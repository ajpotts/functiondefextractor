,Uniq ID,Code,np_calls,num_np_calls,np_calls_w_args,nltk_function
0,/home/amandapotts/git/nltk/nltk/text.py__default_context,"def _default_context(tokens, i):
""""""One left token and one right token, normalized to lowercase""""""
left = tokens[i - 1].lower() if i != 0 else ""*START*""
right = tokens[i + 1].lower() if i != len(tokens) - 1 else ""*END*""
return (left, right)
",[],0,[],/text.py__default_context
1,/home/amandapotts/git/nltk/nltk/text.py_tokens,"def tokens(self):
""""""
:rtype: list(str)
:return: The document that this context index was
created from.
""""""
return self._tokens
",[],0,[],/text.py_tokens
2,/home/amandapotts/git/nltk/nltk/text.py_word_similarity_dict,"def word_similarity_dict(self, word):
""""""
Return a dictionary mapping from words to 'similarity scores,'
indicating how often these two words occur in the same
context.
""""""
word = self._key(word)
word_contexts = set(self._word_to_contexts[word])
scores = {}
for w, w_contexts in self._word_to_contexts.items():
scores[w] = f_measure(word_contexts, set(w_contexts))
return scores
",[],0,[],/text.py_word_similarity_dict
3,/home/amandapotts/git/nltk/nltk/text.py_similar_words,"def similar_words(self, word, n=20):
scores = defaultdict(int)
for c in self._word_to_contexts[self._key(word)]:
for w in self._context_to_words[c]:
if w != word:
scores[w] += (
self._context_to_words[c][word] * self._context_to_words[c][w]
)
return sorted(scores, key=scores.get, reverse=True)[:n]
",[],0,[],/text.py_similar_words
4,/home/amandapotts/git/nltk/nltk/text.py_common_contexts,"def common_contexts(self, words, fail_on_unknown=False):
""""""
Find contexts where the specified words can all appear
return a frequency distribution mapping each context to the
number of times that context was used.
:param words: The words used to seed the similarity search
:type words: str
:param fail_on_unknown: If true, then raise a value error if
any of the given words do not occur at all in the index.
""""""
words = [self._key(w) for w in words]
contexts = [set(self._word_to_contexts[w]) for w in words]
empty = [words[i] for i in range(len(words)) if not contexts[i]]
common = reduce(set.intersection, contexts)
if empty and fail_on_unknown:
raise ValueError(""The following word(s) were not found:"", "" "".join(words))
elif not common:
return FreqDist()
else:
fd = FreqDist(
c for w in words for c in self._word_to_contexts[w] if c in common
)
return fd
",[],0,[],/text.py_common_contexts
5,/home/amandapotts/git/nltk/nltk/text.py_tokens,"def tokens(self):
""""""
:rtype: list(str)
:return: The document that this concordance index was
created from.
""""""
return self._tokens
",[],0,[],/text.py_tokens
6,/home/amandapotts/git/nltk/nltk/text.py_offsets,"def offsets(self, word):
""""""
:rtype: list(int)
:return: A list of the offset positions at which the given
word occurs.  If a key function was specified for the
index, then given word's key will be looked up.
""""""
word = self._key(word)
return self._offsets[word]
",[],0,[],/text.py_offsets
7,/home/amandapotts/git/nltk/nltk/text.py___repr__,"def __repr__(self):
return ""<ConcordanceIndex for %d tokens (%d types)>"" % (
len(self._tokens),
len(self._offsets),
)
",[],0,[],/text.py___repr__
8,/home/amandapotts/git/nltk/nltk/text.py_find_concordance,"def find_concordance(self, word, width=80):
""""""
Find all concordance lines given the query word.
Provided with a list of words, these will be found as a phrase.
""""""
if isinstance(word, list):
phrase = word
else:
phrase = [word]
phrase_str = "" "".join(phrase)
phrase_len = sum(1 for char in phrase_str if not unicodedata.combining(char))
half_width = (width - phrase_len - 2) // 2
context = width // 4  # approx number of words of context
concordance_list = []
offsets = self.offsets(phrase[0])
for i, word in enumerate(phrase[1:]):
word_offsets = {offset - i - 1 for offset in self.offsets(word)}
offsets = sorted(word_offsets.intersection(offsets))
if offsets:
for i in offsets:
query_word = "" "".join(self._tokens[i : i + len(phrase)])
left_context = self._tokens[max(0, i - context) : i]
right_context = self._tokens[i + len(phrase) : i + context]
left_print = cut_string("" "".join(left_context), -half_width).rjust(
half_width
)
right_print = cut_string("" "".join(right_context), half_width)
line_print = "" "".join([left_print, query_word, right_print])
concordance_line = ConcordanceLine(
left_context,
query_word,
right_context,
i,
left_print,
right_print,
line_print,
)
concordance_list.append(concordance_line)
return concordance_list
",[],0,[],/text.py_find_concordance
9,/home/amandapotts/git/nltk/nltk/text.py_print_concordance,"def print_concordance(self, word, width=80, lines=25):
""""""
Print concordance lines given the query word.
:param word: The target word or phrase (a list of strings)
:type word: str or list
:param lines: The number of lines to display (default=25)
:type lines: int
:param width: The width of each line, in characters (default=80)
:type width: int
:param save: The option to save the concordance.
:type save: bool
""""""
concordance_list = self.find_concordance(word, width=width)
if not concordance_list:
print(""no matches"")
else:
lines = min(lines, len(concordance_list))
print(f""Displaying {lines} of {len(concordance_list)} matches:"")
for i, concordance_line in enumerate(concordance_list[:lines]):
print(concordance_line.line)
",[],0,[],/text.py_print_concordance
10,/home/amandapotts/git/nltk/nltk/text.py___init__,"def __init__(self, tokens):
self._raw = """".join(""<"" + w + "">"" for w in tokens)
",[],0,[],/text.py___init__
11,/home/amandapotts/git/nltk/nltk/text.py_findall,"def findall(self, regexp):
""""""
Find instances of the regular expression in the text.
The text is a list of tokens, and a regexp pattern to match
a single token must be surrounded by angle brackets.  E.g.
>>> from nltk.text import TokenSearcher
>>> from nltk.book import text1, text5, text9
>>> text5.findall(""<.*><.*><bro>"")
you rule bro
>>> text1.findall(""<a>(<.*>)<man>"")
monied
mature
pale
brave
>>> text9.findall(""<th.*>{3,}"")
thread through those
that
through the thick
:param regexp: A regular expression
:type regexp: str
""""""
regexp = re.sub(r""\s"", """", regexp)
regexp = re.sub(r""<"", ""(?:<(?:"", regexp)
regexp = re.sub(r"">"", "")>)"", regexp)
regexp = re.sub(r""(?<!\\)\."", ""[^>]"", regexp)
hits = re.findall(regexp, self._raw)
for h in hits:
if not h.startswith(""<"") and h.endswith("">""):
raise ValueError(""Bad regexp for TokenSearcher.findall"")
hits = [h[1:-1].split(""><"") for h in hits]
return hits
",[],0,[],/text.py_findall
12,/home/amandapotts/git/nltk/nltk/text.py___init__,"def __init__(self, tokens, name=None):
""""""
Create a Text object.
:param tokens: The source text.
:type tokens: sequence of str
""""""
if self._COPY_TOKENS:
tokens = list(tokens)
self.tokens = tokens
if name:
self.name = name
elif ""]"" in tokens[:20]:
end = tokens[:20].index(""]"")
self.name = "" "".join(str(tok) for tok in tokens[1:end])
else:
self.name = "" "".join(str(tok) for tok in tokens[:8]) + ""...""
",[],0,[],/text.py___init__
13,/home/amandapotts/git/nltk/nltk/text.py___getitem__,"def __getitem__(self, i):
return self.tokens[i]
",[],0,[],/text.py___getitem__
14,/home/amandapotts/git/nltk/nltk/text.py___len__,"def __len__(self):
return len(self.tokens)
",[],0,[],/text.py___len__
15,/home/amandapotts/git/nltk/nltk/text.py_collocations,"def collocations(self, num=20, window_size=2):
""""""
Print collocations derived from the text, ignoring stopwords.
>>> from nltk.book import text4
>>> text4.collocations() # doctest: +NORMALIZE_WHITESPACE
United States
Government
bless
Almighty God
tribes
:param num: The maximum number of collocations to print.
:type num: int
:param window_size: The number of tokens spanned by a collocation (default=2)
:type window_size: int
""""""
collocation_strings = [
w1 + "" "" + w2 for w1, w2 in self.collocation_list(num, window_size)
]
print(tokenwrap(collocation_strings, separator=""
",[],0,[],/text.py_collocations
16,/home/amandapotts/git/nltk/nltk/text.py_count,"def count(self, word):
""""""
Count the number of times this word appears in the text.
""""""
return self.tokens.count(word)
",[],0,[],/text.py_count
17,/home/amandapotts/git/nltk/nltk/text.py_index,"def index(self, word):
""""""
Find the index of the first occurrence of the word in the text.
""""""
return self.tokens.index(word)
",[],0,[],/text.py_index
18,/home/amandapotts/git/nltk/nltk/text.py_readability,"def readability(self, method):
raise NotImplementedError
",[],0,[],/text.py_readability
19,/home/amandapotts/git/nltk/nltk/text.py_dispersion_plot,"def dispersion_plot(self, words):
""""""
Produce a plot showing the distribution of the words through the text.
Requires pylab to be installed.
:param words: The words to be plotted
:type words: list(str)
:seealso: nltk.draw.dispersion_plot()
""""""
from nltk.draw import dispersion_plot
dispersion_plot(self, words)
",[],0,[],/text.py_dispersion_plot
20,/home/amandapotts/git/nltk/nltk/text.py__train_default_ngram_lm,"def _train_default_ngram_lm(self, tokenized_sents, n=3):
train_data, padded_sents = padded_everygram_pipeline(n, tokenized_sents)
model = MLE(order=n)
model.fit(train_data, padded_sents)
return model
",[],0,[],/text.py__train_default_ngram_lm
21,/home/amandapotts/git/nltk/nltk/text.py_generate,"def generate(self, length=100, text_seed=None, random_seed=42):
""""""
Print random text, generated using a trigram language model.
See also `help(nltk.lm)`.
:param length: The length of text to generate (default=100)
:type length: int
:param text_seed: Generation can be conditioned on preceding context.
:type text_seed: list(str)
:param random_seed: A random seed or an instance of `random.Random`. If provided,
makes the random sampling part of generation reproducible. (default=42)
:type random_seed: int
""""""
self._tokenized_sents = [
sent.split("" "") for sent in sent_tokenize("" "".join(self.tokens))
]
if not hasattr(self, ""_trigram_model""):
print(""Building ngram index..."", file=sys.stderr)
self._trigram_model = self._train_default_ngram_lm(
self._tokenized_sents, n=3
)
generated_tokens = []
assert length > 0, ""The `length` must be more than 0.""
while len(generated_tokens) < length:
for idx, token in enumerate(
self._trigram_model.generate(
length, text_seed=text_seed, random_seed=random_seed
)
):
if token == ""<s>"":
continue
if token == ""</s>"":
break
generated_tokens.append(token)
random_seed += 1
prefix = "" "".join(text_seed) + "" "" if text_seed else """"
output_str = prefix + tokenwrap(generated_tokens[:length])
print(output_str)
return output_str
",[],0,[],/text.py_generate
22,/home/amandapotts/git/nltk/nltk/text.py_plot,"def plot(self, *args):
""""""
See documentation for FreqDist.plot()
:seealso: nltk.prob.FreqDist.plot()
""""""
return self.vocab().plot(*args)
",[],0,[],/text.py_plot
23,/home/amandapotts/git/nltk/nltk/text.py_vocab,"def vocab(self):
""""""
:seealso: nltk.prob.FreqDist
""""""
if ""_vocab"" not in self.__dict__:
self._vocab = FreqDist(self)
return self._vocab
",[],0,[],/text.py_vocab
24,/home/amandapotts/git/nltk/nltk/text.py_findall,"def findall(self, regexp):
""""""
Find instances of the regular expression in the text.
The text is a list of tokens, and a regexp pattern to match
a single token must be surrounded by angle brackets.  E.g.
>>> from nltk.book import text1, text5, text9
>>> text5.findall(""<.*><.*><bro>"")
you rule bro
>>> text1.findall(""<a>(<.*>)<man>"")
monied
mature
pale
brave
>>> text9.findall(""<th.*>{3,}"")
thread through those
that
through the thick
:param regexp: A regular expression
:type regexp: str
""""""
if ""_token_searcher"" not in self.__dict__:
self._token_searcher = TokenSearcher(self)
hits = self._token_searcher.findall(regexp)
hits = ["" "".join(h) for h in hits]
print(tokenwrap(hits, ""
",[],0,[],/text.py_findall
25,/home/amandapotts/git/nltk/nltk/text.py__context,"def _context(self, tokens, i):
""""""
One left & one right token, both case-normalized.  Skip over
non-sentence-final punctuation.  Used by the ``ContextIndex``
that is created for ``similar()`` and ``common_contexts()``.
""""""
j = i - 1
while j >= 0 and not self._CONTEXT_RE.match(tokens[j]):
j -= 1
left = tokens[j] if j != 0 else ""*START*""
j = i + 1
while j < len(tokens) and not self._CONTEXT_RE.match(tokens[j]):
j += 1
right = tokens[j] if j != len(tokens) else ""*END*""
return (left, right)
",[],0,[],/text.py__context
26,/home/amandapotts/git/nltk/nltk/text.py___str__,"def __str__(self):
return ""<Text: %s>"" % self.name
",[],0,[],/text.py___str__
27,/home/amandapotts/git/nltk/nltk/text.py___repr__,"def __repr__(self):
return ""<Text: %s>"" % self.name
",[],0,[],/text.py___repr__
28,/home/amandapotts/git/nltk/nltk/text.py___init__,"def __init__(self, source):
if hasattr(source, ""words""):  # bridge to the text corpus reader
source = [source.words(f) for f in source.fileids()]
self._texts = source
Text.__init__(self, LazyConcatenation(source))
self._idf_cache = {}
",[],0,[],/text.py___init__
29,/home/amandapotts/git/nltk/nltk/text.py_tf,"def tf(self, term, text):
""""""The frequency of the term in text.""""""
return text.count(term) / len(text)
",[],0,[],/text.py_tf
30,/home/amandapotts/git/nltk/nltk/text.py_idf,"def idf(self, term):
""""""The number of texts in the corpus divided by the
number of texts that the term appears in.
If a term does not appear in the corpus, 0.0 is returned.""""""
idf = self._idf_cache.get(term)
if idf is None:
matches = len([True for text in self._texts if term in text])
if len(self._texts) == 0:
raise ValueError(""IDF undefined for empty document collection"")
idf = log(len(self._texts) / matches) if matches else 0.0
self._idf_cache[term] = idf
return idf
",[],0,[],/text.py_idf
31,/home/amandapotts/git/nltk/nltk/text.py_tf_idf,"def tf_idf(self, term, text):
return self.tf(term, text) * self.idf(term)
",[],0,[],/text.py_tf_idf
32,/home/amandapotts/git/nltk/nltk/text.py_demo,"def demo():
from nltk.corpus import brown
text = Text(brown.words(categories=""news""))
print(text)
print()
print(""Concordance:"")
text.concordance(""news"")
print()
print(""Distributionally similar words:"")
text.similar(""news"")
print()
print(""Collocations:"")
text.collocations()
print()
print(""Dispersion plot:"")
text.dispersion_plot([""news"", ""report"", ""said"", ""announced""])
print()
print(""Vocabulary plot:"")
text.plot(50)
print()
print(""Indexing:"")
print(""text[3]:"", text[3])
print(""text[3:5]:"", text[3:5])
print(""text.vocab()['news']:"", text.vocab()[""news""])
",[],0,[],/text.py_demo
33,/home/amandapotts/git/nltk/nltk/data.py_gzip_open_unicode,"def gzip_open_unicode(
filename,
mode=""rb"",
compresslevel=9,
encoding=""utf-8"",
fileobj=None,
errors=None,
newline=None,
",[],0,[],/data.py_gzip_open_unicode
34,/home/amandapotts/git/nltk/nltk/data.py_split_resource_url,"def split_resource_url(resource_url):
""""""
Splits a resource url into ""<protocol>:<path>"".
>>> windows = sys.platform.startswith('win')
>>> split_resource_url('nltk:home/nltk')
('nltk', 'home/nltk')
>>> split_resource_url('nltk:/home/nltk')
('nltk', '/home/nltk')
>>> split_resource_url('file:/home/nltk')
('file', '/home/nltk')
>>> split_resource_url('file:///home/nltk')
('file', '/home/nltk')
>>> split_resource_url('file:///C:/home/nltk')
('file', '/C:/home/nltk')
""""""
protocol, path_ = resource_url.split("":"", 1)
if protocol == ""nltk"":
pass
elif protocol == ""file"":
if path_.startswith(""/""):
path_ = ""/"" + path_.lstrip(""/"")
else:
path_ = re.sub(r""^/{0,2}"", """", path_)
return protocol, path_
",[],0,[],/data.py_split_resource_url
35,/home/amandapotts/git/nltk/nltk/data.py_normalize_resource_url,"def normalize_resource_url(resource_url):
r""""""
Normalizes a resource url
>>> windows = sys.platform.startswith('win')
>>> os.path.normpath(split_resource_url(normalize_resource_url('file:grammar.fcfg'))[1]) == \
... ('\\' if windows else '') + os.path.abspath(os.path.join(os.curdir, 'grammar.fcfg'))
True
>>> not windows or normalize_resource_url('file:C:/dir/file') == 'file:///C:/dir/file'
True
>>> not windows or normalize_resource_url('file:C:\\dir\\file') == 'file:///C:/dir/file'
True
>>> not windows or normalize_resource_url('file:C:\\dir/file') == 'file:///C:/dir/file'
True
>>> not windows or normalize_resource_url('file://C:/dir/file') == 'file:///C:/dir/file'
True
>>> not windows or normalize_resource_url('file:////C:/dir/file') == 'file:///C:/dir/file'
True
>>> not windows or normalize_resource_url('nltk:C:/dir/file') == 'file:///C:/dir/file'
True
>>> not windows or normalize_resource_url('nltk:C:\\dir\\file') == 'file:///C:/dir/file'
True
>>> windows or normalize_resource_url('file:/dir/file/toy.cfg') == 'file:///dir/file/toy.cfg'
True
>>> normalize_resource_url('nltk:home/nltk')
'nltk:home/nltk'
>>> windows or normalize_resource_url('nltk:/home/nltk') == 'file:///home/nltk'
True
>>> normalize_resource_url('https://example.com/dir/file')
'https://example.com/dir/file'
>>> normalize_resource_url('dir/file')
'nltk:dir/file'
""""""
try:
protocol, name = split_resource_url(resource_url)
except ValueError:
protocol = ""nltk""
name = resource_url
if protocol == ""nltk"" and os.path.isabs(name):
protocol = ""file://""
name = normalize_resource_name(name, False, None)
elif protocol == ""file"":
protocol = ""file://""
name = normalize_resource_name(name, False, None)
elif protocol == ""nltk"":
protocol = ""nltk:""
name = normalize_resource_name(name, True)
else:
protocol += ""://""
return """".join([protocol, name])
",[],0,[],/data.py_normalize_resource_url
36,/home/amandapotts/git/nltk/nltk/data.py_normalize_resource_name,"def normalize_resource_name(resource_name, allow_relative=True, relative_path=None):
""""""
:type resource_name: str or unicode
:param resource_name: The name of the resource to search for.
Resource names are posix-style relative path names, such as
``corpora/brown``.  Directory names will automatically
be converted to a platform-appropriate path separator.
Directory trailing slashes are preserved
>>> windows = sys.platform.startswith('win')
>>> normalize_resource_name('.', True)
'./'
>>> normalize_resource_name('./', True)
'./'
>>> windows or normalize_resource_name('dir/file', False, '/') == '/dir/file'
True
>>> not windows or normalize_resource_name('C:/file', False, '/') == '/C:/file'
True
>>> windows or normalize_resource_name('/dir/file', False, '/') == '/dir/file'
True
>>> windows or normalize_resource_name('../dir/file', False, '/') == '/dir/file'
True
>>> not windows or normalize_resource_name('/dir/file', True, '/') == 'dir/file'
True
>>> windows or normalize_resource_name('/dir/file', True, '/') == '/dir/file'
True
""""""
is_dir = bool(re.search(r""[\\/.]$"", resource_name)) or resource_name.endswith(
os.path.sep
)
if sys.platform.startswith(""win""):
resource_name = resource_name.lstrip(""/"")
else:
resource_name = re.sub(r""^/+"", ""/"", resource_name)
if allow_relative:
resource_name = os.path.normpath(resource_name)
else:
if relative_path is None:
relative_path = os.curdir
resource_name = os.path.abspath(os.path.join(relative_path, resource_name))
resource_name = resource_name.replace(""\\"", ""/"").replace(os.path.sep, ""/"")
if sys.platform.startswith(""win"") and os.path.isabs(resource_name):
resource_name = ""/"" + resource_name
if is_dir and not resource_name.endswith(""/""):
resource_name += ""/""
return resource_name
",[],0,[],/data.py_normalize_resource_name
37,/home/amandapotts/git/nltk/nltk/data.py_open,"def open(self, encoding=None):
""""""
Return a seekable read-only stream that can be used to read
the contents of the file identified by this path pointer.
:raise IOError: If the path specified by this pointer does
not contain a readable file.
""""""
",[],0,[],/data.py_open
38,/home/amandapotts/git/nltk/nltk/data.py_file_size,"def file_size(self):
""""""
Return the size of the file pointed to by this path pointer,
in bytes.
:raise IOError: If the path specified by this pointer does
not contain a readable file.
""""""
",[],0,[],/data.py_file_size
39,/home/amandapotts/git/nltk/nltk/data.py_join,"def join(self, fileid):
""""""
Return a new path pointer formed by starting at the path
identified by this pointer, and then following the relative
path given by ``fileid``.  The path components of ``fileid``
should be separated by forward slashes, regardless of
the underlying file system's path separator character.
""""""
",[],0,[],/data.py_join
40,/home/amandapotts/git/nltk/nltk/data.py___init__,"def __init__(self, _path):
""""""
Create a new path pointer for the given absolute path.
:raise IOError: If the given path does not exist.
""""""
_path = os.path.abspath(_path)
if not os.path.exists(_path):
raise OSError(""No such file or directory: %r"" % _path)
self._path = _path
",[],0,[],/data.py___init__
41,/home/amandapotts/git/nltk/nltk/data.py_path,"def path(self):
""""""The absolute path identified by this path pointer.""""""
return self._path
",[],0,[],/data.py_path
42,/home/amandapotts/git/nltk/nltk/data.py_open,"def open(self, encoding=None):
stream = open(self._path, ""rb"")
if encoding is not None:
stream = SeekableUnicodeStreamReader(stream, encoding)
return stream
",[],0,[],/data.py_open
43,/home/amandapotts/git/nltk/nltk/data.py_file_size,"def file_size(self):
return os.stat(self._path).st_size
",[],0,[],/data.py_file_size
44,/home/amandapotts/git/nltk/nltk/data.py_join,"def join(self, fileid):
_path = os.path.join(self._path, fileid)
return FileSystemPathPointer(_path)
",[],0,[],/data.py_join
45,/home/amandapotts/git/nltk/nltk/data.py___repr__,"def __repr__(self):
return ""FileSystemPathPointer(%r)"" % self._path
",[],0,[],/data.py___repr__
46,/home/amandapotts/git/nltk/nltk/data.py___str__,"def __str__(self):
return self._path
",[],0,[],/data.py___str__
47,/home/amandapotts/git/nltk/nltk/data.py___init__,"def __init__(
self, filename=None, mode=None, compresslevel=9, fileobj=None, **kwargs
",[],0,[],/data.py___init__
48,/home/amandapotts/git/nltk/nltk/data.py_write,"def write(self, data):
super().write(data)
",[],0,[],/data.py_write
49,/home/amandapotts/git/nltk/nltk/data.py_open,"def open(self, encoding=None):
stream = GzipFile(self._path, ""rb"")
if encoding:
stream = SeekableUnicodeStreamReader(stream, encoding)
return stream
",[],0,[],/data.py_open
50,/home/amandapotts/git/nltk/nltk/data.py___init__,"def __init__(self, zipfile, entry=""""):
""""""
Create a new path pointer pointing at the specified entry
in the given zipfile.
:raise IOError: If the given zipfile does not exist, or if it
does not contain the specified entry.
""""""
if isinstance(zipfile, str):
zipfile = OpenOnDemandZipFile(os.path.abspath(zipfile))
if entry:
entry = normalize_resource_name(entry, True, ""/"").lstrip(""/"")
try:
zipfile.getinfo(entry)
except Exception as e:
if entry.endswith(""/"") and [
n for n in zipfile.namelist() if n.startswith(entry)
]:
pass  # zipfile contains a file in that directory.
else:
raise OSError(
f""Zipfile {zipfile.filename!r} does not contain {entry!r}""
) from e
self._zipfile = zipfile
self._entry = entry
",[],0,[],/data.py___init__
51,/home/amandapotts/git/nltk/nltk/data.py_zipfile,"def zipfile(self):
""""""
The zipfile.ZipFile object used to access the zip file
containing the entry identified by this path pointer.
""""""
return self._zipfile
",[],0,[],/data.py_zipfile
52,/home/amandapotts/git/nltk/nltk/data.py_entry,"def entry(self):
""""""
The name of the file within zipfile that this path
pointer points to.
""""""
return self._entry
",[],0,[],/data.py_entry
53,/home/amandapotts/git/nltk/nltk/data.py_open,"def open(self, encoding=None):
data = self._zipfile.read(self._entry)
stream = BytesIO(data)
if self._entry.endswith("".gz""):
stream = GzipFile(self._entry, fileobj=stream)
elif encoding is not None:
stream = SeekableUnicodeStreamReader(stream, encoding)
return stream
",[],0,[],/data.py_open
54,/home/amandapotts/git/nltk/nltk/data.py_file_size,"def file_size(self):
return self._zipfile.getinfo(self._entry).file_size
",[],0,[],/data.py_file_size
55,/home/amandapotts/git/nltk/nltk/data.py_join,"def join(self, fileid):
entry = f""{self._entry}/{fileid}""
return ZipFilePathPointer(self._zipfile, entry)
",[],0,[],/data.py_join
56,/home/amandapotts/git/nltk/nltk/data.py___repr__,"def __repr__(self):
return f""ZipFilePathPointer({self._zipfile.filename!r}, {self._entry!r})""
",[],0,[],/data.py___repr__
57,/home/amandapotts/git/nltk/nltk/data.py___str__,"def __str__(self):
return os.path.normpath(os.path.join(self._zipfile.filename, self._entry))
",[],0,[],/data.py___str__
58,/home/amandapotts/git/nltk/nltk/data.py_find,"def find(resource_name, paths=None):
""""""
Find the given resource by searching through the directories and
zip files in paths, where a None or empty string specifies an absolute path.
Returns a corresponding path name.  If the given resource is not
found, raise a ``LookupError``, whose message gives a pointer to
the installation instructions for the NLTK downloader.
Zip File Handling:
- If ``resource_name`` contains a component with a ``.zip``
extension, then it is assumed to be a zipfile
remaining path components are used to look inside the zipfile.
- If any element of ``nltk.data.path`` has a ``.zip`` extension,
then it is assumed to be a zipfile.
- If a given resource name that does not contain any zipfile
component is not found initially, then ``find()`` will make a
second attempt to find that resource, by replacing each
component *p* in the path with *p.zip/p*.  For example, this
allows ``find()`` to map the resource name
``corpora/chat80/cities.pl`` to a zip file path pointer to
``corpora/chat80.zip/chat80/cities.pl``.
- When using ``find()`` to locate a directory contained in a
zipfile, the resource name must end with the forward slash
character.  Otherwise, ``find()`` will not locate the
directory.
:type resource_name: str or unicode
:param resource_name: The name of the resource to search for.
Resource names are posix-style relative path names, such as
``corpora/brown``.  Directory names will be
automatically converted to a platform-appropriate path separator.
:rtype: str
""""""
resource_name = normalize_resource_name(resource_name, True)
if paths is None:
paths = path
m = re.match(r""(.*\.zip)/?(.*)$|"", resource_name)
zipfile, zipentry = m.groups()
for path_ in paths:
if path_ and (os.path.isfile(path_) and path_.endswith("".zip"")):
try:
return ZipFilePathPointer(path_, resource_name)
except OSError:
continue
elif not path_ or os.path.isdir(path_):
if zipfile is None:
p = os.path.join(path_, url2pathname(resource_name))
if os.path.exists(p):
if p.endswith("".gz""):
return GzipFileSystemPathPointer(p)
else:
return FileSystemPathPointer(p)
else:
p = os.path.join(path_, url2pathname(zipfile))
if os.path.exists(p):
try:
return ZipFilePathPointer(p, zipentry)
except OSError:
continue
if zipfile is None:
pieces = resource_name.split(""/"")
for i in range(len(pieces)):
modified_name = ""/"".join(pieces[:i] + [pieces[i] + "".zip""] + pieces[i:])
try:
return find(modified_name, paths)
except LookupError:
pass
resource_zipname = resource_name.split(""/"")[1]
if resource_zipname.endswith("".zip""):
resource_zipname = resource_zipname.rpartition(""."")[0]
msg = str(
""Resource \33[93m{resource}\033[0m not found.\n""
""Please use the NLTK Downloader to obtain the resource:\n\n""
""\33[31m""  # To display red text in terminal.
"">>> import nltk\n""
"">>> nltk.download('{resource}')\n""
""\033[0m""
).format(resource=resource_zipname)
msg = textwrap_indent(msg)
msg += ""\n  For more information see: https://www.nltk.org/data.html\n""
msg += ""\n  Attempted to load \33[93m{resource_name}\033[0m\n"".format(
resource_name=resource_name
)
msg += ""\n  Searched in:"" + """".join(""\n    - %r"" % d for d in paths)
sep = ""*"" * 70
resource_not_found = f""\n{sep}\n{msg}\n{sep}\n""
raise LookupError(resource_not_found)
",[],0,[],/data.py_find
59,/home/amandapotts/git/nltk/nltk/data.py_retrieve,"def retrieve(resource_url, filename=None, verbose=True):
""""""
Copy the given resource to a local file.  If no filename is
specified, then use the URL's filename.  If there is already a
file named ``filename``, then raise a ``ValueError``.
:type resource_url: str
:param resource_url: A URL specifying where the resource should be
loaded from.  The default protocol is ""nltk:"", which searches
for the file in the the NLTK data package.
""""""
resource_url = normalize_resource_url(resource_url)
if filename is None:
if resource_url.startswith(""file:""):
filename = os.path.split(resource_url)[-1]
else:
filename = re.sub(r""(^\w+:)?.*/"", """", resource_url)
if os.path.exists(filename):
filename = os.path.abspath(filename)
raise ValueError(""File %r already exists!"" % filename)
if verbose:
print(f""Retrieving {resource_url!r}, saving to {filename!r}"")
infile = _open(resource_url)
with open(filename, ""wb"") as outfile:
while True:
s = infile.read(1024 * 64)  # 64k blocks.
outfile.write(s)
if not s:
break
infile.close()
",[],0,[],/data.py_retrieve
60,/home/amandapotts/git/nltk/nltk/data.py_load,"def load(
resource_url,
format=""auto"",
cache=True,
verbose=False,
logic_parser=None,
fstruct_reader=None,
encoding=None,
",[],0,[],/data.py_load
61,/home/amandapotts/git/nltk/nltk/data.py_show_cfg,"def show_cfg(resource_url, escape=""##""):
""""""
Write out a grammar file, ignoring escaped and empty lines.
:type resource_url: str
:param resource_url: A URL specifying where the resource should be
loaded from.  The default protocol is ""nltk:"", which searches
for the file in the the NLTK data package.
:type escape: str
:param escape: Prepended string that signals lines to be ignored
""""""
resource_url = normalize_resource_url(resource_url)
resource_val = load(resource_url, format=""text"", cache=False)
lines = resource_val.splitlines()
for l in lines:
if l.startswith(escape):
continue
if re.match(""^$"", l):
continue
print(l)
",[],0,[],/data.py_show_cfg
62,/home/amandapotts/git/nltk/nltk/data.py_clear_cache,"def clear_cache():
""""""
Remove all objects from the resource cache.
:see: load()
""""""
_resource_cache.clear()
",[],0,[],/data.py_clear_cache
63,/home/amandapotts/git/nltk/nltk/data.py__open,"def _open(resource_url):
""""""
Helper function that returns an open file object for a resource,
given its resource URL.  If the given resource URL uses the ""nltk:""
protocol, or uses no protocol, then use ``nltk.data.find`` to find
its path, and open it with the given mode
uses the 'file' protocol, then open the file with the given mode
otherwise, delegate to ``urllib2.urlopen``.
:type resource_url: str
:param resource_url: A URL specifying where the resource should be
loaded from.  The default protocol is ""nltk:"", which searches
for the file in the the NLTK data package.
""""""
resource_url = normalize_resource_url(resource_url)
protocol, path_ = split_resource_url(resource_url)
if protocol is None or protocol.lower() == ""nltk"":
return find(path_, path + [""""]).open()
elif protocol.lower() == ""file"":
return find(path_, [""""]).open()
else:
return urlopen(resource_url)
",[],0,[],/data.py__open
64,/home/amandapotts/git/nltk/nltk/data.py___init__,"def __init__(self, _path):
self._path = _path
",[],0,[],/data.py___init__
65,/home/amandapotts/git/nltk/nltk/data.py___load,"def __load(self):
resource = load(self._path)
self.__dict__ = resource.__dict__
self.__class__ = resource.__class__
",[],0,[],/data.py___load
66,/home/amandapotts/git/nltk/nltk/data.py___getattr__,"def __getattr__(self, attr):
self.__load()
return getattr(self, attr)
",[],0,[],/data.py___getattr__
67,/home/amandapotts/git/nltk/nltk/data.py___repr__,"def __repr__(self):
self.__load()
return repr(self)
",[],0,[],/data.py___repr__
68,/home/amandapotts/git/nltk/nltk/data.py___init__,"def __init__(self, filename):
if not isinstance(filename, str):
raise TypeError(""ReopenableZipFile filename must be a string"")
zipfile.ZipFile.__init__(self, filename)
assert self.filename == filename
self.close()
self._fileRefCnt = 0
",[],0,[],/data.py___init__
69,/home/amandapotts/git/nltk/nltk/data.py_read,"def read(self, name):
assert self.fp is None
self.fp = open(self.filename, ""rb"")
value = zipfile.ZipFile.read(self, name)
self._fileRefCnt += 1
self.close()
return value
",[],0,[],/data.py_read
70,/home/amandapotts/git/nltk/nltk/data.py_write,"def write(self, *args, **kwargs):
"""""":raise NotImplementedError: OpenOnDemandZipfile is read-only""""""
raise NotImplementedError(""OpenOnDemandZipfile is read-only"")
",[],0,[],/data.py_write
71,/home/amandapotts/git/nltk/nltk/data.py_writestr,"def writestr(self, *args, **kwargs):
"""""":raise NotImplementedError: OpenOnDemandZipfile is read-only""""""
raise NotImplementedError(""OpenOnDemandZipfile is read-only"")
",[],0,[],/data.py_writestr
72,/home/amandapotts/git/nltk/nltk/data.py___repr__,"def __repr__(self):
return repr(""OpenOnDemandZipFile(%r)"" % self.filename)
",[],0,[],/data.py___repr__
73,/home/amandapotts/git/nltk/nltk/data.py___init__,"def __init__(self, stream, encoding, errors=""strict""):
stream.seek(0)
self.stream = stream
""""""The underlying stream.""""""
self.encoding = encoding
""""""The name of the encoding that should be used to encode the
underlying stream.""""""
self.errors = errors
""""""The error mode that should be used when decoding data from
the underlying stream.  Can be 'strict', 'ignore', or
'replace'.""""""
self.decode = codecs.getdecoder(encoding)
""""""The function that is used to decode byte strings into
unicode strings.""""""
self.bytebuffer = b""""
""""""A buffer to use bytes that have been read but have not yet
been decoded.  This is only used when the final bytes from
a read do not form a complete encoding for a character.""""""
self.linebuffer = None
""""""A buffer used by ``readline()`` to hold characters that have
been read, but have not yet been returned by ``read()`` or
``readline()``.  This buffer consists of a list of unicode
strings, where each string corresponds to a single line.
The final element of the list may or may not be a complete
line.  Note that the existence of a linebuffer makes the
``tell()`` operation more complex, because it must backtrack
to the beginning of the buffer to determine the correct
file position in the underlying byte stream.""""""
self._rewind_checkpoint = 0
""""""The file position at which the most recent read on the
underlying stream began.  This is used, together with
``_rewind_numchars``, to backtrack to the beginning of
``linebuffer`` (which is required by ``tell()``).""""""
self._rewind_numchars = None
""""""The number of characters that have been returned since the
read that started at ``_rewind_checkpoint``.  This is used,
together with ``_rewind_checkpoint``, to backtrack to the
beginning of ``linebuffer`` (which is required by ``tell()``).""""""
self._bom = self._check_bom()
""""""The length of the byte order marker at the beginning of
the stream (or None for no byte order marker).""""""
",[],0,[],/data.py___init__
74,/home/amandapotts/git/nltk/nltk/data.py_read,"def read(self, size=None):
""""""
Read up to ``size`` bytes, decode them using this reader's
encoding, and return the resulting unicode string.
:param size: The maximum number of bytes to read.  If not
specified, then read as many bytes as possible.
:type size: int
:rtype: unicode
""""""
chars = self._read(size)
if self.linebuffer:
chars = """".join(self.linebuffer) + chars
self.linebuffer = None
self._rewind_numchars = None
return chars
",[],0,[],/data.py_read
75,/home/amandapotts/git/nltk/nltk/data.py_discard_line,"def discard_line(self):
if self.linebuffer and len(self.linebuffer) > 1:
line = self.linebuffer.pop(0)
self._rewind_numchars += len(line)
else:
self.stream.readline()
",[],0,[],/data.py_discard_line
76,/home/amandapotts/git/nltk/nltk/data.py_readline,"def readline(self, size=None):
""""""
Read a line of text, decode it using this reader's encoding,
and return the resulting unicode string.
:param size: The maximum number of bytes to read.  If no
newline is encountered before ``size`` bytes have been read,
then the returned value may not be a complete line of text.
:type size: int
""""""
if self.linebuffer and len(self.linebuffer) > 1:
line = self.linebuffer.pop(0)
self._rewind_numchars += len(line)
return line
readsize = size or 72
chars = """"
if self.linebuffer:
chars += self.linebuffer.pop()
self.linebuffer = None
while True:
startpos = self.stream.tell() - len(self.bytebuffer)
new_chars = self._read(readsize)
if new_chars and new_chars.endswith(""\r""):
new_chars += self._read(1)
chars += new_chars
lines = chars.splitlines(True)
if len(lines) > 1:
line = lines[0]
self.linebuffer = lines[1:]
self._rewind_numchars = len(new_chars) - (len(chars) - len(line))
self._rewind_checkpoint = startpos
break
elif len(lines) == 1:
line0withend = lines[0]
line0withoutend = lines[0].splitlines(False)[0]
if line0withend != line0withoutend:  # complete line
line = line0withend
break
if not new_chars or size is not None:
line = chars
break
if readsize < 8000:
readsize *= 2
return line
",[],0,[],/data.py_readline
77,/home/amandapotts/git/nltk/nltk/data.py_readlines,"def readlines(self, sizehint=None, keepends=True):
""""""
Read this file's contents, decode them using this reader's
encoding, and return it as a list of unicode lines.
:rtype: list(unicode)
:param sizehint: Ignored.
:param keepends: If false, then strip newlines.
""""""
return self.read().splitlines(keepends)
",[],0,[],/data.py_readlines
78,/home/amandapotts/git/nltk/nltk/data.py_next,"def next(self):
""""""Return the next decoded line from the underlying stream.""""""
line = self.readline()
if line:
return line
else:
raise StopIteration
",[],0,[],/data.py_next
79,/home/amandapotts/git/nltk/nltk/data.py___next__,"def __next__(self):
return self.next()
",[],0,[],/data.py___next__
80,/home/amandapotts/git/nltk/nltk/data.py___iter__,"def __iter__(self):
""""""Return self""""""
return self
",[],0,[],/data.py___iter__
81,/home/amandapotts/git/nltk/nltk/data.py___del__,"def __del__(self):
if not self.closed:
self.close()
",[],0,[],/data.py___del__
82,/home/amandapotts/git/nltk/nltk/data.py___enter__,"def __enter__(self):
return self
",[],0,[],/data.py___enter__
83,/home/amandapotts/git/nltk/nltk/data.py___exit__,"def __exit__(self, type, value, traceback):
self.close()
",[],0,[],/data.py___exit__
84,/home/amandapotts/git/nltk/nltk/data.py_xreadlines,"def xreadlines(self):
""""""Return self""""""
return self
",[],0,[],/data.py_xreadlines
85,/home/amandapotts/git/nltk/nltk/data.py_closed,"def closed(self):
""""""True if the underlying stream is closed.""""""
return self.stream.closed
",[],0,[],/data.py_closed
86,/home/amandapotts/git/nltk/nltk/data.py_name,"def name(self):
""""""The name of the underlying stream.""""""
return self.stream.name
",[],0,[],/data.py_name
87,/home/amandapotts/git/nltk/nltk/data.py_mode,"def mode(self):
""""""The mode of the underlying stream.""""""
return self.stream.mode
",[],0,[],/data.py_mode
88,/home/amandapotts/git/nltk/nltk/data.py_close,"def close(self):
""""""
Close the underlying stream.
""""""
self.stream.close()
",[],0,[],/data.py_close
89,/home/amandapotts/git/nltk/nltk/data.py_seek,"def seek(self, offset, whence=0):
""""""
Move the stream to a new file position.  If the reader is
maintaining any buffers, then they will be cleared.
:param offset: A byte count offset.
:param whence: If 0, then the offset is from the start of the file
(offset should be positive), if 1, then the offset is from the
current position (offset may be positive or negative)
then the offset is from the end of the file (offset should
typically be negative).
""""""
if whence == 1:
raise ValueError(
""Relative seek is not supported for ""
""SeekableUnicodeStreamReader -- consider ""
""using char_seek_forward() instead.""
)
self.stream.seek(offset, whence)
self.linebuffer = None
self.bytebuffer = b""""
self._rewind_numchars = None
self._rewind_checkpoint = self.stream.tell()
",[],0,[],/data.py_seek
90,/home/amandapotts/git/nltk/nltk/data.py_char_seek_forward,"def char_seek_forward(self, offset):
""""""
Move the read pointer forward by ``offset`` characters.
""""""
if offset < 0:
raise ValueError(""Negative offsets are not supported"")
self.seek(self.tell())
self._char_seek_forward(offset)
",[],0,[],/data.py_char_seek_forward
91,/home/amandapotts/git/nltk/nltk/data.py__char_seek_forward,"def _char_seek_forward(self, offset, est_bytes=None):
""""""
Move the file position forward by ``offset`` characters,
ignoring all buffers.
:param est_bytes: A hint, giving an estimate of the number of
bytes that will be needed to move forward by ``offset`` chars.
Defaults to ``offset``.
""""""
if est_bytes is None:
est_bytes = offset
bytes = b""""
while True:
newbytes = self.stream.read(est_bytes - len(bytes))
bytes += newbytes
chars, bytes_decoded = self._incr_decode(bytes)
if len(chars) == offset:
self.stream.seek(-len(bytes) + bytes_decoded, 1)
return
if len(chars) > offset:
while len(chars) > offset:
est_bytes += offset - len(chars)
chars, bytes_decoded = self._incr_decode(bytes[:est_bytes])
self.stream.seek(-len(bytes) + bytes_decoded, 1)
return
est_bytes += offset - len(chars)
",[],0,[],/data.py__char_seek_forward
92,/home/amandapotts/git/nltk/nltk/data.py_tell,"def tell(self):
""""""
Return the current file position on the underlying byte
stream.  If this reader is maintaining any buffers, then the
returned file position will be the position of the beginning
of those buffers.
""""""
if self.linebuffer is None:
return self.stream.tell() - len(self.bytebuffer)
orig_filepos = self.stream.tell()
bytes_read = (orig_filepos - len(self.bytebuffer)) - self._rewind_checkpoint
buf_size = sum(len(line) for line in self.linebuffer)
est_bytes = int(
bytes_read * self._rewind_numchars / (self._rewind_numchars + buf_size)
)
self.stream.seek(self._rewind_checkpoint)
self._char_seek_forward(self._rewind_numchars, est_bytes)
filepos = self.stream.tell()
if self.DEBUG:
self.stream.seek(filepos)
check1 = self._incr_decode(self.stream.read(50))[0]
check2 = """".join(self.linebuffer)
assert check1.startswith(check2) or check2.startswith(check1)
self.stream.seek(orig_filepos)
return filepos
",[],0,[],/data.py_tell
93,/home/amandapotts/git/nltk/nltk/data.py__read,"def _read(self, size=None):
""""""
Read up to ``size`` bytes from the underlying stream, decode
them using this reader's encoding, and return the resulting
unicode string.  ``linebuffer`` is not included in the result.
""""""
if size == 0:
return """"
if self._bom and self.stream.tell() == 0:
self.stream.read(self._bom)
if size is None:
new_bytes = self.stream.read()
else:
new_bytes = self.stream.read(size)
bytes = self.bytebuffer + new_bytes
chars, bytes_decoded = self._incr_decode(bytes)
if (size is not None) and (not chars) and (len(new_bytes) > 0):
while not chars:
new_bytes = self.stream.read(1)
if not new_bytes:
break  # end of file.
bytes += new_bytes
chars, bytes_decoded = self._incr_decode(bytes)
self.bytebuffer = bytes[bytes_decoded:]
return chars
",[],0,[],/data.py__read
94,/home/amandapotts/git/nltk/nltk/data.py__incr_decode,"def _incr_decode(self, bytes):
""""""
Decode the given byte string into a unicode string, using this
reader's encoding.  If an exception is encountered that
appears to be caused by a truncation error, then just decode
the byte string without the bytes that cause the trunctaion
error.
Return a tuple ``(chars, num_consumed)``, where ``chars`` is
the decoded unicode string, and ``num_consumed`` is the
number of bytes that were consumed.
""""""
while True:
try:
return self.decode(bytes, ""strict"")
except UnicodeDecodeError as exc:
if exc.end == len(bytes):
return self.decode(bytes[: exc.start], self.errors)
elif self.errors == ""strict"":
raise
else:
return self.decode(bytes, self.errors)
",[],0,[],/data.py__incr_decode
95,/home/amandapotts/git/nltk/nltk/data.py__check_bom,"def _check_bom(self):
enc = re.sub(""[ -]"", """", self.encoding.lower())
bom_info = self._BOM_TABLE.get(enc)
if bom_info:
bytes = self.stream.read(16)
self.stream.seek(0)
for bom, new_encoding in bom_info:
if bytes.startswith(bom):
if new_encoding:
self.encoding = new_encoding
return len(bom)
return None
",[],0,[],/data.py__check_bom
96,/home/amandapotts/git/nltk/nltk/tgrep.py_ancestors,"def ancestors(node):
""""""
Returns the list of all nodes dominating the given tree node.
This method will not work with leaf nodes, since there is no way
to recover the parent.
""""""
results = []
try:
current = node.parent()
except AttributeError:
return results
while current:
results.append(current)
current = current.parent()
return results
",[],0,[],/tgrep.py_ancestors
97,/home/amandapotts/git/nltk/nltk/tgrep.py_unique_ancestors,"def unique_ancestors(node):
""""""
Returns the list of all nodes dominating the given node, where
there is only a single path of descent.
""""""
results = []
try:
current = node.parent()
except AttributeError:
return results
while current and len(current) == 1:
results.append(current)
current = current.parent()
return results
",[],0,[],/tgrep.py_unique_ancestors
98,/home/amandapotts/git/nltk/nltk/tgrep.py__descendants,"def _descendants(node):
""""""
Returns the list of all nodes which are descended from the given
tree node in some way.
""""""
try:
treepos = node.treepositions()
except AttributeError:
return []
return [node[x] for x in treepos[1:]]
",[],0,[],/tgrep.py__descendants
99,/home/amandapotts/git/nltk/nltk/tgrep.py__leftmost_descendants,"def _leftmost_descendants(node):
""""""
Returns the set of all nodes descended in some way through
left branches from this node.
""""""
try:
treepos = node.treepositions()
except AttributeError:
return []
return [node[x] for x in treepos[1:] if all(y == 0 for y in x)]
",[],0,[],/tgrep.py__leftmost_descendants
100,/home/amandapotts/git/nltk/nltk/tgrep.py__rightmost_descendants,"def _rightmost_descendants(node):
""""""
Returns the set of all nodes descended in some way through
right branches from this node.
""""""
try:
rightmost_leaf = max(node.treepositions())
except AttributeError:
return []
return [node[rightmost_leaf[:i]] for i in range(1, len(rightmost_leaf) + 1)]
",[],0,[],/tgrep.py__rightmost_descendants
101,/home/amandapotts/git/nltk/nltk/tgrep.py__istree,"def _istree(obj):
""""""Predicate to check whether `obj` is a nltk.tree.Tree.""""""
return isinstance(obj, nltk.tree.Tree)
",[],0,[],/tgrep.py__istree
102,/home/amandapotts/git/nltk/nltk/tgrep.py__unique_descendants,"def _unique_descendants(node):
""""""
Returns the list of all nodes descended from the given node, where
there is only a single path of descent.
""""""
results = []
current = node
while current and _istree(current) and len(current) == 1:
current = current[0]
results.append(current)
return results
",[],0,[],/tgrep.py__unique_descendants
103,/home/amandapotts/git/nltk/nltk/tgrep.py__before,"def _before(node):
""""""
Returns the set of all nodes that are before the given node.
""""""
try:
pos = node.treeposition()
tree = node.root()
except AttributeError:
return []
return [tree[x] for x in tree.treepositions() if x[: len(pos)] < pos[: len(x)]]
",[],0,[],/tgrep.py__before
104,/home/amandapotts/git/nltk/nltk/tgrep.py__immediately_before,"def _immediately_before(node):
""""""
Returns the set of all nodes that are immediately before the given
node.
Tree node A immediately precedes node B if the last terminal
symbol (word) produced by A immediately precedes the first
terminal symbol produced by B.
""""""
try:
pos = node.treeposition()
tree = node.root()
except AttributeError:
return []
idx = len(pos) - 1
while 0 <= idx and pos[idx] == 0:
idx -= 1
if idx < 0:
return []
pos = list(pos[: idx + 1])
pos[-1] -= 1
before = tree[pos]
return [before] + _rightmost_descendants(before)
",[],0,[],/tgrep.py__immediately_before
105,/home/amandapotts/git/nltk/nltk/tgrep.py__after,"def _after(node):
""""""
Returns the set of all nodes that are after the given node.
""""""
try:
pos = node.treeposition()
tree = node.root()
except AttributeError:
return []
return [tree[x] for x in tree.treepositions() if x[: len(pos)] > pos[: len(x)]]
",[],0,[],/tgrep.py__after
106,/home/amandapotts/git/nltk/nltk/tgrep.py__immediately_after,"def _immediately_after(node):
""""""
Returns the set of all nodes that are immediately after the given
node.
Tree node A immediately follows node B if the first terminal
symbol (word) produced by A immediately follows the last
terminal symbol produced by B.
""""""
try:
pos = node.treeposition()
tree = node.root()
current = node.parent()
except AttributeError:
return []
idx = len(pos) - 1
while 0 <= idx and pos[idx] == len(current) - 1:
idx -= 1
current = current.parent()
if idx < 0:
return []
pos = list(pos[: idx + 1])
pos[-1] += 1
after = tree[pos]
return [after] + _leftmost_descendants(after)
",[],0,[],/tgrep.py__immediately_after
107,/home/amandapotts/git/nltk/nltk/tgrep.py__tgrep_node_literal_value,"def _tgrep_node_literal_value(node):
""""""
Gets the string value of a given parse tree node, for comparison
using the tgrep node literal predicates.
""""""
return node.label() if _istree(node) else str(node)
",[],0,[],/tgrep.py__tgrep_node_literal_value
108,/home/amandapotts/git/nltk/nltk/tgrep.py_macro_use,"def macro_use(n, m=None, l=None):
if m is None or macro_name not in m:
raise TgrepException(f""macro {macro_name} not defined"")
return m[macro_name](n, m, l)
",[],0,[],/tgrep.py_macro_use
109,/home/amandapotts/git/nltk/nltk/tgrep.py_pattern_segment_pred,"def pattern_segment_pred(n, m=None, l=None):
""""""This predicate function ignores its node argument.""""""
if l is None or node_label not in l:
raise TgrepException(f""node_label ={node_label} not bound in pattern"")
node = l[node_label]
return all(pred(node, m, l) for pred in reln_preds)
",[],0,[],/tgrep.py_pattern_segment_pred
110,/home/amandapotts/git/nltk/nltk/tgrep.py__tgrep_node_label_use_action,"def _tgrep_node_label_use_action(_s, _l, tokens):
""""""
Returns the node label used to begin a tgrep_expr_labeled.  See
`_tgrep_segmented_pattern_action`.
Called for expressions like (`tgrep_node_label_use`)::
=s
when they appear as the first element of a `tgrep_expr_labeled`
expression (see `_tgrep_segmented_pattern_action`).
It returns the node label.
""""""
assert len(tokens) == 1
assert tokens[0].startswith(""="")
return tokens[0][1:]
",[],0,[],/tgrep.py__tgrep_node_label_use_action
111,/home/amandapotts/git/nltk/nltk/tgrep.py_node_label_use_pred,"def node_label_use_pred(n, m=None, l=None):
if l is None or node_label not in l:
raise TgrepException(f""node_label ={node_label} not bound in pattern"")
node = l[node_label]
return n is node
",[],0,[],/tgrep.py_node_label_use_pred
112,/home/amandapotts/git/nltk/nltk/tgrep.py_node_label_bind_pred,"def node_label_bind_pred(n, m=None, l=None):
if node_pred(n, m, l):
if l is None:
raise TgrepException(
""cannot bind node_label {}: label_dict is None"".format(
node_label
)
)
l[node_label] = n
return True
else:
return False
",[],0,[],/tgrep.py_node_label_bind_pred
113,/home/amandapotts/git/nltk/nltk/tgrep.py__macro_defn_action,"def _macro_defn_action(_s, _l, tokens):
""""""
Builds a dictionary structure which defines the given macro.
""""""
assert len(tokens) == 3
assert tokens[0] == ""@""
return {tokens[1]: tokens[2]}
",[],0,[],/tgrep.py__macro_defn_action
114,/home/amandapotts/git/nltk/nltk/tgrep.py_top_level_pred,"def top_level_pred(n, m=macro_dict, l=None):
label_dict = {}
return any(predicate(n, m, label_dict) for predicate in tgrep_exprs)
",[],0,[],/tgrep.py_top_level_pred
115,/home/amandapotts/git/nltk/nltk/tgrep.py__build_tgrep_parser,"def _build_tgrep_parser(set_parse_actions=True):
""""""
Builds a pyparsing-based parser object for tokenizing and
interpreting tgrep search strings.
""""""
tgrep_op = pyparsing.Optional(""!"") + pyparsing.Regex(""[$%,.<>][%,.<>0-9-':]*"")
tgrep_qstring = pyparsing.QuotedString(
quoteChar='""', escChar=""\\"", unquoteResults=False
)
tgrep_node_regex = pyparsing.QuotedString(
quoteChar=""/"", escChar=""\\"", unquoteResults=False
)
tgrep_qstring_icase = pyparsing.Regex('i@\\""(?:[^""\\n\\r\\\\]|(?:\\\\.))*\\""')
tgrep_node_regex_icase = pyparsing.Regex(""i@\\/(?:[^/\\n\\r\\\\]|(?:\\\\.))*\\/"")
tgrep_node_literal = pyparsing.Regex(""[^][ \r\t\n
tgrep_expr = pyparsing.Forward()
tgrep_relations = pyparsing.Forward()
tgrep_parens = pyparsing.Literal(""("") + tgrep_expr + "")""
tgrep_nltk_tree_pos = (
pyparsing.Literal(""N("")
+ pyparsing.Optional(
pyparsing.Word(pyparsing.nums)
+ "",""
+ pyparsing.Optional(
pyparsing.delimitedList(pyparsing.Word(pyparsing.nums), delim="","")
+ pyparsing.Optional("","")
)
)
+ "")""
)
tgrep_node_label = pyparsing.Regex(""[A-Za-z0-9]+"")
tgrep_node_label_use = pyparsing.Combine(""="" + tgrep_node_label)
tgrep_node_label_use_pred = tgrep_node_label_use.copy()
macro_name = pyparsing.Regex(""[^]
macro_name.setWhitespaceChars("""")
macro_use = pyparsing.Combine(""@"" + macro_name)
tgrep_node_expr = (
tgrep_node_label_use_pred
| macro_use
| tgrep_nltk_tree_pos
| tgrep_qstring_icase
| tgrep_node_regex_icase
| tgrep_qstring
| tgrep_node_regex
| ""*""
| tgrep_node_literal
)
tgrep_node_expr2 = (
tgrep_node_expr
+ pyparsing.Literal(""="").setWhitespaceChars("""")
+ tgrep_node_label.copy().setWhitespaceChars("""")
) | tgrep_node_expr
tgrep_node = tgrep_parens | (
pyparsing.Optional(""'"")
+ tgrep_node_expr2
+ pyparsing.ZeroOrMore(""|"" + tgrep_node_expr)
)
tgrep_brackets = pyparsing.Optional(""!"") + ""["" + tgrep_relations + ""]""
tgrep_relation = tgrep_brackets | (tgrep_op + tgrep_node)
tgrep_rel_conjunction = pyparsing.Forward()
tgrep_rel_conjunction << (
tgrep_relation
+ pyparsing.ZeroOrMore(pyparsing.Optional(""&"") + tgrep_rel_conjunction)
)
tgrep_relations << tgrep_rel_conjunction + pyparsing.ZeroOrMore(
""|"" + tgrep_relations
)
tgrep_expr << tgrep_node + pyparsing.Optional(tgrep_relations)
tgrep_expr_labeled = tgrep_node_label_use + pyparsing.Optional(tgrep_relations)
tgrep_expr2 = tgrep_expr + pyparsing.ZeroOrMore("":"" + tgrep_expr_labeled)
macro_defn = (
pyparsing.Literal(""@"") + pyparsing.White().suppress() + macro_name + tgrep_expr2
)
tgrep_exprs = (
pyparsing.Optional(macro_defn + pyparsing.ZeroOrMore(""
+ tgrep_expr2
+ pyparsing.ZeroOrMore(""
+ pyparsing.ZeroOrMore(""
)
if set_parse_actions:
tgrep_node_label_use.setParseAction(_tgrep_node_label_use_action)
tgrep_node_label_use_pred.setParseAction(_tgrep_node_label_pred_use_action)
macro_use.setParseAction(_tgrep_macro_use_action)
tgrep_node.setParseAction(_tgrep_node_action)
tgrep_node_expr2.setParseAction(_tgrep_bind_node_label_action)
tgrep_parens.setParseAction(_tgrep_parens_action)
tgrep_nltk_tree_pos.setParseAction(_tgrep_nltk_tree_pos_action)
tgrep_relation.setParseAction(_tgrep_relation_action)
tgrep_rel_conjunction.setParseAction(_tgrep_conjunction_action)
tgrep_relations.setParseAction(_tgrep_rel_disjunction_action)
macro_defn.setParseAction(_macro_defn_action)
tgrep_expr.setParseAction(_tgrep_conjunction_action)
tgrep_expr_labeled.setParseAction(_tgrep_segmented_pattern_action)
tgrep_expr2.setParseAction(
functools.partial(_tgrep_conjunction_action, join_char="":"")
)
tgrep_exprs.setParseAction(_tgrep_exprs_action)
return tgrep_exprs.ignore(""#"" + pyparsing.restOfLine)
",[],0,[],/tgrep.py__build_tgrep_parser
116,/home/amandapotts/git/nltk/nltk/tgrep.py_tgrep_tokenize,"def tgrep_tokenize(tgrep_string):
""""""
Tokenizes a TGrep search string into separate tokens.
""""""
parser = _build_tgrep_parser(False)
if isinstance(tgrep_string, bytes):
tgrep_string = tgrep_string.decode()
return list(parser.parseString(tgrep_string))
",[],0,[],/tgrep.py_tgrep_tokenize
117,/home/amandapotts/git/nltk/nltk/tgrep.py_treepositions_no_leaves,"def treepositions_no_leaves(tree):
""""""
Returns all the tree positions in the given tree which are not
leaf nodes.
""""""
treepositions = tree.treepositions()
prefixes = set()
for pos in treepositions:
for length in range(len(pos)):
prefixes.add(pos[:length])
return [pos for pos in treepositions if pos in prefixes]
",[],0,[],/tgrep.py_treepositions_no_leaves
118,/home/amandapotts/git/nltk/nltk/tgrep.py_tgrep_positions,"def tgrep_positions(pattern, trees, search_leaves=True):
""""""
Return the tree positions in the trees which match the given pattern.
:param pattern: a tgrep search pattern
:type pattern: str or output of tgrep_compile()
:param trees: a sequence of NLTK trees (usually ParentedTrees)
:type trees: iter(ParentedTree) or iter(Tree)
:param search_leaves: whether to return matching leaf nodes
:type search_leaves: bool
:rtype: iter(tree positions)
""""""
if isinstance(pattern, (bytes, str)):
pattern = tgrep_compile(pattern)
for tree in trees:
try:
if search_leaves:
positions = tree.treepositions()
else:
positions = treepositions_no_leaves(tree)
yield [position for position in positions if pattern(tree[position])]
except AttributeError:
yield []
",[],0,[],/tgrep.py_tgrep_positions
119,/home/amandapotts/git/nltk/nltk/tgrep.py_tgrep_nodes,"def tgrep_nodes(pattern, trees, search_leaves=True):
""""""
Return the tree nodes in the trees which match the given pattern.
:param pattern: a tgrep search pattern
:type pattern: str or output of tgrep_compile()
:param trees: a sequence of NLTK trees (usually ParentedTrees)
:type trees: iter(ParentedTree) or iter(Tree)
:param search_leaves: whether to return matching leaf nodes
:type search_leaves: bool
:rtype: iter(tree nodes)
""""""
if isinstance(pattern, (bytes, str)):
pattern = tgrep_compile(pattern)
for tree in trees:
try:
if search_leaves:
positions = tree.treepositions()
else:
positions = treepositions_no_leaves(tree)
yield [tree[position] for position in positions if pattern(tree[position])]
except AttributeError:
yield []
",[],0,[],/tgrep.py_tgrep_nodes
120,/home/amandapotts/git/nltk/nltk/book.py_texts,"def texts():
print(""text1:"", text1.name)
print(""text2:"", text2.name)
print(""text3:"", text3.name)
print(""text4:"", text4.name)
print(""text5:"", text5.name)
print(""text6:"", text6.name)
print(""text7:"", text7.name)
print(""text8:"", text8.name)
print(""text9:"", text9.name)
",[],0,[],/book.py_texts
121,/home/amandapotts/git/nltk/nltk/book.py_sents,"def sents():
print(""sent1:"", "" "".join(sent1))
print(""sent2:"", "" "".join(sent2))
print(""sent3:"", "" "".join(sent3))
print(""sent4:"", "" "".join(sent4))
print(""sent5:"", "" "".join(sent5))
print(""sent6:"", "" "".join(sent6))
print(""sent7:"", "" "".join(sent7))
print(""sent8:"", "" "".join(sent8))
print(""sent9:"", "" "".join(sent9))
",[],0,[],/book.py_sents
122,/home/amandapotts/git/nltk/nltk/featstruct.py___new__,"def __new__(cls, features=None, **morefeatures):
""""""
Construct and return a new feature structure.  If this
constructor is called directly, then the returned feature
structure will be an instance of either the ``FeatDict`` class
or the ``FeatList`` class.
:param features: The initial feature values for this feature
structure:
- FeatStruct(string) -> FeatStructReader().read(string)
- FeatStruct(mapping) -> FeatDict(mapping)
- FeatStruct(sequence) -> FeatList(sequence)
- FeatStruct() -> FeatDict()
:param morefeatures: If ``features`` is a mapping or None,
then ``morefeatures`` provides additional features for the
``FeatDict`` constructor.
""""""
if cls is FeatStruct:
if features is None:
return FeatDict.__new__(FeatDict, **morefeatures)
elif _is_mapping(features):
return FeatDict.__new__(FeatDict, features, **morefeatures)
elif morefeatures:
raise TypeError(
""Keyword arguments may only be specified ""
""if features is None or is a mapping.""
)
if isinstance(features, str):
if FeatStructReader._START_FDICT_RE.match(features):
return FeatDict.__new__(FeatDict, features, **morefeatures)
else:
return FeatList.__new__(FeatList, features, **morefeatures)
elif _is_sequence(features):
return FeatList.__new__(FeatList, features)
else:
raise TypeError(""Expected string or mapping or sequence"")
else:
return super().__new__(cls, features, **morefeatures)
",[],0,[],/featstruct.py___new__
123,/home/amandapotts/git/nltk/nltk/featstruct.py__keys,"def _keys(self):
""""""Return an iterable of the feature identifiers used by this
FeatStruct.""""""
raise NotImplementedError()  # Implemented by subclasses.
",[],0,[],/featstruct.py__keys
124,/home/amandapotts/git/nltk/nltk/featstruct.py__values,"def _values(self):
""""""Return an iterable of the feature values directly defined
by this FeatStruct.""""""
raise NotImplementedError()  # Implemented by subclasses.
",[],0,[],/featstruct.py__values
125,/home/amandapotts/git/nltk/nltk/featstruct.py__items,"def _items(self):
""""""Return an iterable of (fid,fval) pairs, where fid is a
feature identifier and fval is the corresponding feature
value, for all features defined by this FeatStruct.""""""
raise NotImplementedError()  # Implemented by subclasses.
",[],0,[],/featstruct.py__items
126,/home/amandapotts/git/nltk/nltk/featstruct.py_equal_values,"def equal_values(self, other, check_reentrance=False):
""""""
Return True if ``self`` and ``other`` assign the same value to
to every feature.  In particular, return true if
``self[p]==other[p]`` for every feature path *p* such
that ``self[p]`` or ``other[p]`` is a base value (i.e.,
not a nested feature structure).
:param check_reentrance: If True, then also return False if
there is any difference between the reentrances of ``self``
and ``other``.
:note: the ``==`` is equivalent to ``equal_values()`` with
``check_reentrance=True``.
""""""
return self._equal(other, check_reentrance, set(), set(), set())
",[],0,[],/featstruct.py_equal_values
127,/home/amandapotts/git/nltk/nltk/featstruct.py___eq__,"def __eq__(self, other):
""""""
Return true if ``self`` and ``other`` are both feature structures,
assign the same values to all features, and contain the same
reentrances.  I.e., return
``self.equal_values(other, check_reentrance=True)``.
:see: ``equal_values()``
""""""
return self._equal(other, True, set(), set(), set())
",[],0,[],/featstruct.py___eq__
128,/home/amandapotts/git/nltk/nltk/featstruct.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/featstruct.py___ne__
129,/home/amandapotts/git/nltk/nltk/featstruct.py___lt__,"def __lt__(self, other):
if not isinstance(other, FeatStruct):
return self.__class__.__name__ < other.__class__.__name__
else:
return len(self) < len(other)
",[],0,[],/featstruct.py___lt__
130,/home/amandapotts/git/nltk/nltk/featstruct.py___hash__,"def __hash__(self):
""""""
If this feature structure is frozen, return its hash value
otherwise, raise ``TypeError``.
""""""
if not self._frozen:
raise TypeError(""FeatStructs must be frozen before they "" ""can be hashed."")
try:
return self._hash
except AttributeError:
self._hash = self._calculate_hashvalue(set())
return self._hash
",[],0,[],/featstruct.py___hash__
131,/home/amandapotts/git/nltk/nltk/featstruct.py__equal,"def _equal(
self, other, check_reentrance, visited_self, visited_other, visited_pairs
",[],0,[],/featstruct.py__equal
132,/home/amandapotts/git/nltk/nltk/featstruct.py__calculate_hashvalue,"def _calculate_hashvalue(self, visited):
""""""
Return a hash value for this feature structure.
:require: ``self`` must be frozen.
:param visited: A set containing the ids of all feature
structures we've already visited while hashing.
""""""
if id(self) in visited:
return 1
visited.add(id(self))
hashval = 5831
for fname, fval in sorted(self._items()):
hashval *= 37
hashval += hash(fname)
hashval *= 37
if isinstance(fval, FeatStruct):
hashval += fval._calculate_hashvalue(visited)
else:
hashval += hash(fval)
hashval = int(hashval & 0x7FFFFFFF)
return hashval
",[],0,[],/featstruct.py__calculate_hashvalue
133,/home/amandapotts/git/nltk/nltk/featstruct.py_freeze,"def freeze(self):
""""""
Make this feature structure, and any feature structures it
contains, immutable.  Note: this method does not attempt to
'freeze' any feature value that is not a ``FeatStruct``
is recommended that you use only immutable feature values.
""""""
if self._frozen:
return
self._freeze(set())
",[],0,[],/featstruct.py_freeze
134,/home/amandapotts/git/nltk/nltk/featstruct.py_frozen,"def frozen(self):
""""""
Return True if this feature structure is immutable.  Feature
structures can be made immutable with the ``freeze()`` method.
Immutable feature structures may not be made mutable again,
but new mutable copies can be produced with the ``copy()`` method.
""""""
return self._frozen
",[],0,[],/featstruct.py_frozen
135,/home/amandapotts/git/nltk/nltk/featstruct.py__freeze,"def _freeze(self, visited):
""""""
Make this feature structure, and any feature structure it
contains, immutable.
:param visited: A set containing the ids of all feature
structures we've already visited while freezing.
""""""
if id(self) in visited:
return
visited.add(id(self))
self._frozen = True
for fname, fval in sorted(self._items()):
if isinstance(fval, FeatStruct):
fval._freeze(visited)
",[],0,[],/featstruct.py__freeze
136,/home/amandapotts/git/nltk/nltk/featstruct.py_copy,"def copy(self, deep=True):
""""""
Return a new copy of ``self``.  The new copy will not be frozen.
:param deep: If true, create a deep copy
a shallow copy.
""""""
if deep:
return copy.deepcopy(self)
else:
return self.__class__(self)
",[],0,[],/featstruct.py_copy
137,/home/amandapotts/git/nltk/nltk/featstruct.py___deepcopy__,"def __deepcopy__(self, memo):
raise NotImplementedError()  # Implemented by subclasses.
",[],0,[],/featstruct.py___deepcopy__
138,/home/amandapotts/git/nltk/nltk/featstruct.py_cyclic,"def cyclic(self):
""""""
Return True if this feature structure contains itself.
""""""
return self._find_reentrances({})[id(self)]
",[],0,[],/featstruct.py_cyclic
139,/home/amandapotts/git/nltk/nltk/featstruct.py_walk,"def walk(self):
""""""
Return an iterator that generates this feature structure, and
each feature structure it contains.  Each feature structure will
be generated exactly once.
""""""
return self._walk(set())
",[],0,[],/featstruct.py_walk
140,/home/amandapotts/git/nltk/nltk/featstruct.py__walk,"def _walk(self, visited):
""""""
Return an iterator that generates this feature structure, and
each feature structure it contains.
:param visited: A set containing the ids of all feature
structures we've already visited while freezing.
""""""
raise NotImplementedError()  # Implemented by subclasses.
",[],0,[],/featstruct.py__walk
141,/home/amandapotts/git/nltk/nltk/featstruct.py__walk,"def _walk(self, visited):
if id(self) in visited:
return
visited.add(id(self))
yield self
for fval in self._values():
if isinstance(fval, FeatStruct):
yield from fval._walk(visited)
",[],0,[],/featstruct.py__walk
142,/home/amandapotts/git/nltk/nltk/featstruct.py__find_reentrances,"def _find_reentrances(self, reentrances):
""""""
Return a dictionary that maps from the ``id`` of each feature
structure contained in ``self`` (including ``self``) to a
boolean value, indicating whether it is reentrant or not.
""""""
if id(self) in reentrances:
reentrances[id(self)] = True
else:
reentrances[id(self)] = False
for fval in self._values():
if isinstance(fval, FeatStruct):
fval._find_reentrances(reentrances)
return reentrances
",[],0,[],/featstruct.py__find_reentrances
143,/home/amandapotts/git/nltk/nltk/featstruct.py_substitute_bindings,"def substitute_bindings(self, bindings):
"""""":see: ``nltk.featstruct.substitute_bindings()``""""""
return substitute_bindings(self, bindings)
",[],0,[],/featstruct.py_substitute_bindings
144,/home/amandapotts/git/nltk/nltk/featstruct.py_retract_bindings,"def retract_bindings(self, bindings):
"""""":see: ``nltk.featstruct.retract_bindings()``""""""
return retract_bindings(self, bindings)
",[],0,[],/featstruct.py_retract_bindings
145,/home/amandapotts/git/nltk/nltk/featstruct.py_variables,"def variables(self):
"""""":see: ``nltk.featstruct.find_variables()``""""""
return find_variables(self)
",[],0,[],/featstruct.py_variables
146,/home/amandapotts/git/nltk/nltk/featstruct.py_rename_variables,"def rename_variables(self, vars=None, used_vars=(), new_vars=None):
"""""":see: ``nltk.featstruct.rename_variables()``""""""
return rename_variables(self, vars, used_vars, new_vars)
",[],0,[],/featstruct.py_rename_variables
147,/home/amandapotts/git/nltk/nltk/featstruct.py_remove_variables,"def remove_variables(self):
""""""
Return the feature structure that is obtained by deleting
any feature whose value is a ``Variable``.
:rtype: FeatStruct
""""""
return remove_variables(self)
",[],0,[],/featstruct.py_remove_variables
148,/home/amandapotts/git/nltk/nltk/featstruct.py_unify,"def unify(self, other, bindings=None, trace=False, fail=None, rename_vars=True):
return unify(self, other, bindings, trace, fail, rename_vars)
",[],0,[],/featstruct.py_unify
149,/home/amandapotts/git/nltk/nltk/featstruct.py_subsumes,"def subsumes(self, other):
""""""
Return True if ``self`` subsumes ``other``.  I.e., return true
If unifying ``self`` with ``other`` would result in a feature
structure equal to ``other``.
""""""
return subsumes(self, other)
",[],0,[],/featstruct.py_subsumes
150,/home/amandapotts/git/nltk/nltk/featstruct.py___repr__,"def __repr__(self):
""""""
Display a single-line representation of this feature structure,
suitable for embedding in other representations.
""""""
return self._repr(self._find_reentrances({}), {})
",[],0,[],/featstruct.py___repr__
151,/home/amandapotts/git/nltk/nltk/featstruct.py__repr,"def _repr(self, reentrances, reentrance_ids):
""""""
Return a string representation of this feature structure.
:param reentrances: A dictionary that maps from the ``id`` of
each feature value in self, indicating whether that value
is reentrant or not.
:param reentrance_ids: A dictionary mapping from each ``id``
of a feature value to a unique identifier.  This is modified
by ``repr``: the first time a reentrant feature value is
displayed, an identifier is added to ``reentrance_ids`` for it.
""""""
raise NotImplementedError()
",[],0,[],/featstruct.py__repr
152,/home/amandapotts/git/nltk/nltk/featstruct.py__check_frozen,"def _check_frozen(method, indent=""""):
""""""
Given a method function, return a new method function that first
checks if ``self._frozen`` is true
with an appropriate message.  Otherwise, call the method and return
its result.
""""""
",[],0,[],/featstruct.py__check_frozen
153,/home/amandapotts/git/nltk/nltk/featstruct.py_wrapped,"def wrapped(self, *args, **kwargs):
if self._frozen:
raise ValueError(_FROZEN_ERROR)
else:
return method(self, *args, **kwargs)
",[],0,[],/featstruct.py_wrapped
154,/home/amandapotts/git/nltk/nltk/featstruct.py___init__,"def __init__(self, features=None, **morefeatures):
""""""
Create a new feature dictionary, with the specified features.
:param features: The initial value for this feature
dictionary.  If ``features`` is a ``FeatStruct``, then its
features are copied (shallow copy).  If ``features`` is a
dict, then a feature is created for each item, mapping its
key to its value.  If ``features`` is a string, then it is
processed using ``FeatStructReader``.  If ``features`` is a list of
tuples ``(name, val)``, then a feature is created for each tuple.
:param morefeatures: Additional features for the new feature
dictionary.  If a feature is listed under both ``features`` and
``morefeatures``, then the value from ``morefeatures`` will be
used.
""""""
if isinstance(features, str):
FeatStructReader().fromstring(features, self)
self.update(**morefeatures)
else:
self.update(features, **morefeatures)
",[],0,[],/featstruct.py___init__
155,/home/amandapotts/git/nltk/nltk/featstruct.py___getitem__,"def __getitem__(self, name_or_path):
""""""If the feature with the given name or path exists, return
its value
if isinstance(name_or_path, (str, Feature)):
return dict.__getitem__(self, name_or_path)
elif isinstance(name_or_path, tuple):
try:
val = self
for fid in name_or_path:
if not isinstance(val, FeatStruct):
raise KeyError  # path contains base value
val = val[fid]
return val
except (KeyError, IndexError) as e:
raise KeyError(name_or_path) from e
else:
raise TypeError(self._INDEX_ERROR % name_or_path)
",[],0,[],/featstruct.py___getitem__
156,/home/amandapotts/git/nltk/nltk/featstruct.py_get,"def get(self, name_or_path, default=None):
""""""If the feature with the given name or path exists, return its
value
try:
return self[name_or_path]
except KeyError:
return default
",[],0,[],/featstruct.py_get
157,/home/amandapotts/git/nltk/nltk/featstruct.py___contains__,"def __contains__(self, name_or_path):
""""""Return true if a feature with the given name or path exists.""""""
try:
self[name_or_path]
return True
except KeyError:
return False
",[],0,[],/featstruct.py___contains__
158,/home/amandapotts/git/nltk/nltk/featstruct.py_has_key,"def has_key(self, name_or_path):
""""""Return true if a feature with the given name or path exists.""""""
return name_or_path in self
",[],0,[],/featstruct.py_has_key
159,/home/amandapotts/git/nltk/nltk/featstruct.py___delitem__,"def __delitem__(self, name_or_path):
""""""If the feature with the given name or path exists, delete
its value
if self._frozen:
raise ValueError(_FROZEN_ERROR)
if isinstance(name_or_path, (str, Feature)):
return dict.__delitem__(self, name_or_path)
elif isinstance(name_or_path, tuple):
if len(name_or_path) == 0:
raise ValueError(""The path () can not be set"")
else:
parent = self[name_or_path[:-1]]
if not isinstance(parent, FeatStruct):
raise KeyError(name_or_path)  # path contains base value
del parent[name_or_path[-1]]
else:
raise TypeError(self._INDEX_ERROR % name_or_path)
",[],0,[],/featstruct.py___delitem__
160,/home/amandapotts/git/nltk/nltk/featstruct.py___setitem__,"def __setitem__(self, name_or_path, value):
""""""Set the value for the feature with the given name or path
to ``value``.  If ``name_or_path`` is an invalid path, raise
``KeyError``.""""""
if self._frozen:
raise ValueError(_FROZEN_ERROR)
if isinstance(name_or_path, (str, Feature)):
return dict.__setitem__(self, name_or_path, value)
elif isinstance(name_or_path, tuple):
if len(name_or_path) == 0:
raise ValueError(""The path () can not be set"")
else:
parent = self[name_or_path[:-1]]
if not isinstance(parent, FeatStruct):
raise KeyError(name_or_path)  # path contains base value
parent[name_or_path[-1]] = value
else:
raise TypeError(self._INDEX_ERROR % name_or_path)
",[],0,[],/featstruct.py___setitem__
161,/home/amandapotts/git/nltk/nltk/featstruct.py_update,"def update(self, features=None, **morefeatures):
if self._frozen:
raise ValueError(_FROZEN_ERROR)
if features is None:
items = ()
elif hasattr(features, ""items"") and callable(features.items):
items = features.items()
elif hasattr(features, ""__iter__""):
items = features
else:
raise ValueError(""Expected mapping or list of tuples"")
for key, val in items:
if not isinstance(key, (str, Feature)):
raise TypeError(""Feature names must be strings"")
self[key] = val
for key, val in morefeatures.items():
if not isinstance(key, (str, Feature)):
raise TypeError(""Feature names must be strings"")
self[key] = val
",[],0,[],/featstruct.py_update
162,/home/amandapotts/git/nltk/nltk/featstruct.py___deepcopy__,"def __deepcopy__(self, memo):
memo[id(self)] = selfcopy = self.__class__()
for key, val in self._items():
selfcopy[copy.deepcopy(key, memo)] = copy.deepcopy(val, memo)
return selfcopy
",[],0,[],/featstruct.py___deepcopy__
163,/home/amandapotts/git/nltk/nltk/featstruct.py__keys,"def _keys(self):
return self.keys()
",[],0,[],/featstruct.py__keys
164,/home/amandapotts/git/nltk/nltk/featstruct.py__values,"def _values(self):
return self.values()
",[],0,[],/featstruct.py__values
165,/home/amandapotts/git/nltk/nltk/featstruct.py__items,"def _items(self):
return self.items()
",[],0,[],/featstruct.py__items
166,/home/amandapotts/git/nltk/nltk/featstruct.py___str__,"def __str__(self):
""""""
Display a multi-line representation of this feature dictionary
as an FVM (feature value matrix).
""""""
return ""\n"".join(self._str(self._find_reentrances({}), {}))
",[],0,[],/featstruct.py___str__
167,/home/amandapotts/git/nltk/nltk/featstruct.py__repr,"def _repr(self, reentrances, reentrance_ids):
segments = []
prefix = """"
suffix = """"
if reentrances[id(self)]:
assert id(self) not in reentrance_ids
reentrance_ids[id(self)] = repr(len(reentrance_ids) + 1)
for fname, fval in sorted(self.items()):
display = getattr(fname, ""display"", None)
if id(fval) in reentrance_ids:
segments.append(f""{fname}->({reentrance_ids[id(fval)]})"")
elif (
display == ""prefix"" and not prefix and isinstance(fval, (Variable, str))
):
prefix = ""%s"" % fval
elif display == ""slash"" and not suffix:
if isinstance(fval, Variable):
suffix = ""/%s"" % fval.name
else:
suffix = ""/%s"" % repr(fval)
elif isinstance(fval, Variable):
segments.append(f""{fname}={fval.name}"")
elif fval is True:
segments.append(""+%s"" % fname)
elif fval is False:
segments.append(""-%s"" % fname)
elif isinstance(fval, Expression):
segments.append(f""{fname}=<{fval}>"")
elif not isinstance(fval, FeatStruct):
segments.append(f""{fname}={repr(fval)}"")
else:
fval_repr = fval._repr(reentrances, reentrance_ids)
segments.append(f""{fname}={fval_repr}"")
if reentrances[id(self)]:
prefix = f""({reentrance_ids[id(self)]}){prefix}""
return ""{}[{}]{}"".format(prefix, "", "".join(segments), suffix)
",[],0,[],/featstruct.py__repr
168,/home/amandapotts/git/nltk/nltk/featstruct.py__str,"def _str(self, reentrances, reentrance_ids):
""""""
:return: A list of lines composing a string representation of
this feature dictionary.
:param reentrances: A dictionary that maps from the ``id`` of
each feature value in self, indicating whether that value
is reentrant or not.
:param reentrance_ids: A dictionary mapping from each ``id``
of a feature value to a unique identifier.  This is modified
by ``repr``: the first time a reentrant feature value is
displayed, an identifier is added to ``reentrance_ids`` for
it.
""""""
if reentrances[id(self)]:
assert id(self) not in reentrance_ids
reentrance_ids[id(self)] = repr(len(reentrance_ids) + 1)
if len(self) == 0:
if reentrances[id(self)]:
return [""(%s) []"" % reentrance_ids[id(self)]]
else:
return [""[]""]
maxfnamelen = max(len(""%s"" % k) for k in self.keys())
lines = []
for fname, fval in sorted(self.items()):
fname = (""%s"" % fname).ljust(maxfnamelen)
if isinstance(fval, Variable):
lines.append(f""{fname} = {fval.name}"")
elif isinstance(fval, Expression):
lines.append(f""{fname} = <{fval}>"")
elif isinstance(fval, FeatList):
fval_repr = fval._repr(reentrances, reentrance_ids)
lines.append(f""{fname} = {repr(fval_repr)}"")
elif not isinstance(fval, FeatDict):
lines.append(f""{fname} = {repr(fval)}"")
elif id(fval) in reentrance_ids:
lines.append(f""{fname} -> ({reentrance_ids[id(fval)]})"")
else:
if lines and lines[-1] != """":
lines.append("""")
fval_lines = fval._str(reentrances, reentrance_ids)
fval_lines = [("" "" * (maxfnamelen + 3)) + l for l in fval_lines]
nameline = (len(fval_lines) - 1) // 2
fval_lines[nameline] = (
fname + "" ="" + fval_lines[nameline][maxfnamelen + 2 :]
)
lines += fval_lines
lines.append("""")
if lines[-1] == """":
lines.pop()
maxlen = max(len(line) for line in lines)
lines = [""[ {}{} ]"".format(line, "" "" * (maxlen - len(line))) for line in lines]
if reentrances[id(self)]:
idstr = ""(%s) "" % reentrance_ids[id(self)]
lines = [("" "" * len(idstr)) + l for l in lines]
idline = (len(lines) - 1) // 2
lines[idline] = idstr + lines[idline][len(idstr) :]
return lines
",[],0,[],/featstruct.py__str
169,/home/amandapotts/git/nltk/nltk/featstruct.py___init__,"def __init__(self, features=()):
""""""
Create a new feature list, with the specified features.
:param features: The initial list of features for this feature
list.  If ``features`` is a string, then it is paresd using
``FeatStructReader``.  Otherwise, it should be a sequence
of basic values and nested feature structures.
""""""
if isinstance(features, str):
FeatStructReader().fromstring(features, self)
else:
list.__init__(self, features)
",[],0,[],/featstruct.py___init__
170,/home/amandapotts/git/nltk/nltk/featstruct.py___getitem__,"def __getitem__(self, name_or_path):
if isinstance(name_or_path, int):
return list.__getitem__(self, name_or_path)
elif isinstance(name_or_path, tuple):
try:
val = self
for fid in name_or_path:
if not isinstance(val, FeatStruct):
raise KeyError  # path contains base value
val = val[fid]
return val
except (KeyError, IndexError) as e:
raise KeyError(name_or_path) from e
else:
raise TypeError(self._INDEX_ERROR % name_or_path)
",[],0,[],/featstruct.py___getitem__
171,/home/amandapotts/git/nltk/nltk/featstruct.py___delitem__,"def __delitem__(self, name_or_path):
""""""If the feature with the given name or path exists, delete
its value
if self._frozen:
raise ValueError(_FROZEN_ERROR)
if isinstance(name_or_path, (int, slice)):
return list.__delitem__(self, name_or_path)
elif isinstance(name_or_path, tuple):
if len(name_or_path) == 0:
raise ValueError(""The path () can not be set"")
else:
parent = self[name_or_path[:-1]]
if not isinstance(parent, FeatStruct):
raise KeyError(name_or_path)  # path contains base value
del parent[name_or_path[-1]]
else:
raise TypeError(self._INDEX_ERROR % name_or_path)
",[],0,[],/featstruct.py___delitem__
172,/home/amandapotts/git/nltk/nltk/featstruct.py___setitem__,"def __setitem__(self, name_or_path, value):
""""""Set the value for the feature with the given name or path
to ``value``.  If ``name_or_path`` is an invalid path, raise
``KeyError``.""""""
if self._frozen:
raise ValueError(_FROZEN_ERROR)
if isinstance(name_or_path, (int, slice)):
return list.__setitem__(self, name_or_path, value)
elif isinstance(name_or_path, tuple):
if len(name_or_path) == 0:
raise ValueError(""The path () can not be set"")
else:
parent = self[name_or_path[:-1]]
if not isinstance(parent, FeatStruct):
raise KeyError(name_or_path)  # path contains base value
parent[name_or_path[-1]] = value
else:
raise TypeError(self._INDEX_ERROR % name_or_path)
",[],0,[],/featstruct.py___setitem__
173,/home/amandapotts/git/nltk/nltk/featstruct.py___deepcopy__,"def __deepcopy__(self, memo):
memo[id(self)] = selfcopy = self.__class__()
selfcopy.extend(copy.deepcopy(fval, memo) for fval in self)
return selfcopy
",[],0,[],/featstruct.py___deepcopy__
174,/home/amandapotts/git/nltk/nltk/featstruct.py__keys,"def _keys(self):
return list(range(len(self)))
",[],0,[],/featstruct.py__keys
175,/home/amandapotts/git/nltk/nltk/featstruct.py__values,"def _values(self):
return self
",[],0,[],/featstruct.py__values
176,/home/amandapotts/git/nltk/nltk/featstruct.py__items,"def _items(self):
return enumerate(self)
",[],0,[],/featstruct.py__items
177,/home/amandapotts/git/nltk/nltk/featstruct.py__repr,"def _repr(self, reentrances, reentrance_ids):
if reentrances[id(self)]:
assert id(self) not in reentrance_ids
reentrance_ids[id(self)] = repr(len(reentrance_ids) + 1)
prefix = ""(%s)"" % reentrance_ids[id(self)]
else:
prefix = """"
segments = []
for fval in self:
if id(fval) in reentrance_ids:
segments.append(""->(%s)"" % reentrance_ids[id(fval)])
elif isinstance(fval, Variable):
segments.append(fval.name)
elif isinstance(fval, Expression):
segments.append(""%s"" % fval)
elif isinstance(fval, FeatStruct):
segments.append(fval._repr(reentrances, reentrance_ids))
else:
segments.append(""%s"" % repr(fval))
return ""{}[{}]"".format(prefix, "", "".join(segments))
",[],0,[],/featstruct.py__repr
178,/home/amandapotts/git/nltk/nltk/featstruct.py_substitute_bindings,"def substitute_bindings(fstruct, bindings, fs_class=""default""):
""""""
Return the feature structure that is obtained by replacing each
variable bound by ``bindings`` with its binding.  If a variable is
aliased to a bound variable, then it will be replaced by that
variable's value.  If a variable is aliased to an unbound
variable, then it will be replaced by that variable.
:type bindings: dict(Variable -> any)
:param bindings: A dictionary mapping from variables to values.
""""""
if fs_class == ""default"":
fs_class = _default_fs_class(fstruct)
fstruct = copy.deepcopy(fstruct)
_substitute_bindings(fstruct, bindings, fs_class, set())
return fstruct
",[],0,[],/featstruct.py_substitute_bindings
179,/home/amandapotts/git/nltk/nltk/featstruct.py__substitute_bindings,"def _substitute_bindings(fstruct, bindings, fs_class, visited):
if id(fstruct) in visited:
return
visited.add(id(fstruct))
if _is_mapping(fstruct):
items = fstruct.items()
elif _is_sequence(fstruct):
items = enumerate(fstruct)
else:
raise ValueError(""Expected mapping or sequence"")
for fname, fval in items:
while isinstance(fval, Variable) and fval in bindings:
fval = fstruct[fname] = bindings[fval]
if isinstance(fval, fs_class):
_substitute_bindings(fval, bindings, fs_class, visited)
elif isinstance(fval, SubstituteBindingsI):
fstruct[fname] = fval.substitute_bindings(bindings)
",[],0,[],/featstruct.py__substitute_bindings
180,/home/amandapotts/git/nltk/nltk/featstruct.py_retract_bindings,"def retract_bindings(fstruct, bindings, fs_class=""default""):
""""""
Return the feature structure that is obtained by replacing each
feature structure value that is bound by ``bindings`` with the
variable that binds it.  A feature structure value must be
identical to a bound value (i.e., have equal id) to be replaced.
``bindings`` is modified to point to this new feature structure,
rather than the original feature structure.  Feature structure
values in ``bindings`` may be modified if they are contained in
``fstruct``.
""""""
if fs_class == ""default"":
fs_class = _default_fs_class(fstruct)
(fstruct, new_bindings) = copy.deepcopy((fstruct, bindings))
bindings.update(new_bindings)
inv_bindings = {id(val): var for (var, val) in bindings.items()}
_retract_bindings(fstruct, inv_bindings, fs_class, set())
return fstruct
",[],0,[],/featstruct.py_retract_bindings
181,/home/amandapotts/git/nltk/nltk/featstruct.py__retract_bindings,"def _retract_bindings(fstruct, inv_bindings, fs_class, visited):
if id(fstruct) in visited:
return
visited.add(id(fstruct))
if _is_mapping(fstruct):
items = fstruct.items()
elif _is_sequence(fstruct):
items = enumerate(fstruct)
else:
raise ValueError(""Expected mapping or sequence"")
for fname, fval in items:
if isinstance(fval, fs_class):
if id(fval) in inv_bindings:
fstruct[fname] = inv_bindings[id(fval)]
_retract_bindings(fval, inv_bindings, fs_class, visited)
",[],0,[],/featstruct.py__retract_bindings
182,/home/amandapotts/git/nltk/nltk/featstruct.py_find_variables,"def find_variables(fstruct, fs_class=""default""):
""""""
:return: The set of variables used by this feature structure.
:rtype: set(Variable)
""""""
if fs_class == ""default"":
fs_class = _default_fs_class(fstruct)
return _variables(fstruct, set(), fs_class, set())
",[],0,[],/featstruct.py_find_variables
183,/home/amandapotts/git/nltk/nltk/featstruct.py__variables,"def _variables(fstruct, vars, fs_class, visited):
if id(fstruct) in visited:
return
visited.add(id(fstruct))
if _is_mapping(fstruct):
items = fstruct.items()
elif _is_sequence(fstruct):
items = enumerate(fstruct)
else:
raise ValueError(""Expected mapping or sequence"")
for fname, fval in items:
if isinstance(fval, Variable):
vars.add(fval)
elif isinstance(fval, fs_class):
_variables(fval, vars, fs_class, visited)
elif isinstance(fval, SubstituteBindingsI):
vars.update(fval.variables())
return vars
",[],0,[],/featstruct.py__variables
184,/home/amandapotts/git/nltk/nltk/featstruct.py_rename_variables,"def rename_variables(
fstruct, vars=None, used_vars=(), new_vars=None, fs_class=""default""
",[],0,[],/featstruct.py_rename_variables
185,/home/amandapotts/git/nltk/nltk/featstruct.py__rename_variables,"def _rename_variables(fstruct, vars, used_vars, new_vars, fs_class, visited):
if id(fstruct) in visited:
return
visited.add(id(fstruct))
if _is_mapping(fstruct):
items = fstruct.items()
elif _is_sequence(fstruct):
items = enumerate(fstruct)
else:
raise ValueError(""Expected mapping or sequence"")
for fname, fval in items:
if isinstance(fval, Variable):
if fval in new_vars:
fstruct[fname] = new_vars[fval]
elif fval in vars:
new_vars[fval] = _rename_variable(fval, used_vars)
fstruct[fname] = new_vars[fval]
used_vars.add(new_vars[fval])
elif isinstance(fval, fs_class):
_rename_variables(fval, vars, used_vars, new_vars, fs_class, visited)
elif isinstance(fval, SubstituteBindingsI):
for var in fval.variables():
if var in vars and var not in new_vars:
new_vars[var] = _rename_variable(var, used_vars)
used_vars.add(new_vars[var])
fstruct[fname] = fval.substitute_bindings(new_vars)
return fstruct
",[],0,[],/featstruct.py__rename_variables
186,/home/amandapotts/git/nltk/nltk/featstruct.py__rename_variable,"def _rename_variable(var, used_vars):
name, n = re.sub(r""\d+$"", """", var.name), 2
if not name:
name = ""?""
while Variable(f""{name}{n}"") in used_vars:
n += 1
return Variable(f""{name}{n}"")
",[],0,[],/featstruct.py__rename_variable
187,/home/amandapotts/git/nltk/nltk/featstruct.py_remove_variables,"def remove_variables(fstruct, fs_class=""default""):
""""""
:rtype: FeatStruct
:return: The feature structure that is obtained by deleting
all features whose values are ``Variables``.
""""""
if fs_class == ""default"":
fs_class = _default_fs_class(fstruct)
return _remove_variables(copy.deepcopy(fstruct), fs_class, set())
",[],0,[],/featstruct.py_remove_variables
188,/home/amandapotts/git/nltk/nltk/featstruct.py__remove_variables,"def _remove_variables(fstruct, fs_class, visited):
if id(fstruct) in visited:
return
visited.add(id(fstruct))
if _is_mapping(fstruct):
items = list(fstruct.items())
elif _is_sequence(fstruct):
items = list(enumerate(fstruct))
else:
raise ValueError(""Expected mapping or sequence"")
for fname, fval in items:
if isinstance(fval, Variable):
del fstruct[fname]
elif isinstance(fval, fs_class):
_remove_variables(fval, fs_class, visited)
return fstruct
",[],0,[],/featstruct.py__remove_variables
189,/home/amandapotts/git/nltk/nltk/featstruct.py___repr__,"def __repr__(self):
return ""nltk.featstruct.UnificationFailure""
",[],0,[],/featstruct.py___repr__
190,/home/amandapotts/git/nltk/nltk/featstruct.py_unify,"def unify(
fstruct1,
fstruct2,
bindings=None,
trace=False,
fail=None,
rename_vars=True,
fs_class=""default"",
",[],0,[],/featstruct.py_unify
191,/home/amandapotts/git/nltk/nltk/featstruct.py__destructively_unify,"def _destructively_unify(
fstruct1, fstruct2, bindings, forward, trace, fail, fs_class, path
",[],0,[],/featstruct.py__destructively_unify
192,/home/amandapotts/git/nltk/nltk/featstruct.py__unify_feature_values,"def _unify_feature_values(
fname, fval1, fval2, bindings, forward, trace, fail, fs_class, fpath
",[],0,[],/featstruct.py__unify_feature_values
193,/home/amandapotts/git/nltk/nltk/featstruct.py__apply_forwards_to_bindings,"def _apply_forwards_to_bindings(forward, bindings):
""""""
Replace any feature structure that has a forward pointer with
the target of its forward pointer (to preserve reentrancy).
""""""
for var, value in bindings.items():
while id(value) in forward:
value = forward[id(value)]
bindings[var] = value
",[],0,[],/featstruct.py__apply_forwards_to_bindings
194,/home/amandapotts/git/nltk/nltk/featstruct.py__apply_forwards,"def _apply_forwards(fstruct, forward, fs_class, visited):
""""""
Replace any feature structure that has a forward pointer with
the target of its forward pointer (to preserve reentrancy).
""""""
while id(fstruct) in forward:
fstruct = forward[id(fstruct)]
if id(fstruct) in visited:
return
visited.add(id(fstruct))
if _is_mapping(fstruct):
items = fstruct.items()
elif _is_sequence(fstruct):
items = enumerate(fstruct)
else:
raise ValueError(""Expected mapping or sequence"")
for fname, fval in items:
if isinstance(fval, fs_class):
while id(fval) in forward:
fval = forward[id(fval)]
fstruct[fname] = fval
_apply_forwards(fval, forward, fs_class, visited)
return fstruct
",[],0,[],/featstruct.py__apply_forwards
195,/home/amandapotts/git/nltk/nltk/featstruct.py__resolve_aliases,"def _resolve_aliases(bindings):
""""""
Replace any bound aliased vars with their binding
any unbound aliased vars with their representative var.
""""""
for var, value in bindings.items():
while isinstance(value, Variable) and value in bindings:
value = bindings[var] = bindings[value]
",[],0,[],/featstruct.py__resolve_aliases
196,/home/amandapotts/git/nltk/nltk/featstruct.py__trace_unify_start,"def _trace_unify_start(path, fval1, fval2):
if path == ():
print(""\nUnification trace:"")
else:
fullname = ""."".join(""%s"" % n for n in path)
print(""  "" + ""|   "" * (len(path) - 1) + ""|"")
print(""  "" + ""|   "" * (len(path) - 1) + ""| Unify feature: %s"" % fullname)
print(""  "" + ""|   "" * len(path) + "" / "" + _trace_valrepr(fval1))
print(""  "" + ""|   "" * len(path) + ""|\\ "" + _trace_valrepr(fval2))
",[],0,[],/featstruct.py__trace_unify_start
197,/home/amandapotts/git/nltk/nltk/featstruct.py__trace_unify_identity,"def _trace_unify_identity(path, fval1):
print(""  "" + ""|   "" * len(path) + ""|"")
print(""  "" + ""|   "" * len(path) + ""| (identical objects)"")
print(""  "" + ""|   "" * len(path) + ""|"")
print(""  "" + ""|   "" * len(path) + ""+-->"" + repr(fval1))
",[],0,[],/featstruct.py__trace_unify_identity
198,/home/amandapotts/git/nltk/nltk/featstruct.py__trace_unify_fail,"def _trace_unify_fail(path, result):
if result is UnificationFailure:
resume = """"
else:
resume = "" (nonfatal)""
print(""  "" + ""|   "" * len(path) + ""|   |"")
print(""  "" + ""X   "" * len(path) + ""X   X <-- FAIL"" + resume)
",[],0,[],/featstruct.py__trace_unify_fail
199,/home/amandapotts/git/nltk/nltk/featstruct.py__trace_unify_succeed,"def _trace_unify_succeed(path, fval1):
print(""  "" + ""|   "" * len(path) + ""|"")
print(""  "" + ""|   "" * len(path) + ""+-->"" + repr(fval1))
",[],0,[],/featstruct.py__trace_unify_succeed
200,/home/amandapotts/git/nltk/nltk/featstruct.py__trace_valrepr,"def _trace_valrepr(val):
if isinstance(val, Variable):
return ""%s"" % val
else:
return ""%s"" % repr(val)
",[],0,[],/featstruct.py__trace_valrepr
201,/home/amandapotts/git/nltk/nltk/featstruct.py_subsumes,"def subsumes(fstruct1, fstruct2):
""""""
Return True if ``fstruct1`` subsumes ``fstruct2``.  I.e., return
true if unifying ``fstruct1`` with ``fstruct2`` would result in a
feature structure equal to ``fstruct2.``
:rtype: bool
""""""
return fstruct2 == unify(fstruct1, fstruct2)
",[],0,[],/featstruct.py_subsumes
202,/home/amandapotts/git/nltk/nltk/featstruct.py_conflicts,"def conflicts(fstruct1, fstruct2, trace=0):
""""""
Return a list of the feature paths of all features which are
assigned incompatible values by ``fstruct1`` and ``fstruct2``.
:rtype: list(tuple)
""""""
conflict_list = []
",[],0,[],/featstruct.py_conflicts
203,/home/amandapotts/git/nltk/nltk/featstruct.py_add_conflict,"def add_conflict(fval1, fval2, path):
conflict_list.append(path)
return fval1
",[],0,[],/featstruct.py_add_conflict
204,/home/amandapotts/git/nltk/nltk/featstruct.py__is_mapping,"def _is_mapping(v):
return hasattr(v, ""__contains__"") and hasattr(v, ""keys"")
",[],0,[],/featstruct.py__is_mapping
205,/home/amandapotts/git/nltk/nltk/featstruct.py__is_sequence,"def _is_sequence(v):
return hasattr(v, ""__iter__"") and hasattr(v, ""__len__"") and not isinstance(v, str)
",[],0,[],/featstruct.py__is_sequence
206,/home/amandapotts/git/nltk/nltk/featstruct.py__default_fs_class,"def _default_fs_class(obj):
if isinstance(obj, FeatStruct):
return FeatStruct
if isinstance(obj, (dict, list)):
return (dict, list)
else:
raise ValueError(
""To unify objects of type %s, you must specify ""
""fs_class explicitly."" % obj.__class__.__name__
)
",[],0,[],/featstruct.py__default_fs_class
207,/home/amandapotts/git/nltk/nltk/featstruct.py_variables,"def variables(self):
return [elt for elt in self if isinstance(elt, Variable)] + sum(
(
list(elt.variables())
for elt in self
if isinstance(elt, SubstituteBindingsI)
),
[],
)
",[],0,[],/featstruct.py_variables
208,/home/amandapotts/git/nltk/nltk/featstruct.py_substitute_bindings,"def substitute_bindings(self, bindings):
return self.__class__([self.subst(v, bindings) for v in self])
",[],0,[],/featstruct.py_substitute_bindings
209,/home/amandapotts/git/nltk/nltk/featstruct.py_subst,"def subst(self, v, bindings):
if isinstance(v, SubstituteBindingsI):
return v.substitute_bindings(bindings)
else:
return bindings.get(v, v)
",[],0,[],/featstruct.py_subst
210,/home/amandapotts/git/nltk/nltk/featstruct.py___repr__,"def __repr__(self):  # [xx] really use %s here?
if len(self) == 0:
return ""()""
return ""(%s)"" % "", "".join(f""{b}"" for b in self)
",[],0,[],/featstruct.py___repr__
211,/home/amandapotts/git/nltk/nltk/featstruct.py___repr__,"def __repr__(self):  # [xx] really use %s here?
if len(self) == 0:
return ""{/}""  # distinguish from dict.
return ""{%s}"" % "", "".join(sorted(f""{b}"" for b in self))
",[],0,[],/featstruct.py___repr__
212,/home/amandapotts/git/nltk/nltk/featstruct.py___new__,"def __new__(cls, values):
values = _flatten(values, FeatureValueUnion)
if sum(isinstance(v, Variable) for v in values) == 0:
values = _flatten(values, FeatureValueSet)
return FeatureValueSet(values)
if len(values) == 1:
return list(values)[0]
return frozenset.__new__(cls, values)
",[],0,[],/featstruct.py___new__
213,/home/amandapotts/git/nltk/nltk/featstruct.py___repr__,"def __repr__(self):
return ""{%s}"" % ""+"".join(sorted(f""{b}"" for b in self))
",[],0,[],/featstruct.py___repr__
214,/home/amandapotts/git/nltk/nltk/featstruct.py___new__,"def __new__(cls, values):
values = _flatten(values, FeatureValueConcat)
if sum(isinstance(v, Variable) for v in values) == 0:
values = _flatten(values, FeatureValueTuple)
return FeatureValueTuple(values)
if len(values) == 1:
return list(values)[0]
return tuple.__new__(cls, values)
",[],0,[],/featstruct.py___new__
215,/home/amandapotts/git/nltk/nltk/featstruct.py___repr__,"def __repr__(self):
return ""(%s)"" % ""+"".join(f""{b}"" for b in self)
",[],0,[],/featstruct.py___repr__
216,/home/amandapotts/git/nltk/nltk/featstruct.py__flatten,"def _flatten(lst, cls):
""""""
Helper function -- return a copy of list, with all elements of
type ``cls`` spliced in rather than appended in.
""""""
result = []
for elt in lst:
if isinstance(elt, cls):
result.extend(elt)
else:
result.append(elt)
return result
",[],0,[],/featstruct.py__flatten
217,/home/amandapotts/git/nltk/nltk/featstruct.py___init__,"def __init__(self, name, default=None, display=None):
assert display in (None, ""prefix"", ""slash"")
self._name = name  # [xx] rename to .identifier?
self._default = default  # [xx] not implemented yet.
self._display = display
if self._display == ""prefix"":
self._sortkey = (-1, self._name)
elif self._display == ""slash"":
self._sortkey = (1, self._name)
else:
self._sortkey = (0, self._name)
",[],0,[],/featstruct.py___init__
218,/home/amandapotts/git/nltk/nltk/featstruct.py_name,"def name(self):
""""""The name of this feature.""""""
return self._name
",[],0,[],/featstruct.py_name
219,/home/amandapotts/git/nltk/nltk/featstruct.py_default,"def default(self):
""""""Default value for this feature.""""""
return self._default
",[],0,[],/featstruct.py_default
220,/home/amandapotts/git/nltk/nltk/featstruct.py_display,"def display(self):
""""""Custom display location: can be prefix, or slash.""""""
return self._display
",[],0,[],/featstruct.py_display
221,/home/amandapotts/git/nltk/nltk/featstruct.py___repr__,"def __repr__(self):
return ""*%s*"" % self.name
",[],0,[],/featstruct.py___repr__
222,/home/amandapotts/git/nltk/nltk/featstruct.py___lt__,"def __lt__(self, other):
if isinstance(other, str):
return True
if not isinstance(other, Feature):
raise_unorderable_types(""<"", self, other)
return self._sortkey < other._sortkey
",[],0,[],/featstruct.py___lt__
223,/home/amandapotts/git/nltk/nltk/featstruct.py___eq__,"def __eq__(self, other):
return type(self) == type(other) and self._name == other._name
",[],0,[],/featstruct.py___eq__
224,/home/amandapotts/git/nltk/nltk/featstruct.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/featstruct.py___ne__
225,/home/amandapotts/git/nltk/nltk/featstruct.py___hash__,"def __hash__(self):
return hash(self._name)
",[],0,[],/featstruct.py___hash__
226,/home/amandapotts/git/nltk/nltk/featstruct.py_read_value,"def read_value(self, s, position, reentrances, parser):
return parser.read_value(s, position, reentrances)
",[],0,[],/featstruct.py_read_value
227,/home/amandapotts/git/nltk/nltk/featstruct.py_unify_base_values,"def unify_base_values(self, fval1, fval2, bindings):
""""""
If possible, return a single value..  If not, return
the value ``UnificationFailure``.
""""""
if fval1 == fval2:
return fval1
else:
return UnificationFailure
",[],0,[],/featstruct.py_unify_base_values
228,/home/amandapotts/git/nltk/nltk/featstruct.py_read_value,"def read_value(self, s, position, reentrances, parser):
return parser.read_partial(s, position, reentrances)
",[],0,[],/featstruct.py_read_value
229,/home/amandapotts/git/nltk/nltk/featstruct.py_read_value,"def read_value(self, s, position, reentrances, parser):
m = self.RANGE_RE.match(s, position)
if not m:
raise ValueError(""range"", position)
return (int(m.group(1)), int(m.group(2))), m.end()
",[],0,[],/featstruct.py_read_value
230,/home/amandapotts/git/nltk/nltk/featstruct.py_unify_base_values,"def unify_base_values(self, fval1, fval2, bindings):
if fval1 is None:
return fval2
if fval2 is None:
return fval1
rng = max(fval1[0], fval2[0]), min(fval1[1], fval2[1])
if rng[1] < rng[0]:
return UnificationFailure
return rng
",[],0,[],/featstruct.py_unify_base_values
231,/home/amandapotts/git/nltk/nltk/featstruct.py_unify,"def unify(self, other):
""""""
If this base value unifies with ``other``, then return the
unified value.  Otherwise, return ``UnificationFailure``.
""""""
raise NotImplementedError(""abstract base class"")
",[],0,[],/featstruct.py_unify
232,/home/amandapotts/git/nltk/nltk/featstruct.py___eq__,"def __eq__(self, other):
return NotImplemented
",[],0,[],/featstruct.py___eq__
233,/home/amandapotts/git/nltk/nltk/featstruct.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/featstruct.py___ne__
234,/home/amandapotts/git/nltk/nltk/featstruct.py___lt__,"def __lt__(self, other):
return NotImplemented
",[],0,[],/featstruct.py___lt__
235,/home/amandapotts/git/nltk/nltk/featstruct.py___hash__,"def __hash__(self):
raise TypeError(""%s objects or unhashable"" % self.__class__.__name__)
",[],0,[],/featstruct.py___hash__
236,/home/amandapotts/git/nltk/nltk/featstruct.py___init__,"def __init__(
self,
features=(SLASH, TYPE),
fdict_class=FeatStruct,
flist_class=FeatList,
logic_parser=None,
",[],0,[],/featstruct.py___init__
237,/home/amandapotts/git/nltk/nltk/featstruct.py_fromstring,"def fromstring(self, s, fstruct=None):
""""""
Convert a string representation of a feature structure (as
displayed by repr) into a ``FeatStruct``.  This process
imposes the following restrictions on the string
representation:
- Feature names cannot contain any of the following:
whitespace, parentheses, quote marks, equals signs,
dashes, commas, and square brackets.  Feature names may
not begin with plus signs or minus signs.
- Only the following basic feature value are supported:
strings, integers, variables, None, and unquoted
alphanumeric strings.
- For reentrant values, the first mention must specify
a reentrance identifier and a value
mentions must use arrows (``'->'``) to reference the
reentrance identifier.
""""""
s = s.strip()
value, position = self.read_partial(s, 0, {}, fstruct)
if position != len(s):
self._error(s, ""end of string"", position)
return value
",[],0,[],/featstruct.py_fromstring
238,/home/amandapotts/git/nltk/nltk/featstruct.py_read_partial,"def read_partial(self, s, position=0, reentrances=None, fstruct=None):
""""""
Helper function that reads in a feature structure.
:param s: The string to read.
:param position: The position in the string to start parsing.
:param reentrances: A dictionary from reentrance ids to values.
Defaults to an empty dictionary.
:return: A tuple (val, pos) of the feature structure created by
parsing and the position where the parsed feature structure ends.
:rtype: bool
""""""
if reentrances is None:
reentrances = {}
try:
return self._read_partial(s, position, reentrances, fstruct)
except ValueError as e:
if len(e.args) != 2:
raise
self._error(s, *e.args)
",[],0,[],/featstruct.py_read_partial
239,/home/amandapotts/git/nltk/nltk/featstruct.py__read_partial,"def _read_partial(self, s, position, reentrances, fstruct=None):
if fstruct is None:
if self._START_FDICT_RE.match(s, position):
fstruct = self._fdict_class()
else:
fstruct = self._flist_class()
match = self._START_FSTRUCT_RE.match(s, position)
if not match:
match = self._BARE_PREFIX_RE.match(s, position)
if not match:
raise ValueError(""open bracket or identifier"", position)
position = match.end()
if match.group(1):
identifier = match.group(1)
if identifier in reentrances:
raise ValueError(""new identifier"", match.start(1))
reentrances[identifier] = fstruct
if isinstance(fstruct, FeatDict):
fstruct.clear()
return self._read_partial_featdict(s, position, match, reentrances, fstruct)
else:
del fstruct[:]
return self._read_partial_featlist(s, position, match, reentrances, fstruct)
",[],0,[],/featstruct.py__read_partial
240,/home/amandapotts/git/nltk/nltk/featstruct.py__read_partial_featlist,"def _read_partial_featlist(self, s, position, match, reentrances, fstruct):
if match.group(2):
raise ValueError(""open bracket"")
if not match.group(3):
raise ValueError(""open bracket"")
while position < len(s):
match = self._END_FSTRUCT_RE.match(s, position)
if match is not None:
return fstruct, match.end()
match = self._REENTRANCE_RE.match(s, position)
if match:
position = match.end()
match = self._TARGET_RE.match(s, position)
if not match:
raise ValueError(""identifier"", position)
target = match.group(1)
if target not in reentrances:
raise ValueError(""bound identifier"", position)
position = match.end()
fstruct.append(reentrances[target])
else:
value, position = self._read_value(0, s, position, reentrances)
fstruct.append(value)
if self._END_FSTRUCT_RE.match(s, position):
continue
match = self._COMMA_RE.match(s, position)
if match is None:
raise ValueError(""comma"", position)
position = match.end()
raise ValueError(""close bracket"", position)
",[],0,[],/featstruct.py__read_partial_featlist
241,/home/amandapotts/git/nltk/nltk/featstruct.py__read_partial_featdict,"def _read_partial_featdict(self, s, position, match, reentrances, fstruct):
if match.group(2):
if self._prefix_feature is None:
raise ValueError(""open bracket or identifier"", match.start(2))
prefixval = match.group(2).strip()
if prefixval.startswith(""?""):
prefixval = Variable(prefixval)
fstruct[self._prefix_feature] = prefixval
if not match.group(3):
return self._finalize(s, match.end(), reentrances, fstruct)
while position < len(s):
name = value = None
match = self._END_FSTRUCT_RE.match(s, position)
if match is not None:
return self._finalize(s, match.end(), reentrances, fstruct)
match = self._FEATURE_NAME_RE.match(s, position)
if match is None:
raise ValueError(""feature name"", position)
name = match.group(2)
position = match.end()
if name[0] == ""*"" and name[-1] == ""*"":
name = self._features.get(name[1:-1])
if name is None:
raise ValueError(""known special feature"", match.start(2))
if name in fstruct:
raise ValueError(""new name"", match.start(2))
if match.group(1) == ""+"":
value = True
if match.group(1) == ""-"":
value = False
if value is None:
match = self._REENTRANCE_RE.match(s, position)
if match is not None:
position = match.end()
match = self._TARGET_RE.match(s, position)
if not match:
raise ValueError(""identifier"", position)
target = match.group(1)
if target not in reentrances:
raise ValueError(""bound identifier"", position)
position = match.end()
value = reentrances[target]
if value is None:
match = self._ASSIGN_RE.match(s, position)
if match:
position = match.end()
value, position = self._read_value(name, s, position, reentrances)
else:
raise ValueError(""equals sign"", position)
fstruct[name] = value
if self._END_FSTRUCT_RE.match(s, position):
continue
match = self._COMMA_RE.match(s, position)
if match is None:
raise ValueError(""comma"", position)
position = match.end()
raise ValueError(""close bracket"", position)
",[],0,[],/featstruct.py__read_partial_featdict
242,/home/amandapotts/git/nltk/nltk/featstruct.py__finalize,"def _finalize(self, s, pos, reentrances, fstruct):
""""""
Called when we see the close brace -- checks for a slash feature,
and adds in default values.
""""""
match = self._SLASH_RE.match(s, pos)
if match:
name = self._slash_feature
v, pos = self._read_value(name, s, match.end(), reentrances)
fstruct[name] = v
return fstruct, pos
",[],0,[],/featstruct.py__finalize
243,/home/amandapotts/git/nltk/nltk/featstruct.py__read_value,"def _read_value(self, name, s, position, reentrances):
if isinstance(name, Feature):
return name.read_value(s, position, reentrances, self)
else:
return self.read_value(s, position, reentrances)
",[],0,[],/featstruct.py__read_value
244,/home/amandapotts/git/nltk/nltk/featstruct.py_read_value,"def read_value(self, s, position, reentrances):
for handler, regexp in self.VALUE_HANDLERS:
match = regexp.match(s, position)
if match:
handler_func = getattr(self, handler)
return handler_func(s, position, reentrances, match)
raise ValueError(""value"", position)
",[],0,[],/featstruct.py_read_value
245,/home/amandapotts/git/nltk/nltk/featstruct.py__error,"def _error(self, s, expected, position):
lines = s.split(""\n"")
while position > len(lines[0]):
position -= len(lines.pop(0)) + 1  # +1 for the newline.
estr = (
""Error parsing feature structure\n    ""
+ lines[0]
+ ""\n    ""
+ "" "" * position
+ ""^ ""
+ ""Expected %s"" % expected
)
raise ValueError(estr)
",[],0,[],/featstruct.py__error
246,/home/amandapotts/git/nltk/nltk/featstruct.py_read_fstruct_value,"def read_fstruct_value(self, s, position, reentrances, match):
return self.read_partial(s, position, reentrances)
",[],0,[],/featstruct.py_read_fstruct_value
247,/home/amandapotts/git/nltk/nltk/featstruct.py_read_str_value,"def read_str_value(self, s, position, reentrances, match):
return read_str(s, position)
",[],0,[],/featstruct.py_read_str_value
248,/home/amandapotts/git/nltk/nltk/featstruct.py_read_int_value,"def read_int_value(self, s, position, reentrances, match):
return int(match.group()), match.end()
",[],0,[],/featstruct.py_read_int_value
249,/home/amandapotts/git/nltk/nltk/featstruct.py_read_var_value,"def read_var_value(self, s, position, reentrances, match):
return Variable(match.group()), match.end()
",[],0,[],/featstruct.py_read_var_value
250,/home/amandapotts/git/nltk/nltk/featstruct.py_read_sym_value,"def read_sym_value(self, s, position, reentrances, match):
val, end = match.group(), match.end()
return self._SYM_CONSTS.get(val, val), end
",[],0,[],/featstruct.py_read_sym_value
251,/home/amandapotts/git/nltk/nltk/featstruct.py_read_app_value,"def read_app_value(self, s, position, reentrances, match):
""""""Mainly included for backwards compat.""""""
return self._logic_parser.parse(""%s(%s)"" % match.group(2, 3)), match.end()
",[],0,[],/featstruct.py_read_app_value
252,/home/amandapotts/git/nltk/nltk/featstruct.py_read_logic_value,"def read_logic_value(self, s, position, reentrances, match):
try:
try:
expr = self._logic_parser.parse(match.group(1))
except LogicalExpressionException as e:
raise ValueError from e
return expr, match.end()
except ValueError as e:
raise ValueError(""logic expression"", match.start(1)) from e
",[],0,[],/featstruct.py_read_logic_value
253,/home/amandapotts/git/nltk/nltk/featstruct.py_read_tuple_value,"def read_tuple_value(self, s, position, reentrances, match):
return self._read_seq_value(
s, position, reentrances, match, "")"", FeatureValueTuple, FeatureValueConcat
)
",[],0,[],/featstruct.py_read_tuple_value
254,/home/amandapotts/git/nltk/nltk/featstruct.py_read_set_value,"def read_set_value(self, s, position, reentrances, match):
return self._read_seq_value(
s, position, reentrances, match, ""}"", FeatureValueSet, FeatureValueUnion
)
",[],0,[],/featstruct.py_read_set_value
255,/home/amandapotts/git/nltk/nltk/featstruct.py__read_seq_value,"def _read_seq_value(
self, s, position, reentrances, match, close_paren, seq_class, plus_class
",[],0,[],/featstruct.py__read_seq_value
256,/home/amandapotts/git/nltk/nltk/featstruct.py_display_unification,"def display_unification(fs1, fs2, indent=""  ""):
fs1_lines = (""%s"" % fs1).split(""\n"")
fs2_lines = (""%s"" % fs2).split(""\n"")
if len(fs1_lines) > len(fs2_lines):
blankline = ""["" + "" "" * (len(fs2_lines[0]) - 2) + ""]""
fs2_lines += [blankline] * len(fs1_lines)
else:
blankline = ""["" + "" "" * (len(fs1_lines[0]) - 2) + ""]""
fs1_lines += [blankline] * len(fs2_lines)
for fs1_line, fs2_line in zip(fs1_lines, fs2_lines):
print(indent + fs1_line + ""   "" + fs2_line)
print(indent + ""-"" * len(fs1_lines[0]) + ""   "" + ""-"" * len(fs2_lines[0]))
linelen = len(fs1_lines[0]) * 2 + 3
print(indent + ""|               |"".center(linelen))
print(indent + ""+-----UNIFY-----+"".center(linelen))
print(indent + ""|"".center(linelen))
print(indent + ""V"".center(linelen))
bindings = {}
result = fs1.unify(fs2, bindings)
if result is None:
print(indent + ""(FAILED)"".center(linelen))
else:
print(
""\n"".join(indent + l.center(linelen) for l in (""%s"" % result).split(""\n""))
)
if bindings and len(bindings.bound_variables()) > 0:
print(repr(bindings).center(linelen))
return result
",[],0,[],/featstruct.py_display_unification
257,/home/amandapotts/git/nltk/nltk/featstruct.py_interactive_demo,"def interactive_demo(trace=False):
import random
import sys
HELP = """"""
1-%d: Select the corresponding feature structure
q: Quit
t: Turn tracing on or off
l: List all feature structures
?: Help
""""""
print(
""""""
This demo will repeatedly present you with a list of feature
structures, and ask you to choose two for unification.  Whenever a
new feature structure is generated, it is added to the list of
choices that you can pick from.  However, since this can be a
large number of feature structures, the demo will only print out a
random subset for you to choose between at a given time.  If you
want to see the complete lists, type ""l"".  For a list of valid
commands, type ""?"".
""""""
)
print('Press ""Enter"" to continue...')
sys.stdin.readline()
fstruct_strings = [
""[agr=[number=sing, gender=masc]]"",
""[agr=[gender=masc, person=3]]"",
""[agr=[gender=fem, person=3]]"",
""[subj=[agr=(1)[]], agr->(1)]"",
""[obj=?x]"",
""[subj=?x]"",
""[/=None]"",
""[/=NP]"",
""[cat=NP]"",
""[cat=VP]"",
""[cat=PP]"",
""[subj=[agr=[gender=?y]], obj=[agr=[gender=?y]]]"",
""[gender=masc, agr=?C]"",
""[gender=?S, agr=[gender=?S,person=3]]"",
]
all_fstructs = [
(i, FeatStruct(fstruct_strings[i])) for i in range(len(fstruct_strings))
]
",[],0,[],/featstruct.py_interactive_demo
258,/home/amandapotts/git/nltk/nltk/featstruct.py_list_fstructs,"def list_fstructs(fstructs):
for i, fstruct in fstructs:
print()
lines = (""%s"" % fstruct).split(""\n"")
print(""%3d: %s"" % (i + 1, lines[0]))
for line in lines[1:]:
print(""     "" + line)
print()
",[],0,[],/featstruct.py_list_fstructs
259,/home/amandapotts/git/nltk/nltk/featstruct.py_demo,"def demo(trace=False):
""""""
Just for testing
""""""
fstruct_strings = [
""[agr=[number=sing, gender=masc]]"",
""[agr=[gender=masc, person=3]]"",
""[agr=[gender=fem, person=3]]"",
""[subj=[agr=(1)[]], agr->(1)]"",
""[obj=?x]"",
""[subj=?x]"",
""[/=None]"",
""[/=NP]"",
""[cat=NP]"",
""[cat=VP]"",
""[cat=PP]"",
""[subj=[agr=[gender=?y]], obj=[agr=[gender=?y]]]"",
""[gender=masc, agr=?C]"",
""[gender=?S, agr=[gender=?S,person=3]]"",
]
all_fstructs = [FeatStruct(fss) for fss in fstruct_strings]
for fs1 in all_fstructs:
for fs2 in all_fstructs:
print(
""\n*******************\nfs1 is:\n%s\n\nfs2 is:\n%s\n\nresult is:\n%s""
% (fs1, fs2, unify(fs1, fs2))
)
",[],0,[],/featstruct.py_demo
260,/home/amandapotts/git/nltk/nltk/probability.py___init__,"def __init__(self, samples=None):
""""""
Construct a new frequency distribution.  If ``samples`` is
given, then the frequency distribution will be initialized
with the count of each object in ``samples``
will be initialized to be empty.
In particular, ``FreqDist()`` returns an empty frequency
distribution
frequency distribution, and then calls ``update`` with the
list ``samples``.
:param samples: The samples to initialize the frequency
distribution with.
:type samples: Sequence
""""""
Counter.__init__(self, samples)
self._N = None
",[],0,[],/probability.py___init__
261,/home/amandapotts/git/nltk/nltk/probability.py_N,"def N(self):
""""""
Return the total number of sample outcomes that have been
recorded by this FreqDist.  For the number of unique
sample values (or bins) with counts greater than zero, use
``FreqDist.B()``.
:rtype: int
""""""
if self._N is None:
self._N = sum(self.values())
return self._N
",[],0,[],/probability.py_N
262,/home/amandapotts/git/nltk/nltk/probability.py___setitem__,"def __setitem__(self, key, val):
""""""
Override ``Counter.__setitem__()`` to invalidate the cached N
""""""
self._N = None
super().__setitem__(key, val)
",[],0,[],/probability.py___setitem__
263,/home/amandapotts/git/nltk/nltk/probability.py___delitem__,"def __delitem__(self, key):
""""""
Override ``Counter.__delitem__()`` to invalidate the cached N
""""""
self._N = None
super().__delitem__(key)
",[],0,[],/probability.py___delitem__
264,/home/amandapotts/git/nltk/nltk/probability.py_update,"def update(self, *args, **kwargs):
""""""
Override ``Counter.update()`` to invalidate the cached N
""""""
self._N = None
super().update(*args, **kwargs)
",[],0,[],/probability.py_update
265,/home/amandapotts/git/nltk/nltk/probability.py_setdefault,"def setdefault(self, key, val):
""""""
Override ``Counter.setdefault()`` to invalidate the cached N
""""""
self._N = None
super().setdefault(key, val)
",[],0,[],/probability.py_setdefault
266,/home/amandapotts/git/nltk/nltk/probability.py_B,"def B(self):
""""""
Return the total number of sample values (or ""bins"") that
have counts greater than zero.  For the total
number of sample outcomes recorded, use ``FreqDist.N()``.
(FreqDist.B() is the same as len(FreqDist).)
:rtype: int
""""""
return len(self)
",[],0,[],/probability.py_B
267,/home/amandapotts/git/nltk/nltk/probability.py_hapaxes,"def hapaxes(self):
""""""
Return a list of all samples that occur once (hapax legomena)
:rtype: list
""""""
return [item for item in self if self[item] == 1]
",[],0,[],/probability.py_hapaxes
268,/home/amandapotts/git/nltk/nltk/probability.py_Nr,"def Nr(self, r, bins=None):
return self.r_Nr(bins)[r]
",[],0,[],/probability.py_Nr
269,/home/amandapotts/git/nltk/nltk/probability.py_r_Nr,"def r_Nr(self, bins=None):
""""""
Return the dictionary mapping r to Nr, the number of samples with frequency r, where Nr > 0.
:type bins: int
:param bins: The number of possible sample outcomes.  ``bins``
is used to calculate Nr(0).  In particular, Nr(0) is
``bins-self.B()``.  If ``bins`` is not specified, it
defaults to ``self.B()`` (so Nr(0) will be 0).
:rtype: int
""""""
_r_Nr = defaultdict(int)
for count in self.values():
_r_Nr[count] += 1
_r_Nr[0] = bins - self.B() if bins is not None else 0
return _r_Nr
",[],0,[],/probability.py_r_Nr
270,/home/amandapotts/git/nltk/nltk/probability.py__cumulative_frequencies,"def _cumulative_frequencies(self, samples):
""""""
Return the cumulative frequencies of the specified samples.
If no samples are specified, all counts are returned, starting
with the largest.
:param samples: the samples whose frequencies should be returned.
:type samples: any
:rtype: list(float)
""""""
cf = 0.0
for sample in samples:
cf += self[sample]
yield cf
",[],0,[],/probability.py__cumulative_frequencies
271,/home/amandapotts/git/nltk/nltk/probability.py_freq,"def freq(self, sample):
""""""
Return the frequency of a given sample.  The frequency of a
sample is defined as the count of that sample divided by the
total number of sample outcomes that have been recorded by
this FreqDist.  The count of a sample is defined as the
number of times that sample outcome was recorded by this
FreqDist.  Frequencies are always real numbers in the range
[0, 1].
:param sample: the sample whose frequency
should be returned.
:type sample: any
:rtype: float
""""""
n = self.N()
if n == 0:
return 0
return self[sample] / n
",[],0,[],/probability.py_freq
272,/home/amandapotts/git/nltk/nltk/probability.py_max,"def max(self):
""""""
Return the sample with the greatest number of outcomes in this
frequency distribution.  If two or more samples have the same
number of outcomes, return one of them
returned is undefined.  If no outcomes have occurred in this
frequency distribution, return None.
:return: The sample with the maximum number of outcomes in this
frequency distribution.
:rtype: any or None
""""""
if len(self) == 0:
raise ValueError(
""A FreqDist must have at least one sample before max is defined.""
)
return self.most_common(1)[0][0]
",[],0,[],/probability.py_max
273,/home/amandapotts/git/nltk/nltk/probability.py_plot,"def plot(
self, *args, title="""", cumulative=False, percents=False, show=False, **kwargs
",[],0,[],/probability.py_plot
274,/home/amandapotts/git/nltk/nltk/probability.py_tabulate,"def tabulate(self, *args, **kwargs):
""""""
Tabulate the given samples from the frequency distribution (cumulative),
displaying the most frequent sample first.  If an integer
parameter is supplied, stop after this many samples have been
plotted.
:param samples: The samples to plot (default is all samples)
:type samples: list
:param cumulative: A flag to specify whether the freqs are cumulative (default = False)
:type title: bool
""""""
if len(args) == 0:
args = [len(self)]
samples = _get_kwarg(
kwargs, ""samples"", [item for item, _ in self.most_common(*args)]
)
cumulative = _get_kwarg(kwargs, ""cumulative"", False)
if cumulative:
freqs = list(self._cumulative_frequencies(samples))
else:
freqs = [self[sample] for sample in samples]
width = max(len(f""{s}"") for s in samples)
width = max(width, max(len(""%d"" % f) for f in freqs))
for i in range(len(samples)):
print(""%*s"" % (width, samples[i]), end="" "")
print()
for i in range(len(samples)):
print(""%*d"" % (width, freqs[i]), end="" "")
print()
",[],0,[],/probability.py_tabulate
275,/home/amandapotts/git/nltk/nltk/probability.py_copy,"def copy(self):
""""""
Create a copy of this frequency distribution.
:rtype: FreqDist
""""""
return self.__class__(self)
",[],0,[],/probability.py_copy
276,/home/amandapotts/git/nltk/nltk/probability.py___add__,"def __add__(self, other):
""""""
Add counts from two counters.
>>> FreqDist('abbb') + FreqDist('bcc')
FreqDist({'b': 4, 'c': 2, 'a': 1})
""""""
return self.__class__(super().__add__(other))
",[],0,[],/probability.py___add__
277,/home/amandapotts/git/nltk/nltk/probability.py___sub__,"def __sub__(self, other):
""""""
Subtract count, but keep only results with positive counts.
>>> FreqDist('abbbc') - FreqDist('bccd')
FreqDist({'b': 2, 'a': 1})
""""""
return self.__class__(super().__sub__(other))
",[],0,[],/probability.py___sub__
278,/home/amandapotts/git/nltk/nltk/probability.py___or__,"def __or__(self, other):
""""""
Union is the maximum of value in either of the input counters.
>>> FreqDist('abbb') | FreqDist('bcc')
FreqDist({'b': 3, 'c': 2, 'a': 1})
""""""
return self.__class__(super().__or__(other))
",[],0,[],/probability.py___or__
279,/home/amandapotts/git/nltk/nltk/probability.py___and__,"def __and__(self, other):
""""""
Intersection is the minimum of corresponding counts.
>>> FreqDist('abbb') & FreqDist('bcc')
FreqDist({'b': 1})
""""""
return self.__class__(super().__and__(other))
",[],0,[],/probability.py___and__
280,/home/amandapotts/git/nltk/nltk/probability.py___le__,"def __le__(self, other):
""""""
Returns True if this frequency distribution is a subset of the other
and for no key the value exceeds the value of the same key from
the other frequency distribution.
The <= operator forms partial order and satisfying the axioms
reflexivity, antisymmetry and transitivity.
>>> FreqDist('a') <= FreqDist('a')
True
>>> a = FreqDist('abc')
>>> b = FreqDist('aabc')
>>> (a <= b, b <= a)
(True, False)
>>> FreqDist('a') <= FreqDist('abcd')
True
>>> FreqDist('abc') <= FreqDist('xyz')
False
>>> FreqDist('xyz') <= FreqDist('abc')
False
>>> c = FreqDist('a')
>>> d = FreqDist('aa')
>>> e = FreqDist('aaa')
>>> c <= d and d <= e and c <= e
True
""""""
if not isinstance(other, FreqDist):
raise_unorderable_types(""<="", self, other)
return set(self).issubset(other) and all(
self[key] <= other[key] for key in self
)
",[],0,[],/probability.py___le__
281,/home/amandapotts/git/nltk/nltk/probability.py___ge__,"def __ge__(self, other):
if not isinstance(other, FreqDist):
raise_unorderable_types("">="", self, other)
return set(self).issuperset(other) and all(
self[key] >= other[key] for key in other
)
",[],0,[],/probability.py___ge__
282,/home/amandapotts/git/nltk/nltk/probability.py___repr__,"def __repr__(self):
""""""
Return a string representation of this FreqDist.
:rtype: string
""""""
return self.pformat()
",[],0,[],/probability.py___repr__
283,/home/amandapotts/git/nltk/nltk/probability.py_pprint,"def pprint(self, maxlen=10, stream=None):
""""""
Print a string representation of this FreqDist to 'stream'
:param maxlen: The maximum number of items to print
:type maxlen: int
:param stream: The stream to print to. stdout by default
""""""
print(self.pformat(maxlen=maxlen), file=stream)
",[],0,[],/probability.py_pprint
284,/home/amandapotts/git/nltk/nltk/probability.py_pformat,"def pformat(self, maxlen=10):
""""""
Return a string representation of this FreqDist.
:param maxlen: The maximum number of items to display
:type maxlen: int
:rtype: string
""""""
items = [""{!r}: {!r}"".format(*item) for item in self.most_common(maxlen)]
if len(self) > maxlen:
items.append(""..."")
return ""FreqDist({{{0}}})"".format("", "".join(items))
",[],0,[],/probability.py_pformat
285,/home/amandapotts/git/nltk/nltk/probability.py___str__,"def __str__(self):
""""""
Return a string representation of this FreqDist.
:rtype: string
""""""
return ""<FreqDist with %d samples and %d outcomes>"" % (len(self), self.N())
",[],0,[],/probability.py___str__
286,/home/amandapotts/git/nltk/nltk/probability.py___iter__,"def __iter__(self):
""""""
Return an iterator which yields tokens ordered by frequency.
:rtype: iterator
""""""
for token, _ in self.most_common(self.B()):
yield token
",[],0,[],/probability.py___iter__
287,/home/amandapotts/git/nltk/nltk/probability.py___init__,"def __init__(self):
""""""
Classes inheriting from ProbDistI should implement __init__.
""""""
",[],0,[],/probability.py___init__
288,/home/amandapotts/git/nltk/nltk/probability.py_prob,"def prob(self, sample):
""""""
Return the probability for a given sample.  Probabilities
are always real numbers in the range [0, 1].
:param sample: The sample whose probability
should be returned.
:type sample: any
:rtype: float
""""""
",[],0,[],/probability.py_prob
289,/home/amandapotts/git/nltk/nltk/probability.py_logprob,"def logprob(self, sample):
""""""
Return the base 2 logarithm of the probability for a given sample.
:param sample: The sample whose probability
should be returned.
:type sample: any
:rtype: float
""""""
p = self.prob(sample)
return math.log(p, 2) if p != 0 else _NINF
",[],0,[],/probability.py_logprob
290,/home/amandapotts/git/nltk/nltk/probability.py_max,"def max(self):
""""""
Return the sample with the greatest probability.  If two or
more samples have the same probability, return one of them
which sample is returned is undefined.
:rtype: any
""""""
",[],0,[],/probability.py_max
291,/home/amandapotts/git/nltk/nltk/probability.py_samples,"def samples(self):
""""""
Return a list of all samples that have nonzero probabilities.
Use ``prob`` to find the probability of each sample.
:rtype: list
""""""
",[],0,[],/probability.py_samples
292,/home/amandapotts/git/nltk/nltk/probability.py_discount,"def discount(self):
""""""
Return the ratio by which counts are discounted on average: c*/c
:rtype: float
""""""
return 0.0
",[],0,[],/probability.py_discount
293,/home/amandapotts/git/nltk/nltk/probability.py_generate,"def generate(self):
""""""
Return a randomly selected sample from this probability distribution.
The probability of returning each sample ``samp`` is equal to
``self.prob(samp)``.
""""""
p = random.random()
p_init = p
for sample in self.samples():
p -= self.prob(sample)
if p <= 0:
return sample
if p < 0.0001:
return sample
if self.SUM_TO_ONE:
warnings.warn(
""Probability distribution %r sums to %r
"" is returning an arbitrary sample."" % (self, p_init - p)
)
return random.choice(list(self.samples()))
",[],0,[],/probability.py_generate
294,/home/amandapotts/git/nltk/nltk/probability.py___init__,"def __init__(self, samples):
""""""
Construct a new uniform probability distribution, that assigns
equal probability to each sample in ``samples``.
:param samples: The samples that should be given uniform
probability.
:type samples: list
:raise ValueError: If ``samples`` is empty.
""""""
if len(samples) == 0:
raise ValueError(
""A Uniform probability distribution must "" + ""have at least one sample.""
)
self._sampleset = set(samples)
self._prob = 1.0 / len(self._sampleset)
self._samples = list(self._sampleset)
",[],0,[],/probability.py___init__
295,/home/amandapotts/git/nltk/nltk/probability.py_prob,"def prob(self, sample):
return self._prob if sample in self._sampleset else 0
",[],0,[],/probability.py_prob
296,/home/amandapotts/git/nltk/nltk/probability.py_max,"def max(self):
return self._samples[0]
",[],0,[],/probability.py_max
297,/home/amandapotts/git/nltk/nltk/probability.py_samples,"def samples(self):
return self._samples
",[],0,[],/probability.py_samples
298,/home/amandapotts/git/nltk/nltk/probability.py___repr__,"def __repr__(self):
return ""<UniformProbDist with %d samples>"" % len(self._sampleset)
",[],0,[],/probability.py___repr__
299,/home/amandapotts/git/nltk/nltk/probability.py___init__,"def __init__(self, samples):
if len(samples) == 0:
raise ValueError(
""A probability distribution must "" + ""have at least one sample.""
)
self._probs = self.unirand(samples)
self._samples = list(self._probs.keys())
",[],0,[],/probability.py___init__
300,/home/amandapotts/git/nltk/nltk/probability.py_unirand,"def unirand(cls, samples):
""""""
The key function that creates a randomized initial distribution
that still sums to 1. Set as a dictionary of prob values so that
it can still be passed to MutableProbDist and called with identical
syntax to UniformProbDist
""""""
samples = set(samples)
randrow = [random.random() for i in range(len(samples))]
total = sum(randrow)
for i, x in enumerate(randrow):
randrow[i] = x / total
total = sum(randrow)
if total != 1:
randrow[-1] -= total - 1
return {s: randrow[i] for i, s in enumerate(samples)}
",[],0,[],/probability.py_unirand
301,/home/amandapotts/git/nltk/nltk/probability.py_max,"def max(self):
if not hasattr(self, ""_max""):
self._max = max((p, v) for (v, p) in self._probs.items())[1]
return self._max
",[],0,[],/probability.py_max
302,/home/amandapotts/git/nltk/nltk/probability.py_prob,"def prob(self, sample):
return self._probs.get(sample, 0)
",[],0,[],/probability.py_prob
303,/home/amandapotts/git/nltk/nltk/probability.py_samples,"def samples(self):
return self._samples
",[],0,[],/probability.py_samples
304,/home/amandapotts/git/nltk/nltk/probability.py___repr__,"def __repr__(self):
return ""<RandomUniformProbDist with %d samples>"" % len(self._probs)
",[],0,[],/probability.py___repr__
305,/home/amandapotts/git/nltk/nltk/probability.py___init__,"def __init__(self, prob_dict=None, log=False, normalize=False):
""""""
Construct a new probability distribution from the given
dictionary, which maps values to probabilities (or to log
probabilities, if ``log`` is true).  If ``normalize`` is
true, then the probability values are scaled by a constant
factor such that they sum to 1.
If called without arguments, the resulting probability
distribution assigns zero probability to all values.
""""""
self._prob_dict = prob_dict.copy() if prob_dict is not None else {}
self._log = log
if normalize:
if len(prob_dict) == 0:
raise ValueError(
""A DictionaryProbDist must have at least one sample ""
+ ""before it can be normalized.""
)
if log:
value_sum = sum_logs(list(self._prob_dict.values()))
if value_sum <= _NINF:
logp = math.log(1.0 / len(prob_dict), 2)
for x in prob_dict:
self._prob_dict[x] = logp
else:
for x, p in self._prob_dict.items():
self._prob_dict[x] -= value_sum
else:
value_sum = sum(self._prob_dict.values())
if value_sum == 0:
p = 1.0 / len(prob_dict)
for x in prob_dict:
self._prob_dict[x] = p
else:
norm_factor = 1.0 / value_sum
for x, p in self._prob_dict.items():
self._prob_dict[x] *= norm_factor
",[],0,[],/probability.py___init__
306,/home/amandapotts/git/nltk/nltk/probability.py_prob,"def prob(self, sample):
if self._log:
return 2 ** (self._prob_dict[sample]) if sample in self._prob_dict else 0
else:
return self._prob_dict.get(sample, 0)
",[],0,[],/probability.py_prob
307,/home/amandapotts/git/nltk/nltk/probability.py_logprob,"def logprob(self, sample):
if self._log:
return self._prob_dict.get(sample, _NINF)
else:
if sample not in self._prob_dict:
return _NINF
elif self._prob_dict[sample] == 0:
return _NINF
else:
return math.log(self._prob_dict[sample], 2)
",[],0,[],/probability.py_logprob
308,/home/amandapotts/git/nltk/nltk/probability.py_max,"def max(self):
if not hasattr(self, ""_max""):
self._max = max((p, v) for (v, p) in self._prob_dict.items())[1]
return self._max
",[],0,[],/probability.py_max
309,/home/amandapotts/git/nltk/nltk/probability.py_samples,"def samples(self):
return self._prob_dict.keys()
",[],0,[],/probability.py_samples
310,/home/amandapotts/git/nltk/nltk/probability.py___repr__,"def __repr__(self):
return ""<ProbDist with %d samples>"" % len(self._prob_dict)
",[],0,[],/probability.py___repr__
311,/home/amandapotts/git/nltk/nltk/probability.py___init__,"def __init__(self, freqdist, bins=None):
""""""
Use the maximum likelihood estimate to create a probability
distribution for the experiment used to generate ``freqdist``.
:type freqdist: FreqDist
:param freqdist: The frequency distribution that the
probability estimates should be based on.
""""""
self._freqdist = freqdist
",[],0,[],/probability.py___init__
312,/home/amandapotts/git/nltk/nltk/probability.py_freqdist,"def freqdist(self):
""""""
Return the frequency distribution that this probability
distribution is based on.
:rtype: FreqDist
""""""
return self._freqdist
",[],0,[],/probability.py_freqdist
313,/home/amandapotts/git/nltk/nltk/probability.py_prob,"def prob(self, sample):
return self._freqdist.freq(sample)
",[],0,[],/probability.py_prob
314,/home/amandapotts/git/nltk/nltk/probability.py_max,"def max(self):
return self._freqdist.max()
",[],0,[],/probability.py_max
315,/home/amandapotts/git/nltk/nltk/probability.py_samples,"def samples(self):
return self._freqdist.keys()
",[],0,[],/probability.py_samples
316,/home/amandapotts/git/nltk/nltk/probability.py___repr__,"def __repr__(self):
""""""
:rtype: str
:return: A string representation of this ``ProbDist``.
""""""
return ""<MLEProbDist based on %d samples>"" % self._freqdist.N()
",[],0,[],/probability.py___repr__
317,/home/amandapotts/git/nltk/nltk/probability.py___init__,"def __init__(self, freqdist, gamma, bins=None):
""""""
Use the Lidstone estimate to create a probability distribution
for the experiment used to generate ``freqdist``.
:type freqdist: FreqDist
:param freqdist: The frequency distribution that the
probability estimates should be based on.
:type gamma: float
:param gamma: A real number used to parameterize the
estimate.  The Lidstone estimate is equivalent to adding
maximum likelihood estimate of the resulting frequency
distribution.
:type bins: int
:param bins: The number of sample values that can be generated
by the experiment that is described by the probability
distribution.  This value must be correctly set for the
probabilities of the sample values to sum to one.  If
``bins`` is not specified, it defaults to ``freqdist.B()``.
""""""
if (bins == 0) or (bins is None and freqdist.N() == 0):
name = self.__class__.__name__[:-8]
raise ValueError(
""A %s probability distribution "" % name + ""must have at least one bin.""
)
if (bins is not None) and (bins < freqdist.B()):
name = self.__class__.__name__[:-8]
raise ValueError(
""\nThe number of bins in a %s distribution "" % name
+ ""(%d) must be greater than or equal to\n"" % bins
+ ""the number of bins in the FreqDist used ""
+ ""to create it (%d)."" % freqdist.B()
)
self._freqdist = freqdist
self._gamma = float(gamma)
self._N = self._freqdist.N()
if bins is None:
bins = freqdist.B()
self._bins = bins
self._divisor = self._N + bins * gamma
if self._divisor == 0.0:
self._gamma = 0
self._divisor = 1
",[],0,[],/probability.py___init__
318,/home/amandapotts/git/nltk/nltk/probability.py_freqdist,"def freqdist(self):
""""""
Return the frequency distribution that this probability
distribution is based on.
:rtype: FreqDist
""""""
return self._freqdist
",[],0,[],/probability.py_freqdist
319,/home/amandapotts/git/nltk/nltk/probability.py_prob,"def prob(self, sample):
c = self._freqdist[sample]
return (c + self._gamma) / self._divisor
",[],0,[],/probability.py_prob
320,/home/amandapotts/git/nltk/nltk/probability.py_max,"def max(self):
return self._freqdist.max()
",[],0,[],/probability.py_max
321,/home/amandapotts/git/nltk/nltk/probability.py_samples,"def samples(self):
return self._freqdist.keys()
",[],0,[],/probability.py_samples
322,/home/amandapotts/git/nltk/nltk/probability.py_discount,"def discount(self):
gb = self._gamma * self._bins
return gb / (self._N + gb)
",[],0,[],/probability.py_discount
323,/home/amandapotts/git/nltk/nltk/probability.py___repr__,"def __repr__(self):
""""""
Return a string representation of this ``ProbDist``.
:rtype: str
""""""
return ""<LidstoneProbDist based on %d samples>"" % self._freqdist.N()
",[],0,[],/probability.py___repr__
324,/home/amandapotts/git/nltk/nltk/probability.py___init__,"def __init__(self, freqdist, bins=None):
""""""
Use the Laplace estimate to create a probability distribution
for the experiment used to generate ``freqdist``.
:type freqdist: FreqDist
:param freqdist: The frequency distribution that the
probability estimates should be based on.
:type bins: int
:param bins: The number of sample values that can be generated
by the experiment that is described by the probability
distribution.  This value must be correctly set for the
probabilities of the sample values to sum to one.  If
``bins`` is not specified, it defaults to ``freqdist.B()``.
""""""
LidstoneProbDist.__init__(self, freqdist, 1, bins)
",[],0,[],/probability.py___init__
325,/home/amandapotts/git/nltk/nltk/probability.py___repr__,"def __repr__(self):
""""""
:rtype: str
:return: A string representation of this ``ProbDist``.
""""""
return ""<LaplaceProbDist based on %d samples>"" % self._freqdist.N()
",[],0,[],/probability.py___repr__
326,/home/amandapotts/git/nltk/nltk/probability.py___init__,"def __init__(self, freqdist, bins=None):
""""""
Use the expected likelihood estimate to create a probability
distribution for the experiment used to generate ``freqdist``.
:type freqdist: FreqDist
:param freqdist: The frequency distribution that the
probability estimates should be based on.
:type bins: int
:param bins: The number of sample values that can be generated
by the experiment that is described by the probability
distribution.  This value must be correctly set for the
probabilities of the sample values to sum to one.  If
``bins`` is not specified, it defaults to ``freqdist.B()``.
""""""
LidstoneProbDist.__init__(self, freqdist, 0.5, bins)
",[],0,[],/probability.py___init__
327,/home/amandapotts/git/nltk/nltk/probability.py___repr__,"def __repr__(self):
""""""
Return a string representation of this ``ProbDist``.
:rtype: str
""""""
return ""<ELEProbDist based on %d samples>"" % self._freqdist.N()
",[],0,[],/probability.py___repr__
328,/home/amandapotts/git/nltk/nltk/probability.py___init__,"def __init__(self, base_fdist, heldout_fdist, bins=None):
""""""
Use the heldout estimate to create a probability distribution
for the experiment used to generate ``base_fdist`` and
``heldout_fdist``.
:type base_fdist: FreqDist
:param base_fdist: The base frequency distribution.
:type heldout_fdist: FreqDist
:param heldout_fdist: The heldout frequency distribution.
:type bins: int
:param bins: The number of sample values that can be generated
by the experiment that is described by the probability
distribution.  This value must be correctly set for the
probabilities of the sample values to sum to one.  If
``bins`` is not specified, it defaults to ``freqdist.B()``.
""""""
self._base_fdist = base_fdist
self._heldout_fdist = heldout_fdist
self._max_r = base_fdist[base_fdist.max()]
Tr = self._calculate_Tr()
r_Nr = base_fdist.r_Nr(bins)
Nr = [r_Nr[r] for r in range(self._max_r + 1)]
N = heldout_fdist.N()
self._estimate = self._calculate_estimate(Tr, Nr, N)
",[],0,[],/probability.py___init__
329,/home/amandapotts/git/nltk/nltk/probability.py__calculate_Tr,"def _calculate_Tr(self):
""""""
Return the list *Tr*, where *Tr[r]* is the total count in
``heldout_fdist`` for all samples that occur *r*
times in ``base_fdist``.
:rtype: list(float)
""""""
Tr = [0.0] * (self._max_r + 1)
for sample in self._heldout_fdist:
r = self._base_fdist[sample]
Tr[r] += self._heldout_fdist[sample]
return Tr
",[],0,[],/probability.py__calculate_Tr
330,/home/amandapotts/git/nltk/nltk/probability.py__calculate_estimate,"def _calculate_estimate(self, Tr, Nr, N):
""""""
Return the list *estimate*, where *estimate[r]* is the probability
estimate for any sample that occurs *r* times in the base frequency
distribution.  In particular, *estimate[r]* is *Tr[r]/(N[r].N)*.
In the special case that *N[r]=0*, *estimate[r]* will never be used
so we define *estimate[r]=None* for those cases.
:rtype: list(float)
:type Tr: list(float)
:param Tr: the list *Tr*, where *Tr[r]* is the total count in
the heldout distribution for all samples that occur *r*
times in base distribution.
:type Nr: list(float)
:param Nr: The list *Nr*, where *Nr[r]* is the number of
samples that occur *r* times in the base distribution.
:type N: int
:param N: The total number of outcomes recorded by the heldout
frequency distribution.
""""""
estimate = []
for r in range(self._max_r + 1):
if Nr[r] == 0:
estimate.append(None)
else:
estimate.append(Tr[r] / (Nr[r] * N))
return estimate
",[],0,[],/probability.py__calculate_estimate
331,/home/amandapotts/git/nltk/nltk/probability.py_base_fdist,"def base_fdist(self):
""""""
Return the base frequency distribution that this probability
distribution is based on.
:rtype: FreqDist
""""""
return self._base_fdist
",[],0,[],/probability.py_base_fdist
332,/home/amandapotts/git/nltk/nltk/probability.py_heldout_fdist,"def heldout_fdist(self):
""""""
Return the heldout frequency distribution that this
probability distribution is based on.
:rtype: FreqDist
""""""
return self._heldout_fdist
",[],0,[],/probability.py_heldout_fdist
333,/home/amandapotts/git/nltk/nltk/probability.py_samples,"def samples(self):
return self._base_fdist.keys()
",[],0,[],/probability.py_samples
334,/home/amandapotts/git/nltk/nltk/probability.py_prob,"def prob(self, sample):
r = self._base_fdist[sample]
return self._estimate[r]
",[],0,[],/probability.py_prob
335,/home/amandapotts/git/nltk/nltk/probability.py_max,"def max(self):
return self._base_fdist.max()
",[],0,[],/probability.py_max
336,/home/amandapotts/git/nltk/nltk/probability.py_discount,"def discount(self):
raise NotImplementedError()
",[],0,[],/probability.py_discount
337,/home/amandapotts/git/nltk/nltk/probability.py___repr__,"def __repr__(self):
""""""
:rtype: str
:return: A string representation of this ``ProbDist``.
""""""
s = ""<HeldoutProbDist: %d base samples
return s % (self._base_fdist.N(), self._heldout_fdist.N())
",[],0,[],/probability.py___repr__
338,/home/amandapotts/git/nltk/nltk/probability.py___init__,"def __init__(self, freqdists, bins):
""""""
Use the cross-validation estimate to create a probability
distribution for the experiment used to generate
``freqdists``.
:type freqdists: list(FreqDist)
:param freqdists: A list of the frequency distributions
generated by the experiment.
:type bins: int
:param bins: The number of sample values that can be generated
by the experiment that is described by the probability
distribution.  This value must be correctly set for the
probabilities of the sample values to sum to one.  If
``bins`` is not specified, it defaults to ``freqdist.B()``.
""""""
self._freqdists = freqdists
self._heldout_probdists = []
for fdist1 in freqdists:
for fdist2 in freqdists:
if fdist1 is not fdist2:
probdist = HeldoutProbDist(fdist1, fdist2, bins)
self._heldout_probdists.append(probdist)
",[],0,[],/probability.py___init__
339,/home/amandapotts/git/nltk/nltk/probability.py_freqdists,"def freqdists(self):
""""""
Return the list of frequency distributions that this ``ProbDist`` is based on.
:rtype: list(FreqDist)
""""""
return self._freqdists
",[],0,[],/probability.py_freqdists
340,/home/amandapotts/git/nltk/nltk/probability.py_samples,"def samples(self):
return set(sum((list(fd) for fd in self._freqdists), []))
",[],0,[],/probability.py_samples
341,/home/amandapotts/git/nltk/nltk/probability.py_prob,"def prob(self, sample):
prob = 0.0
for heldout_probdist in self._heldout_probdists:
prob += heldout_probdist.prob(sample)
return prob / len(self._heldout_probdists)
",[],0,[],/probability.py_prob
342,/home/amandapotts/git/nltk/nltk/probability.py_discount,"def discount(self):
raise NotImplementedError()
",[],0,[],/probability.py_discount
343,/home/amandapotts/git/nltk/nltk/probability.py___repr__,"def __repr__(self):
""""""
Return a string representation of this ``ProbDist``.
:rtype: str
""""""
return ""<CrossValidationProbDist: %d-way>"" % len(self._freqdists)
",[],0,[],/probability.py___repr__
344,/home/amandapotts/git/nltk/nltk/probability.py___init__,"def __init__(self, freqdist, bins=None):
""""""
Creates a distribution of Witten-Bell probability estimates.  This
distribution allocates uniform probability mass to as yet unseen
events by using the number of events that have only been seen once. The
probability mass reserved for unseen events is equal to *T / (N + T)*
where *T* is the number of observed event types and *N* is the total
number of observed events. This equates to the maximum likelihood
estimate of a new type event occurring. The remaining probability mass
is discounted such that all probability estimates sum to one,
yielding:
- *p = T / Z (N + T)*, if count = 0
- *p = c / (N + T)*, otherwise
The parameters *T* and *N* are taken from the ``freqdist`` parameter
(the ``B()`` and ``N()`` values). The normalizing factor *Z* is
calculated using these values along with the ``bins`` parameter.
:param freqdist: The frequency counts upon which to base the
estimation.
:type freqdist: FreqDist
:param bins: The number of possible event types. This must be at least
as large as the number of bins in the ``freqdist``. If None, then
it's assumed to be equal to that of the ``freqdist``
:type bins: int
""""""
assert bins is None or bins >= freqdist.B(), (
""bins parameter must not be less than %d=freqdist.B()"" % freqdist.B()
)
if bins is None:
bins = freqdist.B()
self._freqdist = freqdist
self._T = self._freqdist.B()
self._Z = bins - self._freqdist.B()
self._N = self._freqdist.N()
if self._N == 0:
self._P0 = 1.0 / self._Z
else:
self._P0 = self._T / (self._Z * (self._N + self._T))
",[],0,[],/probability.py___init__
345,/home/amandapotts/git/nltk/nltk/probability.py_prob,"def prob(self, sample):
c = self._freqdist[sample]
return c / (self._N + self._T) if c != 0 else self._P0
",[],0,[],/probability.py_prob
346,/home/amandapotts/git/nltk/nltk/probability.py_max,"def max(self):
return self._freqdist.max()
",[],0,[],/probability.py_max
347,/home/amandapotts/git/nltk/nltk/probability.py_samples,"def samples(self):
return self._freqdist.keys()
",[],0,[],/probability.py_samples
348,/home/amandapotts/git/nltk/nltk/probability.py_freqdist,"def freqdist(self):
return self._freqdist
",[],0,[],/probability.py_freqdist
349,/home/amandapotts/git/nltk/nltk/probability.py_discount,"def discount(self):
raise NotImplementedError()
",[],0,[],/probability.py_discount
350,/home/amandapotts/git/nltk/nltk/probability.py___repr__,"def __repr__(self):
""""""
Return a string representation of this ``ProbDist``.
:rtype: str
""""""
return ""<WittenBellProbDist based on %d samples>"" % self._freqdist.N()
",[],0,[],/probability.py___repr__
351,/home/amandapotts/git/nltk/nltk/probability.py___init__,"def __init__(self, freqdist, bins=None):
""""""
:param freqdist: The frequency counts upon which to base the
estimation.
:type freqdist: FreqDist
:param bins: The number of possible event types. This must be
larger than the number of bins in the ``freqdist``. If None,
then it's assumed to be equal to ``freqdist``.B() + 1
:type bins: int
""""""
assert (
bins is None or bins > freqdist.B()
), ""bins parameter must not be less than %d=freqdist.B()+1"" % (freqdist.B() + 1)
if bins is None:
bins = freqdist.B() + 1
self._freqdist = freqdist
self._bins = bins
r, nr = self._r_Nr()
self.find_best_fit(r, nr)
self._switch(r, nr)
self._renormalize(r, nr)
",[],0,[],/probability.py___init__
352,/home/amandapotts/git/nltk/nltk/probability.py__r_Nr_non_zero,"def _r_Nr_non_zero(self):
r_Nr = self._freqdist.r_Nr()
del r_Nr[0]
return r_Nr
",[],0,[],/probability.py__r_Nr_non_zero
353,/home/amandapotts/git/nltk/nltk/probability.py__r_Nr,"def _r_Nr(self):
""""""
Split the frequency distribution in two list (r, Nr), where Nr(r) > 0
""""""
nonzero = self._r_Nr_non_zero()
if not nonzero:
return [], []
return zip(*sorted(nonzero.items()))
",[],0,[],/probability.py__r_Nr
354,/home/amandapotts/git/nltk/nltk/probability.py_find_best_fit,"def find_best_fit(self, r, nr):
""""""
Use simple linear regression to tune parameters self._slope and
self._intercept in the log-log space based on count and Nr(count)
(Work in log space to avoid floating point underflow.)
""""""
if not r or not nr:
return
zr = []
for j in range(len(r)):
i = r[j - 1] if j > 0 else 0
k = 2 * r[j] - i if j == len(r) - 1 else r[j + 1]
zr_ = 2.0 * nr[j] / (k - i)
zr.append(zr_)
log_r = [math.log(i) for i in r]
log_zr = [math.log(i) for i in zr]
xy_cov = x_var = 0.0
x_mean = sum(log_r) / len(log_r)
y_mean = sum(log_zr) / len(log_zr)
for x, y in zip(log_r, log_zr):
xy_cov += (x - x_mean) * (y - y_mean)
x_var += (x - x_mean) ** 2
self._slope = xy_cov / x_var if x_var != 0 else 0.0
if self._slope >= -1:
warnings.warn(
""SimpleGoodTuring did not find a proper best fit ""
""line for smoothing probabilities of occurrences. ""
""The probability estimates are likely to be ""
""unreliable.""
)
self._intercept = y_mean - self._slope * x_mean
",[],0,[],/probability.py_find_best_fit
355,/home/amandapotts/git/nltk/nltk/probability.py__switch,"def _switch(self, r, nr):
""""""
Calculate the r frontier where we must switch from Nr to Sr
when estimating E[Nr].
""""""
for i, r_ in enumerate(r):
if len(r) == i + 1 or r[i + 1] != r_ + 1:
self._switch_at = r_
break
Sr = self.smoothedNr
smooth_r_star = (r_ + 1) * Sr(r_ + 1) / Sr(r_)
unsmooth_r_star = (r_ + 1) * nr[i + 1] / nr[i]
std = math.sqrt(self._variance(r_, nr[i], nr[i + 1]))
if abs(unsmooth_r_star - smooth_r_star) <= 1.96 * std:
self._switch_at = r_
break
",[],0,[],/probability.py__switch
356,/home/amandapotts/git/nltk/nltk/probability.py__variance,"def _variance(self, r, nr, nr_1):
r = float(r)
nr = float(nr)
nr_1 = float(nr_1)
return (r + 1.0) ** 2 * (nr_1 / nr**2) * (1.0 + nr_1 / nr)
",[],0,[],/probability.py__variance
357,/home/amandapotts/git/nltk/nltk/probability.py__renormalize,"def _renormalize(self, r, nr):
""""""
It is necessary to renormalize all the probability estimates to
ensure a proper probability distribution results. This can be done
by keeping the estimate of the probability mass for unseen items as
N(1)/N and renormalizing all the estimates for previously seen items
(as Gale and Sampson (1995) propose). (See M&S P.213, 1999)
""""""
prob_cov = 0.0
for r_, nr_ in zip(r, nr):
prob_cov += nr_ * self._prob_measure(r_)
if prob_cov:
self._renormal = (1 - self._prob_measure(0)) / prob_cov
",[],0,[],/probability.py__renormalize
358,/home/amandapotts/git/nltk/nltk/probability.py_smoothedNr,"def smoothedNr(self, r):
""""""
Return the number of samples with count r.
:param r: The amount of frequency.
:type r: int
:rtype: float
""""""
return math.exp(self._intercept + self._slope * math.log(r))
",[],0,[],/probability.py_smoothedNr
359,/home/amandapotts/git/nltk/nltk/probability.py_prob,"def prob(self, sample):
""""""
Return the sample's probability.
:param sample: sample of the event
:type sample: str
:rtype: float
""""""
count = self._freqdist[sample]
p = self._prob_measure(count)
if count == 0:
if self._bins == self._freqdist.B():
p = 0.0
else:
p = p / (self._bins - self._freqdist.B())
else:
p = p * self._renormal
return p
",[],0,[],/probability.py_prob
360,/home/amandapotts/git/nltk/nltk/probability.py__prob_measure,"def _prob_measure(self, count):
if count == 0 and self._freqdist.N() == 0:
return 1.0
elif count == 0 and self._freqdist.N() != 0:
return self._freqdist.Nr(1) / self._freqdist.N()
if self._switch_at > count:
Er_1 = self._freqdist.Nr(count + 1)
Er = self._freqdist.Nr(count)
else:
Er_1 = self.smoothedNr(count + 1)
Er = self.smoothedNr(count)
r_star = (count + 1) * Er_1 / Er
return r_star / self._freqdist.N()
",[],0,[],/probability.py__prob_measure
361,/home/amandapotts/git/nltk/nltk/probability.py_check,"def check(self):
prob_sum = 0.0
for i in range(0, len(self._Nr)):
prob_sum += self._Nr[i] * self._prob_measure(i) / self._renormal
print(""Probability Sum:"", prob_sum)
",[],0,[],/probability.py_check
362,/home/amandapotts/git/nltk/nltk/probability.py_discount,"def discount(self):
""""""
This function returns the total mass of probability transfers from the
seen samples to the unseen samples.
""""""
return self.smoothedNr(1) / self._freqdist.N()
",[],0,[],/probability.py_discount
363,/home/amandapotts/git/nltk/nltk/probability.py_max,"def max(self):
return self._freqdist.max()
",[],0,[],/probability.py_max
364,/home/amandapotts/git/nltk/nltk/probability.py_samples,"def samples(self):
return self._freqdist.keys()
",[],0,[],/probability.py_samples
365,/home/amandapotts/git/nltk/nltk/probability.py_freqdist,"def freqdist(self):
return self._freqdist
",[],0,[],/probability.py_freqdist
366,/home/amandapotts/git/nltk/nltk/probability.py___repr__,"def __repr__(self):
""""""
Return a string representation of this ``ProbDist``.
:rtype: str
""""""
return ""<SimpleGoodTuringProbDist based on %d samples>"" % self._freqdist.N()
",[],0,[],/probability.py___repr__
367,/home/amandapotts/git/nltk/nltk/probability.py___init__,"def __init__(self, prob_dist, samples, store_logs=True):
""""""
Creates the mutable probdist based on the given prob_dist and using
the list of samples given. These values are stored as log
probabilities if the store_logs flag is set.
:param prob_dist: the distribution from which to garner the
probabilities
:type prob_dist: ProbDist
:param samples: the complete set of samples
:type samples: sequence of any
:param store_logs: whether to store the probabilities as logarithms
:type store_logs: bool
""""""
self._samples = samples
self._sample_dict = {samples[i]: i for i in range(len(samples))}
self._data = array.array(""d"", [0.0]) * len(samples)
for i in range(len(samples)):
if store_logs:
self._data[i] = prob_dist.logprob(samples[i])
else:
self._data[i] = prob_dist.prob(samples[i])
self._logs = store_logs
",[],0,[],/probability.py___init__
368,/home/amandapotts/git/nltk/nltk/probability.py_max,"def max(self):
return max((p, v) for (v, p) in self._sample_dict.items())[1]
",[],0,[],/probability.py_max
369,/home/amandapotts/git/nltk/nltk/probability.py_samples,"def samples(self):
return self._samples
",[],0,[],/probability.py_samples
370,/home/amandapotts/git/nltk/nltk/probability.py_prob,"def prob(self, sample):
i = self._sample_dict.get(sample)
if i is None:
return 0.0
return 2 ** (self._data[i]) if self._logs else self._data[i]
",[],0,[],/probability.py_prob
371,/home/amandapotts/git/nltk/nltk/probability.py_logprob,"def logprob(self, sample):
i = self._sample_dict.get(sample)
if i is None:
return float(""-inf"")
return self._data[i] if self._logs else math.log(self._data[i], 2)
",[],0,[],/probability.py_logprob
372,/home/amandapotts/git/nltk/nltk/probability.py_update,"def update(self, sample, prob, log=True):
""""""
Update the probability for the given sample. This may cause the object
to stop being the valid probability distribution - the user must
ensure that they update the sample probabilities such that all samples
have probabilities between 0 and 1 and that all probabilities sum to
one.
:param sample: the sample for which to update the probability
:type sample: any
:param prob: the new probability
:type prob: float
:param log: is the probability already logged
:type log: bool
""""""
i = self._sample_dict.get(sample)
assert i is not None
if self._logs:
self._data[i] = prob if log else math.log(prob, 2)
else:
self._data[i] = 2 ** (prob) if log else prob
",[],0,[],/probability.py_update
373,/home/amandapotts/git/nltk/nltk/probability.py___init__,"def __init__(self, freqdist, bins=None, discount=0.75):
""""""
:param freqdist: The trigram frequency distribution upon which to base
the estimation
:type freqdist: FreqDist
:param bins: Included for compatibility with nltk.tag.hmm
:type bins: int or float
:param discount: The discount applied when retrieving counts of
trigrams
:type discount: float (preferred, but can be set to int)
""""""
if not bins:
self._bins = freqdist.B()
else:
self._bins = bins
self._D = discount
self._cache = {}
self._bigrams = defaultdict(int)
self._trigrams = freqdist
self._wordtypes_after = defaultdict(float)
self._trigrams_contain = defaultdict(float)
self._wordtypes_before = defaultdict(float)
for w0, w1, w2 in freqdist:
self._bigrams[(w0, w1)] += freqdist[(w0, w1, w2)]
self._wordtypes_after[(w0, w1)] += 1
self._trigrams_contain[w1] += 1
self._wordtypes_before[(w1, w2)] += 1
",[],0,[],/probability.py___init__
374,/home/amandapotts/git/nltk/nltk/probability.py_prob,"def prob(self, trigram):
if len(trigram) != 3:
raise ValueError(""Expected an iterable with 3 members."")
trigram = tuple(trigram)
w0, w1, w2 = trigram
if trigram in self._cache:
return self._cache[trigram]
else:
if trigram in self._trigrams:
prob = (self._trigrams[trigram] - self.discount()) / self._bigrams[
(w0, w1)
]
elif (w0, w1) in self._bigrams and (w1, w2) in self._wordtypes_before:
aftr = self._wordtypes_after[(w0, w1)]
bfr = self._wordtypes_before[(w1, w2)]
leftover_prob = (aftr * self.discount()) / self._bigrams[(w0, w1)]
beta = bfr / (self._trigrams_contain[w1] - aftr)
prob = leftover_prob * beta
else:
prob = 0.0
self._cache[trigram] = prob
return prob
",[],0,[],/probability.py_prob
375,/home/amandapotts/git/nltk/nltk/probability.py_discount,"def discount(self):
""""""
Return the value by which counts are discounted. By default set to 0.75.
:rtype: float
""""""
return self._D
",[],0,[],/probability.py_discount
376,/home/amandapotts/git/nltk/nltk/probability.py_set_discount,"def set_discount(self, discount):
""""""
Set the value by which counts are discounted to the value of discount.
:param discount: the new value to discount counts by
:type discount: float (preferred, but int possible)
:rtype: None
""""""
self._D = discount
",[],0,[],/probability.py_set_discount
377,/home/amandapotts/git/nltk/nltk/probability.py_samples,"def samples(self):
return self._trigrams.keys()
",[],0,[],/probability.py_samples
378,/home/amandapotts/git/nltk/nltk/probability.py_max,"def max(self):
return self._trigrams.max()
",[],0,[],/probability.py_max
379,/home/amandapotts/git/nltk/nltk/probability.py___repr__,"def __repr__(self):
""""""
Return a string representation of this ProbDist
:rtype: str
""""""
return f""<KneserNeyProbDist based on {self._trigrams.N()} trigrams""
",[],0,[],/probability.py___repr__
380,/home/amandapotts/git/nltk/nltk/probability.py_log_likelihood,"def log_likelihood(test_pdist, actual_pdist):
if not isinstance(test_pdist, ProbDistI) or not isinstance(actual_pdist, ProbDistI):
raise ValueError(""expected a ProbDist."")
return sum(
actual_pdist.prob(s) * math.log(test_pdist.prob(s), 2) for s in actual_pdist
)
",[],0,[],/probability.py_log_likelihood
381,/home/amandapotts/git/nltk/nltk/probability.py_entropy,"def entropy(pdist):
probs = (pdist.prob(s) for s in pdist.samples())
return -sum(p * math.log(p, 2) for p in probs)
",[],0,[],/probability.py_entropy
382,/home/amandapotts/git/nltk/nltk/probability.py___init__,"def __init__(self, cond_samples=None):
""""""
Construct a new empty conditional frequency distribution.  In
particular, the count for every sample, under every condition,
is zero.
:param cond_samples: The samples to initialize the conditional
frequency distribution with
:type cond_samples: Sequence of (condition, sample) tuples
""""""
defaultdict.__init__(self, FreqDist)
if cond_samples:
for cond, sample in cond_samples:
self[cond][sample] += 1
",[],0,[],/probability.py___init__
383,/home/amandapotts/git/nltk/nltk/probability.py___reduce__,"def __reduce__(self):
kv_pairs = ((cond, self[cond]) for cond in self.conditions())
return (self.__class__, (), None, None, kv_pairs)
",[],0,[],/probability.py___reduce__
384,/home/amandapotts/git/nltk/nltk/probability.py_conditions,"def conditions(self):
""""""
Return a list of the conditions that have been accessed for
this ``ConditionalFreqDist``.  Use the indexing operator to
access the frequency distribution for a given condition.
Note that the frequency distributions for some conditions
may contain zero sample outcomes.
:rtype: list
""""""
return list(self.keys())
",[],0,[],/probability.py_conditions
385,/home/amandapotts/git/nltk/nltk/probability.py_N,"def N(self):
""""""
Return the total number of sample outcomes that have been
recorded by this ``ConditionalFreqDist``.
:rtype: int
""""""
return sum(fdist.N() for fdist in self.values())
",[],0,[],/probability.py_N
386,/home/amandapotts/git/nltk/nltk/probability.py_plot,"def plot(
self,
samples=None,
title="""",
cumulative=False,
percents=False,
conditions=None,
show=False,
",[],0,[],/probability.py_plot
387,/home/amandapotts/git/nltk/nltk/probability.py_tabulate,"def tabulate(self, *args, **kwargs):
""""""
Tabulate the given samples from the conditional frequency distribution.
:param samples: The samples to plot
:type samples: list
:param conditions: The conditions to plot (default is all)
:type conditions: list
:param cumulative: A flag to specify whether the freqs are cumulative (default = False)
:type title: bool
""""""
cumulative = _get_kwarg(kwargs, ""cumulative"", False)
conditions = _get_kwarg(kwargs, ""conditions"", sorted(self.conditions()))
samples = _get_kwarg(
kwargs,
""samples"",
sorted({v for c in conditions if c in self for v in self[c]}),
)  # this computation could be wasted
width = max(len(""%s"" % s) for s in samples)
freqs = dict()
for c in conditions:
if cumulative:
freqs[c] = list(self[c]._cumulative_frequencies(samples))
else:
freqs[c] = [self[c][sample] for sample in samples]
width = max(width, max(len(""%d"" % f) for f in freqs[c]))
condition_size = max(len(""%s"" % c) for c in conditions)
print("" "" * condition_size, end="" "")
for s in samples:
print(""%*s"" % (width, s), end="" "")
print()
for c in conditions:
print(""%*s"" % (condition_size, c), end="" "")
for f in freqs[c]:
print(""%*d"" % (width, f), end="" "")
print()
",[],0,[],/probability.py_tabulate
388,/home/amandapotts/git/nltk/nltk/probability.py___add__,"def __add__(self, other):
""""""
Add counts from two ConditionalFreqDists.
""""""
if not isinstance(other, ConditionalFreqDist):
return NotImplemented
result = self.copy()
for cond in other.conditions():
result[cond] += other[cond]
return result
",[],0,[],/probability.py___add__
389,/home/amandapotts/git/nltk/nltk/probability.py___sub__,"def __sub__(self, other):
""""""
Subtract count, but keep only results with positive counts.
""""""
if not isinstance(other, ConditionalFreqDist):
return NotImplemented
result = self.copy()
for cond in other.conditions():
result[cond] -= other[cond]
if not result[cond]:
del result[cond]
return result
",[],0,[],/probability.py___sub__
390,/home/amandapotts/git/nltk/nltk/probability.py___or__,"def __or__(self, other):
""""""
Union is the maximum of value in either of the input counters.
""""""
if not isinstance(other, ConditionalFreqDist):
return NotImplemented
result = self.copy()
for cond in other.conditions():
result[cond] |= other[cond]
return result
",[],0,[],/probability.py___or__
391,/home/amandapotts/git/nltk/nltk/probability.py___and__,"def __and__(self, other):
""""""
Intersection is the minimum of corresponding counts.
""""""
if not isinstance(other, ConditionalFreqDist):
return NotImplemented
result = ConditionalFreqDist()
for cond in self.conditions():
newfreqdist = self[cond] & other[cond]
if newfreqdist:
result[cond] = newfreqdist
return result
",[],0,[],/probability.py___and__
392,/home/amandapotts/git/nltk/nltk/probability.py___le__,"def __le__(self, other):
if not isinstance(other, ConditionalFreqDist):
raise_unorderable_types(""<="", self, other)
return set(self.conditions()).issubset(other.conditions()) and all(
self[c] <= other[c] for c in self.conditions()
)
",[],0,[],/probability.py___le__
393,/home/amandapotts/git/nltk/nltk/probability.py___lt__,"def __lt__(self, other):
if not isinstance(other, ConditionalFreqDist):
raise_unorderable_types(""<"", self, other)
return self <= other and self != other
",[],0,[],/probability.py___lt__
394,/home/amandapotts/git/nltk/nltk/probability.py___ge__,"def __ge__(self, other):
if not isinstance(other, ConditionalFreqDist):
raise_unorderable_types("">="", self, other)
return other <= self
",[],0,[],/probability.py___ge__
395,/home/amandapotts/git/nltk/nltk/probability.py___gt__,"def __gt__(self, other):
if not isinstance(other, ConditionalFreqDist):
raise_unorderable_types("">"", self, other)
return other < self
",[],0,[],/probability.py___gt__
396,/home/amandapotts/git/nltk/nltk/probability.py_deepcopy,"def deepcopy(self):
from copy import deepcopy
return deepcopy(self)
",[],0,[],/probability.py_deepcopy
397,/home/amandapotts/git/nltk/nltk/probability.py___repr__,"def __repr__(self):
""""""
Return a string representation of this ``ConditionalFreqDist``.
:rtype: str
""""""
return ""<ConditionalFreqDist with %d conditions>"" % len(self)
",[],0,[],/probability.py___repr__
398,/home/amandapotts/git/nltk/nltk/probability.py___init__,"def __init__(self):
""""""
Classes inheriting from ConditionalProbDistI should implement __init__.
""""""
",[],0,[],/probability.py___init__
399,/home/amandapotts/git/nltk/nltk/probability.py_conditions,"def conditions(self):
""""""
Return a list of the conditions that are represented by
this ``ConditionalProbDist``.  Use the indexing operator to
access the probability distribution for a given condition.
:rtype: list
""""""
return list(self.keys())
",[],0,[],/probability.py_conditions
400,/home/amandapotts/git/nltk/nltk/probability.py___repr__,"def __repr__(self):
""""""
Return a string representation of this ``ConditionalProbDist``.
:rtype: str
""""""
return ""<%s with %d conditions>"" % (type(self).__name__, len(self))
",[],0,[],/probability.py___repr__
401,/home/amandapotts/git/nltk/nltk/probability.py___init__,"def __init__(self, cfdist, probdist_factory, *factory_args, **factory_kw_args):
""""""
Construct a new conditional probability distribution, based on
the given conditional frequency distribution and ``ProbDist``
factory.
:type cfdist: ConditionalFreqDist
:param cfdist: The ``ConditionalFreqDist`` specifying the
frequency distribution for each condition.
:type probdist_factory: class or function
:param probdist_factory: The function or class that maps
a condition's frequency distribution to its probability
distribution.  The function is called with the frequency
distribution as its first argument,
``factory_args`` as its remaining arguments, and
``factory_kw_args`` as keyword arguments.
:type factory_args: (any)
:param factory_args: Extra arguments for ``probdist_factory``.
These arguments are usually used to specify extra
properties for the probability distributions of individual
conditions, such as the number of bins they contain.
:type factory_kw_args: (any)
:param factory_kw_args: Extra keyword arguments for ``probdist_factory``.
""""""
self._probdist_factory = probdist_factory
self._factory_args = factory_args
self._factory_kw_args = factory_kw_args
for condition in cfdist:
self[condition] = probdist_factory(
cfdist[condition], *factory_args, **factory_kw_args
)
",[],0,[],/probability.py___init__
402,/home/amandapotts/git/nltk/nltk/probability.py___missing__,"def __missing__(self, key):
self[key] = self._probdist_factory(
FreqDist(), *self._factory_args, **self._factory_kw_args
)
return self[key]
",[],0,[],/probability.py___missing__
403,/home/amandapotts/git/nltk/nltk/probability.py___init__,"def __init__(self, probdist_dict):
""""""
:param probdist_dict: a dictionary containing the probdists indexed
by the conditions
:type probdist_dict: dict any -> probdist
""""""
self.update(probdist_dict)
",[],0,[],/probability.py___init__
404,/home/amandapotts/git/nltk/nltk/probability.py___missing__,"def __missing__(self, key):
self[key] = DictionaryProbDist()
return self[key]
",[],0,[],/probability.py___missing__
405,/home/amandapotts/git/nltk/nltk/probability.py_add_logs,"def add_logs(logx, logy):
""""""
Given two numbers ``logx`` = *log(x)* and ``logy`` = *log(y)*, return
``log(2**(logx)+2**(logy))``, but the actual implementation
avoids overflow errors that could result from direct computation.
""""""
if logx < logy + _ADD_LOGS_MAX_DIFF:
return logy
if logy < logx + _ADD_LOGS_MAX_DIFF:
return logx
base = min(logx, logy)
return base + math.log(2 ** (logx - base) + 2 ** (logy - base), 2)
",[],0,[],/probability.py_add_logs
406,/home/amandapotts/git/nltk/nltk/probability.py_sum_logs,"def sum_logs(logs):
return reduce(add_logs, logs[1:], logs[0]) if len(logs) != 0 else _NINF
",[],0,[],/probability.py_sum_logs
407,/home/amandapotts/git/nltk/nltk/probability.py___init__,"def __init__(self, **kwargs):
""""""
Initialize this object's probability.  This initializer should
be called by subclass constructors.  ``prob`` should generally be
the first argument for those constructors.
:param prob: The probability associated with the object.
:type prob: float
:param logprob: The log of the probability associated with
the object.
:type logprob: float
""""""
if ""prob"" in kwargs:
if ""logprob"" in kwargs:
raise TypeError(""Must specify either prob or logprob "" ""(not both)"")
else:
ProbabilisticMixIn.set_prob(self, kwargs[""prob""])
elif ""logprob"" in kwargs:
ProbabilisticMixIn.set_logprob(self, kwargs[""logprob""])
else:
self.__prob = self.__logprob = None
",[],0,[],/probability.py___init__
408,/home/amandapotts/git/nltk/nltk/probability.py_set_prob,"def set_prob(self, prob):
""""""
Set the probability associated with this object to ``prob``.
:param prob: The new probability
:type prob: float
""""""
self.__prob = prob
self.__logprob = None
",[],0,[],/probability.py_set_prob
409,/home/amandapotts/git/nltk/nltk/probability.py_set_logprob,"def set_logprob(self, logprob):
""""""
Set the log probability associated with this object to
``logprob``.  I.e., set the probability associated with this
object to ``2**(logprob)``.
:param logprob: The new log probability
:type logprob: float
""""""
self.__logprob = logprob
self.__prob = None
",[],0,[],/probability.py_set_logprob
410,/home/amandapotts/git/nltk/nltk/probability.py_prob,"def prob(self):
""""""
Return the probability associated with this object.
:rtype: float
""""""
if self.__prob is None:
if self.__logprob is None:
return None
self.__prob = 2 ** (self.__logprob)
return self.__prob
",[],0,[],/probability.py_prob
411,/home/amandapotts/git/nltk/nltk/probability.py_logprob,"def logprob(self):
""""""
Return ``log(p)``, where ``p`` is the probability associated
with this object.
:rtype: float
""""""
if self.__logprob is None:
if self.__prob is None:
return None
self.__logprob = math.log(self.__prob, 2)
return self.__logprob
",[],0,[],/probability.py_logprob
412,/home/amandapotts/git/nltk/nltk/probability.py_set_prob,"def set_prob(self, prob):
raise ValueError(""%s is immutable"" % self.__class__.__name__)
",[],0,[],/probability.py_set_prob
413,/home/amandapotts/git/nltk/nltk/probability.py_set_logprob,"def set_logprob(self, prob):
raise ValueError(""%s is immutable"" % self.__class__.__name__)
",[],0,[],/probability.py_set_logprob
414,/home/amandapotts/git/nltk/nltk/probability.py__get_kwarg,"def _get_kwarg(kwargs, key, default):
if key in kwargs:
arg = kwargs[key]
del kwargs[key]
else:
arg = default
return arg
",[],0,[],/probability.py__get_kwarg
415,/home/amandapotts/git/nltk/nltk/probability.py__create_rand_fdist,"def _create_rand_fdist(numsamples, numoutcomes):
""""""
Create a new frequency distribution, with random samples.  The
samples are numbers from 1 to ``numsamples``, and are generated by
summing two numbers, each of which has a uniform distribution.
""""""
fdist = FreqDist()
for x in range(numoutcomes):
y = random.randint(1, (1 + numsamples) // 2) + random.randint(
0, numsamples // 2
)
fdist[y] += 1
return fdist
",[],0,[],/probability.py__create_rand_fdist
416,/home/amandapotts/git/nltk/nltk/probability.py__create_sum_pdist,"def _create_sum_pdist(numsamples):
""""""
Return the true probability distribution for the experiment
``_create_rand_fdist(numsamples, x)``.
""""""
fdist = FreqDist()
for x in range(1, (1 + numsamples) // 2 + 1):
for y in range(0, numsamples // 2 + 1):
fdist[x + y] += 1
return MLEProbDist(fdist)
",[],0,[],/probability.py__create_sum_pdist
417,/home/amandapotts/git/nltk/nltk/probability.py_demo,"def demo(numsamples=6, numoutcomes=500):
""""""
A demonstration of frequency distributions and probability
distributions.  This demonstration creates three frequency
distributions with, and uses them to sample a random process with
``numsamples`` samples.  Each frequency distribution is sampled
``numoutcomes`` times.  These three frequency distributions are
then used to build six probability distributions.  Finally, the
probability estimates of these distributions are compared to the
actual probability of each sample.
:type numsamples: int
:param numsamples: The number of samples to use in each demo
frequency distributions.
:type numoutcomes: int
:param numoutcomes: The total number of outcomes for each
demo frequency distribution.  These outcomes are divided into
``numsamples`` bins.
:rtype: None
""""""
fdist1 = _create_rand_fdist(numsamples, numoutcomes)
fdist2 = _create_rand_fdist(numsamples, numoutcomes)
fdist3 = _create_rand_fdist(numsamples, numoutcomes)
pdists = [
MLEProbDist(fdist1),
LidstoneProbDist(fdist1, 0.5, numsamples),
HeldoutProbDist(fdist1, fdist2, numsamples),
HeldoutProbDist(fdist2, fdist1, numsamples),
CrossValidationProbDist([fdist1, fdist2, fdist3], numsamples),
SimpleGoodTuringProbDist(fdist1),
SimpleGoodTuringProbDist(fdist1, 7),
_create_sum_pdist(numsamples),
]
vals = []
for n in range(1, numsamples + 1):
vals.append(tuple([n, fdist1.freq(n)] + [pdist.prob(n) for pdist in pdists]))
print(
""%d samples (1-%d)
% (numsamples, numsamples, numoutcomes)
)
print(""="" * 9 * (len(pdists) + 2))
FORMATSTR = ""      FreqDist "" + ""%8s "" * (len(pdists) - 1) + ""|  Actual""
print(FORMATSTR % tuple(repr(pdist)[1:9] for pdist in pdists[:-1]))
print(""-"" * 9 * (len(pdists) + 2))
FORMATSTR = ""%3d   %8.6f "" + ""%8.6f "" * (len(pdists) - 1) + ""| %8.6f""
for val in vals:
print(FORMATSTR % val)
zvals = list(zip(*vals))
sums = [sum(val) for val in zvals[1:]]
print(""-"" * 9 * (len(pdists) + 2))
FORMATSTR = ""Total "" + ""%8.6f "" * (len(pdists)) + ""| %8.6f""
print(FORMATSTR % tuple(sums))
print(""="" * 9 * (len(pdists) + 2))
if len(""%s"" % fdist1) < 70:
print(""  fdist1: %s"" % fdist1)
print(""  fdist2: %s"" % fdist2)
print(""  fdist3: %s"" % fdist3)
print()
print(""Generating:"")
for pdist in pdists:
fdist = FreqDist(pdist.generate() for i in range(5000))
print(""{:>20} {}"".format(pdist.__class__.__name__[:20], (""%s"" % fdist)[:55]))
print()
",[],0,[],/probability.py_demo
418,/home/amandapotts/git/nltk/nltk/grammar.py___init__,"def __init__(self, symbol):
""""""
Construct a new non-terminal from the given symbol.
:type symbol: any
:param symbol: The node value corresponding to this
``Nonterminal``.  This value must be immutable and
hashable.
""""""
self._symbol = symbol
",[],0,[],/grammar.py___init__
419,/home/amandapotts/git/nltk/nltk/grammar.py_symbol,"def symbol(self):
""""""
Return the node value corresponding to this ``Nonterminal``.
:rtype: (any)
""""""
return self._symbol
",[],0,[],/grammar.py_symbol
420,/home/amandapotts/git/nltk/nltk/grammar.py___eq__,"def __eq__(self, other):
""""""
Return True if this non-terminal is equal to ``other``.  In
particular, return True if ``other`` is a ``Nonterminal``
and this non-terminal's symbol is equal to ``other`` 's symbol.
:rtype: bool
""""""
return type(self) == type(other) and self._symbol == other._symbol
",[],0,[],/grammar.py___eq__
421,/home/amandapotts/git/nltk/nltk/grammar.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/grammar.py___ne__
422,/home/amandapotts/git/nltk/nltk/grammar.py___lt__,"def __lt__(self, other):
if not isinstance(other, Nonterminal):
raise_unorderable_types(""<"", self, other)
return self._symbol < other._symbol
",[],0,[],/grammar.py___lt__
423,/home/amandapotts/git/nltk/nltk/grammar.py___hash__,"def __hash__(self):
return hash(self._symbol)
",[],0,[],/grammar.py___hash__
424,/home/amandapotts/git/nltk/nltk/grammar.py___repr__,"def __repr__(self):
""""""
Return a string representation for this ``Nonterminal``.
:rtype: str
""""""
if isinstance(self._symbol, str):
return ""%s"" % self._symbol
else:
return ""%s"" % repr(self._symbol)
",[],0,[],/grammar.py___repr__
425,/home/amandapotts/git/nltk/nltk/grammar.py___str__,"def __str__(self):
""""""
Return a string representation for this ``Nonterminal``.
:rtype: str
""""""
if isinstance(self._symbol, str):
return ""%s"" % self._symbol
else:
return ""%s"" % repr(self._symbol)
",[],0,[],/grammar.py___str__
426,/home/amandapotts/git/nltk/nltk/grammar.py___div__,"def __div__(self, rhs):
""""""
Return a new nonterminal whose symbol is ``A/B``, where ``A`` is
the symbol for this nonterminal, and ``B`` is the symbol for rhs.
:param rhs: The nonterminal used to form the right hand side
of the new nonterminal.
:type rhs: Nonterminal
:rtype: Nonterminal
""""""
return Nonterminal(f""{self._symbol}/{rhs._symbol}"")
",[],0,[],/grammar.py___div__
427,/home/amandapotts/git/nltk/nltk/grammar.py___truediv__,"def __truediv__(self, rhs):
""""""
Return a new nonterminal whose symbol is ``A/B``, where ``A`` is
the symbol for this nonterminal, and ``B`` is the symbol for rhs.
This function allows use of the slash ``/`` operator with
the future import of division.
:param rhs: The nonterminal used to form the right hand side
of the new nonterminal.
:type rhs: Nonterminal
:rtype: Nonterminal
""""""
return self.__div__(rhs)
",[],0,[],/grammar.py___truediv__
428,/home/amandapotts/git/nltk/nltk/grammar.py_nonterminals,"def nonterminals(symbols):
""""""
Given a string containing a list of symbol names, return a list of
``Nonterminals`` constructed from those symbols.
:param symbols: The symbol name string.  This string can be
delimited by either spaces or commas.
:type symbols: str
:return: A list of ``Nonterminals`` constructed from the symbol
names given in ``symbols``.  The ``Nonterminals`` are sorted
in the same order as the symbols names.
:rtype: list(Nonterminal)
""""""
if "","" in symbols:
symbol_list = symbols.split("","")
else:
symbol_list = symbols.split()
return [Nonterminal(s.strip()) for s in symbol_list]
",[],0,[],/grammar.py_nonterminals
429,/home/amandapotts/git/nltk/nltk/grammar.py___hash__,"def __hash__(self):
self.freeze()
return FeatStruct.__hash__(self)
",[],0,[],/grammar.py___hash__
430,/home/amandapotts/git/nltk/nltk/grammar.py_symbol,"def symbol(self):
return self
",[],0,[],/grammar.py_symbol
431,/home/amandapotts/git/nltk/nltk/grammar.py_is_nonterminal,"def is_nonterminal(item):
""""""
:return: True if the item is a ``Nonterminal``.
:rtype: bool
""""""
return isinstance(item, Nonterminal)
",[],0,[],/grammar.py_is_nonterminal
432,/home/amandapotts/git/nltk/nltk/grammar.py_is_terminal,"def is_terminal(item):
""""""
Return True if the item is a terminal, which currently is
if it is hashable and not a ``Nonterminal``.
:rtype: bool
""""""
return hasattr(item, ""__hash__"") and not isinstance(item, Nonterminal)
",[],0,[],/grammar.py_is_terminal
433,/home/amandapotts/git/nltk/nltk/grammar.py___init__,"def __init__(self, lhs, rhs):
""""""
Construct a new ``Production``.
:param lhs: The left-hand side of the new ``Production``.
:type lhs: Nonterminal
:param rhs: The right-hand side of the new ``Production``.
:type rhs: sequence(Nonterminal and terminal)
""""""
if isinstance(rhs, str):
raise TypeError(
""production right hand side should be a list, "" ""not a string""
)
self._lhs = lhs
self._rhs = tuple(rhs)
",[],0,[],/grammar.py___init__
434,/home/amandapotts/git/nltk/nltk/grammar.py_lhs,"def lhs(self):
""""""
Return the left-hand side of this ``Production``.
:rtype: Nonterminal
""""""
return self._lhs
",[],0,[],/grammar.py_lhs
435,/home/amandapotts/git/nltk/nltk/grammar.py_rhs,"def rhs(self):
""""""
Return the right-hand side of this ``Production``.
:rtype: sequence(Nonterminal and terminal)
""""""
return self._rhs
",[],0,[],/grammar.py_rhs
436,/home/amandapotts/git/nltk/nltk/grammar.py___len__,"def __len__(self):
""""""
Return the length of the right-hand side.
:rtype: int
""""""
return len(self._rhs)
",[],0,[],/grammar.py___len__
437,/home/amandapotts/git/nltk/nltk/grammar.py_is_nonlexical,"def is_nonlexical(self):
""""""
Return True if the right-hand side only contains ``Nonterminals``
:rtype: bool
""""""
return all(is_nonterminal(n) for n in self._rhs)
",[],0,[],/grammar.py_is_nonlexical
438,/home/amandapotts/git/nltk/nltk/grammar.py_is_lexical,"def is_lexical(self):
""""""
Return True if the right-hand contain at least one terminal token.
:rtype: bool
""""""
return not self.is_nonlexical()
",[],0,[],/grammar.py_is_lexical
439,/home/amandapotts/git/nltk/nltk/grammar.py___str__,"def __str__(self):
""""""
Return a verbose string representation of the ``Production``.
:rtype: str
""""""
result = ""%s -> "" % repr(self._lhs)
result += "" "".join(repr(el) for el in self._rhs)
return result
",[],0,[],/grammar.py___str__
440,/home/amandapotts/git/nltk/nltk/grammar.py___repr__,"def __repr__(self):
""""""
Return a concise string representation of the ``Production``.
:rtype: str
""""""
return ""%s"" % self
",[],0,[],/grammar.py___repr__
441,/home/amandapotts/git/nltk/nltk/grammar.py___eq__,"def __eq__(self, other):
""""""
Return True if this ``Production`` is equal to ``other``.
:rtype: bool
""""""
return (
type(self) == type(other)
and self._lhs == other._lhs
and self._rhs == other._rhs
)
",[],0,[],/grammar.py___eq__
442,/home/amandapotts/git/nltk/nltk/grammar.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/grammar.py___ne__
443,/home/amandapotts/git/nltk/nltk/grammar.py___lt__,"def __lt__(self, other):
if not isinstance(other, Production):
raise_unorderable_types(""<"", self, other)
return (self._lhs, self._rhs) < (other._lhs, other._rhs)
",[],0,[],/grammar.py___lt__
444,/home/amandapotts/git/nltk/nltk/grammar.py___hash__,"def __hash__(self):
""""""
Return a hash value for the ``Production``.
:rtype: int
""""""
return hash((self._lhs, self._rhs))
",[],0,[],/grammar.py___hash__
445,/home/amandapotts/git/nltk/nltk/grammar.py___str__,"def __str__(self):
""""""
Return a verbose string representation of the ``DependencyProduction``.
:rtype: str
""""""
result = f""'{self._lhs}' ->""
for elt in self._rhs:
result += f"" '{elt}'""
return result
",[],0,[],/grammar.py___str__
446,/home/amandapotts/git/nltk/nltk/grammar.py___init__,"def __init__(self, lhs, rhs, **prob):
""""""
Construct a new ``ProbabilisticProduction``.
:param lhs: The left-hand side of the new ``ProbabilisticProduction``.
:type lhs: Nonterminal
:param rhs: The right-hand side of the new ``ProbabilisticProduction``.
:type rhs: sequence(Nonterminal and terminal)
:param prob: Probability parameters of the new ``ProbabilisticProduction``.
""""""
ImmutableProbabilisticMixIn.__init__(self, **prob)
Production.__init__(self, lhs, rhs)
",[],0,[],/grammar.py___init__
447,/home/amandapotts/git/nltk/nltk/grammar.py___str__,"def __str__(self):
return super().__str__() + (
"" [1.0]"" if (self.prob() == 1.0) else "" [%g]"" % self.prob()
)
",[],0,[],/grammar.py___str__
448,/home/amandapotts/git/nltk/nltk/grammar.py___eq__,"def __eq__(self, other):
return (
type(self) == type(other)
and self._lhs == other._lhs
and self._rhs == other._rhs
and self.prob() == other.prob()
)
",[],0,[],/grammar.py___eq__
449,/home/amandapotts/git/nltk/nltk/grammar.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/grammar.py___ne__
450,/home/amandapotts/git/nltk/nltk/grammar.py___hash__,"def __hash__(self):
return hash((self._lhs, self._rhs, self.prob()))
",[],0,[],/grammar.py___hash__
451,/home/amandapotts/git/nltk/nltk/grammar.py___init__,"def __init__(self, start, productions, calculate_leftcorners=True):
""""""
Create a new context-free grammar, from the given start state
and set of ``Production`` instances.
:param start: The start symbol
:type start: Nonterminal
:param productions: The list of productions that defines the grammar
:type productions: list(Production)
:param calculate_leftcorners: False if we don't want to calculate the
leftcorner relation. In that case, some optimized chart parsers won't work.
:type calculate_leftcorners: bool
""""""
if not is_nonterminal(start):
raise TypeError(
""start should be a Nonterminal object,""
"" not a %s"" % type(start).__name__
)
self._start = start
self._productions = productions
self._categories = {prod.lhs() for prod in productions}
self._calculate_indexes()
self._calculate_grammar_forms()
if calculate_leftcorners:
self._calculate_leftcorners()
",[],0,[],/grammar.py___init__
452,/home/amandapotts/git/nltk/nltk/grammar.py__calculate_indexes,"def _calculate_indexes(self):
self._lhs_index = {}
self._rhs_index = {}
self._empty_index = {}
self._lexical_index = {}
for prod in self._productions:
lhs = prod._lhs
if lhs not in self._lhs_index:
self._lhs_index[lhs] = []
self._lhs_index[lhs].append(prod)
if prod._rhs:
rhs0 = prod._rhs[0]
if rhs0 not in self._rhs_index:
self._rhs_index[rhs0] = []
self._rhs_index[rhs0].append(prod)
else:
self._empty_index[prod.lhs()] = prod
for token in prod._rhs:
if is_terminal(token):
self._lexical_index.setdefault(token, set()).add(prod)
",[],0,[],/grammar.py__calculate_indexes
453,/home/amandapotts/git/nltk/nltk/grammar.py__calculate_leftcorners,"def _calculate_leftcorners(self):
self._immediate_leftcorner_categories = {cat: {cat} for cat in self._categories}
self._immediate_leftcorner_words = {cat: set() for cat in self._categories}
for prod in self.productions():
if len(prod) > 0:
cat, left = prod.lhs(), prod.rhs()[0]
if is_nonterminal(left):
self._immediate_leftcorner_categories[cat].add(left)
else:
self._immediate_leftcorner_words[cat].add(left)
lc = transitive_closure(self._immediate_leftcorner_categories, reflexive=True)
self._leftcorners = lc
self._leftcorner_parents = invert_graph(lc)
nr_leftcorner_categories = sum(
map(len, self._immediate_leftcorner_categories.values())
)
nr_leftcorner_words = sum(map(len, self._immediate_leftcorner_words.values()))
if nr_leftcorner_words > nr_leftcorner_categories > 10000:
self._leftcorner_words = None
return
self._leftcorner_words = {}
for cat in self._leftcorners:
lefts = self._leftcorners[cat]
lc = self._leftcorner_words[cat] = set()
for left in lefts:
lc.update(self._immediate_leftcorner_words.get(left, set()))
",[],0,[],/grammar.py__calculate_leftcorners
454,/home/amandapotts/git/nltk/nltk/grammar.py_fromstring,"def fromstring(cls, input, encoding=None):
""""""
Return the grammar instance corresponding to the input string(s).
:param input: a grammar, either in the form of a string or as a list of strings.
""""""
start, productions = read_grammar(
input, standard_nonterm_parser, encoding=encoding
)
return cls(start, productions)
",[],0,[],/grammar.py_fromstring
455,/home/amandapotts/git/nltk/nltk/grammar.py_start,"def start(self):
""""""
Return the start symbol of the grammar
:rtype: Nonterminal
""""""
return self._start
",[],0,[],/grammar.py_start
456,/home/amandapotts/git/nltk/nltk/grammar.py_productions,"def productions(self, lhs=None, rhs=None, empty=False):
""""""
Return the grammar productions, filtered by the left-hand side
or the first item in the right-hand side.
:param lhs: Only return productions with the given left-hand side.
:param rhs: Only return productions with the given first item
in the right-hand side.
:param empty: Only return productions with an empty right-hand side.
:return: A list of productions matching the given constraints.
:rtype: list(Production)
""""""
if rhs and empty:
raise ValueError(
""You cannot select empty and non-empty "" ""productions at the same time.""
)
if not lhs and not rhs:
if not empty:
return self._productions
else:
return self._empty_index.values()
elif lhs and not rhs:
if not empty:
return self._lhs_index.get(lhs, [])
elif lhs in self._empty_index:
return [self._empty_index[lhs]]
else:
return []
elif rhs and not lhs:
return self._rhs_index.get(rhs, [])
else:
return [
prod
for prod in self._lhs_index.get(lhs, [])
if prod in self._rhs_index.get(rhs, [])
]
",[],0,[],/grammar.py_productions
457,/home/amandapotts/git/nltk/nltk/grammar.py_leftcorners,"def leftcorners(self, cat):
""""""
Return the set of all nonterminals that the given nonterminal
can start with, including itself.
This is the reflexive, transitive closure of the immediate
leftcorner relation:  (A > B)  iff  (A -> B beta)
:param cat: the parent of the leftcorners
:type cat: Nonterminal
:return: the set of all leftcorners
:rtype: set(Nonterminal)
""""""
return self._leftcorners.get(cat, {cat})
",[],0,[],/grammar.py_leftcorners
458,/home/amandapotts/git/nltk/nltk/grammar.py_is_leftcorner,"def is_leftcorner(self, cat, left):
""""""
True if left is a leftcorner of cat, where left can be a
terminal or a nonterminal.
:param cat: the parent of the leftcorner
:type cat: Nonterminal
:param left: the suggested leftcorner
:type left: Terminal or Nonterminal
:rtype: bool
""""""
if is_nonterminal(left):
return left in self.leftcorners(cat)
elif self._leftcorner_words:
return left in self._leftcorner_words.get(cat, set())
else:
return any(
left in self._immediate_leftcorner_words.get(parent, set())
for parent in self.leftcorners(cat)
)
",[],0,[],/grammar.py_is_leftcorner
459,/home/amandapotts/git/nltk/nltk/grammar.py_leftcorner_parents,"def leftcorner_parents(self, cat):
""""""
Return the set of all nonterminals for which the given category
is a left corner. This is the inverse of the leftcorner relation.
:param cat: the suggested leftcorner
:type cat: Nonterminal
:return: the set of all parents to the leftcorner
:rtype: set(Nonterminal)
""""""
return self._leftcorner_parents.get(cat, {cat})
",[],0,[],/grammar.py_leftcorner_parents
460,/home/amandapotts/git/nltk/nltk/grammar.py_check_coverage,"def check_coverage(self, tokens):
""""""
Check whether the grammar rules cover the given list of tokens.
If not, then raise an exception.
:type tokens: list(str)
""""""
missing = [tok for tok in tokens if not self._lexical_index.get(tok)]
if missing:
missing = "", "".join(f""{w!r}"" for w in missing)
raise ValueError(
""Grammar does not cover some of the "" ""input words: %r."" % missing
)
",[],0,[],/grammar.py_check_coverage
461,/home/amandapotts/git/nltk/nltk/grammar.py__calculate_grammar_forms,"def _calculate_grammar_forms(self):
""""""
Pre-calculate of which form(s) the grammar is.
""""""
prods = self._productions
self._is_lexical = all(p.is_lexical() for p in prods)
self._is_nonlexical = all(p.is_nonlexical() for p in prods if len(p) != 1)
self._min_len = min(len(p) for p in prods)
self._max_len = max(len(p) for p in prods)
self._all_unary_are_lexical = all(p.is_lexical() for p in prods if len(p) == 1)
",[],0,[],/grammar.py__calculate_grammar_forms
462,/home/amandapotts/git/nltk/nltk/grammar.py_is_lexical,"def is_lexical(self):
""""""
Return True if all productions are lexicalised.
""""""
return self._is_lexical
",[],0,[],/grammar.py_is_lexical
463,/home/amandapotts/git/nltk/nltk/grammar.py_is_nonlexical,"def is_nonlexical(self):
""""""
Return True if all lexical rules are ""preterminals"", that is,
unary rules which can be separated in a preprocessing step.
This means that all productions are of the forms
A -> B1 ... Bn (n>=0), or A -> ""s"".
Note: is_lexical() and is_nonlexical() are not opposites.
There are grammars which are neither, and grammars which are both.
""""""
return self._is_nonlexical
",[],0,[],/grammar.py_is_nonlexical
464,/home/amandapotts/git/nltk/nltk/grammar.py_min_len,"def min_len(self):
""""""
Return the right-hand side length of the shortest grammar production.
""""""
return self._min_len
",[],0,[],/grammar.py_min_len
465,/home/amandapotts/git/nltk/nltk/grammar.py_max_len,"def max_len(self):
""""""
Return the right-hand side length of the longest grammar production.
""""""
return self._max_len
",[],0,[],/grammar.py_max_len
466,/home/amandapotts/git/nltk/nltk/grammar.py_is_nonempty,"def is_nonempty(self):
""""""
Return True if there are no empty productions.
""""""
return self._min_len > 0
",[],0,[],/grammar.py_is_nonempty
467,/home/amandapotts/git/nltk/nltk/grammar.py_is_binarised,"def is_binarised(self):
""""""
Return True if all productions are at most binary.
Note that there can still be empty and unary productions.
""""""
return self._max_len <= 2
",[],0,[],/grammar.py_is_binarised
468,/home/amandapotts/git/nltk/nltk/grammar.py_is_flexible_chomsky_normal_form,"def is_flexible_chomsky_normal_form(self):
""""""
Return True if all productions are of the forms
A -> B C, A -> B, or A -> ""s"".
""""""
return self.is_nonempty() and self.is_nonlexical() and self.is_binarised()
",[],0,[],/grammar.py_is_flexible_chomsky_normal_form
469,/home/amandapotts/git/nltk/nltk/grammar.py_is_chomsky_normal_form,"def is_chomsky_normal_form(self):
""""""
Return True if the grammar is of Chomsky Normal Form, i.e. all productions
are of the form A -> B C, or A -> ""s"".
""""""
return self.is_flexible_chomsky_normal_form() and self._all_unary_are_lexical
",[],0,[],/grammar.py_is_chomsky_normal_form
470,/home/amandapotts/git/nltk/nltk/grammar.py_chomsky_normal_form,"def chomsky_normal_form(self, new_token_padding=""@$@"", flexible=False):
""""""
Returns a new Grammar that is in chomsky normal
:param: new_token_padding
Customise new rule formation during binarisation
""""""
if self.is_chomsky_normal_form():
return self
if self.productions(empty=True):
raise ValueError(
""Grammar has Empty rules. "" ""Cannot deal with them at the moment""
)
for rule in self.productions():
if rule.is_lexical() and len(rule.rhs()) > 1:
raise ValueError(
f""Cannot handled mixed rule {rule.lhs()} => {rule.rhs()}""
)
step1 = CFG.eliminate_start(self)
step2 = CFG.binarize(step1, new_token_padding)
if flexible:
return step2
step3 = CFG.remove_unitary_rules(step2)
step4 = CFG(step3.start(), list(set(step3.productions())))
return step4
",[],0,[],/grammar.py_chomsky_normal_form
471,/home/amandapotts/git/nltk/nltk/grammar.py_remove_unitary_rules,"def remove_unitary_rules(cls, grammar):
""""""
Remove nonlexical unitary rules and convert them to
lexical
""""""
result = []
unitary = deque([])
for rule in grammar.productions():
if len(rule) == 1 and rule.is_nonlexical():
unitary.append(rule)
else:
result.append(rule)
while unitary:
rule = unitary.popleft()
for item in grammar.productions(lhs=rule.rhs()[0]):
new_rule = Production(rule.lhs(), item.rhs())
if len(new_rule) != 1 or new_rule.is_lexical():
result.append(new_rule)
else:
unitary.append(new_rule)
n_grammar = CFG(grammar.start(), result)
return n_grammar
",[],0,[],/grammar.py_remove_unitary_rules
472,/home/amandapotts/git/nltk/nltk/grammar.py_binarize,"def binarize(cls, grammar, padding=""@$@""):
""""""
Convert all non-binary rules into binary by introducing
new tokens.
Example::
Original:
A => B C D
After Conversion:
A => B A@$@B
A@$@B => C D
""""""
result = []
for rule in grammar.productions():
if len(rule.rhs()) > 2:
left_side = rule.lhs()
for k in range(0, len(rule.rhs()) - 2):
tsym = rule.rhs()[k]
new_sym = Nonterminal(left_side.symbol() + padding + tsym.symbol())
new_production = Production(left_side, (tsym, new_sym))
left_side = new_sym
result.append(new_production)
last_prd = Production(left_side, rule.rhs()[-2:])
result.append(last_prd)
else:
result.append(rule)
n_grammar = CFG(grammar.start(), result)
return n_grammar
",[],0,[],/grammar.py_binarize
473,/home/amandapotts/git/nltk/nltk/grammar.py_eliminate_start,"def eliminate_start(cls, grammar):
""""""
Eliminate start rule in case it appears on RHS
Example: S -> S0 S1 and S0 -> S1 S
Then another rule S0_Sigma -> S is added
""""""
start = grammar.start()
result = []
need_to_add = None
for rule in grammar.productions():
if start in rule.rhs():
need_to_add = True
result.append(rule)
if need_to_add:
start = Nonterminal(""S0_SIGMA"")
result.append(Production(start, [grammar.start()]))
n_grammar = CFG(start, result)
return n_grammar
return grammar
",[],0,[],/grammar.py_eliminate_start
474,/home/amandapotts/git/nltk/nltk/grammar.py___repr__,"def __repr__(self):
return ""<Grammar with %d productions>"" % len(self._productions)
",[],0,[],/grammar.py___repr__
475,/home/amandapotts/git/nltk/nltk/grammar.py___str__,"def __str__(self):
result = ""Grammar with %d productions"" % len(self._productions)
result += "" (start state = %r)"" % self._start
for production in self._productions:
result += ""\n    %s"" % production
return result
",[],0,[],/grammar.py___str__
476,/home/amandapotts/git/nltk/nltk/grammar.py___init__,"def __init__(self, start, productions):
""""""
Create a new feature-based grammar, from the given start
state and set of ``Productions``.
:param start: The start symbol
:type start: FeatStructNonterminal
:param productions: The list of productions that defines the grammar
:type productions: list(Production)
""""""
CFG.__init__(self, start, productions)
",[],0,[],/grammar.py___init__
477,/home/amandapotts/git/nltk/nltk/grammar.py__calculate_indexes,"def _calculate_indexes(self):
self._lhs_index = {}
self._rhs_index = {}
self._empty_index = {}
self._empty_productions = []
self._lexical_index = {}
for prod in self._productions:
lhs = self._get_type_if_possible(prod._lhs)
if lhs not in self._lhs_index:
self._lhs_index[lhs] = []
self._lhs_index[lhs].append(prod)
if prod._rhs:
rhs0 = self._get_type_if_possible(prod._rhs[0])
if rhs0 not in self._rhs_index:
self._rhs_index[rhs0] = []
self._rhs_index[rhs0].append(prod)
else:
if lhs not in self._empty_index:
self._empty_index[lhs] = []
self._empty_index[lhs].append(prod)
self._empty_productions.append(prod)
for token in prod._rhs:
if is_terminal(token):
self._lexical_index.setdefault(token, set()).add(prod)
",[],0,[],/grammar.py__calculate_indexes
478,/home/amandapotts/git/nltk/nltk/grammar.py_productions,"def productions(self, lhs=None, rhs=None, empty=False):
""""""
Return the grammar productions, filtered by the left-hand side
or the first item in the right-hand side.
:param lhs: Only return productions with the given left-hand side.
:param rhs: Only return productions with the given first item
in the right-hand side.
:param empty: Only return productions with an empty right-hand side.
:rtype: list(Production)
""""""
if rhs and empty:
raise ValueError(
""You cannot select empty and non-empty "" ""productions at the same time.""
)
if not lhs and not rhs:
if empty:
return self._empty_productions
else:
return self._productions
elif lhs and not rhs:
if empty:
return self._empty_index.get(self._get_type_if_possible(lhs), [])
else:
return self._lhs_index.get(self._get_type_if_possible(lhs), [])
elif rhs and not lhs:
return self._rhs_index.get(self._get_type_if_possible(rhs), [])
else:
return [
prod
for prod in self._lhs_index.get(self._get_type_if_possible(lhs), [])
if prod in self._rhs_index.get(self._get_type_if_possible(rhs), [])
]
",[],0,[],/grammar.py_productions
479,/home/amandapotts/git/nltk/nltk/grammar.py_leftcorners,"def leftcorners(self, cat):
""""""
Return the set of all words that the given category can start with.
Also called the ""first set"" in compiler construction.
""""""
raise NotImplementedError(""Not implemented yet"")
",[],0,[],/grammar.py_leftcorners
480,/home/amandapotts/git/nltk/nltk/grammar.py_leftcorner_parents,"def leftcorner_parents(self, cat):
""""""
Return the set of all categories for which the given category
is a left corner.
""""""
raise NotImplementedError(""Not implemented yet"")
",[],0,[],/grammar.py_leftcorner_parents
481,/home/amandapotts/git/nltk/nltk/grammar.py__get_type_if_possible,"def _get_type_if_possible(self, item):
""""""
Helper function which returns the ``TYPE`` feature of the ``item``,
if it exists, otherwise it returns the ``item`` itself
""""""
if isinstance(item, dict) and TYPE in item:
return FeatureValueType(item[TYPE])
else:
return item
",[],0,[],/grammar.py__get_type_if_possible
482,/home/amandapotts/git/nltk/nltk/grammar.py___init__,"def __init__(self, value):
self._value = value
",[],0,[],/grammar.py___init__
483,/home/amandapotts/git/nltk/nltk/grammar.py___repr__,"def __repr__(self):
return ""<%s>"" % self._value
",[],0,[],/grammar.py___repr__
484,/home/amandapotts/git/nltk/nltk/grammar.py___eq__,"def __eq__(self, other):
return type(self) == type(other) and self._value == other._value
",[],0,[],/grammar.py___eq__
485,/home/amandapotts/git/nltk/nltk/grammar.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/grammar.py___ne__
486,/home/amandapotts/git/nltk/nltk/grammar.py___lt__,"def __lt__(self, other):
if not isinstance(other, FeatureValueType):
raise_unorderable_types(""<"", self, other)
return self._value < other._value
",[],0,[],/grammar.py___lt__
487,/home/amandapotts/git/nltk/nltk/grammar.py___hash__,"def __hash__(self):
return hash(self._value)
",[],0,[],/grammar.py___hash__
488,/home/amandapotts/git/nltk/nltk/grammar.py___init__,"def __init__(self, productions):
""""""
Create a new dependency grammar, from the set of ``Productions``.
:param productions: The list of productions that defines the grammar
:type productions: list(Production)
""""""
self._productions = productions
",[],0,[],/grammar.py___init__
489,/home/amandapotts/git/nltk/nltk/grammar.py_fromstring,"def fromstring(cls, input):
productions = []
for linenum, line in enumerate(input.split(""\n"")):
line = line.strip()
if line.startswith(""#"") or line == """":
continue
try:
productions += _read_dependency_production(line)
except ValueError as e:
raise ValueError(f""Unable to parse line {linenum}: {line}"") from e
if len(productions) == 0:
raise ValueError(""No productions found!"")
return cls(productions)
",[],0,[],/grammar.py_fromstring
490,/home/amandapotts/git/nltk/nltk/grammar.py_contains,"def contains(self, head, mod):
""""""
:param head: A head word.
:type head: str
:param mod: A mod word, to test as a modifier of 'head'.
:type mod: str
:return: true if this ``DependencyGrammar`` contains a
``DependencyProduction`` mapping 'head' to 'mod'.
:rtype: bool
""""""
for production in self._productions:
for possibleMod in production._rhs:
if production._lhs == head and possibleMod == mod:
return True
return False
",[],0,[],/grammar.py_contains
491,/home/amandapotts/git/nltk/nltk/grammar.py___contains__,"def __contains__(self, head_mod):
""""""
Return True if this ``DependencyGrammar`` contains a
``DependencyProduction`` mapping 'head' to 'mod'.
:param head_mod: A tuple of a head word and a mod word,
to test as a modifier of 'head'.
:type head: Tuple[str, str]
:rtype: bool
""""""
try:
head, mod = head_mod
except ValueError as e:
raise ValueError(
""Must use a tuple of strings, e.g. `('price', 'of') in grammar`""
) from e
return self.contains(head, mod)
",[],0,[],/grammar.py___contains__
492,/home/amandapotts/git/nltk/nltk/grammar.py___str__,"def __str__(self):
""""""
Return a verbose string representation of the ``DependencyGrammar``
:rtype: str
""""""
str = ""Dependency grammar with %d productions"" % len(self._productions)
for production in self._productions:
str += ""\n  %s"" % production
return str
",[],0,[],/grammar.py___str__
493,/home/amandapotts/git/nltk/nltk/grammar.py___repr__,"def __repr__(self):
""""""
Return a concise string representation of the ``DependencyGrammar``
""""""
return ""Dependency grammar with %d productions"" % len(self._productions)
",[],0,[],/grammar.py___repr__
494,/home/amandapotts/git/nltk/nltk/grammar.py___init__,"def __init__(self, productions, events, tags):
self._productions = productions
self._events = events
self._tags = tags
",[],0,[],/grammar.py___init__
495,/home/amandapotts/git/nltk/nltk/grammar.py_contains,"def contains(self, head, mod):
""""""
Return True if this ``DependencyGrammar`` contains a
``DependencyProduction`` mapping 'head' to 'mod'.
:param head: A head word.
:type head: str
:param mod: A mod word, to test as a modifier of 'head'.
:type mod: str
:rtype: bool
""""""
for production in self._productions:
for possibleMod in production._rhs:
if production._lhs == head and possibleMod == mod:
return True
return False
",[],0,[],/grammar.py_contains
496,/home/amandapotts/git/nltk/nltk/grammar.py___str__,"def __str__(self):
""""""
Return a verbose string representation of the ``ProbabilisticDependencyGrammar``
:rtype: str
""""""
str = ""Statistical dependency grammar with %d productions"" % len(
self._productions
)
for production in self._productions:
str += ""\n  %s"" % production
str += ""\nEvents:""
for event in self._events:
str += ""\n  %d:%s"" % (self._events[event], event)
str += ""\nTags:""
for tag_word in self._tags:
str += f""\n {tag_word}:\t({self._tags[tag_word]})""
return str
",[],0,[],/grammar.py___str__
497,/home/amandapotts/git/nltk/nltk/grammar.py___repr__,"def __repr__(self):
""""""
Return a concise string representation of the ``ProbabilisticDependencyGrammar``
""""""
return ""Statistical Dependency grammar with %d productions"" % len(
self._productions
)
",[],0,[],/grammar.py___repr__
498,/home/amandapotts/git/nltk/nltk/grammar.py___init__,"def __init__(self, start, productions, calculate_leftcorners=True):
""""""
Create a new context-free grammar, from the given start state
and set of ``ProbabilisticProductions``.
:param start: The start symbol
:type start: Nonterminal
:param productions: The list of productions that defines the grammar
:type productions: list(Production)
:raise ValueError: if the set of productions with any left-hand-side
do not have probabilities that sum to a value within
EPSILON of 1.
:param calculate_leftcorners: False if we don't want to calculate the
leftcorner relation. In that case, some optimized chart parsers won't work.
:type calculate_leftcorners: bool
""""""
CFG.__init__(self, start, productions, calculate_leftcorners)
probs = {}
for production in productions:
probs[production.lhs()] = probs.get(production.lhs(), 0) + production.prob()
for lhs, p in probs.items():
if not ((1 - PCFG.EPSILON) < p < (1 + PCFG.EPSILON)):
raise ValueError(""Productions for %r do not sum to 1"" % lhs)
",[],0,[],/grammar.py___init__
499,/home/amandapotts/git/nltk/nltk/grammar.py_fromstring,"def fromstring(cls, input, encoding=None):
""""""
Return a probabilistic context-free grammar corresponding to the
input string(s).
:param input: a grammar, either in the form of a string or else
as a list of strings.
""""""
start, productions = read_grammar(
input, standard_nonterm_parser, probabilistic=True, encoding=encoding
)
return cls(start, productions)
",[],0,[],/grammar.py_fromstring
500,/home/amandapotts/git/nltk/nltk/grammar.py_induce_pcfg,"def induce_pcfg(start, productions):
r""""""
Induce a PCFG grammar from a list of productions.
The probability of a production A -> B C in a PCFG is:
|                count(A -> B C)
|  P(B, C | A) = ---------------       where \* is any right hand side
|                 count(A -> \*)
:param start: The start symbol
:type start: Nonterminal
:param productions: The list of productions that defines the grammar
:type productions: list(Production)
""""""
pcount = {}
lcount = {}
for prod in productions:
lcount[prod.lhs()] = lcount.get(prod.lhs(), 0) + 1
pcount[prod] = pcount.get(prod, 0) + 1
prods = [
ProbabilisticProduction(p.lhs(), p.rhs(), prob=pcount[p] / lcount[p.lhs()])
for p in pcount
]
return PCFG(start, prods)
",[],0,[],/grammar.py_induce_pcfg
501,/home/amandapotts/git/nltk/nltk/grammar.py__read_cfg_production,"def _read_cfg_production(input):
""""""
Return a list of context-free ``Productions``.
""""""
return _read_production(input, standard_nonterm_parser)
",[],0,[],/grammar.py__read_cfg_production
502,/home/amandapotts/git/nltk/nltk/grammar.py__read_pcfg_production,"def _read_pcfg_production(input):
""""""
Return a list of PCFG ``ProbabilisticProductions``.
""""""
return _read_production(input, standard_nonterm_parser, probabilistic=True)
",[],0,[],/grammar.py__read_pcfg_production
503,/home/amandapotts/git/nltk/nltk/grammar.py__read_fcfg_production,"def _read_fcfg_production(input, fstruct_reader):
""""""
Return a list of feature-based ``Productions``.
""""""
return _read_production(input, fstruct_reader)
",[],0,[],/grammar.py__read_fcfg_production
504,/home/amandapotts/git/nltk/nltk/grammar.py__read_production,"def _read_production(line, nonterm_parser, probabilistic=False):
""""""
Parse a grammar rule, given as a string, and return
a list of productions.
""""""
pos = 0
lhs, pos = nonterm_parser(line, pos)
m = _ARROW_RE.match(line, pos)
if not m:
raise ValueError(""Expected an arrow"")
pos = m.end()
probabilities = [0.0]
rhsides = [[]]
while pos < len(line):
m = _PROBABILITY_RE.match(line, pos)
if probabilistic and m:
pos = m.end()
probabilities[-1] = float(m.group(1)[1:-1])
if probabilities[-1] > 1.0:
raise ValueError(
""Production probability %f, ""
""should not be greater than 1.0"" % (probabilities[-1],)
)
elif line[pos] in ""'\"""":
m = _TERMINAL_RE.match(line, pos)
if not m:
raise ValueError(""Unterminated string"")
rhsides[-1].append(m.group(1)[1:-1])
pos = m.end()
elif line[pos] == ""|"":
m = _DISJUNCTION_RE.match(line, pos)
probabilities.append(0.0)
rhsides.append([])
pos = m.end()
else:
nonterm, pos = nonterm_parser(line, pos)
rhsides[-1].append(nonterm)
if probabilistic:
return [
ProbabilisticProduction(lhs, rhs, prob=probability)
for (rhs, probability) in zip(rhsides, probabilities)
]
else:
return [Production(lhs, rhs) for rhs in rhsides]
",[],0,[],/grammar.py__read_production
505,/home/amandapotts/git/nltk/nltk/grammar.py_read_grammar,"def read_grammar(input, nonterm_parser, probabilistic=False, encoding=None):
""""""
Return a pair consisting of a starting category and a list of
``Productions``.
:param input: a grammar, either in the form of a string or else
as a list of strings.
:param nonterm_parser: a function for parsing nonterminals.
It should take a ``(string, position)`` as argument and
return a ``(nonterminal, position)`` as result.
:param probabilistic: are the grammar rules probabilistic?
:type probabilistic: bool
:param encoding: the encoding of the grammar, if it is a binary string
:type encoding: str
""""""
if encoding is not None:
input = input.decode(encoding)
if isinstance(input, str):
lines = input.split(""\n"")
else:
lines = input
start = None
productions = []
continue_line = """"
for linenum, line in enumerate(lines):
line = continue_line + line.strip()
if line.startswith(""#"") or line == """":
continue
if line.endswith(""\\""):
continue_line = line[:-1].rstrip() + "" ""
continue
continue_line = """"
try:
if line[0] == ""%"":
directive, args = line[1:].split(None, 1)
if directive == ""start"":
start, pos = nonterm_parser(args, 0)
if pos != len(args):
raise ValueError(""Bad argument to start directive"")
else:
raise ValueError(""Bad directive"")
else:
productions += _read_production(line, nonterm_parser, probabilistic)
except ValueError as e:
raise ValueError(f""Unable to parse line {linenum + 1}: {line}\n{e}"") from e
if not productions:
raise ValueError(""No productions found!"")
if not start:
start = productions[0].lhs()
return (start, productions)
",[],0,[],/grammar.py_read_grammar
506,/home/amandapotts/git/nltk/nltk/grammar.py_standard_nonterm_parser,"def standard_nonterm_parser(string, pos):
m = _STANDARD_NONTERM_RE.match(string, pos)
if not m:
raise ValueError(""Expected a nonterminal, found: "" + string[pos:])
return (Nonterminal(m.group(1)), m.end())
",[],0,[],/grammar.py_standard_nonterm_parser
507,/home/amandapotts/git/nltk/nltk/grammar.py__read_dependency_production,"def _read_dependency_production(s):
if not _READ_DG_RE.match(s):
raise ValueError(""Bad production string"")
pieces = _SPLIT_DG_RE.split(s)
pieces = [p for i, p in enumerate(pieces) if i % 2 == 1]
lhside = pieces[0].strip(""'\"""")
rhsides = [[]]
for piece in pieces[2:]:
if piece == ""|"":
rhsides.append([])
else:
rhsides[-1].append(piece.strip(""'\""""))
return [DependencyProduction(lhside, rhside) for rhside in rhsides]
",[],0,[],/grammar.py__read_dependency_production
508,/home/amandapotts/git/nltk/nltk/grammar.py_cfg_demo,"def cfg_demo():
""""""
A demonstration showing how ``CFGs`` can be created and used.
""""""
from nltk import CFG, Production, nonterminals
S, NP, VP, PP = nonterminals(""S, NP, VP, PP"")
N, V, P, Det = nonterminals(""N, V, P, Det"")
VP_slash_NP = VP / NP
print(""Some nonterminals:"", [S, NP, VP, PP, N, V, P, Det, VP / NP])
print(""    S.symbol() =>"", repr(S.symbol()))
print()
print(Production(S, [NP]))
grammar = CFG.fromstring(
""""""
S -> NP VP
PP -> P NP
NP -> Det N | NP PP
VP -> V NP | VP PP
Det -> 'a' | 'the'
N -> 'dog' | 'cat'
V -> 'chased' | 'sat'
P -> 'on' | 'in'
""""""
)
print(""A Grammar:"", repr(grammar))
print(""    grammar.start()       =>"", repr(grammar.start()))
print(""    grammar.productions() =>"", end="" "")
print(repr(grammar.productions()).replace("","", "",\n"" + "" "" * 25))
print()
",[],0,[],/grammar.py_cfg_demo
509,/home/amandapotts/git/nltk/nltk/grammar.py_pcfg_demo,"def pcfg_demo():
""""""
A demonstration showing how a ``PCFG`` can be created and used.
""""""
from nltk import induce_pcfg, treetransforms
from nltk.corpus import treebank
from nltk.parse import pchart
toy_pcfg1 = PCFG.fromstring(
""""""
S -> NP VP [1.0]
NP -> Det N [0.5] | NP PP [0.25] | 'John' [0.1] | 'I' [0.15]
Det -> 'the' [0.8] | 'my' [0.2]
N -> 'man' [0.5] | 'telescope' [0.5]
VP -> VP PP [0.1] | V NP [0.7] | V [0.2]
V -> 'ate' [0.35] | 'saw' [0.65]
PP -> P NP [1.0]
P -> 'with' [0.61] | 'under' [0.39]
""""""
)
toy_pcfg2 = PCFG.fromstring(
""""""
S    -> NP VP         [1.0]
VP   -> V NP          [.59]
VP   -> V             [.40]
VP   -> VP PP         [.01]
NP   -> Det N         [.41]
NP   -> Name          [.28]
NP   -> NP PP         [.31]
PP   -> P NP          [1.0]
V    -> 'saw'         [.21]
V    -> 'ate'         [.51]
V    -> 'ran'         [.28]
N    -> 'boy'         [.11]
N    -> 'cookie'      [.12]
N    -> 'table'       [.13]
N    -> 'telescope'   [.14]
N    -> 'hill'        [.5]
Name -> 'Jack'        [.52]
Name -> 'Bob'         [.48]
P    -> 'with'        [.61]
P    -> 'under'       [.39]
Det  -> 'the'         [.41]
Det  -> 'a'           [.31]
Det  -> 'my'          [.28]
""""""
)
pcfg_prods = toy_pcfg1.productions()
pcfg_prod = pcfg_prods[2]
print(""A PCFG production:"", repr(pcfg_prod))
print(""    pcfg_prod.lhs()  =>"", repr(pcfg_prod.lhs()))
print(""    pcfg_prod.rhs()  =>"", repr(pcfg_prod.rhs()))
print(""    pcfg_prod.prob() =>"", repr(pcfg_prod.prob()))
print()
grammar = toy_pcfg2
print(""A PCFG grammar:"", repr(grammar))
print(""    grammar.start()       =>"", repr(grammar.start()))
print(""    grammar.productions() =>"", end="" "")
print(repr(grammar.productions()).replace("","", "",\n"" + "" "" * 26))
print()
print(""Induce PCFG grammar from treebank data:"")
productions = []
item = treebank._fileids[0]
for tree in treebank.parsed_sents(item)[:3]:
tree.collapse_unary(collapsePOS=False)
tree.chomsky_normal_form(horzMarkov=2)
productions += tree.productions()
S = Nonterminal(""S"")
grammar = induce_pcfg(S, productions)
print(grammar)
print()
print(""Parse sentence using induced grammar:"")
parser = pchart.InsideChartParser(grammar)
parser.trace(3)
sent = treebank.parsed_sents(item)[0].leaves()
print(sent)
for parse in parser.parse(sent):
print(parse)
",[],0,[],/grammar.py_pcfg_demo
510,/home/amandapotts/git/nltk/nltk/grammar.py_fcfg_demo,"def fcfg_demo():
import nltk.data
g = nltk.data.load(""grammars/book_grammars/feat0.fcfg"")
print(g)
print()
",[],0,[],/grammar.py_fcfg_demo
511,/home/amandapotts/git/nltk/nltk/grammar.py_dg_demo,"def dg_demo():
""""""
A demonstration showing the creation and inspection of a
``DependencyGrammar``.
""""""
grammar = DependencyGrammar.fromstring(
""""""
'scratch' -> 'cats' | 'walls'
'walls' -> 'the'
'cats' -> 'the'
""""""
)
print(grammar)
",[],0,[],/grammar.py_dg_demo
512,/home/amandapotts/git/nltk/nltk/grammar.py_sdg_demo,"def sdg_demo():
""""""
A demonstration of how to read a string representation of
a CoNLL format dependency tree.
""""""
from nltk.parse import DependencyGraph
dg = DependencyGraph(
""""""
1   Ze                ze                Pron  Pron  per|3|evofmv|nom                 2   su      _  _
2   had               heb               V     V     trans|ovt|1of2of3|ev             0   ROOT    _  _
3   met               met               Prep  Prep  voor                             8   mod     _  _
4   haar              haar              Pron  Pron  bez|3|ev|neut|attr               5   det     _  _
5   moeder            moeder            N     N     soort|ev|neut                    3   obj1    _  _
6   kunnen            kan               V     V     hulp|ott|1of2of3|mv              2   vc      _  _
7   gaan              ga                V     V     hulp|inf                         6   vc      _  _
8   winkelen          winkel            V     V     intrans|inf                      11  cnj     _  _
9   ,                 ,                 Punc  Punc  komma                            8   punct   _  _
10  zwemmen           zwem              V     V     intrans|inf                      11  cnj     _  _
11  of                of                Conj  Conj  neven                            7   vc      _  _
12  terrassen         terras            N     N     soort|mv|neut                    11  cnj     _  _
13  .                 .                 Punc  Punc  punt                             12  punct   _  _
""""""
)
tree = dg.tree()
print(tree.pprint())
",[],0,[],/grammar.py_sdg_demo
513,/home/amandapotts/git/nltk/nltk/grammar.py_demo,"def demo():
cfg_demo()
pcfg_demo()
fcfg_demo()
dg_demo()
sdg_demo()
",[],0,[],/grammar.py_demo
514,/home/amandapotts/git/nltk/nltk/__init__.py__fake_PIPE,"def _fake_PIPE(*args, **kwargs):
raise NotImplementedError(""subprocess.PIPE is not supported."")
",[],0,[],/__init__.py__fake_PIPE
515,/home/amandapotts/git/nltk/nltk/__init__.py__fake_Popen,"def _fake_Popen(*args, **kwargs):
raise NotImplementedError(""subprocess.Popen is not supported."")
",[],0,[],/__init__.py__fake_Popen
516,/home/amandapotts/git/nltk/nltk/__init__.py_demo,"def demo():
print(""To run the demo code for a module, type nltk.module.demo()"")
",[],0,[],/__init__.py_demo
517,/home/amandapotts/git/nltk/nltk/wsd.py_lesk,"def lesk(context_sentence, ambiguous_word, pos=None, synsets=None, lang=""eng""):
""""""Return a synset for an ambiguous word in a context.
:param iter context_sentence: The context sentence where the ambiguous word
occurs, passed as an iterable of words.
:param str ambiguous_word: The ambiguous word that requires WSD.
:param str pos: A specified Part-of-Speech (POS).
:param iter synsets: Possible synsets of the ambiguous word.
:param str lang: WordNet language.
:return: ``lesk_sense`` The Synset() object with the highest signature overlaps.
This function is an implementation of the original Lesk algorithm (1986) [1].
Usage example::
>>> lesk(['I', 'went', 'to', 'the', 'bank', 'to', 'deposit', 'money', '.'], 'bank', 'n')
Synset('savings_bank.n.02')
[1] Lesk, Michael. ""Automatic sense disambiguation using machine
readable dictionaries: how to tell a pine cone from an ice cream
cone."" Proceedings of the 5th Annual International Conference on
Systems Documentation. ACM, 1986.
https://dl.acm.org/citation.cfm?id=318728
""""""
context = set(context_sentence)
if synsets is None:
synsets = wordnet.synsets(ambiguous_word, lang=lang)
if pos:
synsets = [ss for ss in synsets if str(ss.pos()) == pos]
if not synsets:
return None
_, sense = max(
(len(context.intersection(ss.definition().split())), ss) for ss in synsets
)
return sense
",[],0,[],/wsd.py_lesk
518,/home/amandapotts/git/nltk/nltk/cli.py_cli,"def cli():
pass
",[],0,[],/cli.py_cli
519,/home/amandapotts/git/nltk/nltk/cli.py_tokenize_file,"def tokenize_file(language, preserve_line, processes, encoding, delimiter):
""""""This command tokenizes text stream using nltk.word_tokenize""""""
with click.get_text_stream(""stdin"", encoding=encoding) as fin:
with click.get_text_stream(""stdout"", encoding=encoding) as fout:
if processes == 1:
for line in tqdm(fin.readlines()):
print(delimiter.join(word_tokenize(line)), end=""\n"", file=fout)
else:
for outline in parallelize_preprocess(
word_tokenize, fin.readlines(), processes, progress_bar=True
):
print(delimiter.join(outline), end=""\n"", file=fout)
",[],0,[],/cli.py_tokenize_file
520,/home/amandapotts/git/nltk/nltk/lazyimport.py___init__,"def __init__(self, name, locals, globals=None):
""""""Create a LazyModule instance wrapping module name.
The module will later on be registered in locals under the
given module name.
globals is optional and defaults to locals.
""""""
self.__lazymodule_locals = locals
if globals is None:
globals = locals
self.__lazymodule_globals = globals
mainname = globals.get(""__name__"", """")
if mainname:
self.__name__ = mainname + ""."" + name
self.__lazymodule_name = name
else:
self.__name__ = self.__lazymodule_name = name
self.__lazymodule_init = 1
",[],0,[],/lazyimport.py___init__
521,/home/amandapotts/git/nltk/nltk/lazyimport.py___lazymodule_import,"def __lazymodule_import(self):
""""""Import the module now.""""""
local_name = self.__lazymodule_name  # e.g. ""toolbox""
full_name = self.__name__  # e.g. ""nltk.toolbox""
if self.__lazymodule_loaded:
return self.__lazymodule_locals[local_name]
if _debug:
print(""LazyModule: Loading module %r"" % full_name)
self.__lazymodule_locals[local_name] = module = __import__(
full_name, self.__lazymodule_locals, self.__lazymodule_globals, ""*""
)
self.__dict__.update(module.__dict__)
self.__dict__[""__lazymodule_loaded""] = 1
if _debug:
print(""LazyModule: Module %r loaded"" % full_name)
return module
",[],0,[],/lazyimport.py___lazymodule_import
522,/home/amandapotts/git/nltk/nltk/lazyimport.py___getattr__,"def __getattr__(self, name):
""""""Import the module on demand and get the attribute.""""""
if self.__lazymodule_loaded:
raise AttributeError(name)
if _debug:
print(
""LazyModule: ""
""Module load triggered by attribute %r read access"" % name
)
module = self.__lazymodule_import()
return getattr(module, name)
",[],0,[],/lazyimport.py___getattr__
523,/home/amandapotts/git/nltk/nltk/lazyimport.py___setattr__,"def __setattr__(self, name, value):
""""""Import the module on demand and set the attribute.""""""
if not self.__lazymodule_init:
self.__dict__[name] = value
return
if self.__lazymodule_loaded:
self.__lazymodule_locals[self.__lazymodule_name] = value
self.__dict__[name] = value
return
if _debug:
print(
""LazyModule: ""
""Module load triggered by attribute %r write access"" % name
)
module = self.__lazymodule_import()
setattr(module, name, value)
",[],0,[],/lazyimport.py___setattr__
524,/home/amandapotts/git/nltk/nltk/lazyimport.py___repr__,"def __repr__(self):
return ""<LazyModule '%s'>"" % self.__name__
",[],0,[],/lazyimport.py___repr__
525,/home/amandapotts/git/nltk/nltk/help.py_brown_tagset,"def brown_tagset(tagpattern=None):
_format_tagset(""brown_tagset"", tagpattern)
",[],0,[],/help.py_brown_tagset
526,/home/amandapotts/git/nltk/nltk/help.py_claws5_tagset,"def claws5_tagset(tagpattern=None):
_format_tagset(""claws5_tagset"", tagpattern)
",[],0,[],/help.py_claws5_tagset
527,/home/amandapotts/git/nltk/nltk/help.py_upenn_tagset,"def upenn_tagset(tagpattern=None):
_format_tagset(""upenn_tagset"", tagpattern)
",[],0,[],/help.py_upenn_tagset
528,/home/amandapotts/git/nltk/nltk/help.py__print_entries,"def _print_entries(tags, tagdict):
for tag in tags:
entry = tagdict[tag]
defn = [tag + "": "" + entry[0]]
examples = wrap(
entry[1], width=75, initial_indent=""    "", subsequent_indent=""    ""
)
print(""\n"".join(defn + examples))
",[],0,[],/help.py__print_entries
529,/home/amandapotts/git/nltk/nltk/help.py__format_tagset,"def _format_tagset(tagset, tagpattern=None):
tagdict = load(""help/tagsets/"" + tagset + "".pickle"")
if not tagpattern:
_print_entries(sorted(tagdict), tagdict)
elif tagpattern in tagdict:
_print_entries([tagpattern], tagdict)
else:
tagpattern = re.compile(tagpattern)
tags = [tag for tag in sorted(tagdict) if tagpattern.match(tag)]
if tags:
_print_entries(tags, tagdict)
else:
print(""No matching tags found."")
",[],0,[],/help.py__format_tagset
530,/home/amandapotts/git/nltk/nltk/util.py_usage,"def usage(obj):
str(obj)  # In case it's lazy, this will load it.
if not isinstance(obj, type):
obj = obj.__class__
print(f""{obj.__name__} supports the following operations:"")
for name, method in sorted(pydoc.allmethods(obj).items()):
if name.startswith(""_""):
continue
if getattr(method, ""__deprecated__"", False):
continue
try:
sig = str(inspect.signature(method))
except ValueError as e:
if ""builtin"" in str(e):
continue
else:
raise
args = sig.lstrip(""("").rstrip("")"").split("", "")
meth = inspect.getattr_static(obj, name)
if isinstance(meth, (classmethod, staticmethod)):
name = f""cls.{name}""
elif args and args[0] == ""self"":
name = f""self.{name}""
args.pop(0)
print(
textwrap.fill(
f""{name}({', '.join(args)})"",
initial_indent=""  - "",
subsequent_indent="" "" * (len(name) + 5),
)
)
",[],0,[],/util.py_usage
531,/home/amandapotts/git/nltk/nltk/util.py_in_idle,"def in_idle():
""""""
Return True if this function is run within idle.  Tkinter
programs that are run in idle should never call ``Tk.mainloop``
this function should be used to gate all calls to ``Tk.mainloop``.
:warning: This function works by checking ``sys.stdin``.  If the
user has modified ``sys.stdin``, then it may return incorrect
results.
:rtype: bool
""""""
import sys
return sys.stdin.__class__.__name__ in (""PyShell"", ""RPCProxy"")
",[],0,[],/util.py_in_idle
532,/home/amandapotts/git/nltk/nltk/util.py_pr,"def pr(data, start=0, end=None):
""""""
Pretty print a sequence of data items
:param data: the data stream to print
:type data: sequence or iter
:param start: the start position
:type start: int
:param end: the end position
:type end: int
""""""
pprint(list(islice(data, start, end)))
",[],0,[],/util.py_pr
533,/home/amandapotts/git/nltk/nltk/util.py_print_string,"def print_string(s, width=70):
""""""
Pretty print a string, breaking lines on whitespace
:param s: the string to print, consisting of words and spaces
:type s: str
:param width: the display width
:type width: int
""""""
print(""\n"".join(textwrap.wrap(s, width=width)))
",[],0,[],/util.py_print_string
534,/home/amandapotts/git/nltk/nltk/util.py_tokenwrap,"def tokenwrap(tokens, separator="" "", width=70):
""""""
Pretty print a list of text tokens, breaking lines on whitespace
:param tokens: the tokens to print
:type tokens: list
:param separator: the string to use to separate tokens
:type separator: str
:param width: the display width (default=70)
:type width: int
""""""
return ""\n"".join(textwrap.wrap(separator.join(tokens), width=width))
",[],0,[],/util.py_tokenwrap
535,/home/amandapotts/git/nltk/nltk/util.py_cut_string,"def cut_string(s, width=70):
""""""
Cut off and return a given width of a string
Return the same as s[:width] if width >= 0 or s[-width:] if
width < 0, as long as s has no unicode combining characters.
If it has combining characters make sure the returned string's
visible width matches the called-for width.
:param s: the string to cut
:type s: str
:param width: the display_width
:type width: int
""""""
chars_sofar = 0
width_sofar = 0
result = """"
abs_width = abs(width)
max_chars = len(s)
while width_sofar < abs_width and chars_sofar < max_chars:
if width < 0:
char = s[-(chars_sofar + 1)]
result = char + result
else:
char = s[chars_sofar]
result = result + char
chars_sofar += 1
if not unicodedata.combining(char):
width_sofar += 1
return result
",[],0,[],/util.py_cut_string
536,/home/amandapotts/git/nltk/nltk/util.py___init__,"def __init__(self, pairs):
defaultdict.__init__(self, list)
for key, value in pairs:
self[key].append(value)
",[],0,[],/util.py___init__
537,/home/amandapotts/git/nltk/nltk/util.py_re_show,"def re_show(regexp, string, left=""{"", right=""}""):
""""""
Return a string with markers surrounding the matched substrings.
Search str for substrings matching ``regexp`` and wrap the matches
with braces.  This is convenient for learning about regular expressions.
:param regexp: The regular expression.
:type regexp: str
:param string: The string being matched.
:type string: str
:param left: The left delimiter (printed before the matched substring)
:type left: str
:param right: The right delimiter (printed after the matched substring)
:type right: str
:rtype: str
""""""
print(re.compile(regexp, re.M).sub(left + r""\g<0>"" + right, string.rstrip()))
",[],0,[],/util.py_re_show
538,/home/amandapotts/git/nltk/nltk/util.py_filestring,"def filestring(f):
if hasattr(f, ""read""):
return f.read()
elif isinstance(f, str):
with open(f) as infile:
return infile.read()
else:
raise ValueError(""Must be called with a filename or file-like object"")
",[],0,[],/util.py_filestring
539,/home/amandapotts/git/nltk/nltk/util.py_breadth_first,"def breadth_first(tree, children=iter, maxdepth=-1):
""""""Traverse the nodes of a tree in breadth-first order.
(No check for cycles.)
The first argument should be the tree root
children should be a function taking as argument a tree node
and returning an iterator of the node's children.
""""""
queue = deque([(tree, 0)])
while queue:
node, depth = queue.popleft()
yield node
if depth != maxdepth:
try:
queue.extend((c, depth + 1) for c in children(node))
except TypeError:
pass
",[],0,[],/util.py_breadth_first
540,/home/amandapotts/git/nltk/nltk/util.py_edges2dot,"def edges2dot(edges, shapes=None, attr=None):
""""""
:param edges: the set (or list) of edges of a directed graph.
:return dot_string: a representation of 'edges' as a string in the DOT
graph language, which can be converted to an image by the 'dot' program
from the Graphviz package, or nltk.parse.dependencygraph.dot2img(dot_string).
:param shapes: dictionary of strings that trigger a specified shape.
:param attr: dictionary with global graph attributes
>>> import nltk
>>> from nltk.util import edges2dot
>>> print(edges2dot([('A', 'B'), ('A', 'C'), ('B', 'C'), ('C', 'B')]))
digraph G {
""A"" -> ""B""
""A"" -> ""C""
""B"" -> ""C""
""C"" -> ""B""
}
<BLANKLINE>
""""""
if not shapes:
shapes = dict()
if not attr:
attr = dict()
dot_string = ""digraph G {\n""
for pair in attr.items():
dot_string += f""{pair[0]} = {pair[1]}
for edge in edges:
for shape in shapes.items():
for node in range(2):
if shape[0] in repr(edge[node]):
dot_string += f'""{edge[node]}"" [shape = {shape[1]}]
dot_string += f'""{edge[0]}"" -> ""{edge[1]}""
dot_string += ""}\n""
return dot_string
",[],0,[],/util.py_edges2dot
541,/home/amandapotts/git/nltk/nltk/util.py_acyclic_breadth_first,"def acyclic_breadth_first(tree, children=iter, maxdepth=-1):
""""""Traverse the nodes of a tree in breadth-first order,
discarding eventual cycles.
The first argument should be the tree root
children should be a function taking as argument a tree node
and returning an iterator of the node's children.
""""""
traversed = set()
queue = deque([(tree, 0)])
while queue:
node, depth = queue.popleft()
yield node
traversed.add(node)
if depth != maxdepth:
try:
for child in children(node):
if child not in traversed:
queue.append((child, depth + 1))
else:
warnings.warn(
""Discarded redundant search for {} at depth {}"".format(
child, depth + 1
),
stacklevel=2,
)
except TypeError:
pass
",[],0,[],/util.py_acyclic_breadth_first
542,/home/amandapotts/git/nltk/nltk/util.py_acyclic_dic2tree,"def acyclic_dic2tree(node, dic):
""""""Convert acyclic dictionary 'dic', where the keys are nodes, and the
values are lists of children, to output tree suitable for pprint(),
starting at root 'node', with subtrees as nested lists.""""""
return [node] + [acyclic_dic2tree(child, dic) for child in dic[node]]
",[],0,[],/util.py_acyclic_dic2tree
543,/home/amandapotts/git/nltk/nltk/util.py_guess_encoding,"def guess_encoding(data):
""""""
Given a byte string, attempt to decode it.
Tries the standard 'UTF8' and 'latin-1' encodings,
Plus several gathered from locale information.
The calling program *must* first call::
locale.setlocale(locale.LC_ALL, '')
If successful it returns ``(decoded_unicode, successful_encoding)``.
If unsuccessful it raises a ``UnicodeError``.
""""""
successful_encoding = None
encodings = [""utf-8""]
try:
encodings.append(locale.nl_langinfo(locale.CODESET))
except AttributeError:
pass
try:
encodings.append(locale.getlocale()[1])
except (AttributeError, IndexError):
pass
try:
encodings.append(locale.getdefaultlocale()[1])
except (AttributeError, IndexError):
pass
encodings.append(""latin-1"")
for enc in encodings:
if not enc:
continue
try:
decoded = str(data, enc)
successful_encoding = enc
except (UnicodeError, LookupError):
pass
else:
break
if not successful_encoding:
raise UnicodeError(
""Unable to decode input data. ""
""Tried the following encodings: %s.""
% "", "".join([repr(enc) for enc in encodings if enc])
)
else:
return (decoded, successful_encoding)
",[],0,[],/util.py_guess_encoding
544,/home/amandapotts/git/nltk/nltk/util.py_unique_list,"def unique_list(xs):
seen = set()
return [x for x in xs if x not in seen and not seen.add(x)]
",[],0,[],/util.py_unique_list
545,/home/amandapotts/git/nltk/nltk/util.py_invert_dict,"def invert_dict(d):
inverted_dict = defaultdict(list)
for key in d:
if hasattr(d[key], ""__iter__""):
for term in d[key]:
inverted_dict[term].append(key)
else:
inverted_dict[d[key]] = key
return inverted_dict
",[],0,[],/util.py_invert_dict
546,/home/amandapotts/git/nltk/nltk/util.py_transitive_closure,"def transitive_closure(graph, reflexive=False):
""""""
Calculate the transitive closure of a directed graph,
optionally the reflexive transitive closure.
The algorithm is a slight modification of the ""Marking Algorithm"" of
Ioannidis & Ramakrishnan (1998) ""Efficient Transitive Closure Algorithms"".
:param graph: the initial graph, represented as a dictionary of sets
:type graph: dict(set)
:param reflexive: if set, also make the closure reflexive
:type reflexive: bool
:rtype: dict(set)
""""""
if reflexive:
",[],0,[],/util.py_transitive_closure
547,/home/amandapotts/git/nltk/nltk/util.py_invert_graph,"def invert_graph(graph):
""""""
Inverts a directed graph.
:param graph: the graph, represented as a dictionary of sets
:type graph: dict(set)
:return: the inverted graph
:rtype: dict(set)
""""""
inverted = {}
for key in graph:
for value in graph[key]:
inverted.setdefault(value, set()).add(key)
return inverted
",[],0,[],/util.py_invert_graph
548,/home/amandapotts/git/nltk/nltk/util.py_clean_html,"def clean_html(html):
raise NotImplementedError(
""To remove HTML markup, use BeautifulSoup's get_text() function""
)
",[],0,[],/util.py_clean_html
549,/home/amandapotts/git/nltk/nltk/util.py_clean_url,"def clean_url(url):
raise NotImplementedError(
""To remove HTML markup, use BeautifulSoup's get_text() function""
)
",[],0,[],/util.py_clean_url
550,/home/amandapotts/git/nltk/nltk/util.py_flatten,"def flatten(*args):
""""""
Flatten a list.
>>> from nltk.util import flatten
>>> flatten(1, 2, ['b', 'a' , ['c', 'd']], 3)
[1, 2, 'b', 'a', 'c', 'd', 3]
:param args: items and lists to be combined into a single list
:rtype: list
""""""
x = []
for l in args:
if not isinstance(l, (list, tuple)):
l = [l]
for item in l:
if isinstance(item, (list, tuple)):
x.extend(flatten(item))
else:
x.append(item)
return x
",[],0,[],/util.py_flatten
551,/home/amandapotts/git/nltk/nltk/util.py_pad_sequence,"def pad_sequence(
sequence,
n,
pad_left=False,
pad_right=False,
left_pad_symbol=None,
right_pad_symbol=None,
",[],0,[],/util.py_pad_sequence
552,/home/amandapotts/git/nltk/nltk/util.py_ngrams,"def ngrams(sequence, n, **kwargs):
""""""
Return the ngrams generated from a sequence of items, as an iterator.
For example:
>>> from nltk.util import ngrams
>>> list(ngrams([1,2,3,4,5], 3))
[(1, 2, 3), (2, 3, 4), (3, 4, 5)]
Wrap with list for a list version of this function.  Set pad_left
or pad_right to true in order to get additional ngrams:
>>> list(ngrams([1,2,3,4,5], 2, pad_right=True))
[(1, 2), (2, 3), (3, 4), (4, 5), (5, None)]
>>> list(ngrams([1,2,3,4,5], 2, pad_right=True, right_pad_symbol='</s>'))
[(1, 2), (2, 3), (3, 4), (4, 5), (5, '</s>')]
>>> list(ngrams([1,2,3,4,5], 2, pad_left=True, left_pad_symbol='<s>'))
[('<s>', 1), (1, 2), (2, 3), (3, 4), (4, 5)]
>>> list(ngrams([1,2,3,4,5], 2, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))
[('<s>', 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, '</s>')]
:param sequence: the source data to be converted into ngrams
:type sequence: sequence or iter
:param n: the degree of the ngrams
:type n: int
:param pad_left: whether the ngrams should be left-padded
:type pad_left: bool
:param pad_right: whether the ngrams should be right-padded
:type pad_right: bool
:param left_pad_symbol: the symbol to use for left padding (default is None)
:type left_pad_symbol: any
:param right_pad_symbol: the symbol to use for right padding (default is None)
:type right_pad_symbol: any
:rtype: sequence or iter
""""""
sequence = pad_sequence(sequence, n, **kwargs)
it = iter(sequence)
window = deque(islice(it, n), maxlen=n)
if len(window) == n:
yield tuple(window)
for x in it:
window.append(x)
yield tuple(window)
",[],0,[],/util.py_ngrams
553,/home/amandapotts/git/nltk/nltk/util.py_bigrams,"def bigrams(sequence, **kwargs):
""""""
Return the bigrams generated from a sequence of items, as an iterator.
For example:
>>> from nltk.util import bigrams
>>> list(bigrams([1,2,3,4,5]))
[(1, 2), (2, 3), (3, 4), (4, 5)]
Use bigrams for a list version of this function.
:param sequence: the source data to be converted into bigrams
:type sequence: sequence or iter
:rtype: iter(tuple)
""""""
yield from ngrams(sequence, 2, **kwargs)
",[],0,[],/util.py_bigrams
554,/home/amandapotts/git/nltk/nltk/util.py_trigrams,"def trigrams(sequence, **kwargs):
""""""
Return the trigrams generated from a sequence of items, as an iterator.
For example:
>>> from nltk.util import trigrams
>>> list(trigrams([1,2,3,4,5]))
[(1, 2, 3), (2, 3, 4), (3, 4, 5)]
Use trigrams for a list version of this function.
:param sequence: the source data to be converted into trigrams
:type sequence: sequence or iter
:rtype: iter(tuple)
""""""
yield from ngrams(sequence, 3, **kwargs)
",[],0,[],/util.py_trigrams
555,/home/amandapotts/git/nltk/nltk/util.py_everygrams,"def everygrams(
sequence, min_len=1, max_len=-1, pad_left=False, pad_right=False, **kwargs
",[],0,[],/util.py_everygrams
556,/home/amandapotts/git/nltk/nltk/util.py_skipgrams,"def skipgrams(sequence, n, k, **kwargs):
""""""
Returns all possible skipgrams generated from a sequence of items, as an iterator.
Skipgrams are ngrams that allows tokens to be skipped.
Refer to http://homepages.inf.ed.ac.uk/ballison/pdf/lrec_skipgrams.pdf
>>> sent = ""Insurgents killed in ongoing fighting"".split()
>>> list(skipgrams(sent, 2, 2))
[('Insurgents', 'killed'), ('Insurgents', 'in'), ('Insurgents', 'ongoing'), ('killed', 'in'), ('killed', 'ongoing'), ('killed', 'fighting'), ('in', 'ongoing'), ('in', 'fighting'), ('ongoing', 'fighting')]
>>> list(skipgrams(sent, 3, 2))
[('Insurgents', 'killed', 'in'), ('Insurgents', 'killed', 'ongoing'), ('Insurgents', 'killed', 'fighting'), ('Insurgents', 'in', 'ongoing'), ('Insurgents', 'in', 'fighting'), ('Insurgents', 'ongoing', 'fighting'), ('killed', 'in', 'ongoing'), ('killed', 'in', 'fighting'), ('killed', 'ongoing', 'fighting'), ('in', 'ongoing', 'fighting')]
:param sequence: the source data to be converted into trigrams
:type sequence: sequence or iter
:param n: the degree of the ngrams
:type n: int
:param k: the skip distance
:type  k: int
:rtype: iter(tuple)
""""""
if ""pad_left"" in kwargs or ""pad_right"" in kwargs:
sequence = pad_sequence(sequence, n, **kwargs)
SENTINEL = object()
for ngram in ngrams(sequence, n + k, pad_right=True, right_pad_symbol=SENTINEL):
head = ngram[:1]
tail = ngram[1:]
for skip_tail in combinations(tail, n - 1):
if skip_tail[-1] is SENTINEL:
continue
yield head + skip_tail
",[],0,[],/util.py_skipgrams
557,/home/amandapotts/git/nltk/nltk/util.py_binary_search_file,"def binary_search_file(file, key, cache=None, cacheDepth=-1):
""""""
Return the line from the file with first word key.
Searches through a sorted file using the binary search algorithm.
:type file: file
:param file: the file to be searched through.
:type key: str
:param key: the identifier we are searching for.
""""""
key = key + "" ""
keylen = len(key)
start = 0
currentDepth = 0
if hasattr(file, ""name""):
end = os.stat(file.name).st_size - 1
else:
file.seek(0, 2)
end = file.tell() - 1
file.seek(0)
if cache is None:
cache = {}
while start < end:
lastState = start, end
middle = (start + end) // 2
if cache.get(middle):
offset, line = cache[middle]
else:
line = """"
while True:
file.seek(max(0, middle - 1))
if middle > 0:
file.discard_line()
offset = file.tell()
line = file.readline()
if line != """":
break
middle = (start + middle) // 2
if middle == end - 1:
return None
if currentDepth < cacheDepth:
cache[middle] = (offset, line)
if offset > end:
assert end != middle - 1, ""infinite loop""
end = middle - 1
elif line[:keylen] == key:
return line
elif line > key:
assert end != middle - 1, ""infinite loop""
end = middle - 1
elif line < key:
start = offset + len(line) - 1
currentDepth += 1
thisState = start, end
if lastState == thisState:
return None
return None
",[],0,[],/util.py_binary_search_file
558,/home/amandapotts/git/nltk/nltk/util.py_set_proxy,"def set_proxy(proxy, user=None, password=""""):
""""""
Set the HTTP proxy for Python to download through.
If ``proxy`` is None then tries to set proxy from environment or system
settings.
:param proxy: The HTTP proxy server to use. For example:
'http://proxy.example.com:3128/'
:param user: The username to authenticate with. Use None to disable
authentication.
:param password: The password to authenticate with.
""""""
if proxy is None:
try:
proxy = getproxies()[""http""]
except KeyError as e:
raise ValueError(""Could not detect default proxy settings"") from e
proxy_handler = ProxyHandler({""https"": proxy, ""http"": proxy})
opener = build_opener(proxy_handler)
if user is not None:
password_manager = HTTPPasswordMgrWithDefaultRealm()
password_manager.add_password(realm=None, uri=proxy, user=user, passwd=password)
opener.add_handler(ProxyBasicAuthHandler(password_manager))
opener.add_handler(ProxyDigestAuthHandler(password_manager))
install_opener(opener)
",[],0,[],/util.py_set_proxy
559,/home/amandapotts/git/nltk/nltk/util.py_elementtree_indent,"def elementtree_indent(elem, level=0):
""""""
Recursive function to indent an ElementTree._ElementInterface
used for pretty printing. Run indent on elem and then output
in the normal way.
:param elem: element to be indented. will be modified.
:type elem: ElementTree._ElementInterface
:param level: level of indentation for this element
:type level: nonnegative integer
:rtype:   ElementTree._ElementInterface
:return:  Contents of elem indented to reflect its structure
""""""
i = ""\n"" + level * ""  ""
if len(elem):
if not elem.text or not elem.text.strip():
elem.text = i + ""  ""
for elem in elem:
elementtree_indent(elem, level + 1)
if not elem.tail or not elem.tail.strip():
elem.tail = i
else:
if level and (not elem.tail or not elem.tail.strip()):
elem.tail = i
",[],0,[],/util.py_elementtree_indent
560,/home/amandapotts/git/nltk/nltk/util.py_choose,"def choose(n, k):
""""""
This function is a fast way to calculate binomial coefficients, commonly
known as nCk, i.e. the number of combinations of n things taken k at a time.
(https://en.wikipedia.org/wiki/Binomial_coefficient).
This is the *scipy.special.comb()* with long integer computation but this
approximation is faster, see https://github.com/nltk/nltk/issues/1181
>>> choose(4, 2)
6
>>> choose(6, 2)
15
:param n: The number of things.
:type n: int
:param r: The number of times a thing is taken.
:type r: int
""""""
if 0 <= k <= n:
ntok, ktok = 1, 1
for t in range(1, min(k, n - k) + 1):
ntok *= n
ktok *= t
n -= 1
return ntok // ktok
else:
return 0
",[],0,[],/util.py_choose
561,/home/amandapotts/git/nltk/nltk/util.py_pairwise,"def pairwise(iterable):
""""""s -> (s0,s1), (s1,s2), (s2, s3), ...""""""
a, b = tee(iterable)
next(b, None)
return zip(a, b)
",[],0,[],/util.py_pairwise
562,/home/amandapotts/git/nltk/nltk/util.py_parallelize_preprocess,"def parallelize_preprocess(func, iterator, processes, progress_bar=False):
from joblib import Parallel, delayed
from tqdm import tqdm
iterator = tqdm(iterator) if progress_bar else iterator
if processes <= 1:
return map(func, iterator)
return Parallel(n_jobs=processes)(delayed(func)(line) for line in iterator)
",[],0,[],/util.py_parallelize_preprocess
563,/home/amandapotts/git/nltk/nltk/toolbox.py___init__,"def __init__(self, filename=None, encoding=None):
self._encoding = encoding
if filename is not None:
self.open(filename)
",[],0,[],/toolbox.py___init__
564,/home/amandapotts/git/nltk/nltk/toolbox.py_open,"def open(self, sfm_file):
""""""
Open a standard format marker file for sequential reading.
:param sfm_file: name of the standard format marker input file
:type sfm_file: str
""""""
if isinstance(sfm_file, PathPointer):
self._file = sfm_file.open(self._encoding)
else:
self._file = codecs.open(sfm_file, ""r"", self._encoding)
",[],0,[],/toolbox.py_open
565,/home/amandapotts/git/nltk/nltk/toolbox.py_open_string,"def open_string(self, s):
""""""
Open a standard format marker string for sequential reading.
:param s: string to parse as a standard format marker input file
:type s: str
""""""
self._file = StringIO(s)
",[],0,[],/toolbox.py_open_string
566,/home/amandapotts/git/nltk/nltk/toolbox.py_raw_fields,"def raw_fields(self):
""""""
Return an iterator that returns the next field in a (marker, value)
tuple. Linebreaks and trailing white space are preserved except
for the final newline in each field.
:rtype: iter(tuple(str, str))
""""""
join_string = ""\n""
line_regexp = r""^%s(?:\\(\S+)\s*)?(.*)$""
first_line_pat = re.compile(line_regexp % ""(?:\xef\xbb\xbf)?"")
line_pat = re.compile(line_regexp % """")
file_iter = iter(self._file)
try:
line = next(file_iter)
except StopIteration:
return
mobj = re.match(first_line_pat, line)
mkr, line_value = mobj.groups()
value_lines = [line_value]
self.line_num = 0
for line in file_iter:
self.line_num += 1
mobj = re.match(line_pat, line)
line_mkr, line_value = mobj.groups()
if line_mkr:
yield (mkr, join_string.join(value_lines))
mkr = line_mkr
value_lines = [line_value]
else:
value_lines.append(line_value)
self.line_num += 1
yield (mkr, join_string.join(value_lines))
",[],0,[],/toolbox.py_raw_fields
567,/home/amandapotts/git/nltk/nltk/toolbox.py_fields,"def fields(
self,
strip=True,
unwrap=True,
encoding=None,
errors=""strict"",
unicode_fields=None,
",[],0,[],/toolbox.py_fields
568,/home/amandapotts/git/nltk/nltk/toolbox.py_close,"def close(self):
""""""Close a previously opened standard format marker file or string.""""""
self._file.close()
try:
del self.line_num
except AttributeError:
pass
",[],0,[],/toolbox.py_close
569,/home/amandapotts/git/nltk/nltk/toolbox.py_parse,"def parse(self, grammar=None, **kwargs):
if grammar:
return self._chunk_parse(grammar=grammar, **kwargs)
else:
return self._record_parse(**kwargs)
",[],0,[],/toolbox.py_parse
570,/home/amandapotts/git/nltk/nltk/toolbox.py__record_parse,"def _record_parse(self, key=None, **kwargs):
r""""""
Returns an element tree structure corresponding to a toolbox data file with
all markers at the same level.
Thus the following Toolbox database::
\_sh v3.0  400  Rotokas Dictionary
\_DateStampHasFourDigitYear
\lx kaa
\ps V.A
\ge gag
\gp nek i pas
\lx kaa
\ps V.B
\ge strangle
\gp pasim nek
after parsing will end up with the same structure (ignoring the extra
whitespace) as the following XML fragment after being parsed by
ElementTree::
<toolbox_data>
<header>
<_sh>v3.0  400  Rotokas Dictionary</_sh>
<_DateStampHasFourDigitYear/>
</header>
<record>
<lx>kaa</lx>
<ps>V.A</ps>
<ge>gag</ge>
<gp>nek i pas</gp>
</record>
<record>
<lx>kaa</lx>
<ps>V.B</ps>
<ge>strangle</ge>
<gp>pasim nek</gp>
</record>
</toolbox_data>
:param key: Name of key marker at the start of each record. If set to
None (the default value) the first marker that doesn't begin with
an underscore is assumed to be the key.
:type key: str
:param kwargs: Keyword arguments passed to ``StandardFormat.fields()``
:type kwargs: dict
:rtype: ElementTree._ElementInterface
:return: contents of toolbox data divided into header and records
""""""
builder = TreeBuilder()
builder.start(""toolbox_data"", {})
builder.start(""header"", {})
in_records = False
for mkr, value in self.fields(**kwargs):
if key is None and not in_records and mkr[0] != ""_"":
key = mkr
if mkr == key:
if in_records:
builder.end(""record"")
else:
builder.end(""header"")
in_records = True
builder.start(""record"", {})
builder.start(mkr, {})
builder.data(value)
builder.end(mkr)
if in_records:
builder.end(""record"")
else:
builder.end(""header"")
builder.end(""toolbox_data"")
return builder.close()
",[],0,[],/toolbox.py__record_parse
571,/home/amandapotts/git/nltk/nltk/toolbox.py__tree2etree,"def _tree2etree(self, parent):
from nltk.tree import Tree
root = Element(parent.label())
for child in parent:
if isinstance(child, Tree):
root.append(self._tree2etree(child))
else:
text, tag = child
e = SubElement(root, tag)
e.text = text
return root
",[],0,[],/toolbox.py__tree2etree
572,/home/amandapotts/git/nltk/nltk/toolbox.py__chunk_parse,"def _chunk_parse(self, grammar=None, root_label=""record"", trace=0, **kwargs):
""""""
Returns an element tree structure corresponding to a toolbox data file
parsed according to the chunk grammar.
:type grammar: str
:param grammar: Contains the chunking rules used to parse the
database.  See ``chunk.RegExp`` for documentation.
:type root_label: str
:param root_label: The node value that should be used for the
top node of the chunk structure.
:type trace: int
:param trace: The level of tracing that should be used when
parsing a text.  ``0`` will generate no tracing output
``1`` will generate normal tracing output
higher will generate verbose tracing output.
:type kwargs: dict
:param kwargs: Keyword arguments passed to ``toolbox.StandardFormat.fields()``
:rtype: ElementTree._ElementInterface
""""""
from nltk import chunk
from nltk.tree import Tree
cp = chunk.RegexpParser(grammar, root_label=root_label, trace=trace)
db = self.parse(**kwargs)
tb_etree = Element(""toolbox_data"")
header = db.find(""header"")
tb_etree.append(header)
for record in db.findall(""record""):
parsed = cp.parse([(elem.text, elem.tag) for elem in record])
tb_etree.append(self._tree2etree(parsed))
return tb_etree
",[],0,[],/toolbox.py__chunk_parse
573,/home/amandapotts/git/nltk/nltk/toolbox.py_to_sfm_string,"def to_sfm_string(tree, encoding=None, errors=""strict"", unicode_fields=None):
""""""
Return a string with a standard format representation of the toolbox
data in tree (tree can be a toolbox database or a single record).
:param tree: flat representation of toolbox data (whole database or single record)
:type tree: ElementTree._ElementInterface
:param encoding: Name of an encoding to use.
:type encoding: str
:param errors: Error handling scheme for codec. Same as the ``encode()``
builtin string method.
:type errors: str
:param unicode_fields:
:type unicode_fields: dict(str) or set(str)
:rtype: str
""""""
if tree.tag == ""record"":
root = Element(""toolbox_data"")
root.append(tree)
tree = root
if tree.tag != ""toolbox_data"":
raise ValueError(""not a toolbox_data element structure"")
if encoding is None and unicode_fields is not None:
raise ValueError(
""if encoding is not specified then neither should unicode_fields""
)
l = []
for rec in tree:
l.append(""\n"")
for field in rec:
mkr = field.tag
value = field.text
if encoding is not None:
if unicode_fields is not None and mkr in unicode_fields:
cur_encoding = ""utf8""
else:
cur_encoding = encoding
if re.search(_is_value, value):
l.append((f""\\{mkr} {value}\n"").encode(cur_encoding, errors))
else:
l.append((f""\\{mkr}{value}\n"").encode(cur_encoding, errors))
else:
if re.search(_is_value, value):
l.append(f""\\{mkr} {value}\n"")
else:
l.append(f""\\{mkr}{value}\n"")
return """".join(l[1:])
",[],0,[],/toolbox.py_to_sfm_string
574,/home/amandapotts/git/nltk/nltk/toolbox.py___init__,"def __init__(self):
super().__init__()
",[],0,[],/toolbox.py___init__
575,/home/amandapotts/git/nltk/nltk/toolbox.py_parse,"def parse(self, encoding=None, errors=""strict"", **kwargs):
""""""
Return the contents of toolbox settings file with a nested structure.
:param encoding: encoding used by settings file
:type encoding: str
:param errors: Error handling scheme for codec. Same as ``decode()`` builtin method.
:type errors: str
:param kwargs: Keyword arguments passed to ``StandardFormat.fields()``
:type kwargs: dict
:rtype: ElementTree._ElementInterface
""""""
builder = TreeBuilder()
for mkr, value in self.fields(encoding=encoding, errors=errors, **kwargs):
block = mkr[0]
if block in (""+"", ""-""):
mkr = mkr[1:]
else:
block = None
if block == ""+"":
builder.start(mkr, {})
builder.data(value)
elif block == ""-"":
builder.end(mkr)
else:
builder.start(mkr, {})
builder.data(value)
builder.end(mkr)
return builder.close()
",[],0,[],/toolbox.py_parse
576,/home/amandapotts/git/nltk/nltk/toolbox.py_to_settings_string,"def to_settings_string(tree, encoding=None, errors=""strict"", unicode_fields=None):
l = list()
_to_settings_string(
tree.getroot(),
l,
encoding=encoding,
errors=errors,
unicode_fields=unicode_fields,
)
return """".join(l)
",[],0,[],/toolbox.py_to_settings_string
577,/home/amandapotts/git/nltk/nltk/toolbox.py__to_settings_string,"def _to_settings_string(node, l, **kwargs):
tag = node.tag
text = node.text
if len(node) == 0:
if text:
l.append(f""\\{tag} {text}\n"")
else:
l.append(""\\%s\n"" % tag)
else:
if text:
l.append(f""\\+{tag} {text}\n"")
else:
l.append(""\\+%s\n"" % tag)
for n in node:
_to_settings_string(n, l, **kwargs)
l.append(""\\-%s\n"" % tag)
return
",[],0,[],/toolbox.py__to_settings_string
578,/home/amandapotts/git/nltk/nltk/toolbox.py_remove_blanks,"def remove_blanks(elem):
""""""
Remove all elements and subelements with no text and no child elements.
:param elem: toolbox data in an elementtree structure
:type elem: ElementTree._ElementInterface
""""""
out = list()
for child in elem:
remove_blanks(child)
if child.text or len(child) > 0:
out.append(child)
elem[:] = out
",[],0,[],/toolbox.py_remove_blanks
579,/home/amandapotts/git/nltk/nltk/toolbox.py_add_default_fields,"def add_default_fields(elem, default_fields):
""""""
Add blank elements and subelements specified in default_fields.
:param elem: toolbox data in an elementtree structure
:type elem: ElementTree._ElementInterface
:param default_fields: fields to add to each type of element and subelement
:type default_fields: dict(tuple)
""""""
for field in default_fields.get(elem.tag, []):
if elem.find(field) is None:
SubElement(elem, field)
for child in elem:
add_default_fields(child, default_fields)
",[],0,[],/toolbox.py_add_default_fields
580,/home/amandapotts/git/nltk/nltk/toolbox.py_sort_fields,"def sort_fields(elem, field_orders):
""""""
Sort the elements and subelements in order specified in field_orders.
:param elem: toolbox data in an elementtree structure
:type elem: ElementTree._ElementInterface
:param field_orders: order of fields for each type of element and subelement
:type field_orders: dict(tuple)
""""""
order_dicts = dict()
for field, order in field_orders.items():
order_dicts[field] = order_key = dict()
for i, subfield in enumerate(order):
order_key[subfield] = i
_sort_fields(elem, order_dicts)
",[],0,[],/toolbox.py_sort_fields
581,/home/amandapotts/git/nltk/nltk/toolbox.py__sort_fields,"def _sort_fields(elem, orders_dicts):
""""""sort the children of elem""""""
try:
order = orders_dicts[elem.tag]
except KeyError:
pass
else:
tmp = sorted(
((order.get(child.tag, 1e9), i), child) for i, child in enumerate(elem)
)
elem[:] = [child for key, child in tmp]
for child in elem:
if len(child):
_sort_fields(child, orders_dicts)
",[],0,[],/toolbox.py__sort_fields
582,/home/amandapotts/git/nltk/nltk/toolbox.py_add_blank_lines,"def add_blank_lines(tree, blanks_before, blanks_between):
""""""
Add blank lines before all elements and subelements specified in blank_before.
:param elem: toolbox data in an elementtree structure
:type elem: ElementTree._ElementInterface
:param blank_before: elements and subelements to add blank lines before
:type blank_before: dict(tuple)
""""""
try:
before = blanks_before[tree.tag]
between = blanks_between[tree.tag]
except KeyError:
for elem in tree:
if len(elem):
add_blank_lines(elem, blanks_before, blanks_between)
else:
last_elem = None
for elem in tree:
tag = elem.tag
if last_elem is not None and last_elem.tag != tag:
if tag in before and last_elem is not None:
e = last_elem.getiterator()[-1]
e.text = (e.text or """") + ""\n""
else:
if tag in between:
e = last_elem.getiterator()[-1]
e.text = (e.text or """") + ""\n""
if len(elem):
add_blank_lines(elem, blanks_before, blanks_between)
last_elem = elem
",[],0,[],/toolbox.py_add_blank_lines
583,/home/amandapotts/git/nltk/nltk/toolbox.py_demo,"def demo():
from itertools import islice
file_path = find(""corpora/toolbox/rotokas.dic"")
lexicon = ToolboxData(file_path).parse()
print(""first field in fourth record:"")
print(lexicon[3][0].tag)
print(lexicon[3][0].text)
print(""\nfields in sequential order:"")
for field in islice(lexicon.find(""record""), 10):
print(field.tag, field.text)
print(""\nlx fields:"")
for field in islice(lexicon.findall(""record/lx""), 10):
print(field.text)
settings = ToolboxSettings()
file_path = find(""corpora/toolbox/MDF/MDF_AltH.typ"")
settings.open(file_path)
tree = settings.parse(unwrap=False, encoding=""cp1252"")
print(tree.find(""expset/expMDF/rtfPageSetup/paperSize"").text)
settings_tree = ElementTree(tree)
print(to_settings_string(settings_tree).encode(""utf8""))
",[],0,[],/toolbox.py_demo
584,/home/amandapotts/git/nltk/nltk/compat.py_add_py3_data,"def add_py3_data(path):
for item in _PY3_DATA_UPDATES:
if item in str(path) and ""/PY3"" not in str(path):
pos = path.index(item) + len(item)
if path[pos : pos + 4] == "".zip"":
pos += 4
path = path[:pos] + ""/PY3"" + path[pos:]
break
return path
",[],0,[],/compat.py_add_py3_data
585,/home/amandapotts/git/nltk/nltk/compat.py_py3_data,"def py3_data(init_func):
",[],0,[],/compat.py_py3_data
586,/home/amandapotts/git/nltk/nltk/compat.py__decorator,"def _decorator(*args, **kwargs):
args = (args[0], add_py3_data(args[1])) + args[2:]
return init_func(*args, **kwargs)
",[],0,[],/compat.py__decorator
587,/home/amandapotts/git/nltk/nltk/langnames.py_langname,"def langname(tag, typ=""full""):
""""""
Convert a composite BCP-47 tag to a language name
>>> from nltk.langnames import langname
>>> langname('ca-Latn-ES-valencia')
'Catalan: Latin: Spain: Valencian'
>>> langname('ca-Latn-ES-valencia', typ=""short"")
'Catalan'
""""""
tags = tag.split(""-"")
code = tags[0].lower()
if codepattern.fullmatch(code):
if code in iso639retired:  # retired codes
return iso639retired[code]
elif code in iso639short:  # 3-letter codes
code2 = iso639short[code]  # convert to 2-letter code
warn(f""Shortening {code!r} to {code2!r}"", stacklevel=2)
tag = ""-"".join([code2] + tags[1:])
name = bcp47.name(tag)  # parse according to BCP-47
if typ == ""full"":
return name  # include all subtags
elif name:
return name.split("":"")[0]  # only the language subtag
else:
warn(f""Could not find code in {code!r}"", stacklevel=2)
",[],0,[],/langnames.py_langname
588,/home/amandapotts/git/nltk/nltk/langnames.py_langcode,"def langcode(name, typ=2):
""""""
Convert language name to iso639-3 language code. Returns the short 2-letter
code by default, if one is available, and the 3-letter code otherwise:
>>> from nltk.langnames import langcode
>>> langcode('Modern Greek (1453-)')
'el'
Specify 'typ=3' to get the 3-letter code:
>>> langcode('Modern Greek (1453-)', typ=3)
'ell'
""""""
if name in bcp47.langcode:
code = bcp47.langcode[name]
if typ == 3 and code in iso639long:
code = iso639long[code]  # convert to 3-letter code
return code
elif name in iso639code_retired:
return iso639code_retired[name]
else:
warn(f""Could not find language in {name!r}"", stacklevel=2)
",[],0,[],/langnames.py_langcode
589,/home/amandapotts/git/nltk/nltk/langnames.py_tag2q,"def tag2q(tag):
""""""
Convert BCP-47 tag to Wikidata Q-code
>>> tag2q('nds-u-sd-demv')
'Q4289225'
""""""
return bcp47.wiki_q[tag]
",[],0,[],/langnames.py_tag2q
590,/home/amandapotts/git/nltk/nltk/langnames.py_q2tag,"def q2tag(qcode):
""""""
Convert Wikidata Q-code to BCP-47 tag
>>> q2tag('Q4289225')
'nds-u-sd-demv'
""""""
return wiki_bcp47[qcode]
",[],0,[],/langnames.py_q2tag
591,/home/amandapotts/git/nltk/nltk/langnames.py_q2name,"def q2name(qcode, typ=""full""):
""""""
Convert Wikidata Q-code to BCP-47 (full or short) language name
>>> q2name('Q4289225')
'Low German: Mecklenburg-Vorpommern'
>>> q2name('Q4289225', ""short"")
'Low German'
""""""
return langname(q2tag(qcode), typ)
",[],0,[],/langnames.py_q2name
592,/home/amandapotts/git/nltk/nltk/langnames.py_lang2q,"def lang2q(name):
""""""
Convert simple language name to Wikidata Q-code
>>> lang2q('Low German')
'Q25433'
""""""
return tag2q(langcode(name))
",[],0,[],/langnames.py_lang2q
593,/home/amandapotts/git/nltk/nltk/langnames.py_inverse_dict,"def inverse_dict(dic):
""""""Return inverse mapping, but only if it is bijective""""""
if len(dic.keys()) == len(set(dic.values())):
return {val: key for (key, val) in dic.items()}
else:
warn(""This dictionary has no bijective inverse mapping."")
",[],0,[],/langnames.py_inverse_dict
594,/home/amandapotts/git/nltk/nltk/internals.py_config_java,"def config_java(bin=None, options=None, verbose=False):
""""""
Configure nltk's java interface, by letting nltk know where it can
find the Java binary, and what extra options (if any) should be
passed to Java when it is run.
:param bin: The full path to the Java binary.  If not specified,
then nltk will search the system for a Java binary
one is not found, it will raise a ``LookupError`` exception.
:type bin: str
:param options: A list of options that should be passed to the
Java binary when it is called.  A common value is
``'-Xmx512m'``, which tells Java binary to increase
the maximum heap size to 512 megabytes.  If no options are
specified, then do not modify the options list.
:type options: list(str)
""""""
global _java_bin, _java_options
_java_bin = find_binary(
""java"",
bin,
env_vars=[""JAVAHOME"", ""JAVA_HOME""],
verbose=verbose,
binary_names=[""java.exe""],
)
if options is not None:
if isinstance(options, str):
options = options.split()
_java_options = list(options)
",[],0,[],/internals.py_config_java
595,/home/amandapotts/git/nltk/nltk/internals.py_java,"def java(cmd, classpath=None, stdin=None, stdout=None, stderr=None, blocking=True):
""""""
Execute the given java command, by opening a subprocess that calls
Java.  If java has not yet been configured, it will be configured
by calling ``config_java()`` with no arguments.
:param cmd: The java command that should be called, formatted as
a list of strings.  Typically, the first string will be the name
of the java class
for that java class.
:type cmd: list(str)
:param classpath: A ``':'`` separated list of directories, JAR
archives, and ZIP archives to search for class files.
:type classpath: str
:param stdin: Specify the executed program's
standard input file handles, respectively.  Valid values are ``subprocess.PIPE``,
an existing file descriptor (a positive integer), an existing
file object, 'pipe', 'stdout', 'devnull' and None.  ``subprocess.PIPE`` indicates that a
new pipe to the child should be created.  With None, no
redirection will occur
inherited from the parent.  Additionally, stderr can be
``subprocess.STDOUT``, which indicates that the stderr data
from the applications should be captured into the same file
handle as for stdout.
:param stdout: Specify the executed program's standard output file
handle. See ``stdin`` for valid values.
:param stderr: Specify the executed program's standard error file
handle. See ``stdin`` for valid values.
:param blocking: If ``false``, then return immediately after
spawning the subprocess.  In this case, the return value is
the ``Popen`` object, and not a ``(stdout, stderr)`` tuple.
:return: If ``blocking=True``, then return a tuple ``(stdout,
stderr)``, containing the stdout and stderr outputs generated
by the java command if the ``stdout`` and ``stderr`` parameters
were set to ``subprocess.PIPE``
``blocking=False``, then return a ``subprocess.Popen`` object.
:raise OSError: If the java command returns a nonzero return code.
""""""
subprocess_output_dict = {
""pipe"": subprocess.PIPE,
""stdout"": subprocess.STDOUT,
""devnull"": subprocess.DEVNULL,
}
stdin = subprocess_output_dict.get(stdin, stdin)
stdout = subprocess_output_dict.get(stdout, stdout)
stderr = subprocess_output_dict.get(stderr, stderr)
if isinstance(cmd, str):
raise TypeError(""cmd should be a list of strings"")
if _java_bin is None:
config_java()
if isinstance(classpath, str):
classpaths = [classpath]
else:
classpaths = list(classpath)
classpath = os.path.pathsep.join(classpaths)
cmd = list(cmd)
cmd = [""-cp"", classpath] + cmd
cmd = [_java_bin] + _java_options + cmd
p = subprocess.Popen(cmd, stdin=stdin, stdout=stdout, stderr=stderr)
if not blocking:
return p
(stdout, stderr) = p.communicate()
if p.returncode != 0:
print(_decode_stdoutdata(stderr))
raise OSError(""Java command failed : "" + str(cmd))
return (stdout, stderr)
",[],0,[],/internals.py_java
596,/home/amandapotts/git/nltk/nltk/internals.py___init__,"def __init__(self, expected, position):
ValueError.__init__(self, expected, position)
self.expected = expected
self.position = position
",[],0,[],/internals.py___init__
597,/home/amandapotts/git/nltk/nltk/internals.py___str__,"def __str__(self):
return f""Expected {self.expected} at {self.position}""
",[],0,[],/internals.py___str__
598,/home/amandapotts/git/nltk/nltk/internals.py_read_str,"def read_str(s, start_position):
""""""
If a Python string literal begins at the specified position in the
given string, then return a tuple ``(val, end_position)``
containing the value of the string literal and the position where
it ends.  Otherwise, raise a ``ReadError``.
:param s: A string that will be checked to see if within which a
Python string literal exists.
:type s: str
:param start_position: The specified beginning position of the string ``s``
to begin regex matching.
:type start_position: int
:return: A tuple containing the matched string literal evaluated as a
string and the end position of the string literal.
:rtype: tuple(str, int)
:raise ReadError: If the ``_STRING_START_RE`` regex doesn't return a
match in ``s`` at ``start_position``, i.e., open quote. If the
``_STRING_END_RE`` regex doesn't return a match in ``s`` at the
end of the first match, i.e., close quote.
:raise ValueError: If an invalid string (i.e., contains an invalid
escape sequence) is passed into the ``eval``.
:Example:
>>> from nltk.internals import read_str
>>> read_str('""Hello"", World!', 0)
('Hello', 7)
""""""
m = _STRING_START_RE.match(s, start_position)
if not m:
raise ReadError(""open quote"", start_position)
quotemark = m.group(1)
_STRING_END_RE = re.compile(r""\\|%s"" % quotemark)
position = m.end()
while True:
match = _STRING_END_RE.search(s, position)
if not match:
raise ReadError(""close quote"", position)
if match.group(0) == ""\\"":
position = match.end() + 1
else:
break
try:
return eval(s[start_position : match.end()]), match.end()
except ValueError as e:
raise ReadError(""valid escape sequence"", start_position) from e
",[],0,[],/internals.py_read_str
599,/home/amandapotts/git/nltk/nltk/internals.py_read_int,"def read_int(s, start_position):
""""""
If an integer begins at the specified position in the given
string, then return a tuple ``(val, end_position)`` containing the
value of the integer and the position where it ends.  Otherwise,
raise a ``ReadError``.
:param s: A string that will be checked to see if within which a
Python integer exists.
:type s: str
:param start_position: The specified beginning position of the string ``s``
to begin regex matching.
:type start_position: int
:return: A tuple containing the matched integer casted to an int,
and the end position of the int in ``s``.
:rtype: tuple(int, int)
:raise ReadError: If the ``_READ_INT_RE`` regex doesn't return a
match in ``s`` at ``start_position``.
:Example:
>>> from nltk.internals import read_int
>>> read_int('42 is the answer', 0)
(42, 2)
""""""
m = _READ_INT_RE.match(s, start_position)
if not m:
raise ReadError(""integer"", start_position)
return int(m.group()), m.end()
",[],0,[],/internals.py_read_int
600,/home/amandapotts/git/nltk/nltk/internals.py_read_number,"def read_number(s, start_position):
""""""
If an integer or float begins at the specified position in the
given string, then return a tuple ``(val, end_position)``
containing the value of the number and the position where it ends.
Otherwise, raise a ``ReadError``.
:param s: A string that will be checked to see if within which a
Python number exists.
:type s: str
:param start_position: The specified beginning position of the string ``s``
to begin regex matching.
:type start_position: int
:return: A tuple containing the matched number casted to a ``float``,
and the end position of the number in ``s``.
:rtype: tuple(float, int)
:raise ReadError: If the ``_READ_NUMBER_VALUE`` regex doesn't return a
match in ``s`` at ``start_position``.
:Example:
>>> from nltk.internals import read_number
>>> read_number('Pi is 3.14159', 6)
(3.14159, 13)
""""""
m = _READ_NUMBER_VALUE.match(s, start_position)
if not m or not (m.group(1) or m.group(2)):
raise ReadError(""number"", start_position)
if m.group(2):
return float(m.group()), m.end()
else:
return int(m.group()), m.end()
",[],0,[],/internals.py_read_number
601,/home/amandapotts/git/nltk/nltk/internals.py_overridden,"def overridden(method):
""""""
:return: True if ``method`` overrides some method with the same
name in a base class.  This is typically used when defining
abstract base classes or interfaces, to allow subclasses to define
either of two related methods:
>>> class EaterI:
...     '''Subclass must define eat() or batch_eat().'''
...     def eat(self, food):
...         if overridden(self.batch_eat):
...             return self.batch_eat([food])[0]
...         else:
...             raise NotImplementedError()
...     def batch_eat(self, foods):
...         return [self.eat(food) for food in foods]
:type method: instance method
""""""
if isinstance(method, types.MethodType) and method.__self__.__class__ is not None:
name = method.__name__
funcs = [
cls.__dict__[name]
for cls in _mro(method.__self__.__class__)
if name in cls.__dict__
]
return len(funcs) > 1
else:
raise TypeError(""Expected an instance method."")
",[],0,[],/internals.py_overridden
602,/home/amandapotts/git/nltk/nltk/internals.py__mro,"def _mro(cls):
""""""
Return the method resolution order for ``cls`` -- i.e., a list
containing ``cls`` and all its base classes, in the order in which
they would be checked by ``getattr``.  For new-style classes, this
is just cls.__mro__.  For classic classes, this can be obtained by
a depth-first left-to-right traversal of ``__bases__``.
""""""
if isinstance(cls, type):
return cls.__mro__
else:
mro = [cls]
for base in cls.__bases__:
mro.extend(_mro(base))
return mro
",[],0,[],/internals.py__mro
603,/home/amandapotts/git/nltk/nltk/internals.py__add_epytext_field,"def _add_epytext_field(obj, field, message):
""""""Add an epytext @field to a given object's docstring.""""""
indent = """"
if obj.__doc__:
obj.__doc__ = obj.__doc__.rstrip() + ""\n\n""
indents = re.findall(r""(?<=\n)[ ]+(?!\s)"", obj.__doc__.expandtabs())
if indents:
indent = min(indents)
else:
obj.__doc__ = """"
obj.__doc__ += textwrap.fill(
f""@{field}: {message}"",
initial_indent=indent,
subsequent_indent=indent + ""    "",
)
",[],0,[],/internals.py__add_epytext_field
604,/home/amandapotts/git/nltk/nltk/internals.py_deprecated,"def deprecated(message):
""""""
A decorator used to mark functions as deprecated.  This will cause
a warning to be printed the when the function is used.  Usage:
>>> from nltk.internals import deprecated
>>> @deprecated('Use foo() instead')
... def bar(x):
...     print(x/10)
""""""
",[],0,[],/internals.py_deprecated
605,/home/amandapotts/git/nltk/nltk/internals.py_decorator,"def decorator(func):
msg = f""Function {func.__name__}() has been deprecated.  {message}""
msg = ""\n"" + textwrap.fill(msg, initial_indent=""  "", subsequent_indent=""  "")
",[],0,[],/internals.py_decorator
606,/home/amandapotts/git/nltk/nltk/internals.py_newFunc,"def newFunc(*args, **kwargs):
warnings.warn(msg, category=DeprecationWarning, stacklevel=2)
return func(*args, **kwargs)
",[],0,[],/internals.py_newFunc
607,/home/amandapotts/git/nltk/nltk/internals.py___new__,"def __new__(cls, *args, **kwargs):
dep_cls = None
for base in _mro(cls):
if Deprecated in base.__bases__:
dep_cls = base
break
assert dep_cls, ""Unable to determine which base is deprecated.""
doc = dep_cls.__doc__ or """".strip()
doc = re.sub(r""\A\s*@deprecated:"", r"""", doc)
doc = re.sub(r""(?m)^\s*"", """", doc)
name = ""Class %s"" % dep_cls.__name__
if cls != dep_cls:
name += "" (base class for %s)"" % cls.__name__
msg = f""{name} has been deprecated.  {doc}""
msg = ""\n"" + textwrap.fill(msg, initial_indent=""    "", subsequent_indent=""    "")
warnings.warn(msg, category=DeprecationWarning, stacklevel=2)
return object.__new__(cls)
",[],0,[],/internals.py___new__
608,/home/amandapotts/git/nltk/nltk/internals.py___init__,"def __init__(self, initial_value=0):
self._value = initial_value
",[],0,[],/internals.py___init__
609,/home/amandapotts/git/nltk/nltk/internals.py_get,"def get(self):
self._value += 1
return self._value
",[],0,[],/internals.py_get
610,/home/amandapotts/git/nltk/nltk/internals.py_find_file_iter,"def find_file_iter(
filename,
env_vars=(),
searchpath=(),
file_names=None,
url=None,
verbose=False,
finding_dir=False,
",[],0,[],/internals.py_find_file_iter
611,/home/amandapotts/git/nltk/nltk/internals.py_find_file,"def find_file(
filename, env_vars=(), searchpath=(), file_names=None, url=None, verbose=False
",[],0,[],/internals.py_find_file
612,/home/amandapotts/git/nltk/nltk/internals.py_find_dir,"def find_dir(
filename, env_vars=(), searchpath=(), file_names=None, url=None, verbose=False
",[],0,[],/internals.py_find_dir
613,/home/amandapotts/git/nltk/nltk/internals.py_find_binary_iter,"def find_binary_iter(
name,
path_to_bin=None,
env_vars=(),
searchpath=(),
binary_names=None,
url=None,
verbose=False,
",[],0,[],/internals.py_find_binary_iter
614,/home/amandapotts/git/nltk/nltk/internals.py_find_binary,"def find_binary(
name,
path_to_bin=None,
env_vars=(),
searchpath=(),
binary_names=None,
url=None,
verbose=False,
",[],0,[],/internals.py_find_binary
615,/home/amandapotts/git/nltk/nltk/internals.py_find_jar_iter,"def find_jar_iter(
name_pattern,
path_to_jar=None,
env_vars=(),
searchpath=(),
url=None,
verbose=False,
is_regex=False,
",[],0,[],/internals.py_find_jar_iter
616,/home/amandapotts/git/nltk/nltk/internals.py_find_jar,"def find_jar(
name_pattern,
path_to_jar=None,
env_vars=(),
searchpath=(),
url=None,
verbose=False,
is_regex=False,
",[],0,[],/internals.py_find_jar
617,/home/amandapotts/git/nltk/nltk/internals.py_find_jars_within_path,"def find_jars_within_path(path_to_jars):
return [
os.path.join(root, filename)
for root, dirnames, filenames in os.walk(path_to_jars)
for filename in fnmatch.filter(filenames, ""*.jar"")
]
",[],0,[],/internals.py_find_jars_within_path
618,/home/amandapotts/git/nltk/nltk/internals.py__decode_stdoutdata,"def _decode_stdoutdata(stdoutdata):
""""""Convert data read from stdout/stderr to unicode""""""
if not isinstance(stdoutdata, bytes):
return stdoutdata
encoding = getattr(sys.__stdout__, ""encoding"", locale.getpreferredencoding())
if encoding is None:
return stdoutdata.decode()
return stdoutdata.decode(encoding)
",[],0,[],/internals.py__decode_stdoutdata
619,/home/amandapotts/git/nltk/nltk/internals.py_import_from_stdlib,"def import_from_stdlib(module):
""""""
When python is run from within the nltk/ directory tree, the
current directory is included at the beginning of the search path.
Unfortunately, that means that modules within nltk can sometimes
shadow standard library modules.  As an example, the stdlib
'inspect' module will attempt to import the stdlib 'tokenize'
module, but will instead end up importing NLTK's 'tokenize' module
instead (causing the import to fail).
""""""
old_path = sys.path
sys.path = [d for d in sys.path if d not in ("""", ""."")]
m = __import__(module)
sys.path = old_path
return m
",[],0,[],/internals.py_import_from_stdlib
620,/home/amandapotts/git/nltk/nltk/internals.py___new__,"def __new__(cls, etree):
""""""
Create and return a wrapper around a given Element object.
If ``etree`` is an ``ElementWrapper``, then ``etree`` is
returned as-is.
""""""
if isinstance(etree, ElementWrapper):
return etree
else:
return object.__new__(ElementWrapper)
",[],0,[],/internals.py___new__
621,/home/amandapotts/git/nltk/nltk/internals.py___init__,"def __init__(self, etree):
r""""""
Initialize a new Element wrapper for ``etree``.
If ``etree`` is a string, then it will be converted to an
Element object using ``ElementTree.fromstring()`` first:
>>> ElementWrapper(""<test></test>"")
<Element ""<?xml version='1.0' encoding='utf8'?>\n<test />"">
""""""
if isinstance(etree, str):
etree = ElementTree.fromstring(etree)
self.__dict__[""_etree""] = etree
",[],0,[],/internals.py___init__
622,/home/amandapotts/git/nltk/nltk/internals.py_unwrap,"def unwrap(self):
""""""
Return the Element object wrapped by this wrapper.
""""""
return self._etree
",[],0,[],/internals.py_unwrap
623,/home/amandapotts/git/nltk/nltk/internals.py___repr__,"def __repr__(self):
s = ElementTree.tostring(self._etree, encoding=""utf8"").decode(""utf8"")
if len(s) > 60:
e = s.rfind(""<"")
if (len(s) - e) > 30:
e = -20
s = f""{s[:30]}...{s[e:]}""
return ""<Element %r>"" % s
",[],0,[],/internals.py___repr__
624,/home/amandapotts/git/nltk/nltk/internals.py___str__,"def __str__(self):
""""""
:return: the result of applying ``ElementTree.tostring()`` to
the wrapped Element object.
""""""
return (
ElementTree.tostring(self._etree, encoding=""utf8"").decode(""utf8"").rstrip()
)
",[],0,[],/internals.py___str__
625,/home/amandapotts/git/nltk/nltk/internals.py___getattr__,"def __getattr__(self, attrib):
return getattr(self._etree, attrib)
",[],0,[],/internals.py___getattr__
626,/home/amandapotts/git/nltk/nltk/internals.py___setattr__,"def __setattr__(self, attr, value):
return setattr(self._etree, attr, value)
",[],0,[],/internals.py___setattr__
627,/home/amandapotts/git/nltk/nltk/internals.py___delattr__,"def __delattr__(self, attr):
return delattr(self._etree, attr)
",[],0,[],/internals.py___delattr__
628,/home/amandapotts/git/nltk/nltk/internals.py___setitem__,"def __setitem__(self, index, element):
self._etree[index] = element
",[],0,[],/internals.py___setitem__
629,/home/amandapotts/git/nltk/nltk/internals.py___delitem__,"def __delitem__(self, index):
del self._etree[index]
",[],0,[],/internals.py___delitem__
630,/home/amandapotts/git/nltk/nltk/internals.py___setslice__,"def __setslice__(self, start, stop, elements):
self._etree[start:stop] = elements
",[],0,[],/internals.py___setslice__
631,/home/amandapotts/git/nltk/nltk/internals.py___delslice__,"def __delslice__(self, start, stop):
del self._etree[start:stop]
",[],0,[],/internals.py___delslice__
632,/home/amandapotts/git/nltk/nltk/internals.py___len__,"def __len__(self):
return len(self._etree)
",[],0,[],/internals.py___len__
633,/home/amandapotts/git/nltk/nltk/internals.py___getitem__,"def __getitem__(self, index):
return ElementWrapper(self._etree[index])
",[],0,[],/internals.py___getitem__
634,/home/amandapotts/git/nltk/nltk/internals.py___getslice__,"def __getslice__(self, start, stop):
return [ElementWrapper(elt) for elt in self._etree[start:stop]]
",[],0,[],/internals.py___getslice__
635,/home/amandapotts/git/nltk/nltk/internals.py_getchildren,"def getchildren(self):
return [ElementWrapper(elt) for elt in self._etree]
",[],0,[],/internals.py_getchildren
636,/home/amandapotts/git/nltk/nltk/internals.py_getiterator,"def getiterator(self, tag=None):
return (ElementWrapper(elt) for elt in self._etree.getiterator(tag))
",[],0,[],/internals.py_getiterator
637,/home/amandapotts/git/nltk/nltk/internals.py_makeelement,"def makeelement(self, tag, attrib):
return ElementWrapper(self._etree.makeelement(tag, attrib))
",[],0,[],/internals.py_makeelement
638,/home/amandapotts/git/nltk/nltk/internals.py_find,"def find(self, path):
elt = self._etree.find(path)
if elt is None:
return elt
else:
return ElementWrapper(elt)
",[],0,[],/internals.py_find
639,/home/amandapotts/git/nltk/nltk/internals.py_findall,"def findall(self, path):
return [ElementWrapper(elt) for elt in self._etree.findall(path)]
",[],0,[],/internals.py_findall
640,/home/amandapotts/git/nltk/nltk/internals.py_slice_bounds,"def slice_bounds(sequence, slice_obj, allow_step=False):
""""""
Given a slice, return the corresponding (start, stop) bounds,
taking into account None indices and negative indices.  The
following guarantees are made for the returned start and stop values:
- 0 <= start <= len(sequence)
- 0 <= stop <= len(sequence)
- start <= stop
:raise ValueError: If ``slice_obj.step`` is not None.
:param allow_step: If true, then the slice object may have a
non-None step.  If it does, then return a tuple
(start, stop, step).
""""""
start, stop = (slice_obj.start, slice_obj.stop)
if allow_step:
step = slice_obj.step
if step is None:
step = 1
if step < 0:
start, stop = slice_bounds(sequence, slice(stop, start))
else:
start, stop = slice_bounds(sequence, slice(start, stop))
return start, stop, step
elif slice_obj.step not in (None, 1):
raise ValueError(
""slices with steps are not supported by %s"" % sequence.__class__.__name__
)
if start is None:
start = 0
if stop is None:
stop = len(sequence)
if start < 0:
start = max(0, len(sequence) + start)
if stop < 0:
stop = max(0, len(sequence) + stop)
if stop > 0:
try:
sequence[stop - 1]
except IndexError:
stop = len(sequence)
start = min(start, stop)
return start, stop
",[],0,[],/internals.py_slice_bounds
641,/home/amandapotts/git/nltk/nltk/internals.py_is_writable,"def is_writable(path):
if not os.path.exists(path):
return False
if hasattr(os, ""getuid""):
statdata = os.stat(path)
perm = stat.S_IMODE(statdata.st_mode)
if perm & 0o002:
return True
elif statdata.st_uid == os.getuid() and (perm & 0o200):
return True
elif (statdata.st_gid in [os.getgid()] + os.getgroups()) and (perm & 0o020):
return True
else:
return False
return True
",[],0,[],/internals.py_is_writable
642,/home/amandapotts/git/nltk/nltk/internals.py_raise_unorderable_types,"def raise_unorderable_types(ordering, a, b):
raise TypeError(
""unorderable types: %s() %s %s()""
% (type(a).__name__, ordering, type(b).__name__)
)
",[],0,[],/internals.py_raise_unorderable_types
643,/home/amandapotts/git/nltk/nltk/jsontags.py_register_tag,"def register_tag(cls):
""""""
Decorates a class to register it's json tag.
""""""
json_tags[TAG_PREFIX + getattr(cls, ""json_tag"")] = cls
return cls
",[],0,[],/jsontags.py_register_tag
644,/home/amandapotts/git/nltk/nltk/jsontags.py_default,"def default(self, obj):
obj_tag = getattr(obj, ""json_tag"", None)
if obj_tag is None:
return super().default(obj)
obj_tag = TAG_PREFIX + obj_tag
obj = obj.encode_json_obj()
return {obj_tag: obj}
",[],0,[],/jsontags.py_default
645,/home/amandapotts/git/nltk/nltk/jsontags.py_decode,"def decode(self, s):
return self.decode_obj(super().decode(s))
",[],0,[],/jsontags.py_decode
646,/home/amandapotts/git/nltk/nltk/jsontags.py_decode_obj,"def decode_obj(cls, obj):
if isinstance(obj, dict):
obj = {key: cls.decode_obj(val) for (key, val) in obj.items()}
elif isinstance(obj, list):
obj = list(cls.decode_obj(val) for val in obj)
if not isinstance(obj, dict) or len(obj) != 1:
return obj
obj_tag = next(iter(obj.keys()))
if not obj_tag.startswith(""!""):
return obj
if obj_tag not in json_tags:
raise ValueError(""Unknown tag"", obj_tag)
obj_cls = json_tags[obj_tag]
return obj_cls.decode_json_obj(obj[obj_tag])
",[],0,[],/jsontags.py_decode_obj
647,/home/amandapotts/git/nltk/nltk/downloader.py___init__,"def __init__(
self,
id,
url,
name=None,
subdir="""",
size=None,
unzipped_size=None,
checksum=None,
svn_revision=None,
copyright=""Unknown"",
contact=""Unknown"",
license=""Unknown"",
author=""Unknown"",
unzip=True,
",[],0,[],/downloader.py___init__
648,/home/amandapotts/git/nltk/nltk/downloader.py_fromxml,"def fromxml(xml):
if isinstance(xml, str):
xml = ElementTree.parse(xml)
for key in xml.attrib:
xml.attrib[key] = str(xml.attrib[key])
return Package(**xml.attrib)
",[],0,[],/downloader.py_fromxml
649,/home/amandapotts/git/nltk/nltk/downloader.py___lt__,"def __lt__(self, other):
return self.id < other.id
",[],0,[],/downloader.py___lt__
650,/home/amandapotts/git/nltk/nltk/downloader.py___repr__,"def __repr__(self):
return ""<Package %s>"" % self.id
",[],0,[],/downloader.py___repr__
651,/home/amandapotts/git/nltk/nltk/downloader.py___init__,"def __init__(self, id, children, name=None, **kw):
self.id = id
""""""A unique identifier for this collection.""""""
self.name = name or id
""""""A string name for this collection.""""""
self.children = children
""""""A list of the ``Collections`` or ``Packages`` directly
contained by this collection.""""""
self.packages = None
""""""A list of ``Packages`` contained by this collection or any
collections it recursively contains.""""""
self.__dict__.update(kw)
",[],0,[],/downloader.py___init__
652,/home/amandapotts/git/nltk/nltk/downloader.py_fromxml,"def fromxml(xml):
if isinstance(xml, str):
xml = ElementTree.parse(xml)
for key in xml.attrib:
xml.attrib[key] = str(xml.attrib[key])
children = [child.get(""ref"") for child in xml.findall(""item"")]
return Collection(children=children, **xml.attrib)
",[],0,[],/downloader.py_fromxml
653,/home/amandapotts/git/nltk/nltk/downloader.py___lt__,"def __lt__(self, other):
return self.id < other.id
",[],0,[],/downloader.py___lt__
654,/home/amandapotts/git/nltk/nltk/downloader.py___repr__,"def __repr__(self):
return ""<Collection %s>"" % self.id
",[],0,[],/downloader.py___repr__
655,/home/amandapotts/git/nltk/nltk/downloader.py___init__,"def __init__(self, collection):
self.collection = collection
",[],0,[],/downloader.py___init__
656,/home/amandapotts/git/nltk/nltk/downloader.py___init__,"def __init__(self, collection):
self.collection = collection
",[],0,[],/downloader.py___init__
657,/home/amandapotts/git/nltk/nltk/downloader.py___init__,"def __init__(self, package):
self.package = package
",[],0,[],/downloader.py___init__
658,/home/amandapotts/git/nltk/nltk/downloader.py___init__,"def __init__(self, package):
self.package = package
",[],0,[],/downloader.py___init__
659,/home/amandapotts/git/nltk/nltk/downloader.py___init__,"def __init__(self, package):
self.package = package
",[],0,[],/downloader.py___init__
660,/home/amandapotts/git/nltk/nltk/downloader.py___init__,"def __init__(self, package):
self.package = package
",[],0,[],/downloader.py___init__
661,/home/amandapotts/git/nltk/nltk/downloader.py___init__,"def __init__(self, package):
self.package = package
",[],0,[],/downloader.py___init__
662,/home/amandapotts/git/nltk/nltk/downloader.py___init__,"def __init__(self, package):
self.package = package
",[],0,[],/downloader.py___init__
663,/home/amandapotts/git/nltk/nltk/downloader.py___init__,"def __init__(self, package):
self.package = package
",[],0,[],/downloader.py___init__
664,/home/amandapotts/git/nltk/nltk/downloader.py___init__,"def __init__(self, package):
self.package = package
",[],0,[],/downloader.py___init__
665,/home/amandapotts/git/nltk/nltk/downloader.py___init__,"def __init__(self, package, message):
self.package = package
if isinstance(message, Exception):
self.message = str(message)
else:
self.message = message
",[],0,[],/downloader.py___init__
666,/home/amandapotts/git/nltk/nltk/downloader.py___init__,"def __init__(self, progress):
self.progress = progress
",[],0,[],/downloader.py___init__
667,/home/amandapotts/git/nltk/nltk/downloader.py___init__,"def __init__(self, download_dir):
self.download_dir = download_dir
",[],0,[],/downloader.py___init__
668,/home/amandapotts/git/nltk/nltk/downloader.py___init__,"def __init__(self, server_index_url=None, download_dir=None):
self._url = server_index_url or self.DEFAULT_URL
""""""The URL for the data server's index file.""""""
self._collections = {}
""""""Dictionary from collection identifier to ``Collection``""""""
self._packages = {}
""""""Dictionary from package identifier to ``Package``""""""
self._download_dir = download_dir
""""""The default directory to which packages will be downloaded.""""""
self._index = None
""""""The XML index file downloaded from the data server""""""
self._index_timestamp = None
""""""Time at which ``self._index`` was downloaded.  If it is more
than ``INDEX_TIMEOUT`` seconds old, it will be re-downloaded.""""""
self._status_cache = {}
""""""Dictionary from package/collection identifier to status
string (``INSTALLED``, ``NOT_INSTALLED``, ``STALE``, or
``PARTIAL``).  Cache is used for packages only, not
collections.""""""
self._errors = None
""""""Flag for telling if all packages got successfully downloaded or not.""""""
if self._download_dir is None:
self._download_dir = self.default_download_dir()
",[],0,[],/downloader.py___init__
669,/home/amandapotts/git/nltk/nltk/downloader.py_list,"def list(
self,
download_dir=None,
show_packages=True,
show_collections=True,
header=True,
more_prompt=False,
skip_installed=False,
",[],0,[],/downloader.py_list
670,/home/amandapotts/git/nltk/nltk/downloader.py_packages,"def packages(self):
self._update_index()
return self._packages.values()
",[],0,[],/downloader.py_packages
671,/home/amandapotts/git/nltk/nltk/downloader.py_corpora,"def corpora(self):
self._update_index()
return [pkg for (id, pkg) in self._packages.items() if pkg.subdir == ""corpora""]
",[],0,[],/downloader.py_corpora
672,/home/amandapotts/git/nltk/nltk/downloader.py_models,"def models(self):
self._update_index()
return [pkg for (id, pkg) in self._packages.items() if pkg.subdir != ""corpora""]
",[],0,[],/downloader.py_models
673,/home/amandapotts/git/nltk/nltk/downloader.py_collections,"def collections(self):
self._update_index()
return self._collections.values()
",[],0,[],/downloader.py_collections
674,/home/amandapotts/git/nltk/nltk/downloader.py__info_or_id,"def _info_or_id(self, info_or_id):
if isinstance(info_or_id, str):
return self.info(info_or_id)
else:
return info_or_id
",[],0,[],/downloader.py__info_or_id
675,/home/amandapotts/git/nltk/nltk/downloader.py_incr_download,"def incr_download(self, info_or_id, download_dir=None, force=False):
if download_dir is None:
download_dir = self._download_dir
yield SelectDownloadDirMessage(download_dir)
if isinstance(info_or_id, (list, tuple)):
yield from self._download_list(info_or_id, download_dir, force)
return
try:
info = self._info_or_id(info_or_id)
except (OSError, ValueError) as e:
yield ErrorMessage(None, f""Error loading {info_or_id}: {e}"")
return
if isinstance(info, Collection):
yield StartCollectionMessage(info)
yield from self.incr_download(info.children, download_dir, force)
yield FinishCollectionMessage(info)
else:
yield from self._download_package(info, download_dir, force)
",[],0,[],/downloader.py_incr_download
676,/home/amandapotts/git/nltk/nltk/downloader.py__num_packages,"def _num_packages(self, item):
if isinstance(item, Package):
return 1
else:
return len(item.packages)
",[],0,[],/downloader.py__num_packages
677,/home/amandapotts/git/nltk/nltk/downloader.py__download_list,"def _download_list(self, items, download_dir, force):
for i in range(len(items)):
try:
items[i] = self._info_or_id(items[i])
except (OSError, ValueError) as e:
yield ErrorMessage(items[i], e)
return
num_packages = sum(self._num_packages(item) for item in items)
progress = 0
for i, item in enumerate(items):
if isinstance(item, Package):
delta = 1.0 / num_packages
else:
delta = len(item.packages) / num_packages
for msg in self.incr_download(item, download_dir, force):
if isinstance(msg, ProgressMessage):
yield ProgressMessage(progress + msg.progress * delta)
else:
yield msg
progress += 100 * delta
",[],0,[],/downloader.py__download_list
678,/home/amandapotts/git/nltk/nltk/downloader.py__download_package,"def _download_package(self, info, download_dir, force):
yield StartPackageMessage(info)
yield ProgressMessage(0)
status = self.status(info, download_dir)
if not force and status == self.INSTALLED:
yield UpToDateMessage(info)
yield ProgressMessage(100)
yield FinishPackageMessage(info)
return
self._status_cache.pop(info.id, None)
filepath = os.path.join(download_dir, info.filename)
if os.path.exists(filepath):
if status == self.STALE:
yield StaleMessage(info)
os.remove(filepath)
os.makedirs(download_dir, exist_ok=True)
os.makedirs(os.path.join(download_dir, info.subdir), exist_ok=True)
yield StartDownloadMessage(info)
yield ProgressMessage(5)
try:
infile = urlopen(info.url)
with open(filepath, ""wb"") as outfile:
num_blocks = max(1, info.size / (1024 * 16))
for block in itertools.count():
s = infile.read(1024 * 16)  # 16k blocks.
outfile.write(s)
if not s:
break
if block % 2 == 0:  # how often?
yield ProgressMessage(min(80, 5 + 75 * (block / num_blocks)))
infile.close()
except OSError as e:
yield ErrorMessage(
info,
""Error downloading %r from <%s>:"" ""\n  %s"" % (info.id, info.url, e),
)
return
yield FinishDownloadMessage(info)
yield ProgressMessage(80)
if info.filename.endswith("".zip""):
zipdir = os.path.join(download_dir, info.subdir)
if info.unzip or os.path.exists(os.path.join(zipdir, info.id)):
yield StartUnzipMessage(info)
for msg in _unzip_iter(filepath, zipdir, verbose=False):
msg.package = info
yield msg
yield FinishUnzipMessage(info)
yield FinishPackageMessage(info)
",[],0,[],/downloader.py__download_package
679,/home/amandapotts/git/nltk/nltk/downloader.py_download,"def download(
self,
info_or_id=None,
download_dir=None,
quiet=False,
force=False,
prefix=""[nltk_data] "",
halt_on_error=True,
raise_on_error=False,
print_error_to=sys.stderr,
",[],0,[],/downloader.py_download
680,/home/amandapotts/git/nltk/nltk/downloader.py_show,"def show(s, prefix2=""""):
print_to(
textwrap.fill(
s,
initial_indent=prefix + prefix2,
subsequent_indent=prefix + prefix2 + "" "" * 4,
)
)
",[],0,[],/downloader.py_show
681,/home/amandapotts/git/nltk/nltk/downloader.py_is_stale,"def is_stale(self, info_or_id, download_dir=None):
return self.status(info_or_id, download_dir) == self.STALE
",[],0,[],/downloader.py_is_stale
682,/home/amandapotts/git/nltk/nltk/downloader.py_is_installed,"def is_installed(self, info_or_id, download_dir=None):
return self.status(info_or_id, download_dir) == self.INSTALLED
",[],0,[],/downloader.py_is_installed
683,/home/amandapotts/git/nltk/nltk/downloader.py_clear_status_cache,"def clear_status_cache(self, id=None):
if id is None:
self._status_cache.clear()
else:
self._status_cache.pop(id, None)
",[],0,[],/downloader.py_clear_status_cache
684,/home/amandapotts/git/nltk/nltk/downloader.py_status,"def status(self, info_or_id, download_dir=None):
""""""
Return a constant describing the status of the given package
or collection.  Status can be one of ``INSTALLED``,
``NOT_INSTALLED``, ``STALE``, or ``PARTIAL``.
""""""
if download_dir is None:
download_dir = self._download_dir
info = self._info_or_id(info_or_id)
if isinstance(info, Collection):
pkg_status = [self.status(pkg.id) for pkg in info.packages]
if self.STALE in pkg_status:
return self.STALE
elif self.PARTIAL in pkg_status:
return self.PARTIAL
elif self.INSTALLED in pkg_status and self.NOT_INSTALLED in pkg_status:
return self.PARTIAL
elif self.NOT_INSTALLED in pkg_status:
return self.NOT_INSTALLED
else:
return self.INSTALLED
else:
filepath = os.path.join(download_dir, info.filename)
if download_dir != self._download_dir:
return self._pkg_status(info, filepath)
else:
if info.id not in self._status_cache:
self._status_cache[info.id] = self._pkg_status(info, filepath)
return self._status_cache[info.id]
",[],0,[],/downloader.py_status
685,/home/amandapotts/git/nltk/nltk/downloader.py__pkg_status,"def _pkg_status(self, info, filepath):
if not os.path.exists(filepath):
return self.NOT_INSTALLED
try:
filestat = os.stat(filepath)
except OSError:
return self.NOT_INSTALLED
if filestat.st_size != int(info.size):
return self.STALE
if md5_hexdigest(filepath) != info.checksum:
return self.STALE
if filepath.endswith("".zip""):
unzipdir = filepath[:-4]
if not os.path.exists(unzipdir):
return self.INSTALLED  # but not unzipped -- ok!
if not os.path.isdir(unzipdir):
return self.STALE
unzipped_size = sum(
os.stat(os.path.join(d, f)).st_size
for d, _, files in os.walk(unzipdir)
for f in files
)
if unzipped_size != info.unzipped_size:
return self.STALE
return self.INSTALLED
",[],0,[],/downloader.py__pkg_status
686,/home/amandapotts/git/nltk/nltk/downloader.py_update,"def update(self, quiet=False, prefix=""[nltk_data] ""):
""""""
Re-download any packages whose status is STALE.
""""""
self.clear_status_cache()
for pkg in self.packages():
if self.status(pkg) == self.STALE:
self.download(pkg, quiet=quiet, prefix=prefix)
",[],0,[],/downloader.py_update
687,/home/amandapotts/git/nltk/nltk/downloader.py__update_index,"def _update_index(self, url=None):
""""""A helper function that ensures that self._index is
up-to-date.  If the index is older than self.INDEX_TIMEOUT,
then download it again.""""""
if not (
self._index is None
or url is not None
or time.time() - self._index_timestamp > self.INDEX_TIMEOUT
):
return
self._url = url or self._url
self._index = nltk.internals.ElementWrapper(
ElementTree.parse(urlopen(self._url)).getroot()
)
self._index_timestamp = time.time()
packages = [Package.fromxml(p) for p in self._index.findall(""packages/package"")]
self._packages = {p.id: p for p in packages}
collections = [
Collection.fromxml(c) for c in self._index.findall(""collections/collection"")
]
self._collections = {c.id: c for c in collections}
for collection in self._collections.values():
for i, child_id in enumerate(collection.children):
if child_id in self._packages:
collection.children[i] = self._packages[child_id]
elif child_id in self._collections:
collection.children[i] = self._collections[child_id]
else:
print(
""removing collection member with no package: {}"".format(
child_id
)
)
del collection.children[i]
for collection in self._collections.values():
packages = {}
queue = [collection]
for child in queue:
if isinstance(child, Collection):
queue.extend(child.children)
elif isinstance(child, Package):
packages[child.id] = child
else:
pass
collection.packages = packages.values()
self._status_cache.clear()
",[],0,[],/downloader.py__update_index
688,/home/amandapotts/git/nltk/nltk/downloader.py_index,"def index(self):
""""""
Return the XML index describing the packages available from
the data server.  If necessary, this index will be downloaded
from the data server.
""""""
self._update_index()
return self._index
",[],0,[],/downloader.py_index
689,/home/amandapotts/git/nltk/nltk/downloader.py_info,"def info(self, id):
""""""Return the ``Package`` or ``Collection`` record for the
given item.""""""
self._update_index()
if id in self._packages:
return self._packages[id]
if id in self._collections:
return self._collections[id]
raise ValueError(""Package %r not found in index"" % id)
",[],0,[],/downloader.py_info
690,/home/amandapotts/git/nltk/nltk/downloader.py_xmlinfo,"def xmlinfo(self, id):
""""""Return the XML info record for the given item""""""
self._update_index()
for package in self._index.findall(""packages/package""):
if package.get(""id"") == id:
return package
for collection in self._index.findall(""collections/collection""):
if collection.get(""id"") == id:
return collection
raise ValueError(""Package %r not found in index"" % id)
",[],0,[],/downloader.py_xmlinfo
691,/home/amandapotts/git/nltk/nltk/downloader.py__get_url,"def _get_url(self):
""""""The URL for the data server's index file.""""""
return self._url
",[],0,[],/downloader.py__get_url
692,/home/amandapotts/git/nltk/nltk/downloader.py__set_url,"def _set_url(self, url):
""""""
Set a new URL for the data server. If we're unable to contact
the given url, then the original url is kept.
""""""
original_url = self._url
try:
self._update_index(url)
except:
self._url = original_url
raise
",[],0,[],/downloader.py__set_url
693,/home/amandapotts/git/nltk/nltk/downloader.py_default_download_dir,"def default_download_dir(self):
""""""
Return the directory to which packages will be downloaded by
default.  This value can be overridden using the constructor,
or on a case-by-case basis using the ``download_dir`` argument when
calling ``download()``.
On Windows, the default download directory is
``PYTHONHOME/lib/nltk``, where *PYTHONHOME* is the
directory containing Python, e.g. ``C:\\Python25``.
On all other platforms, the default directory is the first of
the following which exists or which can be created with write
permission: ``/usr/share/nltk_data``, ``/usr/local/share/nltk_data``,
``/usr/lib/nltk_data``, ``/usr/local/lib/nltk_data``, ``~/nltk_data``.
""""""
if ""APPENGINE_RUNTIME"" in os.environ:
return
for nltkdir in nltk.data.path:
if os.path.exists(nltkdir) and nltk.internals.is_writable(nltkdir):
return nltkdir
if sys.platform == ""win32"" and ""APPDATA"" in os.environ:
homedir = os.environ[""APPDATA""]
else:
homedir = os.path.expanduser(""~/"")
if homedir == ""~/"":
raise ValueError(""Could not find a default download directory"")
return os.path.join(homedir, ""nltk_data"")
",[],0,[],/downloader.py_default_download_dir
694,/home/amandapotts/git/nltk/nltk/downloader.py__get_download_dir,"def _get_download_dir(self):
""""""
The default directory to which packages will be downloaded.
This defaults to the value returned by ``default_download_dir()``.
To override this default on a case-by-case basis, use the
``download_dir`` argument when calling ``download()``.
""""""
return self._download_dir
",[],0,[],/downloader.py__get_download_dir
695,/home/amandapotts/git/nltk/nltk/downloader.py__set_download_dir,"def _set_download_dir(self, download_dir):
self._download_dir = download_dir
self._status_cache.clear()
",[],0,[],/downloader.py__set_download_dir
696,/home/amandapotts/git/nltk/nltk/downloader.py__interactive_download,"def _interactive_download(self):
if TKINTER:
try:
DownloaderGUI(self).mainloop()
except TclError:
DownloaderShell(self).run()
else:
DownloaderShell(self).run()
",[],0,[],/downloader.py__interactive_download
697,/home/amandapotts/git/nltk/nltk/downloader.py___init__,"def __init__(self, dataserver):
self._ds = dataserver
",[],0,[],/downloader.py___init__
698,/home/amandapotts/git/nltk/nltk/downloader.py__simple_interactive_menu,"def _simple_interactive_menu(self, *options):
print(""-"" * 75)
spc = (68 - sum(len(o) for o in options)) // (len(options) - 1) * "" ""
print(""    "" + spc.join(options))
print(""-"" * 75)
",[],0,[],/downloader.py__simple_interactive_menu
699,/home/amandapotts/git/nltk/nltk/downloader.py_run,"def run(self):
print(""NLTK Downloader"")
while True:
self._simple_interactive_menu(
""d) Download"",
""l) List"",
"" u) Update"",
""c) Config"",
""h) Help"",
""q) Quit"",
)
user_input = input(""Downloader> "").strip()
if not user_input:
print()
continue
command = user_input.lower().split()[0]
args = user_input.split()[1:]
try:
if command == ""l"":
print()
self._ds.list(self._ds.download_dir, header=False, more_prompt=True)
elif command == ""h"":
self._simple_interactive_help()
elif command == ""c"":
self._simple_interactive_config()
elif command in (""q"", ""x""):
return
elif command == ""d"":
self._simple_interactive_download(args)
elif command == ""u"":
self._simple_interactive_update()
else:
print(""Command %r unrecognized"" % user_input)
except HTTPError as e:
print(""Error reading from server: %s"" % e)
except URLError as e:
print(""Error connecting to server: %s"" % e.reason)
print()
",[],0,[],/downloader.py_run
700,/home/amandapotts/git/nltk/nltk/downloader.py__simple_interactive_download,"def _simple_interactive_download(self, args):
if args:
for arg in args:
try:
self._ds.download(arg, prefix=""    "")
except (OSError, ValueError) as e:
print(e)
else:
while True:
print()
print(""Download which package (l=list
user_input = input(""  Identifier> "")
if user_input.lower() == ""l"":
self._ds.list(
self._ds.download_dir,
header=False,
more_prompt=True,
skip_installed=True,
)
continue
elif user_input.lower() in (""x"", ""q"", """"):
return
elif user_input:
for id in user_input.split():
try:
self._ds.download(id, prefix=""    "")
except (OSError, ValueError) as e:
print(e)
break
",[],0,[],/downloader.py__simple_interactive_download
701,/home/amandapotts/git/nltk/nltk/downloader.py__simple_interactive_update,"def _simple_interactive_update(self):
while True:
stale_packages = []
stale = partial = False
for info in sorted(getattr(self._ds, ""packages"")(), key=str):
if self._ds.status(info) == self._ds.STALE:
stale_packages.append((info.id, info.name))
print()
if stale_packages:
print(""Will update following packages (o=ok
for pid, pname in stale_packages:
name = textwrap.fill(
""-"" * 27 + (pname), 75, subsequent_indent=27 * "" ""
)[27:]
print(""  [ ] {} {}"".format(pid.ljust(20, "".""), name))
print()
user_input = input(""  Identifier> "")
if user_input.lower() == ""o"":
for pid, pname in stale_packages:
try:
self._ds.download(pid, prefix=""    "")
except (OSError, ValueError) as e:
print(e)
break
elif user_input.lower() in (""x"", ""q"", """"):
return
else:
print(""Nothing to update."")
return
",[],0,[],/downloader.py__simple_interactive_update
702,/home/amandapotts/git/nltk/nltk/downloader.py__simple_interactive_help,"def _simple_interactive_help(self):
print()
print(""Commands:"")
print(
""  d) Download a package or collection     u) Update out of date packages""
)
print(""  l) List packages & collections          h) Help"")
print(""  c) View & Modify Configuration          q) Quit"")
",[],0,[],/downloader.py__simple_interactive_help
703,/home/amandapotts/git/nltk/nltk/downloader.py__show_config,"def _show_config(self):
print()
print(""Data Server:"")
print(""  - URL: <%s>"" % self._ds.url)
print(""  - %d Package Collections Available"" % len(self._ds.collections()))
print(""  - %d Individual Packages Available"" % len(self._ds.packages()))
print()
print(""Local Machine:"")
print(""  - Data directory: %s"" % self._ds.download_dir)
",[],0,[],/downloader.py__show_config
704,/home/amandapotts/git/nltk/nltk/downloader.py__simple_interactive_config,"def _simple_interactive_config(self):
self._show_config()
while True:
print()
self._simple_interactive_menu(
""s) Show Config"", ""u) Set Server URL"", ""d) Set Data Dir"", ""m) Main Menu""
)
user_input = input(""Config> "").strip().lower()
if user_input == ""s"":
self._show_config()
elif user_input == ""d"":
new_dl_dir = input(""  New Directory> "").strip()
if new_dl_dir in ("""", ""x"", ""q"", ""X"", ""Q""):
print(""  Cancelled!"")
elif os.path.isdir(new_dl_dir):
self._ds.download_dir = new_dl_dir
else:
print(""Directory %r not found!  Create it first."" % new_dl_dir)
elif user_input == ""u"":
new_url = input(""  New URL> "").strip()
if new_url in ("""", ""x"", ""q"", ""X"", ""Q""):
print(""  Cancelled!"")
else:
if not new_url.startswith((""http://"", ""https://"")):
new_url = ""http://"" + new_url
try:
self._ds.url = new_url
except Exception as e:
print(f""Error reading <{new_url!r}>:\n  {e}"")
elif user_input == ""m"":
break
",[],0,[],/downloader.py__simple_interactive_config
705,/home/amandapotts/git/nltk/nltk/downloader.py___init__,"def __init__(self, dataserver, use_threads=True):
self._ds = dataserver
self._use_threads = use_threads
self._download_lock = threading.Lock()
self._download_msg_queue = []
self._download_abort_queue = []
self._downloading = False
self._afterid = {}
self._log_messages = []
self._log_indent = 0
self._log(""NLTK Downloader Started!"")
top = self.top = Tk()
top.geometry(""+50+50"")
top.title(""NLTK Downloader"")
top.configure(background=self._BACKDROP_COLOR[1])
top.bind(""<Control-q>"", self.destroy)
top.bind(""<Control-x>"", self.destroy)
self._destroyed = False
self._column_vars = {}
self._init_widgets()
self._init_menu()
try:
self._fill_table()
except HTTPError as e:
showerror(""Error reading from server"", e)
except URLError as e:
showerror(""Error connecting to server"", e.reason)
self._show_info()
self._select_columns()
self._table.select(0)
self._table.bind(""<Destroy>"", self._destroy)
",[],0,[],/downloader.py___init__
706,/home/amandapotts/git/nltk/nltk/downloader.py__log,"def _log(self, msg):
self._log_messages.append(
""{} {}{}"".format(time.ctime(), "" | "" * self._log_indent, msg)
)
",[],0,[],/downloader.py__log
707,/home/amandapotts/git/nltk/nltk/downloader.py__select_columns,"def _select_columns(self):
for column, var in self._column_vars.items():
if var.get():
self._table.show_column(column)
else:
self._table.hide_column(column)
",[],0,[],/downloader.py__select_columns
708,/home/amandapotts/git/nltk/nltk/downloader.py__refresh,"def _refresh(self):
self._ds.clear_status_cache()
try:
self._fill_table()
except HTTPError as e:
showerror(""Error reading from server"", e)
except URLError as e:
showerror(""Error connecting to server"", e.reason)
self._table.select(0)
",[],0,[],/downloader.py__refresh
709,/home/amandapotts/git/nltk/nltk/downloader.py__info_edit,"def _info_edit(self, info_key):
self._info_save()  # just in case.
(entry, callback) = self._info[info_key]
entry[""state""] = ""normal""
entry[""relief""] = ""sunken""
entry.focus()
",[],0,[],/downloader.py__info_edit
710,/home/amandapotts/git/nltk/nltk/downloader.py__info_save,"def _info_save(self, e=None):
focus = self._table
for entry, callback in self._info.values():
if entry[""state""] == ""disabled"":
continue
if e is not None and e.widget is entry and e.keysym != ""Return"":
focus = entry
else:
entry[""state""] = ""disabled""
entry[""relief""] = ""groove""
callback(entry.get())
focus.focus()
",[],0,[],/downloader.py__info_save
711,/home/amandapotts/git/nltk/nltk/downloader.py__table_reprfunc,"def _table_reprfunc(self, row, col, val):
if self._table.column_names[col].endswith(""Size""):
if isinstance(val, str):
return ""  %s"" % val
elif val < 1024**2:
return ""  %.1f KB"" % (val / 1024.0**1)
elif val < 1024**3:
return ""  %.1f MB"" % (val / 1024.0**2)
else:
return ""  %.1f GB"" % (val / 1024.0**3)
if col in (0, """"):
return str(val)
else:
return ""  %s"" % val
",[],0,[],/downloader.py__table_reprfunc
712,/home/amandapotts/git/nltk/nltk/downloader.py__set_url,"def _set_url(self, url):
if url == self._ds.url:
return
try:
self._ds.url = url
self._fill_table()
except OSError as e:
showerror(""Error Setting Server Index"", str(e))
self._show_info()
",[],0,[],/downloader.py__set_url
713,/home/amandapotts/git/nltk/nltk/downloader.py__set_download_dir,"def _set_download_dir(self, download_dir):
if self._ds.download_dir == download_dir:
return
self._ds.download_dir = download_dir
try:
self._fill_table()
except HTTPError as e:
showerror(""Error reading from server"", e)
except URLError as e:
showerror(""Error connecting to server"", e.reason)
self._show_info()
",[],0,[],/downloader.py__set_download_dir
714,/home/amandapotts/git/nltk/nltk/downloader.py__show_info,"def _show_info(self):
print(""showing info"", self._ds.url)
for entry, cb in self._info.values():
entry[""state""] = ""normal""
entry.delete(0, ""end"")
self._info[""url""][0].insert(0, self._ds.url)
self._info[""download_dir""][0].insert(0, self._ds.download_dir)
for entry, cb in self._info.values():
entry[""state""] = ""disabled""
",[],0,[],/downloader.py__show_info
715,/home/amandapotts/git/nltk/nltk/downloader.py__prev_tab,"def _prev_tab(self, *e):
for i, tab in enumerate(self._tab_names):
if tab.lower() == self._tab and i > 0:
self._tab = self._tab_names[i - 1].lower()
try:
return self._fill_table()
except HTTPError as e:
showerror(""Error reading from server"", e)
except URLError as e:
showerror(""Error connecting to server"", e.reason)
",[],0,[],/downloader.py__prev_tab
716,/home/amandapotts/git/nltk/nltk/downloader.py__next_tab,"def _next_tab(self, *e):
for i, tab in enumerate(self._tab_names):
if tab.lower() == self._tab and i < (len(self._tabs) - 1):
self._tab = self._tab_names[i + 1].lower()
try:
return self._fill_table()
except HTTPError as e:
showerror(""Error reading from server"", e)
except URLError as e:
showerror(""Error connecting to server"", e.reason)
",[],0,[],/downloader.py__next_tab
717,/home/amandapotts/git/nltk/nltk/downloader.py__select_tab,"def _select_tab(self, event):
self._tab = event.widget[""text""].lower()
try:
self._fill_table()
except HTTPError as e:
showerror(""Error reading from server"", e)
except URLError as e:
showerror(""Error connecting to server"", e.reason)
",[],0,[],/downloader.py__select_tab
718,/home/amandapotts/git/nltk/nltk/downloader.py__fill_table,"def _fill_table(self):
selected_row = self._table.selected_row()
self._table.clear()
if self._tab == ""all packages"":
items = self._ds.packages()
elif self._tab == ""corpora"":
items = self._ds.corpora()
elif self._tab == ""models"":
items = self._ds.models()
elif self._tab == ""collections"":
items = self._ds.collections()
else:
assert 0, ""bad tab value %r"" % self._tab
rows = [self._package_to_columns(item) for item in items]
self._table.extend(rows)
for tab, label in self._tabs.items():
if tab == self._tab:
label.configure(
foreground=self._FRONT_TAB_COLOR[0],
background=self._FRONT_TAB_COLOR[1],
)
else:
label.configure(
foreground=self._BACK_TAB_COLOR[0],
background=self._BACK_TAB_COLOR[1],
)
self._table.sort_by(""Identifier"", order=""ascending"")
self._color_table()
self._table.select(selected_row)
self.top.after(150, self._table._scrollbar.set, *self._table._mlb.yview())
self.top.after(300, self._table._scrollbar.set, *self._table._mlb.yview())
",[],0,[],/downloader.py__fill_table
719,/home/amandapotts/git/nltk/nltk/downloader.py__update_table_status,"def _update_table_status(self):
for row_num in range(len(self._table)):
status = self._ds.status(self._table[row_num, ""Identifier""])
self._table[row_num, ""Status""] = status
self._color_table()
",[],0,[],/downloader.py__update_table_status
720,/home/amandapotts/git/nltk/nltk/downloader.py__download,"def _download(self, *e):
if self._use_threads:
return self._download_threaded(*e)
marked = [
self._table[row, ""Identifier""]
for row in range(len(self._table))
if self._table[row, 0] != """"
]
selection = self._table.selected_row()
if not marked and selection is not None:
marked = [self._table[selection, ""Identifier""]]
download_iter = self._ds.incr_download(marked, self._ds.download_dir)
self._log_indent = 0
self._download_cb(download_iter, marked)
",[],0,[],/downloader.py__download
721,/home/amandapotts/git/nltk/nltk/downloader.py__download_cb,"def _download_cb(self, download_iter, ids):
try:
msg = next(download_iter)
except StopIteration:
self._update_table_status()
afterid = self.top.after(10, self._show_progress, 0)
self._afterid[""_download_cb""] = afterid
return
",[],0,[],/downloader.py__download_cb
722,/home/amandapotts/git/nltk/nltk/downloader.py_show,"def show(s):
self._progresslabel[""text""] = s
self._log(s)
",[],0,[],/downloader.py_show
723,/home/amandapotts/git/nltk/nltk/downloader.py__select,"def _select(self, id):
for row in range(len(self._table)):
if self._table[row, ""Identifier""] == id:
self._table.select(row)
return
",[],0,[],/downloader.py__select
724,/home/amandapotts/git/nltk/nltk/downloader.py__color_table,"def _color_table(self):
for row in range(len(self._table)):
bg, sbg = self._ROW_COLOR[self._table[row, ""Status""]]
fg, sfg = (""black"", ""white"")
self._table.rowconfig(
row,
foreground=fg,
selectforeground=sfg,
background=bg,
selectbackground=sbg,
)
self._table.itemconfigure(
row, 0, foreground=self._MARK_COLOR[0], background=self._MARK_COLOR[1]
)
",[],0,[],/downloader.py__color_table
725,/home/amandapotts/git/nltk/nltk/downloader.py__clear_mark,"def _clear_mark(self, id):
for row in range(len(self._table)):
if self._table[row, ""Identifier""] == id:
self._table[row, 0] = """"
",[],0,[],/downloader.py__clear_mark
726,/home/amandapotts/git/nltk/nltk/downloader.py__mark_all,"def _mark_all(self, *e):
for row in range(len(self._table)):
self._table[row, 0] = ""X""
",[],0,[],/downloader.py__mark_all
727,/home/amandapotts/git/nltk/nltk/downloader.py__table_mark,"def _table_mark(self, *e):
selection = self._table.selected_row()
if selection >= 0:
if self._table[selection][0] != """":
self._table[selection, 0] = """"
else:
self._table[selection, 0] = ""X""
self._table.select(delta=1)
",[],0,[],/downloader.py__table_mark
728,/home/amandapotts/git/nltk/nltk/downloader.py__show_log,"def _show_log(self):
text = ""\n"".join(self._log_messages)
ShowText(self.top, ""NLTK Downloader Log"", text)
",[],0,[],/downloader.py__show_log
729,/home/amandapotts/git/nltk/nltk/downloader.py__package_to_columns,"def _package_to_columns(self, pkg):
""""""
Given a package, return a list of values describing that
package, one for each column in ``self.COLUMNS``.
""""""
row = []
for column_index, column_name in enumerate(self.COLUMNS):
if column_index == 0:  # Mark:
row.append("""")
elif column_name == ""Identifier"":
row.append(pkg.id)
elif column_name == ""Status"":
row.append(self._ds.status(pkg))
else:
attr = column_name.lower().replace("" "", ""_"")
row.append(getattr(pkg, attr, ""n/a""))
return row
",[],0,[],/downloader.py__package_to_columns
730,/home/amandapotts/git/nltk/nltk/downloader.py_destroy,"def destroy(self, *e):
if self._destroyed:
return
self.top.destroy()
self._destroyed = True
",[],0,[],/downloader.py_destroy
731,/home/amandapotts/git/nltk/nltk/downloader.py__destroy,"def _destroy(self, *e):
if self.top is not None:
for afterid in self._afterid.values():
self.top.after_cancel(afterid)
if self._downloading and self._use_threads:
self._abort_download()
self._column_vars.clear()
",[],0,[],/downloader.py__destroy
732,/home/amandapotts/git/nltk/nltk/downloader.py_mainloop,"def mainloop(self, *args, **kwargs):
self.top.mainloop(*args, **kwargs)
",[],0,[],/downloader.py_mainloop
733,/home/amandapotts/git/nltk/nltk/downloader.py_help,"def help(self, *e):
try:
ShowText(
self.top,
""Help: NLTK Downloader"",
self.HELP.strip(),
width=75,
font=""fixed"",
)
except:
ShowText(self.top, ""Help: NLTK Downloader"", self.HELP.strip(), width=75)
",[],0,[],/downloader.py_help
734,/home/amandapotts/git/nltk/nltk/downloader.py_about,"def about(self, *e):
ABOUT = ""NLTK Downloader\n"" + ""Written by Edward Loper""
TITLE = ""About: NLTK Downloader""
try:
from tkinter.messagebox import Message
Message(message=ABOUT, title=TITLE).show()
except ImportError:
ShowText(self.top, TITLE, ABOUT)
",[],0,[],/downloader.py_about
735,/home/amandapotts/git/nltk/nltk/downloader.py__init_progressbar,"def _init_progressbar(self):
c = self._progressbar
width, height = int(c[""width""]), int(c[""height""])
for i in range(0, (int(c[""width""]) * 2) // self._gradient_width):
c.create_line(
i * self._gradient_width + 20,
-20,
i * self._gradient_width - height - 20,
height + 20,
width=self._gradient_width,
fill=""#%02x0000"" % (80 + abs(i % 6 - 3) * 12),
)
c.addtag_all(""gradient"")
c.itemconfig(""gradient"", state=""hidden"")
c.addtag_withtag(
""redbox"", c.create_rectangle(0, 0, 0, 0, fill=self._PROGRESS_COLOR[0])
)
",[],0,[],/downloader.py__init_progressbar
736,/home/amandapotts/git/nltk/nltk/downloader.py__show_progress,"def _show_progress(self, percent):
c = self._progressbar
if percent is None:
c.coords(""redbox"", 0, 0, 0, 0)
c.itemconfig(""gradient"", state=""hidden"")
else:
width, height = int(c[""width""]), int(c[""height""])
x = percent * int(width) // 100 + 1
c.coords(""redbox"", 0, 0, x, height + 1)
",[],0,[],/downloader.py__show_progress
737,/home/amandapotts/git/nltk/nltk/downloader.py__progress_alive,"def _progress_alive(self):
c = self._progressbar
if not self._downloading:
c.itemconfig(""gradient"", state=""hidden"")
else:
c.itemconfig(""gradient"", state=""normal"")
x1, y1, x2, y2 = c.bbox(""gradient"")
if x1 <= -100:
c.move(""gradient"", (self._gradient_width * 6) - 4, 0)
else:
c.move(""gradient"", -4, 0)
afterid = self.top.after(200, self._progress_alive)
self._afterid[""_progress_alive""] = afterid
",[],0,[],/downloader.py__progress_alive
738,/home/amandapotts/git/nltk/nltk/downloader.py__download_threaded,"def _download_threaded(self, *e):
if self._downloading:
self._abort_download()
return
self._download_button[""text""] = ""Cancel""
marked = [
self._table[row, ""Identifier""]
for row in range(len(self._table))
if self._table[row, 0] != """"
]
selection = self._table.selected_row()
if not marked and selection is not None:
marked = [self._table[selection, ""Identifier""]]
ds = Downloader(self._ds.url, self._ds.download_dir)
assert self._download_msg_queue == []
assert self._download_abort_queue == []
self._DownloadThread(
ds,
marked,
self._download_lock,
self._download_msg_queue,
self._download_abort_queue,
).start()
self._log_indent = 0
self._downloading = True
self._monitor_message_queue()
self._progress_alive()
",[],0,[],/downloader.py__download_threaded
739,/home/amandapotts/git/nltk/nltk/downloader.py__abort_download,"def _abort_download(self):
if self._downloading:
self._download_lock.acquire()
self._download_abort_queue.append(""abort"")
self._download_lock.release()
",[],0,[],/downloader.py__abort_download
740,/home/amandapotts/git/nltk/nltk/downloader.py__DownloadThread,"class _DownloadThread(threading.Thread):
",[],0,[],/downloader.py__DownloadThread
741,/home/amandapotts/git/nltk/nltk/downloader.py___init__,"def __init__(self, data_server, items, lock, message_queue, abort):
self.data_server = data_server
self.items = items
self.lock = lock
self.message_queue = message_queue
self.abort = abort
threading.Thread.__init__(self)
",[],0,[],/downloader.py___init__
742,/home/amandapotts/git/nltk/nltk/downloader.py_run,"def run(self):
for msg in self.data_server.incr_download(self.items):
self.lock.acquire()
self.message_queue.append(msg)
if self.abort:
self.message_queue.append(""aborted"")
self.lock.release()
return
self.lock.release()
self.lock.acquire()
self.message_queue.append(""finished"")
self.lock.release()
",[],0,[],/downloader.py_run
743,/home/amandapotts/git/nltk/nltk/downloader.py__monitor_message_queue,"def _monitor_message_queue(self):
",[],0,[],/downloader.py__monitor_message_queue
744,/home/amandapotts/git/nltk/nltk/downloader.py_show,"def show(s):
self._progresslabel[""text""] = s
self._log(s)
",[],0,[],/downloader.py_show
745,/home/amandapotts/git/nltk/nltk/downloader.py_md5_hexdigest,"def md5_hexdigest(file):
""""""
Calculate and return the MD5 checksum for a given file.
``file`` may either be a filename or an open stream.
""""""
if isinstance(file, str):
with open(file, ""rb"") as infile:
return _md5_hexdigest(infile)
return _md5_hexdigest(file)
",[],0,[],/downloader.py_md5_hexdigest
746,/home/amandapotts/git/nltk/nltk/downloader.py__md5_hexdigest,"def _md5_hexdigest(fp):
md5_digest = md5()
while True:
block = fp.read(1024 * 16)  # 16k blocks
if not block:
break
md5_digest.update(block)
return md5_digest.hexdigest()
",[],0,[],/downloader.py__md5_hexdigest
747,/home/amandapotts/git/nltk/nltk/downloader.py_unzip,"def unzip(filename, root, verbose=True):
""""""
Extract the contents of the zip file ``filename`` into the
directory ``root``.
""""""
for message in _unzip_iter(filename, root, verbose):
if isinstance(message, ErrorMessage):
raise Exception(message)
",[],0,[],/downloader.py_unzip
748,/home/amandapotts/git/nltk/nltk/downloader.py__unzip_iter,"def _unzip_iter(filename, root, verbose=True):
if verbose:
sys.stdout.write(""Unzipping %s"" % os.path.split(filename)[1])
sys.stdout.flush()
try:
zf = zipfile.ZipFile(filename)
except zipfile.BadZipFile:
yield ErrorMessage(filename, ""Error with downloaded zip file"")
return
except Exception as e:
yield ErrorMessage(filename, e)
return
zf.extractall(root)
if verbose:
print()
",[],0,[],/downloader.py__unzip_iter
749,/home/amandapotts/git/nltk/nltk/downloader.py__indent_xml,"def _indent_xml(xml, prefix=""""):
""""""
Helper for ``build_index()``: Given an XML ``ElementTree``, modify it
(and its descendents) ``text`` and ``tail`` attributes to generate
an indented tree, where each nested element is indented by 2
spaces with respect to its parent.
""""""
if len(xml) > 0:
xml.text = (xml.text or """").strip() + ""\n"" + prefix + ""  ""
for child in xml:
_indent_xml(child, prefix + ""  "")
for child in xml[:-1]:
child.tail = (child.tail or """").strip() + ""\n"" + prefix + ""  ""
xml[-1].tail = (xml[-1].tail or """").strip() + ""\n"" + prefix
",[],0,[],/downloader.py__indent_xml
750,/home/amandapotts/git/nltk/nltk/downloader.py__check_package,"def _check_package(pkg_xml, zipfilename, zf):
""""""
Helper for ``build_index()``: Perform some checks to make sure that
the given package is consistent.
""""""
uid = os.path.splitext(os.path.split(zipfilename)[1])[0]
if pkg_xml.get(""id"") != uid:
raise ValueError(
""package identifier mismatch ({} vs {})"".format(pkg_xml.get(""id""), uid)
)
if sum((name != uid and not name.startswith(uid + ""/"")) for name in zf.namelist()):
raise ValueError(
""Zipfile %s.zip does not expand to a single ""
""subdirectory %s/"" % (uid, uid)
)
",[],0,[],/downloader.py__check_package
751,/home/amandapotts/git/nltk/nltk/downloader.py__svn_revision,"def _svn_revision(filename):
""""""
Helper for ``build_index()``: Calculate the subversion revision
number for a given file (by using ``subprocess`` to run ``svn``).
""""""
p = subprocess.Popen(
[""svn"", ""status"", ""-v"", filename],
stdout=subprocess.PIPE,
stderr=subprocess.PIPE,
)
(stdout, stderr) = p.communicate()
if p.returncode != 0 or stderr or not stdout:
raise ValueError(
""Error determining svn_revision for %s: %s""
% (os.path.split(filename)[1], textwrap.fill(stderr))
)
return stdout.split()[2]
",[],0,[],/downloader.py__svn_revision
752,/home/amandapotts/git/nltk/nltk/downloader.py__find_collections,"def _find_collections(root):
""""""
Helper for ``build_index()``: Yield a list of ElementTree.Element
objects, each holding the xml for a single package collection.
""""""
for dirname, _subdirs, files in os.walk(root):
for filename in files:
if filename.endswith("".xml""):
xmlfile = os.path.join(dirname, filename)
yield ElementTree.parse(xmlfile).getroot()
",[],0,[],/downloader.py__find_collections
753,/home/amandapotts/git/nltk/nltk/downloader.py__find_packages,"def _find_packages(root):
""""""
Helper for ``build_index()``: Yield a list of tuples
``(pkg_xml, zf, subdir)``, where:
- ``pkg_xml`` is an ``ElementTree.Element`` holding the xml for a
package
- ``zf`` is a ``zipfile.ZipFile`` for the package's contents.
- ``subdir`` is the subdirectory (relative to ``root``) where
the package was found (e.g. 'corpora' or 'grammars').
""""""
from nltk.corpus.reader.util import _path_from
packages = []
for dirname, subdirs, files in os.walk(root):
relpath = ""/"".join(_path_from(root, dirname))
for filename in files:
if filename.endswith("".xml""):
xmlfilename = os.path.join(dirname, filename)
zipfilename = xmlfilename[:-4] + "".zip""
try:
zf = zipfile.ZipFile(zipfilename)
except Exception as e:
raise ValueError(f""Error reading file {zipfilename!r}!\n{e}"") from e
try:
pkg_xml = ElementTree.parse(xmlfilename).getroot()
except Exception as e:
raise ValueError(f""Error reading file {xmlfilename!r}!\n{e}"") from e
uid = os.path.split(xmlfilename[:-4])[1]
if pkg_xml.get(""id"") != uid:
raise ValueError(
""package identifier mismatch (%s ""
""vs %s)"" % (pkg_xml.get(""id""), uid)
)
if sum(
(name != uid and not name.startswith(uid + ""/""))
for name in zf.namelist()
):
raise ValueError(
""Zipfile %s.zip does not expand to a ""
""single subdirectory %s/"" % (uid, uid)
)
yield pkg_xml, zf, relpath
elif filename.endswith("".zip""):
resourcename = os.path.splitext(filename)[0]
xmlfilename = os.path.join(dirname, resourcename + "".xml"")
if not os.path.exists(xmlfilename):
warnings.warn(
f""{filename} exists, but {resourcename + '.xml'} cannot be found! ""
f""This could mean that {resourcename} can not be downloaded."",
stacklevel=2,
)
try:
subdirs.remove("".svn"")
except ValueError:
pass
",[],0,[],/downloader.py__find_packages
754,/home/amandapotts/git/nltk/nltk/downloader.py_download_shell,"def download_shell():
DownloaderShell(_downloader).run()
",[],0,[],/downloader.py_download_shell
755,/home/amandapotts/git/nltk/nltk/downloader.py_download_gui,"def download_gui():
DownloaderGUI(_downloader).mainloop()
",[],0,[],/downloader.py_download_gui
756,/home/amandapotts/git/nltk/nltk/downloader.py_update,"def update():
_downloader.update()
",[],0,[],/downloader.py_update
757,/home/amandapotts/git/nltk/nltk/collections.py___init__,"def __init__(self, data=None, **kwargs):
self._keys = self.keys(data, kwargs.get(""keys""))
self._default_factory = kwargs.get(""default_factory"")
if data is None:
dict.__init__(self)
else:
dict.__init__(self, data)
",[],0,[],/collections.py___init__
758,/home/amandapotts/git/nltk/nltk/collections.py___delitem__,"def __delitem__(self, key):
dict.__delitem__(self, key)
self._keys.remove(key)
",[],0,[],/collections.py___delitem__
759,/home/amandapotts/git/nltk/nltk/collections.py___getitem__,"def __getitem__(self, key):
try:
return dict.__getitem__(self, key)
except KeyError:
return self.__missing__(key)
",[],0,[],/collections.py___getitem__
760,/home/amandapotts/git/nltk/nltk/collections.py___iter__,"def __iter__(self):
return (key for key in self.keys())
",[],0,[],/collections.py___iter__
761,/home/amandapotts/git/nltk/nltk/collections.py___missing__,"def __missing__(self, key):
if not self._default_factory and key not in self._keys:
raise KeyError()
return self._default_factory()
",[],0,[],/collections.py___missing__
762,/home/amandapotts/git/nltk/nltk/collections.py___setitem__,"def __setitem__(self, key, item):
dict.__setitem__(self, key, item)
if key not in self._keys:
self._keys.append(key)
",[],0,[],/collections.py___setitem__
763,/home/amandapotts/git/nltk/nltk/collections.py_clear,"def clear(self):
dict.clear(self)
self._keys.clear()
",[],0,[],/collections.py_clear
764,/home/amandapotts/git/nltk/nltk/collections.py_copy,"def copy(self):
d = dict.copy(self)
d._keys = self._keys
return d
",[],0,[],/collections.py_copy
765,/home/amandapotts/git/nltk/nltk/collections.py_items,"def items(self):
return zip(self.keys(), self.values())
",[],0,[],/collections.py_items
766,/home/amandapotts/git/nltk/nltk/collections.py_keys,"def keys(self, data=None, keys=None):
if data:
if keys:
assert isinstance(keys, list)
assert len(data) == len(keys)
return keys
else:
assert (
isinstance(data, dict)
or isinstance(data, OrderedDict)
or isinstance(data, list)
)
if isinstance(data, dict) or isinstance(data, OrderedDict):
return data.keys()
elif isinstance(data, list):
return [key for (key, value) in data]
elif ""_keys"" in self.__dict__:
return self._keys
else:
return []
",[],0,[],/collections.py_keys
767,/home/amandapotts/git/nltk/nltk/collections.py_popitem,"def popitem(self):
if not self._keys:
raise KeyError()
key = self._keys.pop()
value = self[key]
del self[key]
return (key, value)
",[],0,[],/collections.py_popitem
768,/home/amandapotts/git/nltk/nltk/collections.py_setdefault,"def setdefault(self, key, failobj=None):
dict.setdefault(self, key, failobj)
if key not in self._keys:
self._keys.append(key)
",[],0,[],/collections.py_setdefault
769,/home/amandapotts/git/nltk/nltk/collections.py_update,"def update(self, data):
dict.update(self, data)
for key in self.keys(data):
if key not in self._keys:
self._keys.append(key)
",[],0,[],/collections.py_update
770,/home/amandapotts/git/nltk/nltk/collections.py_values,"def values(self):
return map(self.get, self._keys)
",[],0,[],/collections.py_values
771,/home/amandapotts/git/nltk/nltk/collections.py___len__,"def __len__(self):
""""""
Return the number of tokens in the corpus file underlying this
corpus view.
""""""
raise NotImplementedError(""should be implemented by subclass"")
",[],0,[],/collections.py___len__
772,/home/amandapotts/git/nltk/nltk/collections.py_iterate_from,"def iterate_from(self, start):
""""""
Return an iterator that generates the tokens in the corpus
file underlying this corpus view, starting at the token number
``start``.  If ``start>=len(self)``, then this iterator will
generate no tokens.
""""""
raise NotImplementedError(""should be implemented by subclass"")
",[],0,[],/collections.py_iterate_from
773,/home/amandapotts/git/nltk/nltk/collections.py___getitem__,"def __getitem__(self, i):
""""""
Return the *i* th token in the corpus file underlying this
corpus view.  Negative indices and spans are both supported.
""""""
if isinstance(i, slice):
start, stop = slice_bounds(self, i)
return LazySubsequence(self, start, stop)
else:
if i < 0:
i += len(self)
if i < 0:
raise IndexError(""index out of range"")
try:
return next(self.iterate_from(i))
except StopIteration as e:
raise IndexError(""index out of range"") from e
",[],0,[],/collections.py___getitem__
774,/home/amandapotts/git/nltk/nltk/collections.py___iter__,"def __iter__(self):
""""""Return an iterator that generates the tokens in the corpus
file underlying this corpus view.""""""
return self.iterate_from(0)
",[],0,[],/collections.py___iter__
775,/home/amandapotts/git/nltk/nltk/collections.py_count,"def count(self, value):
""""""Return the number of times this list contains ``value``.""""""
return sum(1 for elt in self if elt == value)
",[],0,[],/collections.py_count
776,/home/amandapotts/git/nltk/nltk/collections.py_index,"def index(self, value, start=None, stop=None):
""""""Return the index of the first occurrence of ``value`` in this
list that is greater than or equal to ``start`` and less than
``stop``.  Negative start and stop values are treated like negative
slice bounds -- i.e., they count from the end of the list.""""""
start, stop = slice_bounds(self, slice(start, stop))
for i, elt in enumerate(islice(self, start, stop)):
if elt == value:
return i + start
raise ValueError(""index(x): x not in list"")
",[],0,[],/collections.py_index
777,/home/amandapotts/git/nltk/nltk/collections.py___contains__,"def __contains__(self, value):
""""""Return true if this list contains ``value``.""""""
return bool(self.count(value))
",[],0,[],/collections.py___contains__
778,/home/amandapotts/git/nltk/nltk/collections.py___add__,"def __add__(self, other):
""""""Return a list concatenating self with other.""""""
return LazyConcatenation([self, other])
",[],0,[],/collections.py___add__
779,/home/amandapotts/git/nltk/nltk/collections.py___radd__,"def __radd__(self, other):
""""""Return a list concatenating other with self.""""""
return LazyConcatenation([other, self])
",[],0,[],/collections.py___radd__
780,/home/amandapotts/git/nltk/nltk/collections.py___mul__,"def __mul__(self, count):
""""""Return a list concatenating self with itself ``count`` times.""""""
return LazyConcatenation([self] * count)
",[],0,[],/collections.py___mul__
781,/home/amandapotts/git/nltk/nltk/collections.py___rmul__,"def __rmul__(self, count):
""""""Return a list concatenating self with itself ``count`` times.""""""
return LazyConcatenation([self] * count)
",[],0,[],/collections.py___rmul__
782,/home/amandapotts/git/nltk/nltk/collections.py___repr__,"def __repr__(self):
""""""
Return a string representation for this corpus view that is
similar to a list's representation
than 60 characters long, it is truncated.
""""""
pieces = []
length = 5
for elt in self:
pieces.append(repr(elt))
length += len(pieces[-1]) + 2
if length > self._MAX_REPR_SIZE and len(pieces) > 2:
return ""[%s, ...]"" % "", "".join(pieces[:-1])
return ""[%s]"" % "", "".join(pieces)
",[],0,[],/collections.py___repr__
783,/home/amandapotts/git/nltk/nltk/collections.py___eq__,"def __eq__(self, other):
return type(self) == type(other) and list(self) == list(other)
",[],0,[],/collections.py___eq__
784,/home/amandapotts/git/nltk/nltk/collections.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/collections.py___ne__
785,/home/amandapotts/git/nltk/nltk/collections.py___lt__,"def __lt__(self, other):
if type(other) != type(self):
raise_unorderable_types(""<"", self, other)
return list(self) < list(other)
",[],0,[],/collections.py___lt__
786,/home/amandapotts/git/nltk/nltk/collections.py___hash__,"def __hash__(self):
""""""
:raise ValueError: Corpus view objects are unhashable.
""""""
raise ValueError(""%s objects are unhashable"" % self.__class__.__name__)
",[],0,[],/collections.py___hash__
787,/home/amandapotts/git/nltk/nltk/collections.py___new__,"def __new__(cls, source, start, stop):
""""""
Construct a new slice from a given underlying sequence.  The
``start`` and ``stop`` indices should be absolute indices --
i.e., they should not be negative (for indexing from the back
of a list) or greater than the length of ``source``.
""""""
if stop - start < cls.MIN_SIZE:
return list(islice(source.iterate_from(start), stop - start))
else:
return object.__new__(cls)
",[],0,[],/collections.py___new__
788,/home/amandapotts/git/nltk/nltk/collections.py___init__,"def __init__(self, source, start, stop):
self._source = source
self._start = start
self._stop = stop
",[],0,[],/collections.py___init__
789,/home/amandapotts/git/nltk/nltk/collections.py___len__,"def __len__(self):
return self._stop - self._start
",[],0,[],/collections.py___len__
790,/home/amandapotts/git/nltk/nltk/collections.py_iterate_from,"def iterate_from(self, start):
return islice(
self._source.iterate_from(start + self._start), max(0, len(self) - start)
)
",[],0,[],/collections.py_iterate_from
791,/home/amandapotts/git/nltk/nltk/collections.py___init__,"def __init__(self, list_of_lists):
self._list = list_of_lists
self._offsets = [0]
",[],0,[],/collections.py___init__
792,/home/amandapotts/git/nltk/nltk/collections.py___len__,"def __len__(self):
if len(self._offsets) <= len(self._list):
for _ in self.iterate_from(self._offsets[-1]):
pass
return self._offsets[-1]
",[],0,[],/collections.py___len__
793,/home/amandapotts/git/nltk/nltk/collections.py_iterate_from,"def iterate_from(self, start_index):
if start_index < self._offsets[-1]:
sublist_index = bisect.bisect_right(self._offsets, start_index) - 1
else:
sublist_index = len(self._offsets) - 1
index = self._offsets[sublist_index]
if isinstance(self._list, AbstractLazySequence):
sublist_iter = self._list.iterate_from(sublist_index)
else:
sublist_iter = islice(self._list, sublist_index, None)
for sublist in sublist_iter:
if sublist_index == (len(self._offsets) - 1):
assert (
index + len(sublist) >= self._offsets[-1]
), ""offsets not monotonic increasing!""
self._offsets.append(index + len(sublist))
else:
assert self._offsets[sublist_index + 1] == index + len(
sublist
), ""inconsistent list value (num elts)""
yield from sublist[max(0, start_index - index) :]
index += len(sublist)
sublist_index += 1
",[],0,[],/collections.py_iterate_from
794,/home/amandapotts/git/nltk/nltk/collections.py___init__,"def __init__(self, function, *lists, **config):
""""""
:param function: The function that should be applied to
elements of ``lists``.  It should take as many arguments
as there are ``lists``.
:param lists: The underlying lists.
:param cache_size: Determines the size of the cache used
by this lazy map.  (default=5)
""""""
if not lists:
raise TypeError(""LazyMap requires at least two args"")
self._lists = lists
self._func = function
self._cache_size = config.get(""cache_size"", 5)
self._cache = {} if self._cache_size > 0 else None
self._all_lazy = sum(
isinstance(lst, AbstractLazySequence) for lst in lists
) == len(lists)
",[],0,[],/collections.py___init__
795,/home/amandapotts/git/nltk/nltk/collections.py_iterate_from,"def iterate_from(self, index):
if len(self._lists) == 1 and self._all_lazy:
for value in self._lists[0].iterate_from(index):
yield self._func(value)
return
elif len(self._lists) == 1:
while True:
try:
yield self._func(self._lists[0][index])
except IndexError:
return
index += 1
elif self._all_lazy:
iterators = [lst.iterate_from(index) for lst in self._lists]
while True:
elements = []
for iterator in iterators:
try:
elements.append(next(iterator))
except:  # FIXME: What is this except really catching? StopIteration?
elements.append(None)
if elements == [None] * len(self._lists):
return
yield self._func(*elements)
index += 1
else:
while True:
try:
elements = [lst[index] for lst in self._lists]
except IndexError:
elements = [None] * len(self._lists)
for i, lst in enumerate(self._lists):
try:
elements[i] = lst[index]
except IndexError:
pass
if elements == [None] * len(self._lists):
return
yield self._func(*elements)
index += 1
",[],0,[],/collections.py_iterate_from
796,/home/amandapotts/git/nltk/nltk/collections.py___getitem__,"def __getitem__(self, index):
if isinstance(index, slice):
sliced_lists = [lst[index] for lst in self._lists]
return LazyMap(self._func, *sliced_lists)
else:
if index < 0:
index += len(self)
if index < 0:
raise IndexError(""index out of range"")
if self._cache is not None and index in self._cache:
return self._cache[index]
try:
val = next(self.iterate_from(index))
except StopIteration as e:
raise IndexError(""index out of range"") from e
if self._cache is not None:
if len(self._cache) > self._cache_size:
self._cache.popitem()  # discard random entry
self._cache[index] = val
return val
",[],0,[],/collections.py___getitem__
797,/home/amandapotts/git/nltk/nltk/collections.py___len__,"def __len__(self):
return max(len(lst) for lst in self._lists)
",[],0,[],/collections.py___len__
798,/home/amandapotts/git/nltk/nltk/collections.py_iterate_from,"def iterate_from(self, index):
iterator = LazyMap.iterate_from(self, index)
while index < len(self):
yield next(iterator)
index += 1
return
",[],0,[],/collections.py_iterate_from
799,/home/amandapotts/git/nltk/nltk/collections.py___len__,"def __len__(self):
return min(len(lst) for lst in self._lists)
",[],0,[],/collections.py___len__
800,/home/amandapotts/git/nltk/nltk/collections.py___init__,"def __init__(self, lst):
""""""
:param lst: the underlying list
:type lst: list
""""""
LazyZip.__init__(self, range(len(lst)), lst)
",[],0,[],/collections.py___init__
801,/home/amandapotts/git/nltk/nltk/collections.py___init__,"def __init__(self, it, known_len=None):
self._it = it
self._len = known_len
self._cache = []
",[],0,[],/collections.py___init__
802,/home/amandapotts/git/nltk/nltk/collections.py___len__,"def __len__(self):
if self._len:
return self._len
for _ in self.iterate_from(len(self._cache)):
pass
self._len = len(self._cache)
return self._len
",[],0,[],/collections.py___len__
803,/home/amandapotts/git/nltk/nltk/collections.py_iterate_from,"def iterate_from(self, start):
""""""Create a new iterator over this list starting at the given offset.""""""
while len(self._cache) < start:
v = next(self._it)
self._cache.append(v)
i = start
while i < len(self._cache):
yield self._cache[i]
i += 1
try:
while True:
v = next(self._it)
self._cache.append(v)
yield v
except StopIteration:
pass
",[],0,[],/collections.py_iterate_from
804,/home/amandapotts/git/nltk/nltk/collections.py___add__,"def __add__(self, other):
""""""Return a list concatenating self with other.""""""
return type(self)(chain(self, other))
",[],0,[],/collections.py___add__
805,/home/amandapotts/git/nltk/nltk/collections.py___radd__,"def __radd__(self, other):
""""""Return a list concatenating other with self.""""""
return type(self)(chain(other, self))
",[],0,[],/collections.py___radd__
806,/home/amandapotts/git/nltk/nltk/collections.py___init__,"def __init__(self, strings=None):
""""""Builds a Trie object, which is built around a ``dict``
If ``strings`` is provided, it will add the ``strings``, which
consist of a ``list`` of ``strings``, to the Trie.
Otherwise, it'll construct an empty Trie.
:param strings: List of strings to insert into the trie
(Default is ``None``)
:type strings: list(str)
""""""
super().__init__()
if strings:
for string in strings:
self.insert(string)
",[],0,[],/collections.py___init__
807,/home/amandapotts/git/nltk/nltk/collections.py_insert,"def insert(self, string):
""""""Inserts ``string`` into the Trie
:param string: String to insert into the trie
:type string: str
:Example:
>>> from nltk.collections import Trie
>>> trie = Trie([""abc"", ""def""])
>>> expected = {'a': {'b': {'c': {True: None}}}, \
'd': {'e': {'f': {True: None}}}}
>>> trie == expected
True
""""""
if len(string):
self[string[0]].insert(string[1:])
else:
self[Trie.LEAF] = None
",[],0,[],/collections.py_insert
808,/home/amandapotts/git/nltk/nltk/collections.py___missing__,"def __missing__(self, key):
self[key] = Trie()
return self[key]
",[],0,[],/collections.py___missing__
809,/home/amandapotts/git/nltk/nltk/decorators.py___legacysignature,"def __legacysignature(signature):
""""""
For retrocompatibility reasons, we don't use a standard Signature.
Instead, we use the string generated by this method.
Basically, from a Signature we create a string and remove the default values.
""""""
listsignature = str(signature)[1:-1].split("","")
for counter, param in enumerate(listsignature):
if param.count(""="") > 0:
listsignature[counter] = param[0 : param.index(""="")].strip()
else:
listsignature[counter] = param.strip()
return "", "".join(listsignature)
",[],0,[],/decorators.py___legacysignature
810,/home/amandapotts/git/nltk/nltk/decorators.py_getinfo,"def getinfo(func):
""""""
Returns an info dictionary containing:
- name (the name of the function : str)
- argnames (the names of the arguments : list)
- defaults (the values of the default arguments : tuple)
- signature (the signature : str)
- fullsignature (the full signature : Signature)
- doc (the docstring : str)
- module (the module name : str)
- dict (the function __dict__ : str)
>>> def f(self, x=1, y=2, *args, **kw): pass
>>> info = getinfo(f)
>>> info[""name""]
'f'
>>> info[""argnames""]
['self', 'x', 'y', 'args', 'kw']
>>> info[""defaults""]
(1, 2)
>>> info[""signature""]
'self, x, y, *args, **kw'
>>> info[""fullsignature""]
<Signature (self, x=1, y=2, *args, **kw)>
""""""
assert inspect.ismethod(func) or inspect.isfunction(func)
argspec = inspect.getfullargspec(func)
regargs, varargs, varkwargs = argspec[:3]
argnames = list(regargs)
if varargs:
argnames.append(varargs)
if varkwargs:
argnames.append(varkwargs)
fullsignature = inspect.signature(func)
signature = __legacysignature(fullsignature)
if hasattr(func, ""__closure__""):
_closure = func.__closure__
_globals = func.__globals__
else:
_closure = func.func_closure
_globals = func.func_globals
return dict(
name=func.__name__,
argnames=argnames,
signature=signature,
fullsignature=fullsignature,
defaults=func.__defaults__,
doc=func.__doc__,
module=func.__module__,
dict=func.__dict__,
globals=_globals,
closure=_closure,
)
",[],0,[],/decorators.py_getinfo
811,/home/amandapotts/git/nltk/nltk/decorators.py_update_wrapper,"def update_wrapper(wrapper, model, infodict=None):
""akin to functools.update_wrapper""
infodict = infodict or getinfo(model)
wrapper.__name__ = infodict[""name""]
wrapper.__doc__ = infodict[""doc""]
wrapper.__module__ = infodict[""module""]
wrapper.__dict__.update(infodict[""dict""])
wrapper.__defaults__ = infodict[""defaults""]
wrapper.undecorated = model
return wrapper
",[],0,[],/decorators.py_update_wrapper
812,/home/amandapotts/git/nltk/nltk/decorators.py_decorator_factory,"def decorator_factory(cls):
""""""
Take a class with a ``.caller`` method and return a callable decorator
object. It works by adding a suitable __call__ method to the class
it raises a TypeError if the class already has a nontrivial __call__
method.
""""""
attrs = set(dir(cls))
if ""__call__"" in attrs:
raise TypeError(
""You cannot decorate a class with a nontrivial "" ""__call__ method""
)
if ""call"" not in attrs:
raise TypeError(""You cannot decorate a class without a "" "".call method"")
cls.__call__ = __call__
return cls
",[],0,[],/decorators.py_decorator_factory
813,/home/amandapotts/git/nltk/nltk/decorators.py_decorator,"def decorator(caller):
""""""
General purpose decorator factory: takes a caller function as
input and returns a decorator with the same attributes.
A caller function is any function like this::
def caller(func, *args, **kw):
return func(*args, **kw)
Here is an example of usage:
>>> @decorator
... def chatty(f, *args, **kw):
...     print(""Calling %r"" % f.__name__)
...     return f(*args, **kw)
>>> chatty.__name__
'chatty'
>>> @chatty
... def f(): pass
...
>>> f()
Calling 'f'
decorator can also take in input a class with a .caller method
case it converts the class into a factory of callable decorator objects.
See the documentation for an example.
""""""
if inspect.isclass(caller):
return decorator_factory(caller)
",[],0,[],/decorators.py_decorator
814,/home/amandapotts/git/nltk/nltk/decorators.py_getattr_,"def getattr_(obj, name, default_thunk):
""Similar to .setdefault in dictionaries.""
try:
return getattr(obj, name)
except AttributeError:
default = default_thunk()
setattr(obj, name, default)
return default
",[],0,[],/decorators.py_getattr_
815,/home/amandapotts/git/nltk/nltk/decorators.py_memoize,"def memoize(func, *args):
dic = getattr_(func, ""memoize_dic"", dict)
if args in dic:
return dic[args]
result = func(*args)
dic[args] = result
return result
",[],0,[],/decorators.py_memoize
816,/home/amandapotts/git/nltk/nltk/collocations.py___init__,"def __init__(self, word_fd, ngram_fd):
self.word_fd = word_fd
self.N = word_fd.N()
self.ngram_fd = ngram_fd
",[],0,[],/collocations.py___init__
817,/home/amandapotts/git/nltk/nltk/collocations.py__build_new_documents,"def _build_new_documents(
cls, documents, window_size, pad_left=False, pad_right=False, pad_symbol=None
",[],0,[],/collocations.py__build_new_documents
818,/home/amandapotts/git/nltk/nltk/collocations.py_from_documents,"def from_documents(cls, documents):
""""""Constructs a collocation finder given a collection of documents,
each of which is a list (or iterable) of tokens.
""""""
return cls.from_words(
cls._build_new_documents(documents, cls.default_ws, pad_right=True)
)
",[],0,[],/collocations.py_from_documents
819,/home/amandapotts/git/nltk/nltk/collocations.py__ngram_freqdist,"def _ngram_freqdist(words, n):
return FreqDist(tuple(words[i : i + n]) for i in range(len(words) - 1))
",[],0,[],/collocations.py__ngram_freqdist
820,/home/amandapotts/git/nltk/nltk/collocations.py__score_ngrams,"def _score_ngrams(self, score_fn):
""""""Generates of (ngram, score) pairs as determined by the scoring
function provided.
""""""
for tup in self.ngram_fd:
score = self.score_ngram(score_fn, *tup)
if score is not None:
yield tup, score
",[],0,[],/collocations.py__score_ngrams
821,/home/amandapotts/git/nltk/nltk/collocations.py_nbest,"def nbest(self, score_fn, n):
""""""Returns the top n ngrams when scored by the given function.""""""
return [p for p, s in self.score_ngrams(score_fn)[:n]]
",[],0,[],/collocations.py_nbest
822,/home/amandapotts/git/nltk/nltk/collocations.py_above_score,"def above_score(self, score_fn, min_score):
""""""Returns a sequence of ngrams, ordered by decreasing score, whose
scores each exceed the given minimum score.
""""""
for ngram, score in self.score_ngrams(score_fn):
if score > min_score:
yield ngram
else:
break
",[],0,[],/collocations.py_above_score
823,/home/amandapotts/git/nltk/nltk/collocations.py___init__,"def __init__(self, word_fd, bigram_fd, window_size=2):
""""""Construct a BigramCollocationFinder, given FreqDists for
appearances of words and (possibly non-contiguous) bigrams.
""""""
AbstractCollocationFinder.__init__(self, word_fd, bigram_fd)
self.window_size = window_size
",[],0,[],/collocations.py___init__
824,/home/amandapotts/git/nltk/nltk/collocations.py_from_words,"def from_words(cls, words, window_size=2):
""""""Construct a BigramCollocationFinder for all bigrams in the given
sequence.  When window_size > 2, count non-contiguous bigrams, in the
style of Church and Hanks's (1990) association ratio.
""""""
wfd = FreqDist()
bfd = FreqDist()
if window_size < 2:
raise ValueError(""Specify window_size at least 2"")
for window in ngrams(words, window_size, pad_right=True):
w1 = window[0]
if w1 is None:
continue
wfd[w1] += 1
for w2 in window[1:]:
if w2 is not None:
bfd[(w1, w2)] += 1
return cls(wfd, bfd, window_size=window_size)
",[],0,[],/collocations.py_from_words
825,/home/amandapotts/git/nltk/nltk/collocations.py_score_ngram,"def score_ngram(self, score_fn, w1, w2):
""""""Returns the score for a given bigram using the given scoring
function.  Following Church and Hanks (1990), counts are scaled by
a factor of 1/(window_size - 1).
""""""
n_all = self.N
n_ii = self.ngram_fd[(w1, w2)] / (self.window_size - 1.0)
if not n_ii:
return
n_ix = self.word_fd[w1]
n_xi = self.word_fd[w2]
return score_fn(n_ii, (n_ix, n_xi), n_all)
",[],0,[],/collocations.py_score_ngram
826,/home/amandapotts/git/nltk/nltk/collocations.py___init__,"def __init__(self, word_fd, bigram_fd, wildcard_fd, trigram_fd):
""""""Construct a TrigramCollocationFinder, given FreqDists for
appearances of words, bigrams, two words with any word between them,
and trigrams.
""""""
AbstractCollocationFinder.__init__(self, word_fd, trigram_fd)
self.wildcard_fd = wildcard_fd
self.bigram_fd = bigram_fd
",[],0,[],/collocations.py___init__
827,/home/amandapotts/git/nltk/nltk/collocations.py_from_words,"def from_words(cls, words, window_size=3):
""""""Construct a TrigramCollocationFinder for all trigrams in the given
sequence.
""""""
if window_size < 3:
raise ValueError(""Specify window_size at least 3"")
wfd = FreqDist()
wildfd = FreqDist()
bfd = FreqDist()
tfd = FreqDist()
for window in ngrams(words, window_size, pad_right=True):
w1 = window[0]
if w1 is None:
continue
for w2, w3 in _itertools.combinations(window[1:], 2):
wfd[w1] += 1
if w2 is None:
continue
bfd[(w1, w2)] += 1
if w3 is None:
continue
wildfd[(w1, w3)] += 1
tfd[(w1, w2, w3)] += 1
return cls(wfd, bfd, wildfd, tfd)
",[],0,[],/collocations.py_from_words
828,/home/amandapotts/git/nltk/nltk/collocations.py_bigram_finder,"def bigram_finder(self):
""""""Constructs a bigram collocation finder with the bigram and unigram
data from this finder. Note that this does not include any filtering
applied to this finder.
""""""
return BigramCollocationFinder(self.word_fd, self.bigram_fd)
",[],0,[],/collocations.py_bigram_finder
829,/home/amandapotts/git/nltk/nltk/collocations.py_score_ngram,"def score_ngram(self, score_fn, w1, w2, w3):
""""""Returns the score for a given trigram using the given scoring
function.
""""""
n_all = self.N
n_iii = self.ngram_fd[(w1, w2, w3)]
if not n_iii:
return
n_iix = self.bigram_fd[(w1, w2)]
n_ixi = self.wildcard_fd[(w1, w3)]
n_xii = self.bigram_fd[(w2, w3)]
n_ixx = self.word_fd[w1]
n_xix = self.word_fd[w2]
n_xxi = self.word_fd[w3]
return score_fn(n_iii, (n_iix, n_ixi, n_xii), (n_ixx, n_xix, n_xxi), n_all)
",[],0,[],/collocations.py_score_ngram
830,/home/amandapotts/git/nltk/nltk/collocations.py___init__,"def __init__(self, word_fd, quadgram_fd, ii, iii, ixi, ixxi, iixi, ixii):
""""""Construct a QuadgramCollocationFinder, given FreqDists for appearances of words,
bigrams, trigrams, two words with one word and two words between them, three words
with a word between them in both variations.
""""""
AbstractCollocationFinder.__init__(self, word_fd, quadgram_fd)
self.iii = iii
self.ii = ii
self.ixi = ixi
self.ixxi = ixxi
self.iixi = iixi
self.ixii = ixii
",[],0,[],/collocations.py___init__
831,/home/amandapotts/git/nltk/nltk/collocations.py_from_words,"def from_words(cls, words, window_size=4):
if window_size < 4:
raise ValueError(""Specify window_size at least 4"")
ixxx = FreqDist()
iiii = FreqDist()
ii = FreqDist()
iii = FreqDist()
ixi = FreqDist()
ixxi = FreqDist()
iixi = FreqDist()
ixii = FreqDist()
for window in ngrams(words, window_size, pad_right=True):
w1 = window[0]
if w1 is None:
continue
for w2, w3, w4 in _itertools.combinations(window[1:], 3):
ixxx[w1] += 1
if w2 is None:
continue
ii[(w1, w2)] += 1
if w3 is None:
continue
iii[(w1, w2, w3)] += 1
ixi[(w1, w3)] += 1
if w4 is None:
continue
iiii[(w1, w2, w3, w4)] += 1
ixxi[(w1, w4)] += 1
ixii[(w1, w3, w4)] += 1
iixi[(w1, w2, w4)] += 1
return cls(ixxx, iiii, ii, iii, ixi, ixxi, iixi, ixii)
",[],0,[],/collocations.py_from_words
832,/home/amandapotts/git/nltk/nltk/collocations.py_score_ngram,"def score_ngram(self, score_fn, w1, w2, w3, w4):
n_all = self.N
n_iiii = self.ngram_fd[(w1, w2, w3, w4)]
if not n_iiii:
return
n_iiix = self.iii[(w1, w2, w3)]
n_xiii = self.iii[(w2, w3, w4)]
n_iixi = self.iixi[(w1, w2, w4)]
n_ixii = self.ixii[(w1, w3, w4)]
n_iixx = self.ii[(w1, w2)]
n_xxii = self.ii[(w3, w4)]
n_xiix = self.ii[(w2, w3)]
n_ixix = self.ixi[(w1, w3)]
n_ixxi = self.ixxi[(w1, w4)]
n_xixi = self.ixi[(w2, w4)]
n_ixxx = self.word_fd[w1]
n_xixx = self.word_fd[w2]
n_xxix = self.word_fd[w3]
n_xxxi = self.word_fd[w4]
return score_fn(
n_iiii,
(n_iiix, n_iixi, n_ixii, n_xiii),
(n_iixx, n_ixix, n_ixxi, n_xixi, n_xxii, n_xiix),
(n_ixxx, n_xixx, n_xxix, n_xxxi),
n_all,
)
",[],0,[],/collocations.py_score_ngram
833,/home/amandapotts/git/nltk/nltk/collocations.py_demo,"def demo(scorer=None, compare_scorer=None):
""""""Finds bigram collocations in the files of the WebText corpus.""""""
from nltk.metrics import (
BigramAssocMeasures,
ranks_from_scores,
spearman_correlation,
)
if scorer is None:
scorer = BigramAssocMeasures.likelihood_ratio
if compare_scorer is None:
compare_scorer = BigramAssocMeasures.raw_freq
from nltk.corpus import stopwords, webtext
ignored_words = stopwords.words(""english"")
",[],0,[],/collocations.py_demo
834,/home/amandapotts/git/nltk/nltk/app/nemo_app.py___init__,"def __init__(self, image, initialField, initialText):
frm = Frame(root)
frm.config(background=""white"")
self.image = PhotoImage(format=""gif"", data=images[image.upper()])
self.imageDimmed = PhotoImage(format=""gif"", data=images[image])
self.img = Label(frm)
self.img.config(borderwidth=0)
self.img.pack(side=""left"")
self.fld = Text(frm, **fieldParams)
self.initScrollText(frm, self.fld, initialField)
frm = Frame(root)
self.txt = Text(frm, **textParams)
self.initScrollText(frm, self.txt, initialText)
for i in range(2):
self.txt.tag_config(colors[i], background=colors[i])
self.txt.tag_config(""emph"" + colors[i], foreground=emphColors[i])
",[],0,[],/app/nemo_app.py___init__
835,/home/amandapotts/git/nltk/nltk/app/nemo_app.py_initScrollText,"def initScrollText(self, frm, txt, contents):
scl = Scrollbar(frm)
scl.config(command=txt.yview)
scl.pack(side=""right"", fill=""y"")
txt.pack(side=""left"", expand=True, fill=""x"")
txt.config(yscrollcommand=scl.set)
txt.insert(""1.0"", contents)
frm.pack(fill=""x"")
Frame(height=2, bd=1, relief=""ridge"").pack(fill=""x"")
",[],0,[],/app/nemo_app.py_initScrollText
836,/home/amandapotts/git/nltk/nltk/app/nemo_app.py_refresh,"def refresh(self):
self.colorCycle = itertools.cycle(colors)
try:
self.substitute()
self.img.config(image=self.image)
except re.error:
self.img.config(image=self.imageDimmed)
",[],0,[],/app/nemo_app.py_refresh
837,/home/amandapotts/git/nltk/nltk/app/nemo_app.py_addTags,"def addTags(self, m):
color = next(self.colorCycle)
self.txt.tag_add(color, ""1.0+%sc"" % m.start(), ""1.0+%sc"" % m.end())
try:
self.txt.tag_add(
""emph"" + color, ""1.0+%sc"" % m.start(""emph""), ""1.0+%sc"" % m.end(""emph"")
)
except:
pass
",[],0,[],/app/nemo_app.py_addTags
838,/home/amandapotts/git/nltk/nltk/app/nemo_app.py_substitute,"def substitute(self, *args):
for color in colors:
self.txt.tag_remove(color, ""1.0"", ""end"")
self.txt.tag_remove(""emph"" + color, ""1.0"", ""end"")
self.rex = re.compile("""")  # default value in case of malformed regexp
self.rex = re.compile(self.fld.get(""1.0"", ""end"")[:-1], re.MULTILINE)
try:
re.compile(""(?P<emph>%s)"" % self.fld.get(SEL_FIRST, SEL_LAST))
self.rexSel = re.compile(
""%s(?P<emph>%s)%s""
% (
self.fld.get(""1.0"", SEL_FIRST),
self.fld.get(SEL_FIRST, SEL_LAST),
self.fld.get(SEL_LAST, ""end"")[:-1],
),
re.MULTILINE,
)
except:
self.rexSel = self.rex
self.rexSel.sub(self.addTags, self.txt.get(""1.0"", ""end""))
",[],0,[],/app/nemo_app.py_substitute
839,/home/amandapotts/git/nltk/nltk/app/nemo_app.py_addTags,"def addTags(self, m):
s = sz.rex.sub(self.repl, m.group())
self.txt.delete(
""1.0+%sc"" % (m.start() + self.diff), ""1.0+%sc"" % (m.end() + self.diff)
)
self.txt.insert(""1.0+%sc"" % (m.start() + self.diff), s, next(self.colorCycle))
self.diff += len(s) - (m.end() - m.start())
",[],0,[],/app/nemo_app.py_addTags
840,/home/amandapotts/git/nltk/nltk/app/nemo_app.py_substitute,"def substitute(self):
self.txt.delete(""1.0"", ""end"")
self.txt.insert(""1.0"", sz.txt.get(""1.0"", ""end"")[:-1])
self.diff = 0
self.repl = rex0.sub(r""\\g<\1>"", self.fld.get(""1.0"", ""end"")[:-1])
sz.rex.sub(self.addTags, sz.txt.get(""1.0"", ""end"")[:-1])
",[],0,[],/app/nemo_app.py_substitute
841,/home/amandapotts/git/nltk/nltk/app/nemo_app.py_launchRefresh,"def launchRefresh(_):
sz.fld.after_idle(sz.refresh)
rz.fld.after_idle(rz.refresh)
",[],0,[],/app/nemo_app.py_launchRefresh
842,/home/amandapotts/git/nltk/nltk/app/nemo_app.py_app,"def app():
global root, sz, rz, rex0
root = Tk()
root.resizable(height=False, width=True)
root.title(windowTitle)
root.minsize(width=250, height=0)
sz = FindZone(""find"", initialFind, initialText)
sz.fld.bind(""<Button-1>"", launchRefresh)
sz.fld.bind(""<ButtonRelease-1>"", launchRefresh)
sz.fld.bind(""<B1-Motion>"", launchRefresh)
sz.rexSel = re.compile("""")
rz = ReplaceZone(""repl"", initialRepl, """")
rex0 = re.compile(r""(?<!\\)\\([0-9]+)"")
root.bind_all(""<Key>"", launchRefresh)
launchRefresh(None)
root.mainloop()
",[],0,[],/app/nemo_app.py_app
843,/home/amandapotts/git/nltk/nltk/app/concordance_app.py___init__,"def __init__(self):
self.queue = q.Queue()
self.model = ConcordanceSearchModel(self.queue)
self.top = Tk()
self._init_top(self.top)
self._init_menubar()
self._init_widgets(self.top)
self.load_corpus(self.model.DEFAULT_CORPUS)
self.after = self.top.after(POLL_INTERVAL, self._poll)
",[],0,[],/app/concordance_app.py___init__
844,/home/amandapotts/git/nltk/nltk/app/concordance_app.py__init_top,"def _init_top(self, top):
top.geometry(""950x680+50+50"")
top.title(""NLTK Concordance Search"")
top.bind(""<Control-q>"", self.destroy)
top.protocol(""WM_DELETE_WINDOW"", self.destroy)
top.minsize(950, 680)
",[],0,[],/app/concordance_app.py__init_top
845,/home/amandapotts/git/nltk/nltk/app/concordance_app.py__init_widgets,"def _init_widgets(self, parent):
self.main_frame = Frame(
parent, dict(background=self._BACKGROUND_COLOUR, padx=1, pady=1, border=1)
)
self._init_corpus_select(self.main_frame)
self._init_query_box(self.main_frame)
self._init_results_box(self.main_frame)
self._init_paging(self.main_frame)
self._init_status(self.main_frame)
self.main_frame.pack(fill=""both"", expand=True)
",[],0,[],/app/concordance_app.py__init_widgets
846,/home/amandapotts/git/nltk/nltk/app/concordance_app.py__init_menubar,"def _init_menubar(self):
self._result_size = IntVar(self.top)
self._cntx_bf_len = IntVar(self.top)
self._cntx_af_len = IntVar(self.top)
menubar = Menu(self.top)
filemenu = Menu(menubar, tearoff=0, borderwidth=0)
filemenu.add_command(
label=""Exit"", underline=1, command=self.destroy, accelerator=""Ctrl-q""
)
menubar.add_cascade(label=""File"", underline=0, menu=filemenu)
editmenu = Menu(menubar, tearoff=0)
rescntmenu = Menu(editmenu, tearoff=0)
rescntmenu.add_radiobutton(
label=""20"",
variable=self._result_size,
underline=0,
value=20,
command=self.set_result_size,
)
rescntmenu.add_radiobutton(
label=""50"",
variable=self._result_size,
underline=0,
value=50,
command=self.set_result_size,
)
rescntmenu.add_radiobutton(
label=""100"",
variable=self._result_size,
underline=0,
value=100,
command=self.set_result_size,
)
rescntmenu.invoke(1)
editmenu.add_cascade(label=""Result Count"", underline=0, menu=rescntmenu)
cntxmenu = Menu(editmenu, tearoff=0)
cntxbfmenu = Menu(cntxmenu, tearoff=0)
cntxbfmenu.add_radiobutton(
label=""60 characters"",
variable=self._cntx_bf_len,
underline=0,
value=60,
command=self.set_cntx_bf_len,
)
cntxbfmenu.add_radiobutton(
label=""80 characters"",
variable=self._cntx_bf_len,
underline=0,
value=80,
command=self.set_cntx_bf_len,
)
cntxbfmenu.add_radiobutton(
label=""100 characters"",
variable=self._cntx_bf_len,
underline=0,
value=100,
command=self.set_cntx_bf_len,
)
cntxbfmenu.invoke(1)
cntxmenu.add_cascade(label=""Before"", underline=0, menu=cntxbfmenu)
cntxafmenu = Menu(cntxmenu, tearoff=0)
cntxafmenu.add_radiobutton(
label=""70 characters"",
variable=self._cntx_af_len,
underline=0,
value=70,
command=self.set_cntx_af_len,
)
cntxafmenu.add_radiobutton(
label=""90 characters"",
variable=self._cntx_af_len,
underline=0,
value=90,
command=self.set_cntx_af_len,
)
cntxafmenu.add_radiobutton(
label=""110 characters"",
variable=self._cntx_af_len,
underline=0,
value=110,
command=self.set_cntx_af_len,
)
cntxafmenu.invoke(1)
cntxmenu.add_cascade(label=""After"", underline=0, menu=cntxafmenu)
editmenu.add_cascade(label=""Context"", underline=0, menu=cntxmenu)
menubar.add_cascade(label=""Edit"", underline=0, menu=editmenu)
self.top.config(menu=menubar)
",[],0,[],/app/concordance_app.py__init_menubar
847,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_set_result_size,"def set_result_size(self, **kwargs):
self.model.result_count = self._result_size.get()
",[],0,[],/app/concordance_app.py_set_result_size
848,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_set_cntx_af_len,"def set_cntx_af_len(self, **kwargs):
self._char_after = self._cntx_af_len.get()
",[],0,[],/app/concordance_app.py_set_cntx_af_len
849,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_set_cntx_bf_len,"def set_cntx_bf_len(self, **kwargs):
self._char_before = self._cntx_bf_len.get()
",[],0,[],/app/concordance_app.py_set_cntx_bf_len
850,/home/amandapotts/git/nltk/nltk/app/concordance_app.py__init_corpus_select,"def _init_corpus_select(self, parent):
innerframe = Frame(parent, background=self._BACKGROUND_COLOUR)
self.var = StringVar(innerframe)
self.var.set(self.model.DEFAULT_CORPUS)
Label(
innerframe,
justify=LEFT,
text="" Corpus: "",
background=self._BACKGROUND_COLOUR,
padx=2,
pady=1,
border=0,
).pack(side=""left"")
other_corpora = list(self.model.CORPORA.keys()).remove(
self.model.DEFAULT_CORPUS
)
om = OptionMenu(
innerframe,
self.var,
self.model.DEFAULT_CORPUS,
command=self.corpus_selected,
)
om[""borderwidth""] = 0
om[""highlightthickness""] = 1
om.pack(side=""left"")
innerframe.pack(side=""top"", fill=""x"", anchor=""n"")
",[],0,[],/app/concordance_app.py__init_corpus_select
851,/home/amandapotts/git/nltk/nltk/app/concordance_app.py__init_status,"def _init_status(self, parent):
self.status = Label(
parent,
justify=LEFT,
relief=SUNKEN,
background=self._BACKGROUND_COLOUR,
border=0,
padx=1,
pady=0,
)
self.status.pack(side=""top"", anchor=""sw"")
",[],0,[],/app/concordance_app.py__init_status
852,/home/amandapotts/git/nltk/nltk/app/concordance_app.py__init_query_box,"def _init_query_box(self, parent):
innerframe = Frame(parent, background=self._BACKGROUND_COLOUR)
another = Frame(innerframe, background=self._BACKGROUND_COLOUR)
self.query_box = Entry(another, width=60)
self.query_box.pack(side=""left"", fill=""x"", pady=25, anchor=""center"")
self.search_button = Button(
another,
text=""Search"",
command=self.search,
borderwidth=1,
highlightthickness=1,
)
self.search_button.pack(side=""left"", fill=""x"", pady=25, anchor=""center"")
self.query_box.bind(""<KeyPress-Return>"", self.search_enter_keypress_handler)
another.pack()
innerframe.pack(side=""top"", fill=""x"", anchor=""n"")
",[],0,[],/app/concordance_app.py__init_query_box
853,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_search_enter_keypress_handler,"def search_enter_keypress_handler(self, *event):
self.search()
",[],0,[],/app/concordance_app.py_search_enter_keypress_handler
854,/home/amandapotts/git/nltk/nltk/app/concordance_app.py__init_results_box,"def _init_results_box(self, parent):
innerframe = Frame(parent)
i1 = Frame(innerframe)
i2 = Frame(innerframe)
vscrollbar = Scrollbar(i1, borderwidth=1)
hscrollbar = Scrollbar(i2, borderwidth=1, orient=""horiz"")
self.results_box = Text(
i1,
font=Font(family=""courier"", size=""16""),
state=""disabled"",
borderwidth=1,
yscrollcommand=vscrollbar.set,
xscrollcommand=hscrollbar.set,
wrap=""none"",
width=""40"",
height=""20"",
exportselection=1,
)
self.results_box.pack(side=""left"", fill=""both"", expand=True)
self.results_box.tag_config(
self._HIGHLIGHT_WORD_TAG, foreground=self._HIGHLIGHT_WORD_COLOUR
)
self.results_box.tag_config(
self._HIGHLIGHT_LABEL_TAG, foreground=self._HIGHLIGHT_LABEL_COLOUR
)
vscrollbar.pack(side=""left"", fill=""y"", anchor=""e"")
vscrollbar.config(command=self.results_box.yview)
hscrollbar.pack(side=""left"", fill=""x"", expand=True, anchor=""w"")
hscrollbar.config(command=self.results_box.xview)
Label(i2, text=""   "", background=self._BACKGROUND_COLOUR).pack(
side=""left"", anchor=""e""
)
i1.pack(side=""top"", fill=""both"", expand=True, anchor=""n"")
i2.pack(side=""bottom"", fill=""x"", anchor=""s"")
innerframe.pack(side=""top"", fill=""both"", expand=True)
",[],0,[],/app/concordance_app.py__init_results_box
855,/home/amandapotts/git/nltk/nltk/app/concordance_app.py__init_paging,"def _init_paging(self, parent):
innerframe = Frame(parent, background=self._BACKGROUND_COLOUR)
self.prev = prev = Button(
innerframe,
text=""Previous"",
command=self.previous,
width=""10"",
borderwidth=1,
highlightthickness=1,
state=""disabled"",
)
prev.pack(side=""left"", anchor=""center"")
self.next = next = Button(
innerframe,
text=""Next"",
command=self.__next__,
width=""10"",
borderwidth=1,
highlightthickness=1,
state=""disabled"",
)
next.pack(side=""right"", anchor=""center"")
innerframe.pack(side=""top"", fill=""y"")
self.current_page = 0
",[],0,[],/app/concordance_app.py__init_paging
856,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_previous,"def previous(self):
self.clear_results_box()
self.freeze_editable()
self.model.prev(self.current_page - 1)
",[],0,[],/app/concordance_app.py_previous
857,/home/amandapotts/git/nltk/nltk/app/concordance_app.py___next__,"def __next__(self):
self.clear_results_box()
self.freeze_editable()
self.model.next(self.current_page + 1)
",[],0,[],/app/concordance_app.py___next__
858,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_about,"def about(self, *e):
ABOUT = ""NLTK Concordance Search Demo\n""
TITLE = ""About: NLTK Concordance Search Demo""
try:
from tkinter.messagebox import Message
Message(message=ABOUT, title=TITLE, parent=self.main_frame).show()
except:
ShowText(self.top, TITLE, ABOUT)
",[],0,[],/app/concordance_app.py_about
859,/home/amandapotts/git/nltk/nltk/app/concordance_app.py__bind_event_handlers,"def _bind_event_handlers(self):
self.top.bind(CORPUS_LOADED_EVENT, self.handle_corpus_loaded)
self.top.bind(SEARCH_TERMINATED_EVENT, self.handle_search_terminated)
self.top.bind(SEARCH_ERROR_EVENT, self.handle_search_error)
self.top.bind(ERROR_LOADING_CORPUS_EVENT, self.handle_error_loading_corpus)
",[],0,[],/app/concordance_app.py__bind_event_handlers
860,/home/amandapotts/git/nltk/nltk/app/concordance_app.py__poll,"def _poll(self):
try:
event = self.queue.get(block=False)
except q.Empty:
pass
else:
if event == CORPUS_LOADED_EVENT:
self.handle_corpus_loaded(event)
elif event == SEARCH_TERMINATED_EVENT:
self.handle_search_terminated(event)
elif event == SEARCH_ERROR_EVENT:
self.handle_search_error(event)
elif event == ERROR_LOADING_CORPUS_EVENT:
self.handle_error_loading_corpus(event)
self.after = self.top.after(POLL_INTERVAL, self._poll)
",[],0,[],/app/concordance_app.py__poll
861,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_handle_error_loading_corpus,"def handle_error_loading_corpus(self, event):
self.status[""text""] = ""Error in loading "" + self.var.get()
self.unfreeze_editable()
self.clear_all()
self.freeze_editable()
",[],0,[],/app/concordance_app.py_handle_error_loading_corpus
862,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_handle_corpus_loaded,"def handle_corpus_loaded(self, event):
self.status[""text""] = self.var.get() + "" is loaded""
self.unfreeze_editable()
self.clear_all()
self.query_box.focus_set()
",[],0,[],/app/concordance_app.py_handle_corpus_loaded
863,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_handle_search_terminated,"def handle_search_terminated(self, event):
results = self.model.get_results()
self.write_results(results)
self.status[""text""] = """"
if len(results) == 0:
self.status[""text""] = ""No results found for "" + self.model.query
else:
self.current_page = self.model.last_requested_page
self.unfreeze_editable()
self.results_box.xview_moveto(self._FRACTION_LEFT_TEXT)
",[],0,[],/app/concordance_app.py_handle_search_terminated
864,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_handle_search_error,"def handle_search_error(self, event):
self.status[""text""] = ""Error in query "" + self.model.query
self.unfreeze_editable()
",[],0,[],/app/concordance_app.py_handle_search_error
865,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_corpus_selected,"def corpus_selected(self, *args):
new_selection = self.var.get()
self.load_corpus(new_selection)
",[],0,[],/app/concordance_app.py_corpus_selected
866,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_load_corpus,"def load_corpus(self, selection):
if self.model.selected_corpus != selection:
self.status[""text""] = ""Loading "" + selection + ""...""
self.freeze_editable()
self.model.load_corpus(selection)
",[],0,[],/app/concordance_app.py_load_corpus
867,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_search,"def search(self):
self.current_page = 0
self.clear_results_box()
self.model.reset_results()
query = self.query_box.get()
if len(query.strip()) == 0:
return
self.status[""text""] = ""Searching for "" + query
self.freeze_editable()
self.model.search(query, self.current_page + 1)
",[],0,[],/app/concordance_app.py_search
868,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_write_results,"def write_results(self, results):
self.results_box[""state""] = ""normal""
row = 1
for each in results:
sent, pos1, pos2 = each[0].strip(), each[1], each[2]
if len(sent) != 0:
if pos1 < self._char_before:
sent, pos1, pos2 = self.pad(sent, pos1, pos2)
sentence = sent[pos1 - self._char_before : pos1 + self._char_after]
if not row == len(results):
sentence += ""\n""
self.results_box.insert(str(row) + "".0"", sentence)
word_markers, label_markers = self.words_and_labels(sent, pos1, pos2)
for marker in word_markers:
self.results_box.tag_add(
self._HIGHLIGHT_WORD_TAG,
str(row) + ""."" + str(marker[0]),
str(row) + ""."" + str(marker[1]),
)
for marker in label_markers:
self.results_box.tag_add(
self._HIGHLIGHT_LABEL_TAG,
str(row) + ""."" + str(marker[0]),
str(row) + ""."" + str(marker[1]),
)
row += 1
self.results_box[""state""] = ""disabled""
",[],0,[],/app/concordance_app.py_write_results
869,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_words_and_labels,"def words_and_labels(self, sentence, pos1, pos2):
search_exp = sentence[pos1:pos2]
words, labels = [], []
labeled_words = search_exp.split("" "")
index = 0
for each in labeled_words:
if each == """":
index += 1
else:
word, label = each.split(""/"")
words.append(
(self._char_before + index, self._char_before + index + len(word))
)
index += len(word) + 1
labels.append(
(self._char_before + index, self._char_before + index + len(label))
)
index += len(label)
index += 1
return words, labels
",[],0,[],/app/concordance_app.py_words_and_labels
870,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_pad,"def pad(self, sent, hstart, hend):
if hstart >= self._char_before:
return sent, hstart, hend
d = self._char_before - hstart
sent = """".join(["" ""] * d) + sent
return sent, hstart + d, hend + d
",[],0,[],/app/concordance_app.py_pad
871,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_destroy,"def destroy(self, *e):
if self.top is None:
return
self.top.after_cancel(self.after)
self.top.destroy()
self.top = None
",[],0,[],/app/concordance_app.py_destroy
872,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_clear_all,"def clear_all(self):
self.query_box.delete(0, END)
self.model.reset_query()
self.clear_results_box()
",[],0,[],/app/concordance_app.py_clear_all
873,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_clear_results_box,"def clear_results_box(self):
self.results_box[""state""] = ""normal""
self.results_box.delete(""1.0"", END)
self.results_box[""state""] = ""disabled""
",[],0,[],/app/concordance_app.py_clear_results_box
874,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_freeze_editable,"def freeze_editable(self):
self.query_box[""state""] = ""disabled""
self.search_button[""state""] = ""disabled""
self.prev[""state""] = ""disabled""
self.next[""state""] = ""disabled""
",[],0,[],/app/concordance_app.py_freeze_editable
875,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_unfreeze_editable,"def unfreeze_editable(self):
self.query_box[""state""] = ""normal""
self.search_button[""state""] = ""normal""
self.set_paging_button_states()
",[],0,[],/app/concordance_app.py_unfreeze_editable
876,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_set_paging_button_states,"def set_paging_button_states(self):
if self.current_page == 0 or self.current_page == 1:
self.prev[""state""] = ""disabled""
else:
self.prev[""state""] = ""normal""
if self.model.has_more_pages(self.current_page):
self.next[""state""] = ""normal""
else:
self.next[""state""] = ""disabled""
",[],0,[],/app/concordance_app.py_set_paging_button_states
877,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_fire_event,"def fire_event(self, event):
self.top.event_generate(event, when=""tail"")
",[],0,[],/app/concordance_app.py_fire_event
878,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_mainloop,"def mainloop(self, *args, **kwargs):
if in_idle():
return
self.top.mainloop(*args, **kwargs)
",[],0,[],/app/concordance_app.py_mainloop
879,/home/amandapotts/git/nltk/nltk/app/concordance_app.py___init__,"def __init__(self, queue):
self.queue = queue
self.CORPORA = _CORPORA
self.DEFAULT_CORPUS = _DEFAULT
self.selected_corpus = None
self.reset_query()
self.reset_results()
self.result_count = None
self.last_sent_searched = 0
",[],0,[],/app/concordance_app.py___init__
880,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_non_default_corpora,"def non_default_corpora(self):
copy = []
copy.extend(list(self.CORPORA.keys()))
copy.remove(self.DEFAULT_CORPUS)
copy.sort()
return copy
",[],0,[],/app/concordance_app.py_non_default_corpora
881,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_load_corpus,"def load_corpus(self, name):
self.selected_corpus = name
self.tagged_sents = []
runner_thread = self.LoadCorpus(name, self)
runner_thread.start()
",[],0,[],/app/concordance_app.py_load_corpus
882,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_search,"def search(self, query, page):
self.query = query
self.last_requested_page = page
self.SearchCorpus(self, page, self.result_count).start()
",[],0,[],/app/concordance_app.py_search
883,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_next,"def next(self, page):
self.last_requested_page = page
if len(self.results) < page:
self.search(self.query, page)
else:
self.queue.put(SEARCH_TERMINATED_EVENT)
",[],0,[],/app/concordance_app.py_next
884,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_prev,"def prev(self, page):
self.last_requested_page = page
self.queue.put(SEARCH_TERMINATED_EVENT)
",[],0,[],/app/concordance_app.py_prev
885,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_reset_results,"def reset_results(self):
self.last_sent_searched = 0
self.results = []
self.last_page = None
",[],0,[],/app/concordance_app.py_reset_results
886,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_reset_query,"def reset_query(self):
self.query = None
",[],0,[],/app/concordance_app.py_reset_query
887,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_set_results,"def set_results(self, page, resultset):
self.results.insert(page - 1, resultset)
",[],0,[],/app/concordance_app.py_set_results
888,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_get_results,"def get_results(self):
return self.results[self.last_requested_page - 1]
",[],0,[],/app/concordance_app.py_get_results
889,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_has_more_pages,"def has_more_pages(self, page):
if self.results == [] or self.results[0] == []:
return False
if self.last_page is None:
return True
return page < self.last_page
",[],0,[],/app/concordance_app.py_has_more_pages
890,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_LoadCorpus,"class LoadCorpus(threading.Thread):
",[],0,[],/app/concordance_app.py_LoadCorpus
891,/home/amandapotts/git/nltk/nltk/app/concordance_app.py___init__,"def __init__(self, name, model):
threading.Thread.__init__(self)
self.model, self.name = model, name
",[],0,[],/app/concordance_app.py___init__
892,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_run,"def run(self):
try:
ts = self.model.CORPORA[self.name]()
self.model.tagged_sents = [
"" "".join(w + ""/"" + t for (w, t) in sent) for sent in ts
]
self.model.queue.put(CORPUS_LOADED_EVENT)
except Exception as e:
print(e)
self.model.queue.put(ERROR_LOADING_CORPUS_EVENT)
",[],0,[],/app/concordance_app.py_run
893,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_SearchCorpus,"class SearchCorpus(threading.Thread):
",[],0,[],/app/concordance_app.py_SearchCorpus
894,/home/amandapotts/git/nltk/nltk/app/concordance_app.py___init__,"def __init__(self, model, page, count):
self.model, self.count, self.page = model, count, page
threading.Thread.__init__(self)
",[],0,[],/app/concordance_app.py___init__
895,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_run,"def run(self):
q = self.processed_query()
sent_pos, i, sent_count = [], 0, 0
for sent in self.model.tagged_sents[self.model.last_sent_searched :]:
try:
m = re.search(q, sent)
except re.error:
self.model.reset_results()
self.model.queue.put(SEARCH_ERROR_EVENT)
return
if m:
sent_pos.append((sent, m.start(), m.end()))
i += 1
if i > self.count:
self.model.last_sent_searched += sent_count - 1
break
sent_count += 1
if self.count >= len(sent_pos):
self.model.last_sent_searched += sent_count - 1
self.model.last_page = self.page
self.model.set_results(self.page, sent_pos)
else:
self.model.set_results(self.page, sent_pos[:-1])
self.model.queue.put(SEARCH_TERMINATED_EVENT)
",[],0,[],/app/concordance_app.py_run
896,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_processed_query,"def processed_query(self):
new = []
for term in self.model.query.split():
term = re.sub(r""\."", r""[^/ ]"", term)
if re.match(""[A-Z]+$"", term):
new.append(BOUNDARY + WORD_OR_TAG + ""/"" + term + BOUNDARY)
elif ""/"" in term:
new.append(BOUNDARY + term + BOUNDARY)
else:
new.append(BOUNDARY + term + ""/"" + WORD_OR_TAG + BOUNDARY)
return "" "".join(new)
",[],0,[],/app/concordance_app.py_processed_query
897,/home/amandapotts/git/nltk/nltk/app/concordance_app.py_app,"def app():
d = ConcordanceSearchView()
d.mainloop()
",[],0,[],/app/concordance_app.py_app
898,/home/amandapotts/git/nltk/nltk/app/srparser_app.py___init__,"def __init__(self, grammar, sent, trace=0):
self._sent = sent
self._parser = SteppingShiftReduceParser(grammar, trace)
self._top = Tk()
self._top.title(""Shift Reduce Parser Application"")
self._animating_lock = 0
self._animate = IntVar(self._top)
self._animate.set(10)  # = medium
self._show_grammar = IntVar(self._top)
self._show_grammar.set(1)
self._init_fonts(self._top)
self._init_bindings()
self._init_menubar(self._top)
self._init_buttons(self._top)
self._init_feedback(self._top)
self._init_grammar(self._top)
self._init_canvas(self._top)
self._reduce_menu = Menu(self._canvas, tearoff=0)
self.reset()
self._lastoper1[""text""] = """"
",[],0,[],/app/srparser_app.py___init__
899,/home/amandapotts/git/nltk/nltk/app/srparser_app.py__init_fonts,"def _init_fonts(self, root):
self._sysfont = Font(font=Button()[""font""])
root.option_add(""*Font"", self._sysfont)
self._size = IntVar(root)
self._size.set(self._sysfont.cget(""size""))
self._boldfont = Font(family=""helvetica"", weight=""bold"", size=self._size.get())
self._font = Font(family=""helvetica"", size=self._size.get())
",[],0,[],/app/srparser_app.py__init_fonts
900,/home/amandapotts/git/nltk/nltk/app/srparser_app.py__init_grammar,"def _init_grammar(self, parent):
self._prodframe = listframe = Frame(parent)
self._prodframe.pack(fill=""both"", side=""left"", padx=2)
self._prodlist_label = Label(
self._prodframe, font=self._boldfont, text=""Available Reductions""
)
self._prodlist_label.pack()
self._prodlist = Listbox(
self._prodframe,
selectmode=""single"",
relief=""groove"",
background=""white"",
foreground=""#909090"",
font=self._font,
selectforeground=""#004040"",
selectbackground=""#c0f0c0"",
)
self._prodlist.pack(side=""right"", fill=""both"", expand=1)
self._productions = list(self._parser.grammar().productions())
for production in self._productions:
self._prodlist.insert(""end"", ("" %s"" % production))
self._prodlist.config(height=min(len(self._productions), 25))
if 1:  # len(self._productions) > 25:
listscroll = Scrollbar(self._prodframe, orient=""vertical"")
self._prodlist.config(yscrollcommand=listscroll.set)
listscroll.config(command=self._prodlist.yview)
listscroll.pack(side=""left"", fill=""y"")
self._prodlist.bind(""<<ListboxSelect>>"", self._prodlist_select)
self._hover = -1
self._prodlist.bind(""<Motion>"", self._highlight_hover)
self._prodlist.bind(""<Leave>"", self._clear_hover)
",[],0,[],/app/srparser_app.py__init_grammar
901,/home/amandapotts/git/nltk/nltk/app/srparser_app.py__init_buttons,"def _init_buttons(self, parent):
self._buttonframe = buttonframe = Frame(parent)
buttonframe.pack(fill=""none"", side=""bottom"")
Button(
buttonframe,
text=""Step"",
background=""#90c0d0"",
foreground=""black"",
command=self.step,
).pack(side=""left"")
Button(
buttonframe,
text=""Shift"",
underline=0,
background=""#90f090"",
foreground=""black"",
command=self.shift,
).pack(side=""left"")
Button(
buttonframe,
text=""Reduce"",
underline=0,
background=""#90f090"",
foreground=""black"",
command=self.reduce,
).pack(side=""left"")
Button(
buttonframe,
text=""Undo"",
underline=0,
background=""#f0a0a0"",
foreground=""black"",
command=self.undo,
).pack(side=""left"")
",[],0,[],/app/srparser_app.py__init_buttons
902,/home/amandapotts/git/nltk/nltk/app/srparser_app.py__init_menubar,"def _init_menubar(self, parent):
menubar = Menu(parent)
filemenu = Menu(menubar, tearoff=0)
filemenu.add_command(
label=""Reset Parser"", underline=0, command=self.reset, accelerator=""Del""
)
filemenu.add_command(
label=""Print to Postscript"",
underline=0,
command=self.postscript,
accelerator=""Ctrl-p"",
)
filemenu.add_command(
label=""Exit"", underline=1, command=self.destroy, accelerator=""Ctrl-x""
)
menubar.add_cascade(label=""File"", underline=0, menu=filemenu)
editmenu = Menu(menubar, tearoff=0)
editmenu.add_command(
label=""Edit Grammar"",
underline=5,
command=self.edit_grammar,
accelerator=""Ctrl-g"",
)
editmenu.add_command(
label=""Edit Text"",
underline=5,
command=self.edit_sentence,
accelerator=""Ctrl-t"",
)
menubar.add_cascade(label=""Edit"", underline=0, menu=editmenu)
rulemenu = Menu(menubar, tearoff=0)
rulemenu.add_command(
label=""Step"", underline=1, command=self.step, accelerator=""Space""
)
rulemenu.add_separator()
rulemenu.add_command(
label=""Shift"", underline=0, command=self.shift, accelerator=""Ctrl-s""
)
rulemenu.add_command(
label=""Reduce"", underline=0, command=self.reduce, accelerator=""Ctrl-r""
)
rulemenu.add_separator()
rulemenu.add_command(
label=""Undo"", underline=0, command=self.undo, accelerator=""Ctrl-u""
)
menubar.add_cascade(label=""Apply"", underline=0, menu=rulemenu)
viewmenu = Menu(menubar, tearoff=0)
viewmenu.add_checkbutton(
label=""Show Grammar"",
underline=0,
variable=self._show_grammar,
command=self._toggle_grammar,
)
viewmenu.add_separator()
viewmenu.add_radiobutton(
label=""Tiny"",
variable=self._size,
underline=0,
value=10,
command=self.resize,
)
viewmenu.add_radiobutton(
label=""Small"",
variable=self._size,
underline=0,
value=12,
command=self.resize,
)
viewmenu.add_radiobutton(
label=""Medium"",
variable=self._size,
underline=0,
value=14,
command=self.resize,
)
viewmenu.add_radiobutton(
label=""Large"",
variable=self._size,
underline=0,
value=18,
command=self.resize,
)
viewmenu.add_radiobutton(
label=""Huge"",
variable=self._size,
underline=0,
value=24,
command=self.resize,
)
menubar.add_cascade(label=""View"", underline=0, menu=viewmenu)
animatemenu = Menu(menubar, tearoff=0)
animatemenu.add_radiobutton(
label=""No Animation"", underline=0, variable=self._animate, value=0
)
animatemenu.add_radiobutton(
label=""Slow Animation"",
underline=0,
variable=self._animate,
value=20,
accelerator=""-"",
)
animatemenu.add_radiobutton(
label=""Normal Animation"",
underline=0,
variable=self._animate,
value=10,
accelerator=""="",
)
animatemenu.add_radiobutton(
label=""Fast Animation"",
underline=0,
variable=self._animate,
value=4,
accelerator=""+"",
)
menubar.add_cascade(label=""Animate"", underline=1, menu=animatemenu)
helpmenu = Menu(menubar, tearoff=0)
helpmenu.add_command(label=""About"", underline=0, command=self.about)
helpmenu.add_command(
label=""Instructions"", underline=0, command=self.help, accelerator=""F1""
)
menubar.add_cascade(label=""Help"", underline=0, menu=helpmenu)
parent.config(menu=menubar)
",[],0,[],/app/srparser_app.py__init_menubar
903,/home/amandapotts/git/nltk/nltk/app/srparser_app.py__init_feedback,"def _init_feedback(self, parent):
self._feedbackframe = feedbackframe = Frame(parent)
feedbackframe.pack(fill=""x"", side=""bottom"", padx=3, pady=3)
self._lastoper_label = Label(
feedbackframe, text=""Last Operation:"", font=self._font
)
self._lastoper_label.pack(side=""left"")
lastoperframe = Frame(feedbackframe, relief=""sunken"", border=1)
lastoperframe.pack(fill=""x"", side=""right"", expand=1, padx=5)
self._lastoper1 = Label(
lastoperframe, foreground=""#007070"", background=""#f0f0f0"", font=self._font
)
self._lastoper2 = Label(
lastoperframe,
anchor=""w"",
width=30,
foreground=""#004040"",
background=""#f0f0f0"",
font=self._font,
)
self._lastoper1.pack(side=""left"")
self._lastoper2.pack(side=""left"", fill=""x"", expand=1)
",[],0,[],/app/srparser_app.py__init_feedback
904,/home/amandapotts/git/nltk/nltk/app/srparser_app.py__init_canvas,"def _init_canvas(self, parent):
self._cframe = CanvasFrame(
parent,
background=""white"",
width=525,
closeenough=10,
border=2,
relief=""sunken"",
)
self._cframe.pack(expand=1, fill=""both"", side=""top"", pady=2)
canvas = self._canvas = self._cframe.canvas()
self._stackwidgets = []
self._rtextwidgets = []
self._titlebar = canvas.create_rectangle(
0, 0, 0, 0, fill=""#c0f0f0"", outline=""black""
)
self._exprline = canvas.create_line(0, 0, 0, 0, dash=""."")
self._stacktop = canvas.create_line(0, 0, 0, 0, fill=""#408080"")
size = self._size.get() + 4
self._stacklabel = TextWidget(
canvas, ""Stack"", color=""#004040"", font=self._boldfont
)
self._rtextlabel = TextWidget(
canvas, ""Remaining Text"", color=""#004040"", font=self._boldfont
)
self._cframe.add_widget(self._stacklabel)
self._cframe.add_widget(self._rtextlabel)
",[],0,[],/app/srparser_app.py__init_canvas
905,/home/amandapotts/git/nltk/nltk/app/srparser_app.py__redraw,"def _redraw(self):
scrollregion = self._canvas[""scrollregion""].split()
(cx1, cy1, cx2, cy2) = (int(c) for c in scrollregion)
for stackwidget in self._stackwidgets:
self._cframe.destroy_widget(stackwidget)
self._stackwidgets = []
for rtextwidget in self._rtextwidgets:
self._cframe.destroy_widget(rtextwidget)
self._rtextwidgets = []
(x1, y1, x2, y2) = self._stacklabel.bbox()
y = y2 - y1 + 10
self._canvas.coords(self._titlebar, -5000, 0, 5000, y - 4)
self._canvas.coords(self._exprline, 0, y * 2 - 10, 5000, y * 2 - 10)
(x1, y1, x2, y2) = self._stacklabel.bbox()
self._stacklabel.move(5 - x1, 3 - y1)
(x1, y1, x2, y2) = self._rtextlabel.bbox()
self._rtextlabel.move(cx2 - x2 - 5, 3 - y1)
stackx = 5
for tok in self._parser.stack():
if isinstance(tok, Tree):
attribs = {
""tree_color"": ""#4080a0"",
""tree_width"": 2,
""node_font"": self._boldfont,
""node_color"": ""#006060"",
""leaf_color"": ""#006060"",
""leaf_font"": self._font,
}
widget = tree_to_treesegment(self._canvas, tok, **attribs)
widget.label()[""color""] = ""#000000""
else:
widget = TextWidget(self._canvas, tok, color=""#000000"", font=self._font)
widget.bind_click(self._popup_reduce)
self._stackwidgets.append(widget)
self._cframe.add_widget(widget, stackx, y)
stackx = widget.bbox()[2] + 10
rtextwidth = 0
for tok in self._parser.remaining_text():
widget = TextWidget(self._canvas, tok, color=""#000000"", font=self._font)
self._rtextwidgets.append(widget)
self._cframe.add_widget(widget, rtextwidth, y)
rtextwidth = widget.bbox()[2] + 4
if len(self._rtextwidgets) > 0:
stackx += self._rtextwidgets[0].width()
stackx = max(stackx, self._stacklabel.width() + 25)
rlabelwidth = self._rtextlabel.width() + 10
if stackx >= cx2 - max(rtextwidth, rlabelwidth):
cx2 = stackx + max(rtextwidth, rlabelwidth)
for rtextwidget in self._rtextwidgets:
rtextwidget.move(4 + cx2 - rtextwidth, 0)
self._rtextlabel.move(cx2 - self._rtextlabel.bbox()[2] - 5, 0)
midx = (stackx + cx2 - max(rtextwidth, rlabelwidth)) / 2
self._canvas.coords(self._stacktop, midx, 0, midx, 5000)
(x1, y1, x2, y2) = self._stacklabel.bbox()
if len(self._rtextwidgets) > 0:
",[],0,[],/app/srparser_app.py__redraw
906,/home/amandapotts/git/nltk/nltk/app/srparser_app.py_drag_shift,"def drag_shift(widget, midx=midx, self=self):
if widget.bbox()[0] < midx:
self.shift()
else:
self._redraw()
",[],0,[],/app/srparser_app.py_drag_shift
907,/home/amandapotts/git/nltk/nltk/app/srparser_app.py__draw_stack_top,"def _draw_stack_top(self, widget):
midx = widget.bbox()[2] + 50
self._canvas.coords(self._stacktop, midx, 0, midx, 5000)
",[],0,[],/app/srparser_app.py__draw_stack_top
908,/home/amandapotts/git/nltk/nltk/app/srparser_app.py__highlight_productions,"def _highlight_productions(self):
self._prodlist.selection_clear(0, ""end"")
for prod in self._parser.reducible_productions():
index = self._productions.index(prod)
self._prodlist.selection_set(index)
",[],0,[],/app/srparser_app.py__highlight_productions
909,/home/amandapotts/git/nltk/nltk/app/srparser_app.py_destroy,"def destroy(self, *e):
if self._top is None:
return
self._top.destroy()
self._top = None
",[],0,[],/app/srparser_app.py_destroy
910,/home/amandapotts/git/nltk/nltk/app/srparser_app.py_reset,"def reset(self, *e):
self._parser.initialize(self._sent)
self._lastoper1[""text""] = ""Reset App""
self._lastoper2[""text""] = """"
self._redraw()
",[],0,[],/app/srparser_app.py_reset
911,/home/amandapotts/git/nltk/nltk/app/srparser_app.py_step,"def step(self, *e):
if self.reduce():
return True
elif self.shift():
return True
else:
if list(self._parser.parses()):
self._lastoper1[""text""] = ""Finished:""
self._lastoper2[""text""] = ""Success""
else:
self._lastoper1[""text""] = ""Finished:""
self._lastoper2[""text""] = ""Failure""
",[],0,[],/app/srparser_app.py_step
912,/home/amandapotts/git/nltk/nltk/app/srparser_app.py_shift,"def shift(self, *e):
if self._animating_lock:
return
if self._parser.shift():
tok = self._parser.stack()[-1]
self._lastoper1[""text""] = ""Shift:""
self._lastoper2[""text""] = ""%r"" % tok
if self._animate.get():
self._animate_shift()
else:
self._redraw()
return True
return False
",[],0,[],/app/srparser_app.py_shift
913,/home/amandapotts/git/nltk/nltk/app/srparser_app.py_reduce,"def reduce(self, *e):
if self._animating_lock:
return
production = self._parser.reduce()
if production:
self._lastoper1[""text""] = ""Reduce:""
self._lastoper2[""text""] = ""%s"" % production
if self._animate.get():
self._animate_reduce()
else:
self._redraw()
return production
",[],0,[],/app/srparser_app.py_reduce
914,/home/amandapotts/git/nltk/nltk/app/srparser_app.py_undo,"def undo(self, *e):
if self._animating_lock:
return
if self._parser.undo():
self._redraw()
",[],0,[],/app/srparser_app.py_undo
915,/home/amandapotts/git/nltk/nltk/app/srparser_app.py_postscript,"def postscript(self, *e):
self._cframe.print_to_file()
",[],0,[],/app/srparser_app.py_postscript
916,/home/amandapotts/git/nltk/nltk/app/srparser_app.py_mainloop,"def mainloop(self, *args, **kwargs):
""""""
Enter the Tkinter mainloop.  This function must be called if
this demo is created from a non-interactive program (e.g.
from a secript)
the script completes.
""""""
if in_idle():
return
self._top.mainloop(*args, **kwargs)
",[],0,[],/app/srparser_app.py_mainloop
917,/home/amandapotts/git/nltk/nltk/app/srparser_app.py_resize,"def resize(self, size=None):
if size is not None:
self._size.set(size)
size = self._size.get()
self._font.configure(size=-(abs(size)))
self._boldfont.configure(size=-(abs(size)))
self._sysfont.configure(size=-(abs(size)))
self._redraw()
",[],0,[],/app/srparser_app.py_resize
918,/home/amandapotts/git/nltk/nltk/app/srparser_app.py_help,"def help(self, *e):
try:
ShowText(
self._top,
""Help: Shift-Reduce Parser Application"",
(__doc__ or """").strip(),
width=75,
font=""fixed"",
)
except:
ShowText(
self._top,
""Help: Shift-Reduce Parser Application"",
(__doc__ or """").strip(),
width=75,
)
",[],0,[],/app/srparser_app.py_help
919,/home/amandapotts/git/nltk/nltk/app/srparser_app.py_about,"def about(self, *e):
ABOUT = ""NLTK Shift-Reduce Parser Application\n"" + ""Written by Edward Loper""
TITLE = ""About: Shift-Reduce Parser Application""
try:
from tkinter.messagebox import Message
Message(message=ABOUT, title=TITLE).show()
except:
ShowText(self._top, TITLE, ABOUT)
",[],0,[],/app/srparser_app.py_about
920,/home/amandapotts/git/nltk/nltk/app/srparser_app.py_edit_grammar,"def edit_grammar(self, *e):
CFGEditor(self._top, self._parser.grammar(), self.set_grammar)
",[],0,[],/app/srparser_app.py_edit_grammar
921,/home/amandapotts/git/nltk/nltk/app/srparser_app.py_set_grammar,"def set_grammar(self, grammar):
self._parser.set_grammar(grammar)
self._productions = list(grammar.productions())
self._prodlist.delete(0, ""end"")
for production in self._productions:
self._prodlist.insert(""end"", ("" %s"" % production))
",[],0,[],/app/srparser_app.py_set_grammar
922,/home/amandapotts/git/nltk/nltk/app/srparser_app.py_edit_sentence,"def edit_sentence(self, *e):
sentence = "" "".join(self._sent)
title = ""Edit Text""
instr = ""Enter a new sentence to parse.""
EntryDialog(self._top, sentence, instr, self.set_sentence, title)
",[],0,[],/app/srparser_app.py_edit_sentence
923,/home/amandapotts/git/nltk/nltk/app/srparser_app.py_set_sentence,"def set_sentence(self, sent):
self._sent = sent.split()  # [XX] use tagged?
self.reset()
",[],0,[],/app/srparser_app.py_set_sentence
924,/home/amandapotts/git/nltk/nltk/app/srparser_app.py__toggle_grammar,"def _toggle_grammar(self, *e):
if self._show_grammar.get():
self._prodframe.pack(
fill=""both"", side=""left"", padx=2, after=self._feedbackframe
)
self._lastoper1[""text""] = ""Show Grammar""
else:
self._prodframe.pack_forget()
self._lastoper1[""text""] = ""Hide Grammar""
self._lastoper2[""text""] = """"
",[],0,[],/app/srparser_app.py__toggle_grammar
925,/home/amandapotts/git/nltk/nltk/app/srparser_app.py__prodlist_select,"def _prodlist_select(self, event):
selection = self._prodlist.curselection()
if len(selection) != 1:
return
index = int(selection[0])
production = self._parser.reduce(self._productions[index])
if production:
self._lastoper1[""text""] = ""Reduce:""
self._lastoper2[""text""] = ""%s"" % production
if self._animate.get():
self._animate_reduce()
else:
self._redraw()
else:
self._prodlist.selection_clear(0, ""end"")
for prod in self._parser.reducible_productions():
index = self._productions.index(prod)
self._prodlist.selection_set(index)
",[],0,[],/app/srparser_app.py__prodlist_select
926,/home/amandapotts/git/nltk/nltk/app/srparser_app.py__popup_reduce,"def _popup_reduce(self, widget):
productions = self._parser.reducible_productions()
if len(productions) == 0:
return
self._reduce_menu.delete(0, ""end"")
for production in productions:
self._reduce_menu.add_command(label=str(production), command=self.reduce)
self._reduce_menu.post(
self._canvas.winfo_pointerx(), self._canvas.winfo_pointery()
)
",[],0,[],/app/srparser_app.py__popup_reduce
927,/home/amandapotts/git/nltk/nltk/app/srparser_app.py__animate_shift,"def _animate_shift(self):
widget = self._rtextwidgets[0]
right = widget.bbox()[0]
if len(self._stackwidgets) == 0:
left = 5
else:
left = self._stackwidgets[-1].bbox()[2] + 10
dt = self._animate.get()
dx = (left - right) * 1.0 / dt
self._animate_shift_frame(dt, widget, dx)
",[],0,[],/app/srparser_app.py__animate_shift
928,/home/amandapotts/git/nltk/nltk/app/srparser_app.py__animate_shift_frame,"def _animate_shift_frame(self, frame, widget, dx):
if frame > 0:
self._animating_lock = 1
widget.move(dx, 0)
self._top.after(10, self._animate_shift_frame, frame - 1, widget, dx)
else:
del self._rtextwidgets[0]
self._stackwidgets.append(widget)
self._animating_lock = 0
self._draw_stack_top(widget)
self._highlight_productions()
",[],0,[],/app/srparser_app.py__animate_shift_frame
929,/home/amandapotts/git/nltk/nltk/app/srparser_app.py__animate_reduce,"def _animate_reduce(self):
numwidgets = len(self._parser.stack()[-1])  # number of children
widgets = self._stackwidgets[-numwidgets:]
if isinstance(widgets[0], TreeSegmentWidget):
ydist = 15 + widgets[0].label().height()
else:
ydist = 15 + widgets[0].height()
dt = self._animate.get()
dy = ydist * 2.0 / dt
self._animate_reduce_frame(dt / 2, widgets, dy)
",[],0,[],/app/srparser_app.py__animate_reduce
930,/home/amandapotts/git/nltk/nltk/app/srparser_app.py__animate_reduce_frame,"def _animate_reduce_frame(self, frame, widgets, dy):
if frame > 0:
self._animating_lock = 1
for widget in widgets:
widget.move(0, dy)
self._top.after(10, self._animate_reduce_frame, frame - 1, widgets, dy)
else:
del self._stackwidgets[-len(widgets) :]
for widget in widgets:
self._cframe.remove_widget(widget)
tok = self._parser.stack()[-1]
if not isinstance(tok, Tree):
raise ValueError()
label = TextWidget(
self._canvas, str(tok.label()), color=""#006060"", font=self._boldfont
)
widget = TreeSegmentWidget(self._canvas, label, widgets, width=2)
(x1, y1, x2, y2) = self._stacklabel.bbox()
y = y2 - y1 + 10
if not self._stackwidgets:
x = 5
else:
x = self._stackwidgets[-1].bbox()[2] + 10
self._cframe.add_widget(widget, x, y)
self._stackwidgets.append(widget)
self._draw_stack_top(widget)
self._highlight_productions()
self._animating_lock = 0
",[],0,[],/app/srparser_app.py__animate_reduce_frame
931,/home/amandapotts/git/nltk/nltk/app/srparser_app.py__highlight_hover,"def _highlight_hover(self, event):
index = self._prodlist.nearest(event.y)
if self._hover == index:
return
self._clear_hover()
selection = [int(s) for s in self._prodlist.curselection()]
if index in selection:
rhslen = len(self._productions[index].rhs())
for stackwidget in self._stackwidgets[-rhslen:]:
if isinstance(stackwidget, TreeSegmentWidget):
stackwidget.label()[""color""] = ""#00a000""
else:
stackwidget[""color""] = ""#00a000""
self._hover = index
",[],0,[],/app/srparser_app.py__highlight_hover
932,/home/amandapotts/git/nltk/nltk/app/srparser_app.py__clear_hover,"def _clear_hover(self, *event):
if self._hover == -1:
return
self._hover = -1
for stackwidget in self._stackwidgets:
if isinstance(stackwidget, TreeSegmentWidget):
stackwidget.label()[""color""] = ""black""
else:
stackwidget[""color""] = ""black""
",[],0,[],/app/srparser_app.py__clear_hover
933,/home/amandapotts/git/nltk/nltk/app/srparser_app.py_app,"def app():
""""""
Create a shift reduce parser app, using a simple grammar and
text.
""""""
from nltk.grammar import CFG, Nonterminal, Production
nonterminals = ""S VP NP PP P N Name V Det""
(S, VP, NP, PP, P, N, Name, V, Det) = (Nonterminal(s) for s in nonterminals.split())
productions = (
Production(S, [NP, VP]),
Production(NP, [Det, N]),
Production(NP, [NP, PP]),
Production(VP, [VP, PP]),
Production(VP, [V, NP, PP]),
Production(VP, [V, NP]),
Production(PP, [P, NP]),
Production(NP, [""I""]),
Production(Det, [""the""]),
Production(Det, [""a""]),
Production(N, [""man""]),
Production(V, [""saw""]),
Production(P, [""in""]),
Production(P, [""with""]),
Production(N, [""park""]),
Production(N, [""dog""]),
Production(N, [""statue""]),
Production(Det, [""my""]),
)
grammar = CFG(S, productions)
sent = ""my dog saw a man in the park with a statue"".split()
ShiftReduceApp(grammar, sent).mainloop()
",[],0,[],/app/srparser_app.py_app
934,/home/amandapotts/git/nltk/nltk/app/wordfreq_app.py_plot_word_freq_dist,"def plot_word_freq_dist(text):
fd = text.vocab()
samples = [item for item, _ in fd.most_common(50)]
values = [fd[sample] for sample in samples]
values = [sum(values[: i + 1]) * 100.0 / fd.N() for i in range(len(values))]
pylab.title(text.name)
pylab.xlabel(""Samples"")
pylab.ylabel(""Cumulative Percentage"")
pylab.plot(values)
pylab.xticks(range(len(samples)), [str(s) for s in samples], rotation=90)
pylab.show()
",[],0,[],/app/wordfreq_app.py_plot_word_freq_dist
935,/home/amandapotts/git/nltk/nltk/app/wordfreq_app.py_app,"def app():
t1 = Text(gutenberg.words(""melville-moby_dick.txt""))
plot_word_freq_dist(t1)
",[],0,[],/app/wordfreq_app.py_app
936,/home/amandapotts/git/nltk/nltk/app/collocations_app.py___init__,"def __init__(self):
self.queue = q.Queue()
self.model = CollocationsModel(self.queue)
self.top = Tk()
self._init_top(self.top)
self._init_menubar()
self._init_widgets(self.top)
self.load_corpus(self.model.DEFAULT_CORPUS)
self.after = self.top.after(POLL_INTERVAL, self._poll)
",[],0,[],/app/collocations_app.py___init__
937,/home/amandapotts/git/nltk/nltk/app/collocations_app.py__init_top,"def _init_top(self, top):
top.geometry(""550x650+50+50"")
top.title(""NLTK Collocations List"")
top.bind(""<Control-q>"", self.destroy)
top.protocol(""WM_DELETE_WINDOW"", self.destroy)
top.minsize(550, 650)
",[],0,[],/app/collocations_app.py__init_top
938,/home/amandapotts/git/nltk/nltk/app/collocations_app.py__init_widgets,"def _init_widgets(self, parent):
self.main_frame = Frame(
parent, dict(background=self._BACKGROUND_COLOUR, padx=1, pady=1, border=1)
)
self._init_corpus_select(self.main_frame)
self._init_results_box(self.main_frame)
self._init_paging(self.main_frame)
self._init_status(self.main_frame)
self.main_frame.pack(fill=""both"", expand=True)
",[],0,[],/app/collocations_app.py__init_widgets
939,/home/amandapotts/git/nltk/nltk/app/collocations_app.py__init_corpus_select,"def _init_corpus_select(self, parent):
innerframe = Frame(parent, background=self._BACKGROUND_COLOUR)
self.var = StringVar(innerframe)
self.var.set(self.model.DEFAULT_CORPUS)
Label(
innerframe,
justify=LEFT,
text="" Corpus: "",
background=self._BACKGROUND_COLOUR,
padx=2,
pady=1,
border=0,
).pack(side=""left"")
other_corpora = list(self.model.CORPORA.keys()).remove(
self.model.DEFAULT_CORPUS
)
om = OptionMenu(
innerframe,
self.var,
self.model.DEFAULT_CORPUS,
command=self.corpus_selected,
)
om[""borderwidth""] = 0
om[""highlightthickness""] = 1
om.pack(side=""left"")
innerframe.pack(side=""top"", fill=""x"", anchor=""n"")
",[],0,[],/app/collocations_app.py__init_corpus_select
940,/home/amandapotts/git/nltk/nltk/app/collocations_app.py__init_status,"def _init_status(self, parent):
self.status = Label(
parent,
justify=LEFT,
relief=SUNKEN,
background=self._BACKGROUND_COLOUR,
border=0,
padx=1,
pady=0,
)
self.status.pack(side=""top"", anchor=""sw"")
",[],0,[],/app/collocations_app.py__init_status
941,/home/amandapotts/git/nltk/nltk/app/collocations_app.py__init_menubar,"def _init_menubar(self):
self._result_size = IntVar(self.top)
menubar = Menu(self.top)
filemenu = Menu(menubar, tearoff=0, borderwidth=0)
filemenu.add_command(
label=""Exit"", underline=1, command=self.destroy, accelerator=""Ctrl-q""
)
menubar.add_cascade(label=""File"", underline=0, menu=filemenu)
editmenu = Menu(menubar, tearoff=0)
rescntmenu = Menu(editmenu, tearoff=0)
rescntmenu.add_radiobutton(
label=""20"",
variable=self._result_size,
underline=0,
value=20,
command=self.set_result_size,
)
rescntmenu.add_radiobutton(
label=""50"",
variable=self._result_size,
underline=0,
value=50,
command=self.set_result_size,
)
rescntmenu.add_radiobutton(
label=""100"",
variable=self._result_size,
underline=0,
value=100,
command=self.set_result_size,
)
rescntmenu.invoke(1)
editmenu.add_cascade(label=""Result Count"", underline=0, menu=rescntmenu)
menubar.add_cascade(label=""Edit"", underline=0, menu=editmenu)
self.top.config(menu=menubar)
",[],0,[],/app/collocations_app.py__init_menubar
942,/home/amandapotts/git/nltk/nltk/app/collocations_app.py_set_result_size,"def set_result_size(self, **kwargs):
self.model.result_count = self._result_size.get()
",[],0,[],/app/collocations_app.py_set_result_size
943,/home/amandapotts/git/nltk/nltk/app/collocations_app.py__init_results_box,"def _init_results_box(self, parent):
innerframe = Frame(parent)
i1 = Frame(innerframe)
i2 = Frame(innerframe)
vscrollbar = Scrollbar(i1, borderwidth=1)
hscrollbar = Scrollbar(i2, borderwidth=1, orient=""horiz"")
self.results_box = Text(
i1,
font=Font(family=""courier"", size=""16""),
state=""disabled"",
borderwidth=1,
yscrollcommand=vscrollbar.set,
xscrollcommand=hscrollbar.set,
wrap=""none"",
width=""40"",
height=""20"",
exportselection=1,
)
self.results_box.pack(side=""left"", fill=""both"", expand=True)
vscrollbar.pack(side=""left"", fill=""y"", anchor=""e"")
vscrollbar.config(command=self.results_box.yview)
hscrollbar.pack(side=""left"", fill=""x"", expand=True, anchor=""w"")
hscrollbar.config(command=self.results_box.xview)
Label(i2, text=""   "", background=self._BACKGROUND_COLOUR).pack(
side=""left"", anchor=""e""
)
i1.pack(side=""top"", fill=""both"", expand=True, anchor=""n"")
i2.pack(side=""bottom"", fill=""x"", anchor=""s"")
innerframe.pack(side=""top"", fill=""both"", expand=True)
",[],0,[],/app/collocations_app.py__init_results_box
944,/home/amandapotts/git/nltk/nltk/app/collocations_app.py__init_paging,"def _init_paging(self, parent):
innerframe = Frame(parent, background=self._BACKGROUND_COLOUR)
self.prev = prev = Button(
innerframe,
text=""Previous"",
command=self.previous,
width=""10"",
borderwidth=1,
highlightthickness=1,
state=""disabled"",
)
prev.pack(side=""left"", anchor=""center"")
self.next = next = Button(
innerframe,
text=""Next"",
command=self.__next__,
width=""10"",
borderwidth=1,
highlightthickness=1,
state=""disabled"",
)
next.pack(side=""right"", anchor=""center"")
innerframe.pack(side=""top"", fill=""y"")
self.reset_current_page()
",[],0,[],/app/collocations_app.py__init_paging
945,/home/amandapotts/git/nltk/nltk/app/collocations_app.py_reset_current_page,"def reset_current_page(self):
self.current_page = -1
",[],0,[],/app/collocations_app.py_reset_current_page
946,/home/amandapotts/git/nltk/nltk/app/collocations_app.py__poll,"def _poll(self):
try:
event = self.queue.get(block=False)
except q.Empty:
pass
else:
if event == CORPUS_LOADED_EVENT:
self.handle_corpus_loaded(event)
elif event == ERROR_LOADING_CORPUS_EVENT:
self.handle_error_loading_corpus(event)
self.after = self.top.after(POLL_INTERVAL, self._poll)
",[],0,[],/app/collocations_app.py__poll
947,/home/amandapotts/git/nltk/nltk/app/collocations_app.py_handle_error_loading_corpus,"def handle_error_loading_corpus(self, event):
self.status[""text""] = ""Error in loading "" + self.var.get()
self.unfreeze_editable()
self.clear_results_box()
self.freeze_editable()
self.reset_current_page()
",[],0,[],/app/collocations_app.py_handle_error_loading_corpus
948,/home/amandapotts/git/nltk/nltk/app/collocations_app.py_handle_corpus_loaded,"def handle_corpus_loaded(self, event):
self.status[""text""] = self.var.get() + "" is loaded""
self.unfreeze_editable()
self.clear_results_box()
self.reset_current_page()
collocations = self.model.next(self.current_page + 1)
self.write_results(collocations)
self.current_page += 1
",[],0,[],/app/collocations_app.py_handle_corpus_loaded
949,/home/amandapotts/git/nltk/nltk/app/collocations_app.py_corpus_selected,"def corpus_selected(self, *args):
new_selection = self.var.get()
self.load_corpus(new_selection)
",[],0,[],/app/collocations_app.py_corpus_selected
950,/home/amandapotts/git/nltk/nltk/app/collocations_app.py_previous,"def previous(self):
self.freeze_editable()
collocations = self.model.prev(self.current_page - 1)
self.current_page = self.current_page - 1
self.clear_results_box()
self.write_results(collocations)
self.unfreeze_editable()
",[],0,[],/app/collocations_app.py_previous
951,/home/amandapotts/git/nltk/nltk/app/collocations_app.py___next__,"def __next__(self):
self.freeze_editable()
collocations = self.model.next(self.current_page + 1)
self.clear_results_box()
self.write_results(collocations)
self.current_page += 1
self.unfreeze_editable()
",[],0,[],/app/collocations_app.py___next__
952,/home/amandapotts/git/nltk/nltk/app/collocations_app.py_load_corpus,"def load_corpus(self, selection):
if self.model.selected_corpus != selection:
self.status[""text""] = ""Loading "" + selection + ""...""
self.freeze_editable()
self.model.load_corpus(selection)
",[],0,[],/app/collocations_app.py_load_corpus
953,/home/amandapotts/git/nltk/nltk/app/collocations_app.py_freeze_editable,"def freeze_editable(self):
self.prev[""state""] = ""disabled""
self.next[""state""] = ""disabled""
",[],0,[],/app/collocations_app.py_freeze_editable
954,/home/amandapotts/git/nltk/nltk/app/collocations_app.py_clear_results_box,"def clear_results_box(self):
self.results_box[""state""] = ""normal""
self.results_box.delete(""1.0"", END)
self.results_box[""state""] = ""disabled""
",[],0,[],/app/collocations_app.py_clear_results_box
955,/home/amandapotts/git/nltk/nltk/app/collocations_app.py_fire_event,"def fire_event(self, event):
self.top.event_generate(event, when=""tail"")
",[],0,[],/app/collocations_app.py_fire_event
956,/home/amandapotts/git/nltk/nltk/app/collocations_app.py_destroy,"def destroy(self, *e):
if self.top is None:
return
self.top.after_cancel(self.after)
self.top.destroy()
self.top = None
",[],0,[],/app/collocations_app.py_destroy
957,/home/amandapotts/git/nltk/nltk/app/collocations_app.py_mainloop,"def mainloop(self, *args, **kwargs):
if in_idle():
return
self.top.mainloop(*args, **kwargs)
",[],0,[],/app/collocations_app.py_mainloop
958,/home/amandapotts/git/nltk/nltk/app/collocations_app.py_unfreeze_editable,"def unfreeze_editable(self):
self.set_paging_button_states()
",[],0,[],/app/collocations_app.py_unfreeze_editable
959,/home/amandapotts/git/nltk/nltk/app/collocations_app.py_set_paging_button_states,"def set_paging_button_states(self):
if self.current_page == -1 or self.current_page == 0:
self.prev[""state""] = ""disabled""
else:
self.prev[""state""] = ""normal""
if self.model.is_last_page(self.current_page):
self.next[""state""] = ""disabled""
else:
self.next[""state""] = ""normal""
",[],0,[],/app/collocations_app.py_set_paging_button_states
960,/home/amandapotts/git/nltk/nltk/app/collocations_app.py_write_results,"def write_results(self, results):
self.results_box[""state""] = ""normal""
row = 1
for each in results:
self.results_box.insert(str(row) + "".0"", each[0] + "" "" + each[1] + ""\n"")
row += 1
self.results_box[""state""] = ""disabled""
",[],0,[],/app/collocations_app.py_write_results
961,/home/amandapotts/git/nltk/nltk/app/collocations_app.py___init__,"def __init__(self, queue):
self.result_count = None
self.selected_corpus = None
self.collocations = None
self.CORPORA = _CORPORA
self.DEFAULT_CORPUS = _DEFAULT
self.queue = queue
self.reset_results()
",[],0,[],/app/collocations_app.py___init__
962,/home/amandapotts/git/nltk/nltk/app/collocations_app.py_reset_results,"def reset_results(self):
self.result_pages = []
self.results_returned = 0
",[],0,[],/app/collocations_app.py_reset_results
963,/home/amandapotts/git/nltk/nltk/app/collocations_app.py_load_corpus,"def load_corpus(self, name):
self.selected_corpus = name
self.collocations = None
runner_thread = self.LoadCorpus(name, self)
runner_thread.start()
self.reset_results()
",[],0,[],/app/collocations_app.py_load_corpus
964,/home/amandapotts/git/nltk/nltk/app/collocations_app.py_non_default_corpora,"def non_default_corpora(self):
copy = []
copy.extend(list(self.CORPORA.keys()))
copy.remove(self.DEFAULT_CORPUS)
copy.sort()
return copy
",[],0,[],/app/collocations_app.py_non_default_corpora
965,/home/amandapotts/git/nltk/nltk/app/collocations_app.py_is_last_page,"def is_last_page(self, number):
if number < len(self.result_pages):
return False
return self.results_returned + (
number - len(self.result_pages)
) * self.result_count >= len(self.collocations)
",[],0,[],/app/collocations_app.py_is_last_page
966,/home/amandapotts/git/nltk/nltk/app/collocations_app.py_next,"def next(self, page):
if (len(self.result_pages) - 1) < page:
for i in range(page - (len(self.result_pages) - 1)):
self.result_pages.append(
self.collocations[
self.results_returned : self.results_returned
+ self.result_count
]
)
self.results_returned += self.result_count
return self.result_pages[page]
",[],0,[],/app/collocations_app.py_next
967,/home/amandapotts/git/nltk/nltk/app/collocations_app.py_prev,"def prev(self, page):
if page == -1:
return []
return self.result_pages[page]
",[],0,[],/app/collocations_app.py_prev
968,/home/amandapotts/git/nltk/nltk/app/collocations_app.py_LoadCorpus,"class LoadCorpus(threading.Thread):
",[],0,[],/app/collocations_app.py_LoadCorpus
969,/home/amandapotts/git/nltk/nltk/app/collocations_app.py___init__,"def __init__(self, name, model):
threading.Thread.__init__(self)
self.model, self.name = model, name
",[],0,[],/app/collocations_app.py___init__
970,/home/amandapotts/git/nltk/nltk/app/collocations_app.py_run,"def run(self):
try:
words = self.model.CORPORA[self.name]()
from operator import itemgetter
text = [w for w in words if len(w) > 2]
fd = FreqDist(tuple(text[i : i + 2]) for i in range(len(text) - 1))
vocab = FreqDist(text)
scored = [
((w1, w2), fd[(w1, w2)] ** 3 / (vocab[w1] * vocab[w2]))
for w1, w2 in fd
]
scored.sort(key=itemgetter(1), reverse=True)
self.model.collocations = list(map(itemgetter(0), scored))
self.model.queue.put(CORPUS_LOADED_EVENT)
except Exception as e:
print(e)
self.model.queue.put(ERROR_LOADING_CORPUS_EVENT)
",[],0,[],/app/collocations_app.py_run
971,/home/amandapotts/git/nltk/nltk/app/collocations_app.py_app,"def app():
c = CollocationsView()
c.mainloop()
",[],0,[],/app/collocations_app.py_app
972,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py___init__,"def __init__(self, grammar, sent, trace=0):
self._sent = sent
self._parser = SteppingRecursiveDescentParser(grammar, trace)
self._top = Tk()
self._top.title(""Recursive Descent Parser Application"")
self._init_bindings()
self._init_fonts(self._top)
self._animation_frames = IntVar(self._top)
self._animation_frames.set(5)
self._animating_lock = 0
self._autostep = 0
self._show_grammar = IntVar(self._top)
self._show_grammar.set(1)
self._init_menubar(self._top)
self._init_buttons(self._top)
self._init_feedback(self._top)
self._init_grammar(self._top)
self._init_canvas(self._top)
self._parser.initialize(self._sent)
self._canvas.bind(""<Configure>"", self._configure)
",[],0,[],/app/rdparser_app.py___init__
973,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py__init_fonts,"def _init_fonts(self, root):
self._sysfont = Font(font=Button()[""font""])
root.option_add(""*Font"", self._sysfont)
self._size = IntVar(root)
self._size.set(self._sysfont.cget(""size""))
self._boldfont = Font(family=""helvetica"", weight=""bold"", size=self._size.get())
self._font = Font(family=""helvetica"", size=self._size.get())
if self._size.get() < 0:
big = self._size.get() - 2
else:
big = self._size.get() + 2
self._bigfont = Font(family=""helvetica"", weight=""bold"", size=big)
",[],0,[],/app/rdparser_app.py__init_fonts
974,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py__init_grammar,"def _init_grammar(self, parent):
self._prodframe = listframe = Frame(parent)
self._prodframe.pack(fill=""both"", side=""left"", padx=2)
self._prodlist_label = Label(
self._prodframe, font=self._boldfont, text=""Available Expansions""
)
self._prodlist_label.pack()
self._prodlist = Listbox(
self._prodframe,
selectmode=""single"",
relief=""groove"",
background=""white"",
foreground=""#909090"",
font=self._font,
selectforeground=""#004040"",
selectbackground=""#c0f0c0"",
)
self._prodlist.pack(side=""right"", fill=""both"", expand=1)
self._productions = list(self._parser.grammar().productions())
for production in self._productions:
self._prodlist.insert(""end"", (""  %s"" % production))
self._prodlist.config(height=min(len(self._productions), 25))
if len(self._productions) > 25:
listscroll = Scrollbar(self._prodframe, orient=""vertical"")
self._prodlist.config(yscrollcommand=listscroll.set)
listscroll.config(command=self._prodlist.yview)
listscroll.pack(side=""left"", fill=""y"")
self._prodlist.bind(""<<ListboxSelect>>"", self._prodlist_select)
",[],0,[],/app/rdparser_app.py__init_grammar
975,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py__init_bindings,"def _init_bindings(self):
self._top.bind(""<Control-q>"", self.destroy)
self._top.bind(""<Control-x>"", self.destroy)
self._top.bind(""<Escape>"", self.destroy)
self._top.bind(""e"", self.expand)
self._top.bind(""m"", self.match)
self._top.bind(""<Alt-m>"", self.match)
self._top.bind(""<Control-m>"", self.match)
self._top.bind(""b"", self.backtrack)
self._top.bind(""<Alt-b>"", self.backtrack)
self._top.bind(""<Control-b>"", self.backtrack)
self._top.bind(""<Control-z>"", self.backtrack)
self._top.bind(""<BackSpace>"", self.backtrack)
self._top.bind(""a"", self.autostep)
self._top.bind(""<Control-space>"", self.autostep)
self._top.bind(""<Control-c>"", self.cancel_autostep)
self._top.bind(""<space>"", self.step)
self._top.bind(""<Delete>"", self.reset)
self._top.bind(""<Control-p>"", self.postscript)
self._top.bind(""<Control-h>"", self.help)
self._top.bind(""<F1>"", self.help)
self._top.bind(""<Control-g>"", self.edit_grammar)
self._top.bind(""<Control-t>"", self.edit_sentence)
",[],0,[],/app/rdparser_app.py__init_bindings
976,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py__init_buttons,"def _init_buttons(self, parent):
self._buttonframe = buttonframe = Frame(parent)
buttonframe.pack(fill=""none"", side=""bottom"", padx=3, pady=2)
Button(
buttonframe,
text=""Step"",
background=""#90c0d0"",
foreground=""black"",
command=self.step,
).pack(side=""left"")
Button(
buttonframe,
text=""Autostep"",
background=""#90c0d0"",
foreground=""black"",
command=self.autostep,
).pack(side=""left"")
Button(
buttonframe,
text=""Expand"",
underline=0,
background=""#90f090"",
foreground=""black"",
command=self.expand,
).pack(side=""left"")
Button(
buttonframe,
text=""Match"",
underline=0,
background=""#90f090"",
foreground=""black"",
command=self.match,
).pack(side=""left"")
Button(
buttonframe,
text=""Backtrack"",
underline=0,
background=""#f0a0a0"",
foreground=""black"",
command=self.backtrack,
).pack(side=""left"")
",[],0,[],/app/rdparser_app.py__init_buttons
977,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py__configure,"def _configure(self, event):
self._autostep = 0
(x1, y1, x2, y2) = self._cframe.scrollregion()
y2 = event.height - 6
self._canvas[""scrollregion""] = ""%d %d %d %d"" % (x1, y1, x2, y2)
self._redraw()
",[],0,[],/app/rdparser_app.py__configure
978,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py__init_feedback,"def _init_feedback(self, parent):
self._feedbackframe = feedbackframe = Frame(parent)
feedbackframe.pack(fill=""x"", side=""bottom"", padx=3, pady=3)
self._lastoper_label = Label(
feedbackframe, text=""Last Operation:"", font=self._font
)
self._lastoper_label.pack(side=""left"")
lastoperframe = Frame(feedbackframe, relief=""sunken"", border=1)
lastoperframe.pack(fill=""x"", side=""right"", expand=1, padx=5)
self._lastoper1 = Label(
lastoperframe, foreground=""#007070"", background=""#f0f0f0"", font=self._font
)
self._lastoper2 = Label(
lastoperframe,
anchor=""w"",
width=30,
foreground=""#004040"",
background=""#f0f0f0"",
font=self._font,
)
self._lastoper1.pack(side=""left"")
self._lastoper2.pack(side=""left"", fill=""x"", expand=1)
",[],0,[],/app/rdparser_app.py__init_feedback
979,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py__init_canvas,"def _init_canvas(self, parent):
self._cframe = CanvasFrame(
parent,
background=""white"",
closeenough=10,
border=2,
relief=""sunken"",
)
self._cframe.pack(expand=1, fill=""both"", side=""top"", pady=2)
canvas = self._canvas = self._cframe.canvas()
self._tree = None
self._textwidgets = []
self._textline = None
",[],0,[],/app/rdparser_app.py__init_canvas
980,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py__init_menubar,"def _init_menubar(self, parent):
menubar = Menu(parent)
filemenu = Menu(menubar, tearoff=0)
filemenu.add_command(
label=""Reset Parser"", underline=0, command=self.reset, accelerator=""Del""
)
filemenu.add_command(
label=""Print to Postscript"",
underline=0,
command=self.postscript,
accelerator=""Ctrl-p"",
)
filemenu.add_command(
label=""Exit"", underline=1, command=self.destroy, accelerator=""Ctrl-x""
)
menubar.add_cascade(label=""File"", underline=0, menu=filemenu)
editmenu = Menu(menubar, tearoff=0)
editmenu.add_command(
label=""Edit Grammar"",
underline=5,
command=self.edit_grammar,
accelerator=""Ctrl-g"",
)
editmenu.add_command(
label=""Edit Text"",
underline=5,
command=self.edit_sentence,
accelerator=""Ctrl-t"",
)
menubar.add_cascade(label=""Edit"", underline=0, menu=editmenu)
rulemenu = Menu(menubar, tearoff=0)
rulemenu.add_command(
label=""Step"", underline=1, command=self.step, accelerator=""Space""
)
rulemenu.add_separator()
rulemenu.add_command(
label=""Match"", underline=0, command=self.match, accelerator=""Ctrl-m""
)
rulemenu.add_command(
label=""Expand"", underline=0, command=self.expand, accelerator=""Ctrl-e""
)
rulemenu.add_separator()
rulemenu.add_command(
label=""Backtrack"", underline=0, command=self.backtrack, accelerator=""Ctrl-b""
)
menubar.add_cascade(label=""Apply"", underline=0, menu=rulemenu)
viewmenu = Menu(menubar, tearoff=0)
viewmenu.add_checkbutton(
label=""Show Grammar"",
underline=0,
variable=self._show_grammar,
command=self._toggle_grammar,
)
viewmenu.add_separator()
viewmenu.add_radiobutton(
label=""Tiny"",
variable=self._size,
underline=0,
value=10,
command=self.resize,
)
viewmenu.add_radiobutton(
label=""Small"",
variable=self._size,
underline=0,
value=12,
command=self.resize,
)
viewmenu.add_radiobutton(
label=""Medium"",
variable=self._size,
underline=0,
value=14,
command=self.resize,
)
viewmenu.add_radiobutton(
label=""Large"",
variable=self._size,
underline=0,
value=18,
command=self.resize,
)
viewmenu.add_radiobutton(
label=""Huge"",
variable=self._size,
underline=0,
value=24,
command=self.resize,
)
menubar.add_cascade(label=""View"", underline=0, menu=viewmenu)
animatemenu = Menu(menubar, tearoff=0)
animatemenu.add_radiobutton(
label=""No Animation"", underline=0, variable=self._animation_frames, value=0
)
animatemenu.add_radiobutton(
label=""Slow Animation"",
underline=0,
variable=self._animation_frames,
value=10,
accelerator=""-"",
)
animatemenu.add_radiobutton(
label=""Normal Animation"",
underline=0,
variable=self._animation_frames,
value=5,
accelerator=""="",
)
animatemenu.add_radiobutton(
label=""Fast Animation"",
underline=0,
variable=self._animation_frames,
value=2,
accelerator=""+"",
)
menubar.add_cascade(label=""Animate"", underline=1, menu=animatemenu)
helpmenu = Menu(menubar, tearoff=0)
helpmenu.add_command(label=""About"", underline=0, command=self.about)
helpmenu.add_command(
label=""Instructions"", underline=0, command=self.help, accelerator=""F1""
)
menubar.add_cascade(label=""Help"", underline=0, menu=helpmenu)
parent.config(menu=menubar)
",[],0,[],/app/rdparser_app.py__init_menubar
981,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py__get,"def _get(self, widget, treeloc):
for i in treeloc:
widget = widget.subtrees()[i]
if isinstance(widget, TreeSegmentWidget):
widget = widget.label()
return widget
",[],0,[],/app/rdparser_app.py__get
982,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py__redraw,"def _redraw(self):
canvas = self._canvas
if self._tree is not None:
self._cframe.destroy_widget(self._tree)
for twidget in self._textwidgets:
self._cframe.destroy_widget(twidget)
if self._textline is not None:
self._canvas.delete(self._textline)
helv = (""helvetica"", -self._size.get())
bold = (""helvetica"", -self._size.get(), ""bold"")
attribs = {
""tree_color"": ""#000000"",
""tree_width"": 2,
""node_font"": bold,
""leaf_font"": helv,
}
tree = self._parser.tree()
self._tree = tree_to_treesegment(canvas, tree, **attribs)
self._cframe.add_widget(self._tree, 30, 5)
helv = (""helvetica"", -self._size.get())
bottom = y = self._cframe.scrollregion()[3]
self._textwidgets = [
TextWidget(canvas, word, font=self._font) for word in self._sent
]
for twidget in self._textwidgets:
self._cframe.add_widget(twidget, 0, 0)
twidget.move(0, bottom - twidget.bbox()[3] - 5)
y = min(y, twidget.bbox()[1])
self._textline = canvas.create_line(-5000, y - 5, 5000, y - 5, dash=""."")
self._highlight_nodes()
self._highlight_prodlist()
self._position_text()
",[],0,[],/app/rdparser_app.py__redraw
983,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py__redraw_quick,"def _redraw_quick(self):
self._highlight_nodes()
self._highlight_prodlist()
self._position_text()
",[],0,[],/app/rdparser_app.py__redraw_quick
984,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py__highlight_nodes,"def _highlight_nodes(self):
bold = (""helvetica"", -self._size.get(), ""bold"")
for treeloc in self._parser.frontier()[:1]:
self._get(self._tree, treeloc)[""color""] = ""#20a050""
self._get(self._tree, treeloc)[""font""] = bold
for treeloc in self._parser.frontier()[1:]:
self._get(self._tree, treeloc)[""color""] = ""#008080""
",[],0,[],/app/rdparser_app.py__highlight_nodes
985,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py__highlight_prodlist,"def _highlight_prodlist(self):
self._prodlist.delete(0, ""end"")
expandable = self._parser.expandable_productions()
untried = self._parser.untried_expandable_productions()
productions = self._productions
for index in range(len(productions)):
if productions[index] in expandable:
if productions[index] in untried:
self._prodlist.insert(index, "" %s"" % productions[index])
else:
self._prodlist.insert(index, "" %s (TRIED)"" % productions[index])
self._prodlist.selection_set(index)
else:
self._prodlist.insert(index, "" %s"" % productions[index])
",[],0,[],/app/rdparser_app.py__highlight_prodlist
986,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py__position_text,"def _position_text(self):
numwords = len(self._sent)
num_matched = numwords - len(self._parser.remaining_text())
leaves = self._tree_leaves()[:num_matched]
xmax = self._tree.bbox()[0]
for i in range(0, len(leaves)):
widget = self._textwidgets[i]
leaf = leaves[i]
widget[""color""] = ""#006040""
leaf[""color""] = ""#006040""
widget.move(leaf.bbox()[0] - widget.bbox()[0], 0)
xmax = widget.bbox()[2] + 10
for i in range(len(leaves), numwords):
widget = self._textwidgets[i]
widget[""color""] = ""#a0a0a0""
widget.move(xmax - widget.bbox()[0], 0)
xmax = widget.bbox()[2] + 10
if self._parser.currently_complete():
for twidget in self._textwidgets:
twidget[""color""] = ""#00a000""
for i in range(0, len(leaves)):
widget = self._textwidgets[i]
leaf = leaves[i]
dy = widget.bbox()[1] - leaf.bbox()[3] - 10.0
dy = max(dy, leaf.parent().label().bbox()[3] - leaf.bbox()[3] + 10)
leaf.move(0, dy)
",[],0,[],/app/rdparser_app.py__position_text
987,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py__tree_leaves,"def _tree_leaves(self, tree=None):
if tree is None:
tree = self._tree
if isinstance(tree, TreeSegmentWidget):
leaves = []
for child in tree.subtrees():
leaves += self._tree_leaves(child)
return leaves
else:
return [tree]
",[],0,[],/app/rdparser_app.py__tree_leaves
988,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py_destroy,"def destroy(self, *e):
self._autostep = 0
if self._top is None:
return
self._top.destroy()
self._top = None
",[],0,[],/app/rdparser_app.py_destroy
989,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py_reset,"def reset(self, *e):
self._autostep = 0
self._parser.initialize(self._sent)
self._lastoper1[""text""] = ""Reset Application""
self._lastoper2[""text""] = """"
self._redraw()
",[],0,[],/app/rdparser_app.py_reset
990,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py_autostep,"def autostep(self, *e):
if self._animation_frames.get() == 0:
self._animation_frames.set(2)
if self._autostep:
self._autostep = 0
else:
self._autostep = 1
self._step()
",[],0,[],/app/rdparser_app.py_autostep
991,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py_cancel_autostep,"def cancel_autostep(self, *e):
self._autostep = 0
",[],0,[],/app/rdparser_app.py_cancel_autostep
992,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py_step,"def step(self, *e):
self._autostep = 0
self._step()
",[],0,[],/app/rdparser_app.py_step
993,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py_match,"def match(self, *e):
self._autostep = 0
self._match()
",[],0,[],/app/rdparser_app.py_match
994,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py_expand,"def expand(self, *e):
self._autostep = 0
self._expand()
",[],0,[],/app/rdparser_app.py_expand
995,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py_backtrack,"def backtrack(self, *e):
self._autostep = 0
self._backtrack()
",[],0,[],/app/rdparser_app.py_backtrack
996,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py__step,"def _step(self):
if self._animating_lock:
return
if self._expand():
pass
elif self._parser.untried_match() and self._match():
pass
elif self._backtrack():
pass
else:
self._lastoper1[""text""] = ""Finished""
self._lastoper2[""text""] = """"
self._autostep = 0
if self._parser.currently_complete():
self._autostep = 0
self._lastoper2[""text""] += ""    [COMPLETE PARSE]""
",[],0,[],/app/rdparser_app.py__step
997,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py__expand,"def _expand(self, *e):
if self._animating_lock:
return
old_frontier = self._parser.frontier()
rv = self._parser.expand()
if rv is not None:
self._lastoper1[""text""] = ""Expand:""
self._lastoper2[""text""] = rv
self._prodlist.selection_clear(0, ""end"")
index = self._productions.index(rv)
self._prodlist.selection_set(index)
self._animate_expand(old_frontier[0])
return True
else:
self._lastoper1[""text""] = ""Expand:""
self._lastoper2[""text""] = ""(all expansions tried)""
return False
",[],0,[],/app/rdparser_app.py__expand
998,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py__match,"def _match(self, *e):
if self._animating_lock:
return
old_frontier = self._parser.frontier()
rv = self._parser.match()
if rv is not None:
self._lastoper1[""text""] = ""Match:""
self._lastoper2[""text""] = rv
self._animate_match(old_frontier[0])
return True
else:
self._lastoper1[""text""] = ""Match:""
self._lastoper2[""text""] = ""(failed)""
return False
",[],0,[],/app/rdparser_app.py__match
999,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py__backtrack,"def _backtrack(self, *e):
if self._animating_lock:
return
if self._parser.backtrack():
elt = self._parser.tree()
for i in self._parser.frontier()[0]:
elt = elt[i]
self._lastoper1[""text""] = ""Backtrack""
self._lastoper2[""text""] = """"
if isinstance(elt, Tree):
self._animate_backtrack(self._parser.frontier()[0])
else:
self._animate_match_backtrack(self._parser.frontier()[0])
return True
else:
self._autostep = 0
self._lastoper1[""text""] = ""Finished""
self._lastoper2[""text""] = """"
return False
",[],0,[],/app/rdparser_app.py__backtrack
1000,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py_about,"def about(self, *e):
ABOUT = (
""NLTK Recursive Descent Parser Application\n"" + ""Written by Edward Loper""
)
TITLE = ""About: Recursive Descent Parser Application""
try:
from tkinter.messagebox import Message
Message(message=ABOUT, title=TITLE).show()
except:
ShowText(self._top, TITLE, ABOUT)
",[],0,[],/app/rdparser_app.py_about
1001,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py_help,"def help(self, *e):
self._autostep = 0
try:
ShowText(
self._top,
""Help: Recursive Descent Parser Application"",
(__doc__ or """").strip(),
width=75,
font=""fixed"",
)
except:
ShowText(
self._top,
""Help: Recursive Descent Parser Application"",
(__doc__ or """").strip(),
width=75,
)
",[],0,[],/app/rdparser_app.py_help
1002,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py_postscript,"def postscript(self, *e):
self._autostep = 0
self._cframe.print_to_file()
",[],0,[],/app/rdparser_app.py_postscript
1003,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py_mainloop,"def mainloop(self, *args, **kwargs):
""""""
Enter the Tkinter mainloop.  This function must be called if
this demo is created from a non-interactive program (e.g.
from a secript)
the script completes.
""""""
if in_idle():
return
self._top.mainloop(*args, **kwargs)
",[],0,[],/app/rdparser_app.py_mainloop
1004,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py_resize,"def resize(self, size=None):
if size is not None:
self._size.set(size)
size = self._size.get()
self._font.configure(size=-(abs(size)))
self._boldfont.configure(size=-(abs(size)))
self._sysfont.configure(size=-(abs(size)))
self._bigfont.configure(size=-(abs(size + 2)))
self._redraw()
",[],0,[],/app/rdparser_app.py_resize
1005,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py__toggle_grammar,"def _toggle_grammar(self, *e):
if self._show_grammar.get():
self._prodframe.pack(
fill=""both"", side=""left"", padx=2, after=self._feedbackframe
)
self._lastoper1[""text""] = ""Show Grammar""
else:
self._prodframe.pack_forget()
self._lastoper1[""text""] = ""Hide Grammar""
self._lastoper2[""text""] = """"
",[],0,[],/app/rdparser_app.py__toggle_grammar
1006,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py__prodlist_select,"def _prodlist_select(self, event):
selection = self._prodlist.curselection()
if len(selection) != 1:
return
index = int(selection[0])
old_frontier = self._parser.frontier()
production = self._parser.expand(self._productions[index])
if production:
self._lastoper1[""text""] = ""Expand:""
self._lastoper2[""text""] = production
self._prodlist.selection_clear(0, ""end"")
self._prodlist.selection_set(index)
self._animate_expand(old_frontier[0])
else:
self._prodlist.selection_clear(0, ""end"")
for prod in self._parser.expandable_productions():
index = self._productions.index(prod)
self._prodlist.selection_set(index)
",[],0,[],/app/rdparser_app.py__prodlist_select
1007,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py__animate_expand,"def _animate_expand(self, treeloc):
oldwidget = self._get(self._tree, treeloc)
oldtree = oldwidget.parent()
top = not isinstance(oldtree.parent(), TreeSegmentWidget)
tree = self._parser.tree()
for i in treeloc:
tree = tree[i]
widget = tree_to_treesegment(
self._canvas,
tree,
node_font=self._boldfont,
leaf_color=""white"",
tree_width=2,
tree_color=""white"",
node_color=""white"",
leaf_font=self._font,
)
widget.label()[""color""] = ""#20a050""
(oldx, oldy) = oldtree.label().bbox()[:2]
(newx, newy) = widget.label().bbox()[:2]
widget.move(oldx - newx, oldy - newy)
if top:
self._cframe.add_widget(widget, 0, 5)
widget.move(30 - widget.label().bbox()[0], 0)
self._tree = widget
else:
oldtree.parent().replace_child(oldtree, widget)
if widget.subtrees():
dx = (
oldx
+ widget.label().width() / 2
- widget.subtrees()[0].bbox()[0] / 2
- widget.subtrees()[0].bbox()[2] / 2
)
for subtree in widget.subtrees():
subtree.move(dx, 0)
self._makeroom(widget)
if top:
self._cframe.destroy_widget(oldtree)
else:
oldtree.destroy()
colors = [
""gray%d"" % (10 * int(10 * x / self._animation_frames.get()))
for x in range(self._animation_frames.get(), 0, -1)
]
dy = widget.bbox()[3] + 30 - self._canvas.coords(self._textline)[1]
if dy > 0:
for twidget in self._textwidgets:
twidget.move(0, dy)
self._canvas.move(self._textline, 0, dy)
self._animate_expand_frame(widget, colors)
",[],0,[],/app/rdparser_app.py__animate_expand
1008,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py__makeroom,"def _makeroom(self, treeseg):
""""""
Make sure that no sibling tree bbox's overlap.
""""""
parent = treeseg.parent()
if not isinstance(parent, TreeSegmentWidget):
return
index = parent.subtrees().index(treeseg)
rsiblings = parent.subtrees()[index + 1 :]
if rsiblings:
dx = treeseg.bbox()[2] - rsiblings[0].bbox()[0] + 10
for sibling in rsiblings:
sibling.move(dx, 0)
if index > 0:
lsibling = parent.subtrees()[index - 1]
dx = max(0, lsibling.bbox()[2] - treeseg.bbox()[0] + 10)
treeseg.move(dx, 0)
self._makeroom(parent)
",[],0,[],/app/rdparser_app.py__makeroom
1009,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py__animate_expand_frame,"def _animate_expand_frame(self, widget, colors):
if len(colors) > 0:
self._animating_lock = 1
widget[""color""] = colors[0]
for subtree in widget.subtrees():
if isinstance(subtree, TreeSegmentWidget):
subtree.label()[""color""] = colors[0]
else:
subtree[""color""] = colors[0]
self._top.after(50, self._animate_expand_frame, widget, colors[1:])
else:
widget[""color""] = ""black""
for subtree in widget.subtrees():
if isinstance(subtree, TreeSegmentWidget):
subtree.label()[""color""] = ""black""
else:
subtree[""color""] = ""black""
self._redraw_quick()
widget.label()[""color""] = ""black""
self._animating_lock = 0
if self._autostep:
self._step()
",[],0,[],/app/rdparser_app.py__animate_expand_frame
1010,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py__animate_backtrack,"def _animate_backtrack(self, treeloc):
if self._animation_frames.get() == 0:
colors = []
else:
colors = [""#a00000"", ""#000000"", ""#a00000""]
colors += [
""gray%d"" % (10 * int(10 * x / (self._animation_frames.get())))
for x in range(1, self._animation_frames.get() + 1)
]
widgets = [self._get(self._tree, treeloc).parent()]
for subtree in widgets[0].subtrees():
if isinstance(subtree, TreeSegmentWidget):
widgets.append(subtree.label())
else:
widgets.append(subtree)
self._animate_backtrack_frame(widgets, colors)
",[],0,[],/app/rdparser_app.py__animate_backtrack
1011,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py__animate_backtrack_frame,"def _animate_backtrack_frame(self, widgets, colors):
if len(colors) > 0:
self._animating_lock = 1
for widget in widgets:
widget[""color""] = colors[0]
self._top.after(50, self._animate_backtrack_frame, widgets, colors[1:])
else:
for widget in widgets[0].subtrees():
widgets[0].remove_child(widget)
widget.destroy()
self._redraw_quick()
self._animating_lock = 0
if self._autostep:
self._step()
",[],0,[],/app/rdparser_app.py__animate_backtrack_frame
1012,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py__animate_match_backtrack,"def _animate_match_backtrack(self, treeloc):
widget = self._get(self._tree, treeloc)
node = widget.parent().label()
dy = (node.bbox()[3] - widget.bbox()[1] + 14) / max(
1, self._animation_frames.get()
)
self._animate_match_backtrack_frame(self._animation_frames.get(), widget, dy)
",[],0,[],/app/rdparser_app.py__animate_match_backtrack
1013,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py__animate_match,"def _animate_match(self, treeloc):
widget = self._get(self._tree, treeloc)
dy = (self._textwidgets[0].bbox()[1] - widget.bbox()[3] - 10.0) / max(
1, self._animation_frames.get()
)
self._animate_match_frame(self._animation_frames.get(), widget, dy)
",[],0,[],/app/rdparser_app.py__animate_match
1014,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py__animate_match_frame,"def _animate_match_frame(self, frame, widget, dy):
if frame > 0:
self._animating_lock = 1
widget.move(0, dy)
self._top.after(10, self._animate_match_frame, frame - 1, widget, dy)
else:
widget[""color""] = ""#006040""
self._redraw_quick()
self._animating_lock = 0
if self._autostep:
self._step()
",[],0,[],/app/rdparser_app.py__animate_match_frame
1015,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py__animate_match_backtrack_frame,"def _animate_match_backtrack_frame(self, frame, widget, dy):
if frame > 0:
self._animating_lock = 1
widget.move(0, dy)
self._top.after(
10, self._animate_match_backtrack_frame, frame - 1, widget, dy
)
else:
widget.parent().remove_child(widget)
widget.destroy()
self._animating_lock = 0
if self._autostep:
self._step()
",[],0,[],/app/rdparser_app.py__animate_match_backtrack_frame
1016,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py_edit_grammar,"def edit_grammar(self, *e):
CFGEditor(self._top, self._parser.grammar(), self.set_grammar)
",[],0,[],/app/rdparser_app.py_edit_grammar
1017,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py_set_grammar,"def set_grammar(self, grammar):
self._parser.set_grammar(grammar)
self._productions = list(grammar.productions())
self._prodlist.delete(0, ""end"")
for production in self._productions:
self._prodlist.insert(""end"", ("" %s"" % production))
",[],0,[],/app/rdparser_app.py_set_grammar
1018,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py_edit_sentence,"def edit_sentence(self, *e):
sentence = "" "".join(self._sent)
title = ""Edit Text""
instr = ""Enter a new sentence to parse.""
EntryDialog(self._top, sentence, instr, self.set_sentence, title)
",[],0,[],/app/rdparser_app.py_edit_sentence
1019,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py_set_sentence,"def set_sentence(self, sentence):
self._sent = sentence.split()  # [XX] use tagged?
self.reset()
",[],0,[],/app/rdparser_app.py_set_sentence
1020,/home/amandapotts/git/nltk/nltk/app/rdparser_app.py_app,"def app():
""""""
Create a recursive descent parser demo, using a simple grammar and
text.
""""""
from nltk.grammar import CFG
grammar = CFG.fromstring(
""""""
S -> NP VP
NP -> Det N PP | Det N
VP -> V NP PP | V NP | V
PP -> P NP
NP -> 'I'
Det -> 'the' | 'a'
N -> 'man' | 'park' | 'dog' | 'telescope'
V -> 'ate' | 'saw'
P -> 'in' | 'under' | 'with'
""""""
)
sent = ""the dog saw a man in the park"".split()
RecursiveDescentApp(grammar, sent).mainloop()
",[],0,[],/app/rdparser_app.py_app
1021,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py_do_HEAD,"def do_HEAD(self):
self.send_head()
",[],0,[],/app/wordnet_app.py_do_HEAD
1022,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py_do_GET,"def do_GET(self):
global firstClient
sp = self.path[1:]
if unquote_plus(sp) == ""SHUTDOWN THE SERVER"":
if server_mode:
page = ""Server must be killed with SIGTERM.""
type = ""text/plain""
else:
print(""Server shutting down!"")
os._exit(0)
elif sp == """":  # First request.
type = ""text/html""
if not server_mode and firstClient:
firstClient = False
page = get_static_index_page(True)
else:
page = get_static_index_page(False)
word = ""green""
elif sp.endswith("".html""):  # Trying to fetch a HTML file TODO:
type = ""text/html""
usp = unquote_plus(sp)
if usp == ""NLTK Wordnet Browser Database Info.html"":
word = ""* Database Info *""
if os.path.isfile(usp):
with open(usp) as infile:
page = infile.read()
else:
page = (
(html_header % word) + ""<p>The database info file:""
""<p><b>""
+ usp
+ ""</b>""
+ ""<p>was not found. Run this:""
+ ""<p><b>python dbinfo_html.py</b>""
+ ""<p>to produce it.""
+ html_trailer
)
else:
word = sp
try:
page = get_static_page_by_path(usp)
except FileNotFoundError:
page = ""Internal error: Path for static page '%s' is unknown"" % usp
type = ""text/plain""
elif sp.startswith(""search""):
type = ""text/html""
parts = (sp.split(""?"")[1]).split(""&"")
word = [
p.split(""="")[1].replace(""+"", "" "")
for p in parts
if p.startswith(""nextWord"")
][0]
page, word = page_from_word(word)
elif sp.startswith(""lookup_""):
type = ""text/html""
sp = sp[len(""lookup_"") :]
page, word = page_from_href(sp)
elif sp == ""start_page"":
type = ""text/html""
page, word = page_from_word(""wordnet"")
else:
type = ""text/plain""
page = ""Could not parse request: '%s'"" % sp
self.send_head(type)
self.wfile.write(page.encode(""utf8""))
",[],0,[],/app/wordnet_app.py_do_GET
1023,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py_send_head,"def send_head(self, type=None):
self.send_response(200)
self.send_header(""Content-type"", type)
self.end_headers()
",[],0,[],/app/wordnet_app.py_send_head
1024,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py_log_message,"def log_message(self, format, *args):
global logfile
if logfile:
logfile.write(
""%s - - [%s] %s\n""
% (self.address_string(), self.log_date_time_string(), format % args)
)
",[],0,[],/app/wordnet_app.py_log_message
1025,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py_get_unique_counter_from_url,"def get_unique_counter_from_url(sp):
""""""
Extract the unique counter from the URL if it has one.  Otherwise return
null.
""""""
pos = sp.rfind(""%23"")
if pos != -1:
return int(sp[(pos + 3) :])
else:
return None
",[],0,[],/app/wordnet_app.py_get_unique_counter_from_url
1026,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py_wnb,"def wnb(port=8000, runBrowser=True, logfilename=None):
""""""
Run NLTK Wordnet Browser Server.
:param port: The port number for the server to listen on, defaults to
8000
:type  port: int
:param runBrowser: True to start a web browser and point it at the web
server.
:type  runBrowser: bool
""""""
global server_mode, logfile
server_mode = not runBrowser
if logfilename:
try:
logfile = open(logfilename, ""a"", 1)  # 1 means 'line buffering'
except OSError as e:
sys.stderr.write(""Couldn't open %s for writing: %s"", logfilename, e)
sys.exit(1)
else:
logfile = None
url = ""http://localhost:"" + str(port)
server_ready = None
browser_thread = None
if runBrowser:
server_ready = threading.Event()
browser_thread = startBrowser(url, server_ready)
server = HTTPServer(("""", port), MyServerHandler)
if logfile:
logfile.write(""NLTK Wordnet browser server running serving: %s\n"" % url)
if runBrowser:
server_ready.set()
try:
server.serve_forever()
except KeyboardInterrupt:
pass
if runBrowser:
browser_thread.join()
if logfile:
logfile.close()
",[],0,[],/app/wordnet_app.py_wnb
1027,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py_startBrowser,"def startBrowser(url, server_ready):
",[],0,[],/app/wordnet_app.py_startBrowser
1028,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py_run,"def run():
server_ready.wait()
time.sleep(1)  # Wait a little bit more, there's still the chance of
webbrowser.open(url, new=2, autoraise=1)
",[],0,[],/app/wordnet_app.py_run
1029,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py__pos_tuples,"def _pos_tuples():
return [
(wn.NOUN, ""N"", ""noun""),
(wn.VERB, ""V"", ""verb""),
(wn.ADJ, ""J"", ""adj""),
(wn.ADV, ""R"", ""adv""),
]
",[],0,[],/app/wordnet_app.py__pos_tuples
1030,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py__pos_match,"def _pos_match(pos_tuple):
""""""
This function returns the complete pos tuple for the partial pos
tuple given to it.  It attempts to match it against the first
non-null component of the given pos tuple.
""""""
if pos_tuple[0] == ""s"":
pos_tuple = (""a"", pos_tuple[1], pos_tuple[2])
for n, x in enumerate(pos_tuple):
if x is not None:
break
for pt in _pos_tuples():
if pt[n] == pos_tuple[n]:
return pt
return None
",[],0,[],/app/wordnet_app.py__pos_match
1031,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py_lemma_property,"def lemma_property(word, synset, func):
",[],0,[],/app/wordnet_app.py_lemma_property
1032,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py_flattern,"def flattern(l):
if l == []:
return []
else:
return l[0] + flattern(l[1:])
",[],0,[],/app/wordnet_app.py_flattern
1033,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py_rebuild_tree,"def rebuild_tree(orig_tree):
node = orig_tree[0]
children = orig_tree[1:]
return (node, [rebuild_tree(t) for t in children])
",[],0,[],/app/wordnet_app.py_rebuild_tree
1034,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py__bold,"def _bold(txt):
return ""<b>%s</b>"" % txt
",[],0,[],/app/wordnet_app.py__bold
1035,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py__center,"def _center(txt):
return ""<center>%s</center>"" % txt
",[],0,[],/app/wordnet_app.py__center
1036,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py__hlev,"def _hlev(n, txt):
return ""<h%d>%s</h%d>"" % (n, txt, n)
",[],0,[],/app/wordnet_app.py__hlev
1037,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py__italic,"def _italic(txt):
return ""<i>%s</i>"" % txt
",[],0,[],/app/wordnet_app.py__italic
1038,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py__li,"def _li(txt):
return ""<li>%s</li>"" % txt
",[],0,[],/app/wordnet_app.py__li
1039,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py_pg,"def pg(word, body):
""""""
Return a HTML page of NLTK Browser format constructed from the
word and body
:param word: The word that the body corresponds to
:type word: str
:param body: The HTML body corresponding to the word
:type body: str
:return: a HTML page for the word-body combination
:rtype: str
""""""
return (html_header % word) + body + html_trailer
",[],0,[],/app/wordnet_app.py_pg
1040,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py__ul,"def _ul(txt):
return ""<ul>"" + txt + ""</ul>""
",[],0,[],/app/wordnet_app.py__ul
1041,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py__abbc,"def _abbc(txt):
""""""
abbc = asterisks, breaks, bold, center
""""""
return _center(_bold(""<br>"" * 10 + ""*"" * 10 + "" "" + txt + "" "" + ""*"" * 10))
",[],0,[],/app/wordnet_app.py__abbc
1042,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py__get_synset,"def _get_synset(synset_key):
""""""
The synset key is the unique name of the synset, this can be
retrieved via synset.name()
""""""
return wn.synset(synset_key)
",[],0,[],/app/wordnet_app.py__get_synset
1043,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py__collect_one_synset,"def _collect_one_synset(word, synset, synset_relations):
""""""
Returns the HTML string for one synset or word
:param word: the current word
:type word: str
:param synset: a synset
:type synset: synset
:param synset_relations: information about which synset relations
to display.
:type synset_relations: dict(synset_key, set(relation_id))
:return: The HTML string built for this synset
:rtype: str
""""""
if isinstance(synset, tuple):  # It's a word
raise NotImplementedError(""word not supported by _collect_one_synset"")
typ = ""S""
pos_tuple = _pos_match((synset.pos(), None, None))
assert pos_tuple is not None, ""pos_tuple is null: synset.pos(): %s"" % synset.pos()
descr = pos_tuple[2]
ref = copy.deepcopy(Reference(word, synset_relations))
ref.toggle_synset(synset)
synset_label = typ + ""
if synset.name() in synset_relations:
synset_label = _bold(synset_label)
s = f""<li>{make_lookup_link(ref, synset_label)} ({descr}) ""
",[],0,[],/app/wordnet_app.py__collect_one_synset
1044,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py_format_lemma,"def format_lemma(w):
w = w.replace(""_"", "" "")
if w.lower() == word:
return _bold(w)
else:
ref = Reference(w)
return make_lookup_link(ref, w)
",[],0,[],/app/wordnet_app.py_format_lemma
1045,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py__collect_all_synsets,"def _collect_all_synsets(word, pos, synset_relations=dict()):
""""""
Return a HTML unordered list of synsets for the given word and
part of speech.
""""""
return ""<ul>%s\n</ul>\n"" % """".join(
_collect_one_synset(word, synset, synset_relations)
for synset in wn.synsets(word, pos)
)
",[],0,[],/app/wordnet_app.py__collect_all_synsets
1046,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py__synset_relations,"def _synset_relations(word, synset, synset_relations):
""""""
Builds the HTML string for the relations of a synset
:param word: The current word
:type word: str
:param synset: The synset for which we're building the relations.
:type synset: Synset
:param synset_relations: synset keys and relation types for which to display relations.
:type synset_relations: dict(synset_key, set(relation_type))
:return: The HTML for a synset's relations
:rtype: str
""""""
if not synset.name() in synset_relations:
return """"
ref = Reference(word, synset_relations)
",[],0,[],/app/wordnet_app.py__synset_relations
1047,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py_relation_html,"def relation_html(r):
if isinstance(r, Synset):
return make_lookup_link(Reference(r.lemma_names()[0]), r.lemma_names()[0])
elif isinstance(r, Lemma):
return relation_html(r.synset())
elif isinstance(r, tuple):
return ""{}\n<ul>{}</ul>\n"".format(
relation_html(r[0]),
"""".join(""<li>%s</li>\n"" % relation_html(sr) for sr in r[1]),
)
else:
raise TypeError(
""r must be a synset, lemma or list, it was: type(r) = %s, r = %s""
% (type(r), r)
)
",[],0,[],/app/wordnet_app.py_relation_html
1048,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py_make_synset_html,"def make_synset_html(db_name, disp_name, rels):
synset_html = ""<i>%s</i>\n"" % make_lookup_link(
copy.deepcopy(ref).toggle_synset_relation(synset, db_name),
disp_name,
)
if db_name in ref.synset_relations[synset.name()]:
synset_html += ""<ul>%s</ul>\n"" % """".join(
""<li>%s</li>\n"" % relation_html(r) for r in rels
)
return synset_html
",[],0,[],/app/wordnet_app.py_make_synset_html
1049,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py_find_class,"def find_class(self, module, name):
raise pickle.UnpicklingError(f""global '{module}.{name}' is forbidden"")
",[],0,[],/app/wordnet_app.py_find_class
1050,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py___init__,"def __init__(self, word, synset_relations=dict()):
""""""
Build a reference to a new page.
word is the word or words (separated by commas) for which to
search for synsets of
synset_relations is a dictionary of synset keys to sets of
synset relation identifaiers to unfold a list of synset
relations for.
""""""
self.word = word
self.synset_relations = synset_relations
",[],0,[],/app/wordnet_app.py___init__
1051,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py_encode,"def encode(self):
""""""
Encode this reference into a string to be used in a URL.
""""""
string = pickle.dumps((self.word, self.synset_relations), -1)
return base64.urlsafe_b64encode(string).decode()
",[],0,[],/app/wordnet_app.py_encode
1052,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py_decode,"def decode(string):
""""""
Decode a reference encoded with Reference.encode
""""""
string = base64.urlsafe_b64decode(string.encode())
word, synset_relations = RestrictedUnpickler(io.BytesIO(string)).load()
return Reference(word, synset_relations)
",[],0,[],/app/wordnet_app.py_decode
1053,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py_toggle_synset_relation,"def toggle_synset_relation(self, synset, relation):
""""""
Toggle the display of the relations for the given synset and
relation type.
This function will throw a KeyError if the synset is currently
not being displayed.
""""""
if relation in self.synset_relations[synset.name()]:
self.synset_relations[synset.name()].remove(relation)
else:
self.synset_relations[synset.name()].add(relation)
return self
",[],0,[],/app/wordnet_app.py_toggle_synset_relation
1054,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py_toggle_synset,"def toggle_synset(self, synset):
""""""
Toggle displaying of the relation types for the given synset
""""""
if synset.name() in self.synset_relations:
del self.synset_relations[synset.name()]
else:
self.synset_relations[synset.name()] = set()
return self
",[],0,[],/app/wordnet_app.py_toggle_synset
1055,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py_make_lookup_link,"def make_lookup_link(ref, label):
return f'<a href=""lookup_{ref.encode()}"">{label}</a>'
",[],0,[],/app/wordnet_app.py_make_lookup_link
1056,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py_page_from_word,"def page_from_word(word):
""""""
Return a HTML page for the given word.
:type word: str
:param word: The currently active word
:return: A tuple (page,word), where page is the new current HTML page
to be sent to the browser and
word is the new current word
:rtype: A tuple (str,str)
""""""
return page_from_reference(Reference(word))
",[],0,[],/app/wordnet_app.py_page_from_word
1057,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py_page_from_href,"def page_from_href(href):
""""""
Returns a tuple of the HTML page built and the new current word
:param href: The hypertext reference to be solved
:type href: str
:return: A tuple (page,word), where page is the new current HTML page
to be sent to the browser and
word is the new current word
:rtype: A tuple (str,str)
""""""
return page_from_reference(Reference.decode(href))
",[],0,[],/app/wordnet_app.py_page_from_href
1058,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py_page_from_reference,"def page_from_reference(href):
""""""
Returns a tuple of the HTML page built and the new current word
:param href: The hypertext reference to be solved
:type href: str
:return: A tuple (page,word), where page is the new current HTML page
to be sent to the browser and
word is the new current word
:rtype: A tuple (str,str)
""""""
word = href.word
pos_forms = defaultdict(list)
words = word.split("","")
words = [w for w in [w.strip().lower().replace("" "", ""_"") for w in words] if w != """"]
if len(words) == 0:
return """", ""Please specify a word to search for.""
for w in words:
for pos in [wn.NOUN, wn.VERB, wn.ADJ, wn.ADV]:
form = wn.morphy(w, pos)
if form and form not in pos_forms[pos]:
pos_forms[pos].append(form)
body = """"
for pos, pos_str, name in _pos_tuples():
if pos in pos_forms:
body += _hlev(3, name) + ""\n""
for w in pos_forms[pos]:
try:
body += _collect_all_synsets(w, pos, href.synset_relations)
except KeyError:
pass
if not body:
body = ""The word or words '%s' were not found in the dictionary."" % word
return body, word
",[],0,[],/app/wordnet_app.py_page_from_reference
1059,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py_get_static_page_by_path,"def get_static_page_by_path(path):
""""""
Return a static HTML page from the path given.
""""""
if path == ""index_2.html"":
return get_static_index_page(False)
elif path == ""index.html"":
return get_static_index_page(True)
elif path == ""NLTK Wordnet Browser Database Info.html"":
return ""Display of Wordnet Database Statistics is not supported""
elif path == ""upper_2.html"":
return get_static_upper_page(False)
elif path == ""upper.html"":
return get_static_upper_page(True)
elif path == ""web_help.html"":
return get_static_web_help_page()
elif path == ""wx_help.html"":
return get_static_wx_help_page()
raise FileNotFoundError()
",[],0,[],/app/wordnet_app.py_get_static_page_by_path
1060,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py_get_static_web_help_page,"def get_static_web_help_page():
""""""
Return the static web help page.
""""""
return """"""
",[],0,[],/app/wordnet_app.py_get_static_web_help_page
1061,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py_get_static_welcome_message,"def get_static_welcome_message():
""""""
Get the static welcome page.
""""""
return """"""
",[],0,[],/app/wordnet_app.py_get_static_welcome_message
1062,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py_get_static_index_page,"def get_static_index_page(with_shutdown):
""""""
Get the static index page.
""""""
template = """"""
",[],0,[],/app/wordnet_app.py_get_static_index_page
1063,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py_get_static_upper_page,"def get_static_upper_page(with_shutdown):
""""""
Return the upper frame page,
If with_shutdown is True then a 'shutdown' button is also provided
to shutdown the server.
""""""
template = """"""
",[],0,[],/app/wordnet_app.py_get_static_upper_page
1064,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py_usage,"def usage():
""""""
Display the command line help message.
""""""
print(__doc__)
",[],0,[],/app/wordnet_app.py_usage
1065,/home/amandapotts/git/nltk/nltk/app/wordnet_app.py_app,"def app():
(opts, _) = getopt.getopt(
argv[1:], ""l:p:sh"", [""logfile="", ""port="", ""server-mode"", ""help""]
)
port = 8000
server_mode = False
help_mode = False
logfilename = None
for opt, value in opts:
if (opt == ""-l"") or (opt == ""--logfile""):
logfilename = str(value)
elif (opt == ""-p"") or (opt == ""--port""):
port = int(value)
elif (opt == ""-s"") or (opt == ""--server-mode""):
server_mode = True
elif (opt == ""-h"") or (opt == ""--help""):
help_mode = True
if help_mode:
usage()
else:
wnb(port, not server_mode, logfilename)
",[],0,[],/app/wordnet_app.py_app
1066,/home/amandapotts/git/nltk/nltk/app/chunkparser_app.py_normalize_grammar,"def normalize_grammar(self, grammar):
grammar = re.sub(r""((\\.|[^#])*)(#.*)?"", r""\1"", grammar)
grammar = re.sub("" +"", "" "", grammar)
grammar = re.sub(r""\n\s+"", r""\n"", grammar)
grammar = grammar.strip()
grammar = re.sub(r""([^\\])\$"", r""\1\\$"", grammar)
return grammar
",[],0,[],/app/chunkparser_app.py_normalize_grammar
1067,/home/amandapotts/git/nltk/nltk/app/chunkparser_app.py___init__,"def __init__(
self,
devset_name=""conll2000"",
devset=None,
grammar="""",
chunk_label=""NP"",
tagset=None,
",[],0,[],/app/chunkparser_app.py___init__
1068,/home/amandapotts/git/nltk/nltk/app/chunkparser_app.py__init_fonts,"def _init_fonts(self, top):
self._size = IntVar(top)
self._size.set(20)
self._font = Font(family=""helvetica"", size=-self._size.get())
self._smallfont = Font(
family=""helvetica"", size=-(int(self._size.get() * 14 // 20))
)
",[],0,[],/app/chunkparser_app.py__init_fonts
1069,/home/amandapotts/git/nltk/nltk/app/chunkparser_app.py__init_menubar,"def _init_menubar(self, parent):
menubar = Menu(parent)
filemenu = Menu(menubar, tearoff=0)
filemenu.add_command(label=""Reset Application"", underline=0, command=self.reset)
filemenu.add_command(
label=""Save Current Grammar"",
underline=0,
accelerator=""Ctrl-s"",
command=self.save_grammar,
)
filemenu.add_command(
label=""Load Grammar"",
underline=0,
accelerator=""Ctrl-o"",
command=self.load_grammar,
)
filemenu.add_command(
label=""Save Grammar History"", underline=13, command=self.save_history
)
filemenu.add_command(
label=""Exit"", underline=1, command=self.destroy, accelerator=""Ctrl-q""
)
menubar.add_cascade(label=""File"", underline=0, menu=filemenu)
viewmenu = Menu(menubar, tearoff=0)
viewmenu.add_radiobutton(
label=""Tiny"",
variable=self._size,
underline=0,
value=10,
command=self.resize,
)
viewmenu.add_radiobutton(
label=""Small"",
variable=self._size,
underline=0,
value=16,
command=self.resize,
)
viewmenu.add_radiobutton(
label=""Medium"",
variable=self._size,
underline=0,
value=20,
command=self.resize,
)
viewmenu.add_radiobutton(
label=""Large"",
variable=self._size,
underline=0,
value=24,
command=self.resize,
)
viewmenu.add_radiobutton(
label=""Huge"",
variable=self._size,
underline=0,
value=34,
command=self.resize,
)
menubar.add_cascade(label=""View"", underline=0, menu=viewmenu)
devsetmenu = Menu(menubar, tearoff=0)
devsetmenu.add_radiobutton(
label=""50 sentences"",
variable=self._devset_size,
value=50,
command=self.set_devset_size,
)
devsetmenu.add_radiobutton(
label=""100 sentences"",
variable=self._devset_size,
value=100,
command=self.set_devset_size,
)
devsetmenu.add_radiobutton(
label=""200 sentences"",
variable=self._devset_size,
value=200,
command=self.set_devset_size,
)
devsetmenu.add_radiobutton(
label=""500 sentences"",
variable=self._devset_size,
value=500,
command=self.set_devset_size,
)
menubar.add_cascade(label=""Development-Set"", underline=0, menu=devsetmenu)
helpmenu = Menu(menubar, tearoff=0)
helpmenu.add_command(label=""About"", underline=0, command=self.about)
menubar.add_cascade(label=""Help"", underline=0, menu=helpmenu)
parent.config(menu=menubar)
",[],0,[],/app/chunkparser_app.py__init_menubar
1070,/home/amandapotts/git/nltk/nltk/app/chunkparser_app.py_toggle_show_trace,"def toggle_show_trace(self, *e):
if self._showing_trace:
self.show_devset()
else:
self.show_trace()
return ""break""
",[],0,[],/app/chunkparser_app.py_toggle_show_trace
1071,/home/amandapotts/git/nltk/nltk/app/chunkparser_app.py__eval_plot,"def _eval_plot(self, *e, **config):
width = config.get(""width"", self.evalbox.winfo_width())
height = config.get(""height"", self.evalbox.winfo_height())
self.evalbox.delete(""all"")
tag = self.evalbox.create_text(
10, height // 2 - 10, justify=""left"", anchor=""w"", text=""Precision""
)
left, right = self.evalbox.bbox(tag)[2] + 5, width - 10
tag = self.evalbox.create_text(
left + (width - left) // 2,
height - 10,
anchor=""s"",
text=""Recall"",
justify=""center"",
)
top, bot = 10, self.evalbox.bbox(tag)[1] - 10
bg = self._EVALBOX_PARAMS[""background""]
self.evalbox.lower(
self.evalbox.create_rectangle(0, 0, left - 1, 5000, fill=bg, outline=bg)
)
self.evalbox.lower(
self.evalbox.create_rectangle(0, bot + 1, 5000, 5000, fill=bg, outline=bg)
)
if self._autoscale.get() and len(self._history) > 1:
max_precision = max_recall = 0
min_precision = min_recall = 1
for i in range(1, min(len(self._history), self._SCALE_N + 1)):
grammar, precision, recall, fmeasure = self._history[-i]
min_precision = min(precision, min_precision)
min_recall = min(recall, min_recall)
max_precision = max(precision, max_precision)
max_recall = max(recall, max_recall)
min_precision = max(min_precision - 0.01, 0)
min_recall = max(min_recall - 0.01, 0)
max_precision = min(max_precision + 0.01, 1)
max_recall = min(max_recall + 0.01, 1)
else:
min_precision = min_recall = 0
max_precision = max_recall = 1
for i in range(11):
x = left + (right - left) * (
(i / 10.0 - min_recall) / (max_recall - min_recall)
)
y = bot - (bot - top) * (
(i / 10.0 - min_precision) / (max_precision - min_precision)
)
if left < x < right:
self.evalbox.create_line(x, top, x, bot, fill=""#888"")
if top < y < bot:
self.evalbox.create_line(left, y, right, y, fill=""#888"")
self.evalbox.create_line(left, top, left, bot)
self.evalbox.create_line(left, bot, right, bot)
self.evalbox.create_text(
left - 3,
bot,
justify=""right"",
anchor=""se"",
text=""%d%%"" % (100 * min_precision),
)
self.evalbox.create_text(
left - 3,
top,
justify=""right"",
anchor=""ne"",
text=""%d%%"" % (100 * max_precision),
)
self.evalbox.create_text(
left,
bot + 3,
justify=""center"",
anchor=""nw"",
text=""%d%%"" % (100 * min_recall),
)
self.evalbox.create_text(
right,
bot + 3,
justify=""center"",
anchor=""ne"",
text=""%d%%"" % (100 * max_recall),
)
prev_x = prev_y = None
for i, (_, precision, recall, fscore) in enumerate(self._history):
x = left + (right - left) * (
(recall - min_recall) / (max_recall - min_recall)
)
y = bot - (bot - top) * (
(precision - min_precision) / (max_precision - min_precision)
)
if i == self._history_index:
self.evalbox.create_oval(
x - 2, y - 2, x + 2, y + 2, fill=""#0f0"", outline=""#000""
)
self.status[""text""] = (
""Precision: %.2f%%\t"" % (precision * 100)
+ ""Recall: %.2f%%\t"" % (recall * 100)
+ ""F-score: %.2f%%"" % (fscore * 100)
)
else:
self.evalbox.lower(
self.evalbox.create_oval(
x - 2, y - 2, x + 2, y + 2, fill=""#afa"", outline=""#8c8""
)
)
if prev_x is not None and self._eval_lines.get():
self.evalbox.lower(
self.evalbox.create_line(prev_x, prev_y, x, y, fill=""#8c8"")
)
prev_x, prev_y = x, y
",[],0,[],/app/chunkparser_app.py__eval_plot
1072,/home/amandapotts/git/nltk/nltk/app/chunkparser_app.py__eval_demon,"def _eval_demon(self):
if self.top is None:
return
if self.chunker is None:
self._eval_demon_running = False
return
t0 = time.time()
if (
time.time() - self._last_keypress < self._EVAL_DELAY
and self.normalized_grammar != self._eval_normalized_grammar
):
self._eval_demon_running = True
return self.top.after(int(self._EVAL_FREQ * 1000), self._eval_demon)
if self.normalized_grammar != self._eval_normalized_grammar:
for g, p, r, f in self._history:
if self.normalized_grammar == self.normalize_grammar(g):
self._history.append((g, p, r, f))
self._history_index = len(self._history) - 1
self._eval_plot()
self._eval_demon_running = False
self._eval_normalized_grammar = None
return
self._eval_index = 0
self._eval_score = ChunkScore(chunk_label=self._chunk_label)
self._eval_grammar = self.grammar
self._eval_normalized_grammar = self.normalized_grammar
if self.normalized_grammar.strip() == """":
self._eval_demon_running = False
return
for gold in self.devset[
self._eval_index : min(
self._eval_index + self._EVAL_CHUNK, self._devset_size.get()
)
]:
guess = self._chunkparse(gold.leaves())
self._eval_score.score(gold, guess)
self._eval_index += self._EVAL_CHUNK
if self._eval_index >= self._devset_size.get():
self._history.append(
(
self._eval_grammar,
self._eval_score.precision(),
self._eval_score.recall(),
self._eval_score.f_measure(),
)
)
self._history_index = len(self._history) - 1
self._eval_plot()
self._eval_demon_running = False
self._eval_normalized_grammar = None
else:
progress = 100 * self._eval_index / self._devset_size.get()
self.status[""text""] = ""Evaluating on Development Set (%d%%)"" % progress
self._eval_demon_running = True
self._adaptively_modify_eval_chunk(time.time() - t0)
self.top.after(int(self._EVAL_FREQ * 1000), self._eval_demon)
",[],0,[],/app/chunkparser_app.py__eval_demon
1073,/home/amandapotts/git/nltk/nltk/app/chunkparser_app.py__adaptively_modify_eval_chunk,"def _adaptively_modify_eval_chunk(self, t):
""""""
Modify _EVAL_CHUNK to try to keep the amount of time that the
eval demon takes between _EVAL_DEMON_MIN and _EVAL_DEMON_MAX.
:param t: The amount of time that the eval demon took.
""""""
if t > self._EVAL_DEMON_MAX and self._EVAL_CHUNK > 5:
self._EVAL_CHUNK = min(
self._EVAL_CHUNK - 1,
max(
int(self._EVAL_CHUNK * (self._EVAL_DEMON_MAX / t)),
self._EVAL_CHUNK - 10,
),
)
elif t < self._EVAL_DEMON_MIN:
self._EVAL_CHUNK = max(
self._EVAL_CHUNK + 1,
min(
int(self._EVAL_CHUNK * (self._EVAL_DEMON_MIN / t)),
self._EVAL_CHUNK + 10,
),
)
",[],0,[],/app/chunkparser_app.py__adaptively_modify_eval_chunk
1074,/home/amandapotts/git/nltk/nltk/app/chunkparser_app.py_show_trace,"def show_trace(self, *e):
self._showing_trace = True
self.trace_button[""state""] = ""disabled""
self.devset_button[""state""] = ""normal""
self.devsetbox[""state""] = ""normal""
self.devsetbox.delete(""1.0"", ""end"")
self.devsetlabel[""text""] = ""Development Set (%d/%d)"" % (
(self.devset_index + 1, self._devset_size.get())
)
if self.chunker is None:
self.devsetbox.insert(""1.0"", ""Trace: waiting for a valid grammar."")
self.devsetbox.tag_add(""error"", ""1.0"", ""end"")
return  # can't do anything more
gold_tree = self.devset[self.devset_index]
rules = self.chunker.rules()
tagseq = ""\t""
charnum = [1]
for wordnum, (word, pos) in enumerate(gold_tree.leaves()):
tagseq += ""%s "" % pos
charnum.append(len(tagseq))
self.charnum = {
(i, j): charnum[j]
for i in range(len(rules) + 1)
for j in range(len(charnum))
}
self.linenum = {i: i * 2 + 2 for i in range(len(rules) + 1)}
for i in range(len(rules) + 1):
if i == 0:
self.devsetbox.insert(""end"", ""Start:\n"")
self.devsetbox.tag_add(""trace"", ""end -2c linestart"", ""end -2c"")
else:
self.devsetbox.insert(""end"", ""Apply %s:\n"" % rules[i - 1])
self.devsetbox.tag_add(""trace"", ""end -2c linestart"", ""end -2c"")
self.devsetbox.insert(""end"", tagseq + ""\n"")
self.devsetbox.tag_add(""wrapindent"", ""end -2c linestart"", ""end -2c"")
chunker = RegexpChunkParser(rules[:i])
test_tree = self._chunkparse(gold_tree.leaves())
gold_chunks = self._chunks(gold_tree)
test_chunks = self._chunks(test_tree)
for chunk in gold_chunks.intersection(test_chunks):
self._color_chunk(i, chunk, ""true-pos"")
for chunk in gold_chunks - test_chunks:
self._color_chunk(i, chunk, ""false-neg"")
for chunk in test_chunks - gold_chunks:
self._color_chunk(i, chunk, ""false-pos"")
self.devsetbox.insert(""end"", ""Finished.\n"")
self.devsetbox.tag_add(""trace"", ""end -2c linestart"", ""end -2c"")
self.top.after(100, self.devset_xscroll.set, 0, 0.3)
",[],0,[],/app/chunkparser_app.py_show_trace
1075,/home/amandapotts/git/nltk/nltk/app/chunkparser_app.py__history_prev,"def _history_prev(self, *e):
self._view_history(self._history_index - 1)
return ""break""
",[],0,[],/app/chunkparser_app.py__history_prev
1076,/home/amandapotts/git/nltk/nltk/app/chunkparser_app.py__history_next,"def _history_next(self, *e):
self._view_history(self._history_index + 1)
return ""break""
",[],0,[],/app/chunkparser_app.py__history_next
1077,/home/amandapotts/git/nltk/nltk/app/chunkparser_app.py__view_history,"def _view_history(self, index):
index = max(0, min(len(self._history) - 1, index))
if not self._history:
return
if index == self._history_index:
return
self.grammarbox[""state""] = ""normal""
self.grammarbox.delete(""1.0"", ""end"")
self.grammarbox.insert(""end"", self._history[index][0])
self.grammarbox.mark_set(""insert"", ""1.0"")
self._history_index = index
self._syntax_highlight_grammar(self._history[index][0])
self.normalized_grammar = self.normalize_grammar(self._history[index][0])
if self.normalized_grammar:
rules = [
RegexpChunkRule.fromstring(line)
for line in self.normalized_grammar.split(""\n"")
]
else:
rules = []
self.chunker = RegexpChunkParser(rules)
self._eval_plot()
self._highlight_devset()
if self._showing_trace:
self.show_trace()
if self._history_index < len(self._history) - 1:
self.grammarlabel[""text""] = ""Grammar {}/{}:"".format(
self._history_index + 1,
len(self._history),
)
else:
self.grammarlabel[""text""] = ""Grammar:""
",[],0,[],/app/chunkparser_app.py__view_history
1078,/home/amandapotts/git/nltk/nltk/app/chunkparser_app.py__devset_next,"def _devset_next(self, *e):
self._devset_scroll(""scroll"", 1, ""page"")
return ""break""
",[],0,[],/app/chunkparser_app.py__devset_next
1079,/home/amandapotts/git/nltk/nltk/app/chunkparser_app.py__devset_prev,"def _devset_prev(self, *e):
self._devset_scroll(""scroll"", -1, ""page"")
return ""break""
",[],0,[],/app/chunkparser_app.py__devset_prev
1080,/home/amandapotts/git/nltk/nltk/app/chunkparser_app.py_destroy,"def destroy(self, *e):
if self.top is None:
return
self.top.destroy()
self.top = None
",[],0,[],/app/chunkparser_app.py_destroy
1081,/home/amandapotts/git/nltk/nltk/app/chunkparser_app.py__devset_scroll,"def _devset_scroll(self, command, *args):
N = 1  # size of a page -- one sentence.
showing_trace = self._showing_trace
if command == ""scroll"" and args[1].startswith(""unit""):
self.show_devset(self.devset_index + int(args[0]))
elif command == ""scroll"" and args[1].startswith(""page""):
self.show_devset(self.devset_index + N * int(args[0]))
elif command == ""moveto"":
self.show_devset(int(float(args[0]) * self._devset_size.get()))
else:
assert 0, f""bad scroll command {command} {args}""
if showing_trace:
self.show_trace()
",[],0,[],/app/chunkparser_app.py__devset_scroll
1082,/home/amandapotts/git/nltk/nltk/app/chunkparser_app.py_show_devset,"def show_devset(self, index=None):
if index is None:
index = self.devset_index
index = min(max(0, index), self._devset_size.get() - 1)
if index == self.devset_index and not self._showing_trace:
return
self.devset_index = index
self._showing_trace = False
self.trace_button[""state""] = ""normal""
self.devset_button[""state""] = ""disabled""
self.devsetbox[""state""] = ""normal""
self.devsetbox[""wrap""] = ""word""
self.devsetbox.delete(""1.0"", ""end"")
self.devsetlabel[""text""] = ""Development Set (%d/%d)"" % (
(self.devset_index + 1, self._devset_size.get())
)
sample = self.devset[self.devset_index : self.devset_index + 1]
self.charnum = {}
self.linenum = {0: 1}
for sentnum, sent in enumerate(sample):
linestr = """"
for wordnum, (word, pos) in enumerate(sent.leaves()):
self.charnum[sentnum, wordnum] = len(linestr)
linestr += f""{word}/{pos} ""
self.charnum[sentnum, wordnum + 1] = len(linestr)
self.devsetbox.insert(""end"", linestr[:-1] + ""\n\n"")
if self.chunker is not None:
self._highlight_devset()
self.devsetbox[""state""] = ""disabled""
first = self.devset_index / self._devset_size.get()
last = (self.devset_index + 2) / self._devset_size.get()
self.devset_scroll.set(first, last)
",[],0,[],/app/chunkparser_app.py_show_devset
1083,/home/amandapotts/git/nltk/nltk/app/chunkparser_app.py__chunks,"def _chunks(self, tree):
chunks = set()
wordnum = 0
for child in tree:
if isinstance(child, Tree):
if child.label() == self._chunk_label:
chunks.add((wordnum, wordnum + len(child)))
wordnum += len(child)
else:
wordnum += 1
return chunks
",[],0,[],/app/chunkparser_app.py__chunks
1084,/home/amandapotts/git/nltk/nltk/app/chunkparser_app.py__syntax_highlight_grammar,"def _syntax_highlight_grammar(self, grammar):
if self.top is None:
return
self.grammarbox.tag_remove(""comment"", ""1.0"", ""end"")
self.grammarbox.tag_remove(""angle"", ""1.0"", ""end"")
self.grammarbox.tag_remove(""brace"", ""1.0"", ""end"")
self.grammarbox.tag_add(""hangindent"", ""1.0"", ""end"")
for lineno, line in enumerate(grammar.split(""\n"")):
if not line.strip():
continue
m = re.match(r""(\\.|[^#])*(#.*)?"", line)
comment_start = None
if m.group(2):
comment_start = m.start(2)
s = ""%d.%d"" % (lineno + 1, m.start(2))
e = ""%d.%d"" % (lineno + 1, m.end(2))
self.grammarbox.tag_add(""comment"", s, e)
for m in re.finditer(""[<>{}]"", line):
if comment_start is not None and m.start() >= comment_start:
break
s = ""%d.%d"" % (lineno + 1, m.start())
e = ""%d.%d"" % (lineno + 1, m.end())
if m.group() in ""<>"":
self.grammarbox.tag_add(""angle"", s, e)
else:
self.grammarbox.tag_add(""brace"", s, e)
",[],0,[],/app/chunkparser_app.py__syntax_highlight_grammar
1085,/home/amandapotts/git/nltk/nltk/app/chunkparser_app.py__grammarcheck,"def _grammarcheck(self, grammar):
if self.top is None:
return
self.grammarbox.tag_remove(""error"", ""1.0"", ""end"")
self._grammarcheck_errs = []
for lineno, line in enumerate(grammar.split(""\n"")):
line = re.sub(r""((\\.|[^#])*)(#.*)?"", r""\1"", line)
line = line.strip()
if line:
try:
RegexpChunkRule.fromstring(line)
except ValueError as e:
self.grammarbox.tag_add(
""error"", ""%s.0"" % (lineno + 1), ""%s.0 lineend"" % (lineno + 1)
)
self.status[""text""] = """"
",[],0,[],/app/chunkparser_app.py__grammarcheck
1086,/home/amandapotts/git/nltk/nltk/app/chunkparser_app.py_update,"def update(self, *event):
if event:
self._last_keypress = time.time()
self.grammar = grammar = self.grammarbox.get(""1.0"", ""end"")
normalized_grammar = self.normalize_grammar(grammar)
if normalized_grammar == self.normalized_grammar:
return
else:
self.normalized_grammar = normalized_grammar
if self._history_index < len(self._history) - 1:
self.grammarlabel[""text""] = ""Grammar:""
self._syntax_highlight_grammar(grammar)
try:
if normalized_grammar:
rules = [
RegexpChunkRule.fromstring(line)
for line in normalized_grammar.split(""\n"")
]
else:
rules = []
except ValueError as e:
self._grammarcheck(grammar)
self.chunker = None
return
self.chunker = RegexpChunkParser(rules)
self.grammarbox.tag_remove(""error"", ""1.0"", ""end"")
self.grammar_changed = time.time()
if self._showing_trace:
self.show_trace()
else:
self._highlight_devset()
if not self._eval_demon_running:
self._eval_demon()
",[],0,[],/app/chunkparser_app.py_update
1087,/home/amandapotts/git/nltk/nltk/app/chunkparser_app.py__highlight_devset,"def _highlight_devset(self, sample=None):
if sample is None:
sample = self.devset[self.devset_index : self.devset_index + 1]
self.devsetbox.tag_remove(""true-pos"", ""1.0"", ""end"")
self.devsetbox.tag_remove(""false-neg"", ""1.0"", ""end"")
self.devsetbox.tag_remove(""false-pos"", ""1.0"", ""end"")
for sentnum, gold_tree in enumerate(sample):
test_tree = self._chunkparse(gold_tree.leaves())
gold_chunks = self._chunks(gold_tree)
test_chunks = self._chunks(test_tree)
for chunk in gold_chunks.intersection(test_chunks):
self._color_chunk(sentnum, chunk, ""true-pos"")
for chunk in gold_chunks - test_chunks:
self._color_chunk(sentnum, chunk, ""false-neg"")
for chunk in test_chunks - gold_chunks:
self._color_chunk(sentnum, chunk, ""false-pos"")
",[],0,[],/app/chunkparser_app.py__highlight_devset
1088,/home/amandapotts/git/nltk/nltk/app/chunkparser_app.py__chunkparse,"def _chunkparse(self, words):
try:
return self.chunker.parse(words)
except (ValueError, IndexError) as e:
self.grammarbox.tag_add(""error"", ""1.0"", ""end"")
return words
",[],0,[],/app/chunkparser_app.py__chunkparse
1089,/home/amandapotts/git/nltk/nltk/app/chunkparser_app.py__color_chunk,"def _color_chunk(self, sentnum, chunk, tag):
start, end = chunk
self.devsetbox.tag_add(
tag,
f""{self.linenum[sentnum]}.{self.charnum[sentnum, start]}"",
f""{self.linenum[sentnum]}.{self.charnum[sentnum, end] - 1}"",
)
",[],0,[],/app/chunkparser_app.py__color_chunk
1090,/home/amandapotts/git/nltk/nltk/app/chunkparser_app.py_reset,"def reset(self):
self.chunker = None
self.grammar = None
self.normalized_grammar = None
self.grammar_changed = 0
self._history = []
self._history_index = 0
self.grammarbox.delete(""1.0"", ""end"")
self.show_devset(0)
self.update()
",[],0,[],/app/chunkparser_app.py_reset
1091,/home/amandapotts/git/nltk/nltk/app/chunkparser_app.py_save_grammar,"def save_grammar(self, filename=None):
if not filename:
ftypes = [(""Chunk Gramamr"", "".chunk""), (""All files"", ""*"")]
filename = asksaveasfilename(filetypes=ftypes, defaultextension="".chunk"")
if not filename:
return
if self._history and self.normalized_grammar == self.normalize_grammar(
self._history[-1][0]
):
precision, recall, fscore = (
""%.2f%%"" % (100 * v) for v in self._history[-1][1:]
)
elif self.chunker is None:
precision = recall = fscore = ""Grammar not well formed""
else:
precision = recall = fscore = ""Not finished evaluation yet""
with open(filename, ""w"") as outfile:
outfile.write(
self.SAVE_GRAMMAR_TEMPLATE
% dict(
date=time.ctime(),
devset=self.devset_name,
precision=precision,
recall=recall,
fscore=fscore,
grammar=self.grammar.strip(),
)
)
",[],0,[],/app/chunkparser_app.py_save_grammar
1092,/home/amandapotts/git/nltk/nltk/app/chunkparser_app.py_load_grammar,"def load_grammar(self, filename=None):
if not filename:
ftypes = [(""Chunk Gramamr"", "".chunk""), (""All files"", ""*"")]
filename = askopenfilename(filetypes=ftypes, defaultextension="".chunk"")
if not filename:
return
self.grammarbox.delete(""1.0"", ""end"")
self.update()
with open(filename) as infile:
grammar = infile.read()
grammar = re.sub(
r""^\# Regexp Chunk Parsing Grammar[\s\S]*"" ""F-score:.*\n"", """", grammar
).lstrip()
self.grammarbox.insert(""1.0"", grammar)
self.update()
",[],0,[],/app/chunkparser_app.py_load_grammar
1093,/home/amandapotts/git/nltk/nltk/app/chunkparser_app.py_save_history,"def save_history(self, filename=None):
if not filename:
ftypes = [(""Chunk Gramamr History"", "".txt""), (""All files"", ""*"")]
filename = asksaveasfilename(filetypes=ftypes, defaultextension="".txt"")
if not filename:
return
with open(filename, ""w"") as outfile:
outfile.write(""# Regexp Chunk Parsing Grammar History\n"")
outfile.write(""# Saved %s\n"" % time.ctime())
outfile.write(""# Development set: %s\n"" % self.devset_name)
for i, (g, p, r, f) in enumerate(self._history):
hdr = (
""Grammar %d/%d (precision=%.2f%%, recall=%.2f%%, ""
""fscore=%.2f%%)""
% (i + 1, len(self._history), p * 100, r * 100, f * 100)
)
outfile.write(""\n%s\n"" % hdr)
outfile.write("""".join(""  %s\n"" % line for line in g.strip().split()))
if not (
self._history
and self.normalized_grammar
== self.normalize_grammar(self._history[-1][0])
):
if self.chunker is None:
outfile.write(""\nCurrent Grammar (not well-formed)\n"")
else:
outfile.write(""\nCurrent Grammar (not evaluated)\n"")
outfile.write(
"""".join(""  %s\n"" % line for line in self.grammar.strip().split())
)
",[],0,[],/app/chunkparser_app.py_save_history
1094,/home/amandapotts/git/nltk/nltk/app/chunkparser_app.py_about,"def about(self, *e):
ABOUT = ""NLTK RegExp Chunk Parser Application\n"" + ""Written by Edward Loper""
TITLE = ""About: Regular Expression Chunk Parser Application""
try:
from tkinter.messagebox import Message
Message(message=ABOUT, title=TITLE).show()
except:
ShowText(self.top, TITLE, ABOUT)
",[],0,[],/app/chunkparser_app.py_about
1095,/home/amandapotts/git/nltk/nltk/app/chunkparser_app.py_set_devset_size,"def set_devset_size(self, size=None):
if size is not None:
self._devset_size.set(size)
self._devset_size.set(min(len(self.devset), self._devset_size.get()))
self.show_devset(1)
self.show_devset(0)
",[],0,[],/app/chunkparser_app.py_set_devset_size
1096,/home/amandapotts/git/nltk/nltk/app/chunkparser_app.py_resize,"def resize(self, size=None):
if size is not None:
self._size.set(size)
size = self._size.get()
self._font.configure(size=-(abs(size)))
self._smallfont.configure(size=min(-10, -(abs(size)) * 14 // 20))
",[],0,[],/app/chunkparser_app.py_resize
1097,/home/amandapotts/git/nltk/nltk/app/chunkparser_app.py_mainloop,"def mainloop(self, *args, **kwargs):
""""""
Enter the Tkinter mainloop.  This function must be called if
this demo is created from a non-interactive program (e.g.
from a secript)
the script completes.
""""""
if in_idle():
return
self.top.mainloop(*args, **kwargs)
",[],0,[],/app/chunkparser_app.py_mainloop
1098,/home/amandapotts/git/nltk/nltk/app/chunkparser_app.py_app,"def app():
RegexpChunkApp().mainloop()
",[],0,[],/app/chunkparser_app.py_app
1099,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__init_colortags,"def _init_colortags(self, textwidget, options):
textwidget.tag_config(""terminal"", foreground=""#006000"")
textwidget.tag_config(""arrow"", font=""symbol"", underline=""0"")
textwidget.tag_config(""dot"", foreground=""#000000"")
textwidget.tag_config(
""nonterminal"", foreground=""blue"", font=(""helvetica"", -12, ""bold"")
)
",[],0,[],/app/chartparser_app.py__init_colortags
1100,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__item_repr,"def _item_repr(self, item):
contents = []
contents.append((""%s\t"" % item.lhs(), ""nonterminal""))
contents.append((self.ARROW, ""arrow""))
for i, elt in enumerate(item.rhs()):
if i == item.dot():
contents.append(("" *"", ""dot""))
if isinstance(elt, Nonterminal):
contents.append(("" %s"" % elt.symbol(), ""nonterminal""))
else:
contents.append(("" %r"" % elt, ""terminal""))
if item.is_complete():
contents.append(("" *"", ""dot""))
return contents
",[],0,[],/app/chartparser_app.py__item_repr
1101,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py___init__,"def __init__(
self, parent, chart, toplevel=True, title=""Chart Matrix"", show_numedges=False
",[],0,[],/app/chartparser_app.py___init__
1102,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__init_quit,"def _init_quit(self, root):
quit = Button(root, text=""Quit"", command=self.destroy)
quit.pack(side=""bottom"", expand=0, fill=""none"")
",[],0,[],/app/chartparser_app.py__init_quit
1103,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__init_matrix,"def _init_matrix(self, root):
cframe = Frame(root, border=2, relief=""sunken"")
cframe.pack(expand=0, fill=""none"", padx=1, pady=3, side=""top"")
self._canvas = Canvas(cframe, width=200, height=200, background=""white"")
self._canvas.pack(expand=0, fill=""none"")
",[],0,[],/app/chartparser_app.py__init_matrix
1104,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__init_numedges,"def _init_numedges(self, root):
self._numedges_label = Label(root, text=""0 edges"")
self._numedges_label.pack(expand=0, fill=""none"", side=""top"")
",[],0,[],/app/chartparser_app.py__init_numedges
1105,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__init_list,"def _init_list(self, root):
self._list = EdgeList(root, [], width=20, height=5)
self._list.pack(side=""top"", expand=1, fill=""both"", pady=3)
",[],0,[],/app/chartparser_app.py__init_list
1106,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_cb,"def cb(edge, self=self):
self._fire_callbacks(""select"", edge)
",[],0,[],/app/chartparser_app.py_cb
1107,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_destroy,"def destroy(self, *e):
if self._root is None:
return
try:
self._root.destroy()
except:
pass
self._root = None
",[],0,[],/app/chartparser_app.py_destroy
1108,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_set_chart,"def set_chart(self, chart):
if chart is not self._chart:
self._chart = chart
self._num_edges = 0
self.draw()
",[],0,[],/app/chartparser_app.py_set_chart
1109,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_update,"def update(self):
if self._root is None:
return
N = len(self._cells)
cell_edges = [[0 for i in range(N)] for j in range(N)]
for edge in self._chart:
cell_edges[edge.start()][edge.end()] += 1
for i in range(N):
for j in range(i, N):
if cell_edges[i][j] == 0:
color = ""gray20""
else:
color = ""#00{:02x}{:02x}"".format(
min(255, 50 + 128 * cell_edges[i][j] / 10),
max(0, 128 - 128 * cell_edges[i][j] / 10),
)
cell_tag = self._cells[i][j]
self._canvas.itemconfig(cell_tag, fill=color)
if (i, j) == self._selected_cell:
self._canvas.itemconfig(cell_tag, outline=""#00ffff"", width=3)
self._canvas.tag_raise(cell_tag)
else:
self._canvas.itemconfig(cell_tag, outline=""black"", width=1)
edges = list(self._chart.select(span=self._selected_cell))
self._list.set(edges)
self._num_edges = self._chart.num_edges()
if self._numedges_label is not None:
self._numedges_label[""text""] = ""%d edges"" % self._num_edges
",[],0,[],/app/chartparser_app.py_update
1110,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_activate,"def activate(self):
self._canvas.itemconfig(""inactivebox"", state=""hidden"")
self.update()
",[],0,[],/app/chartparser_app.py_activate
1111,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_inactivate,"def inactivate(self):
self._canvas.itemconfig(""inactivebox"", state=""normal"")
self.update()
",[],0,[],/app/chartparser_app.py_inactivate
1112,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_add_callback,"def add_callback(self, event, func):
self._callbacks.setdefault(event, {})[func] = 1
",[],0,[],/app/chartparser_app.py_add_callback
1113,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_remove_callback,"def remove_callback(self, event, func=None):
if func is None:
del self._callbacks[event]
else:
try:
del self._callbacks[event][func]
except:
pass
",[],0,[],/app/chartparser_app.py_remove_callback
1114,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__fire_callbacks,"def _fire_callbacks(self, event, *args):
if event not in self._callbacks:
return
for cb_func in list(self._callbacks[event].keys()):
cb_func(*args)
",[],0,[],/app/chartparser_app.py__fire_callbacks
1115,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_select_cell,"def select_cell(self, i, j):
if self._root is None:
return
if (i, j) == self._selected_cell and self._chart.num_edges() == self._num_edges:
return
self._selected_cell = (i, j)
self.update()
self._fire_callbacks(""select_cell"", i, j)
",[],0,[],/app/chartparser_app.py_select_cell
1116,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_deselect_cell,"def deselect_cell(self):
if self._root is None:
return
self._selected_cell = None
self._list.set([])
self.update()
",[],0,[],/app/chartparser_app.py_deselect_cell
1117,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__click_cell,"def _click_cell(self, i, j):
if self._selected_cell == (i, j):
self.deselect_cell()
else:
self.select_cell(i, j)
",[],0,[],/app/chartparser_app.py__click_cell
1118,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_view_edge,"def view_edge(self, edge):
self.select_cell(*edge.span())
self._list.view(edge)
",[],0,[],/app/chartparser_app.py_view_edge
1119,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_mark_edge,"def mark_edge(self, edge):
if self._root is None:
return
self.select_cell(*edge.span())
self._list.mark(edge)
",[],0,[],/app/chartparser_app.py_mark_edge
1120,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_unmark_edge,"def unmark_edge(self, edge=None):
if self._root is None:
return
self._list.unmark(edge)
",[],0,[],/app/chartparser_app.py_unmark_edge
1121,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_markonly_edge,"def markonly_edge(self, edge):
if self._root is None:
return
self.select_cell(*edge.span())
self._list.markonly(edge)
",[],0,[],/app/chartparser_app.py_markonly_edge
1122,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_draw,"def draw(self):
if self._root is None:
return
LEFT_MARGIN = BOT_MARGIN = 15
TOP_MARGIN = 5
c = self._canvas
c.delete(""all"")
N = self._chart.num_leaves() + 1
dx = (int(c[""width""]) - LEFT_MARGIN) / N
dy = (int(c[""height""]) - TOP_MARGIN - BOT_MARGIN) / N
c.delete(""all"")
for i in range(N):
c.create_text(
LEFT_MARGIN - 2, i * dy + dy / 2 + TOP_MARGIN, text=repr(i), anchor=""e""
)
c.create_text(
i * dx + dx / 2 + LEFT_MARGIN,
N * dy + TOP_MARGIN + 1,
text=repr(i),
anchor=""n"",
)
c.create_line(
LEFT_MARGIN,
dy * (i + 1) + TOP_MARGIN,
dx * N + LEFT_MARGIN,
dy * (i + 1) + TOP_MARGIN,
dash=""."",
)
c.create_line(
dx * i + LEFT_MARGIN,
TOP_MARGIN,
dx * i + LEFT_MARGIN,
dy * N + TOP_MARGIN,
dash=""."",
)
c.create_rectangle(
LEFT_MARGIN, TOP_MARGIN, LEFT_MARGIN + dx * N, dy * N + TOP_MARGIN, width=2
)
self._cells = [[None for i in range(N)] for j in range(N)]
for i in range(N):
for j in range(i, N):
t = c.create_rectangle(
j * dx + LEFT_MARGIN,
i * dy + TOP_MARGIN,
(j + 1) * dx + LEFT_MARGIN,
(i + 1) * dy + TOP_MARGIN,
fill=""gray20"",
)
self._cells[i][j] = t
",[],0,[],/app/chartparser_app.py_draw
1123,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_cb,"def cb(event, self=self, i=i, j=j):
self._click_cell(i, j)
",[],0,[],/app/chartparser_app.py_cb
1124,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_pack,"def pack(self, *args, **kwargs):
self._root.pack(*args, **kwargs)
",[],0,[],/app/chartparser_app.py_pack
1125,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py___init__,"def __init__(self, parent, chart, grammar, toplevel=True):
self._chart = chart
self._grammar = grammar
self._trees = []
self._y = 10
self._treewidgets = []
self._selection = None
self._selectbox = None
if toplevel:
self._root = Toplevel(parent)
self._root.title(""Chart Parser Application: Results"")
self._root.bind(""<Control-q>"", self.destroy)
else:
self._root = Frame(parent)
if toplevel:
buttons = Frame(self._root)
buttons.pack(side=""bottom"", expand=0, fill=""x"")
Button(buttons, text=""Quit"", command=self.destroy).pack(side=""right"")
Button(buttons, text=""Print All"", command=self.print_all).pack(side=""left"")
Button(buttons, text=""Print Selection"", command=self.print_selection).pack(
side=""left""
)
self._cframe = CanvasFrame(self._root, closeenough=20)
self._cframe.pack(side=""top"", expand=1, fill=""both"")
self.update()
",[],0,[],/app/chartparser_app.py___init__
1126,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_update,"def update(self, edge=None):
if self._root is None:
return
if edge is not None:
if edge.lhs() != self._grammar.start():
return
if edge.span() != (0, self._chart.num_leaves()):
return
for parse in self._chart.parses(self._grammar.start()):
if parse not in self._trees:
self._add(parse)
",[],0,[],/app/chartparser_app.py_update
1127,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__add,"def _add(self, parse):
self._trees.append(parse)
c = self._cframe.canvas()
treewidget = tree_to_treesegment(c, parse)
self._treewidgets.append(treewidget)
self._cframe.add_widget(treewidget, 10, self._y)
treewidget.bind_click(self._click)
self._y = treewidget.bbox()[3] + 10
",[],0,[],/app/chartparser_app.py__add
1128,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__click,"def _click(self, widget):
c = self._cframe.canvas()
if self._selection is not None:
c.delete(self._selectbox)
self._selection = widget
(x1, y1, x2, y2) = widget.bbox()
self._selectbox = c.create_rectangle(x1, y1, x2, y2, width=2, outline=""#088"")
",[],0,[],/app/chartparser_app.py__click
1129,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__color,"def _color(self, treewidget, color):
treewidget.label()[""color""] = color
for child in treewidget.subtrees():
if isinstance(child, TreeSegmentWidget):
self._color(child, color)
else:
child[""color""] = color
",[],0,[],/app/chartparser_app.py__color
1130,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_print_all,"def print_all(self, *e):
if self._root is None:
return
self._cframe.print_to_file()
",[],0,[],/app/chartparser_app.py_print_all
1131,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_print_selection,"def print_selection(self, *e):
if self._root is None:
return
if self._selection is None:
showerror(""Print Error"", ""No tree selected"")
else:
c = self._cframe.canvas()
for widget in self._treewidgets:
if widget is not self._selection:
self._cframe.destroy_widget(widget)
c.delete(self._selectbox)
(x1, y1, x2, y2) = self._selection.bbox()
self._selection.move(10 - x1, 10 - y1)
c[""scrollregion""] = f""0 0 {x2 - x1 + 20} {y2 - y1 + 20}""
self._cframe.print_to_file()
self._treewidgets = [self._selection]
self.clear()
self.update()
",[],0,[],/app/chartparser_app.py_print_selection
1132,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_clear,"def clear(self):
if self._root is None:
return
for treewidget in self._treewidgets:
self._cframe.destroy_widget(treewidget)
self._trees = []
self._treewidgets = []
if self._selection is not None:
self._cframe.canvas().delete(self._selectbox)
self._selection = None
self._y = 10
",[],0,[],/app/chartparser_app.py_clear
1133,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_set_chart,"def set_chart(self, chart):
self.clear()
self._chart = chart
self.update()
",[],0,[],/app/chartparser_app.py_set_chart
1134,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_set_grammar,"def set_grammar(self, grammar):
self.clear()
self._grammar = grammar
self.update()
",[],0,[],/app/chartparser_app.py_set_grammar
1135,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_destroy,"def destroy(self, *e):
if self._root is None:
return
try:
self._root.destroy()
except:
pass
self._root = None
",[],0,[],/app/chartparser_app.py_destroy
1136,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_pack,"def pack(self, *args, **kwargs):
self._root.pack(*args, **kwargs)
",[],0,[],/app/chartparser_app.py_pack
1137,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py___init__,"def __init__(self, *chart_filenames):
faketok = [""""] * 8
self._emptychart = Chart(faketok)
self._left_name = ""None""
self._right_name = ""None""
self._left_chart = self._emptychart
self._right_chart = self._emptychart
self._charts = {""None"": self._emptychart}
self._out_chart = self._emptychart
self._operator = None
self._root = Tk()
self._root.title(""Chart Comparison"")
self._root.bind(""<Control-q>"", self.destroy)
self._root.bind(""<Control-x>"", self.destroy)
self._init_menubar(self._root)
self._init_chartviews(self._root)
self._init_divider(self._root)
self._init_buttons(self._root)
self._init_bindings(self._root)
for filename in chart_filenames:
self.load_chart(filename)
",[],0,[],/app/chartparser_app.py___init__
1138,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_destroy,"def destroy(self, *e):
if self._root is None:
return
try:
self._root.destroy()
except:
pass
self._root = None
",[],0,[],/app/chartparser_app.py_destroy
1139,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_mainloop,"def mainloop(self, *args, **kwargs):
return
self._root.mainloop(*args, **kwargs)
",[],0,[],/app/chartparser_app.py_mainloop
1140,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__init_menubar,"def _init_menubar(self, root):
menubar = Menu(root)
filemenu = Menu(menubar, tearoff=0)
filemenu.add_command(
label=""Load Chart"",
accelerator=""Ctrl-o"",
underline=0,
command=self.load_chart_dialog,
)
filemenu.add_command(
label=""Save Output"",
accelerator=""Ctrl-s"",
underline=0,
command=self.save_chart_dialog,
)
filemenu.add_separator()
filemenu.add_command(
label=""Exit"", underline=1, command=self.destroy, accelerator=""Ctrl-x""
)
menubar.add_cascade(label=""File"", underline=0, menu=filemenu)
opmenu = Menu(menubar, tearoff=0)
opmenu.add_command(
label=""Intersection"", command=self._intersection, accelerator=""+""
)
opmenu.add_command(label=""Union"", command=self._union, accelerator=""*"")
opmenu.add_command(
label=""Difference"", command=self._difference, accelerator=""-""
)
opmenu.add_separator()
opmenu.add_command(label=""Swap Charts"", command=self._swapcharts)
menubar.add_cascade(label=""Compare"", underline=0, menu=opmenu)
self._root.config(menu=menubar)
",[],0,[],/app/chartparser_app.py__init_menubar
1141,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__init_divider,"def _init_divider(self, root):
divider = Frame(root, border=2, relief=""sunken"")
divider.pack(side=""top"", fill=""x"", ipady=2)
",[],0,[],/app/chartparser_app.py__init_divider
1142,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__init_chartviews,"def _init_chartviews(self, root):
opfont = (""symbol"", -36)  # Font for operator.
eqfont = (""helvetica"", -36)  # Font for equals sign.
frame = Frame(root, background=""#c0c0c0"")
frame.pack(side=""top"", expand=1, fill=""both"")
cv1_frame = Frame(frame, border=3, relief=""groove"")
cv1_frame.pack(side=""left"", padx=8, pady=7, expand=1, fill=""both"")
self._left_selector = MutableOptionMenu(
cv1_frame, list(self._charts.keys()), command=self._select_left
)
self._left_selector.pack(side=""top"", pady=5, fill=""x"")
self._left_matrix = ChartMatrixView(
cv1_frame, self._emptychart, toplevel=False, show_numedges=True
)
self._left_matrix.pack(side=""bottom"", padx=5, pady=5, expand=1, fill=""both"")
self._left_matrix.add_callback(""select"", self.select_edge)
self._left_matrix.add_callback(""select_cell"", self.select_cell)
self._left_matrix.inactivate()
self._op_label = Label(
frame, text="" "", width=3, background=""#c0c0c0"", font=opfont
)
self._op_label.pack(side=""left"", padx=5, pady=5)
cv2_frame = Frame(frame, border=3, relief=""groove"")
cv2_frame.pack(side=""left"", padx=8, pady=7, expand=1, fill=""both"")
self._right_selector = MutableOptionMenu(
cv2_frame, list(self._charts.keys()), command=self._select_right
)
self._right_selector.pack(side=""top"", pady=5, fill=""x"")
self._right_matrix = ChartMatrixView(
cv2_frame, self._emptychart, toplevel=False, show_numedges=True
)
self._right_matrix.pack(side=""bottom"", padx=5, pady=5, expand=1, fill=""both"")
self._right_matrix.add_callback(""select"", self.select_edge)
self._right_matrix.add_callback(""select_cell"", self.select_cell)
self._right_matrix.inactivate()
Label(frame, text=""="", width=3, background=""#c0c0c0"", font=eqfont).pack(
side=""left"", padx=5, pady=5
)
out_frame = Frame(frame, border=3, relief=""groove"")
out_frame.pack(side=""left"", padx=8, pady=7, expand=1, fill=""both"")
self._out_label = Label(out_frame, text=""Output"")
self._out_label.pack(side=""top"", pady=9)
self._out_matrix = ChartMatrixView(
out_frame, self._emptychart, toplevel=False, show_numedges=True
)
self._out_matrix.pack(side=""bottom"", padx=5, pady=5, expand=1, fill=""both"")
self._out_matrix.add_callback(""select"", self.select_edge)
self._out_matrix.add_callback(""select_cell"", self.select_cell)
self._out_matrix.inactivate()
",[],0,[],/app/chartparser_app.py__init_chartviews
1143,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__init_buttons,"def _init_buttons(self, root):
buttons = Frame(root)
buttons.pack(side=""bottom"", pady=5, fill=""x"", expand=0)
Button(buttons, text=""Intersection"", command=self._intersection).pack(
side=""left""
)
Button(buttons, text=""Union"", command=self._union).pack(side=""left"")
Button(buttons, text=""Difference"", command=self._difference).pack(side=""left"")
Frame(buttons, width=20).pack(side=""left"")
Button(buttons, text=""Swap Charts"", command=self._swapcharts).pack(side=""left"")
Button(buttons, text=""Detach Output"", command=self._detach_out).pack(
side=""right""
)
",[],0,[],/app/chartparser_app.py__init_buttons
1144,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__init_bindings,"def _init_bindings(self, root):
root.bind(""<Control-o>"", self.load_chart_dialog)
",[],0,[],/app/chartparser_app.py__init_bindings
1145,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__select_left,"def _select_left(self, name):
self._left_name = name
self._left_chart = self._charts[name]
self._left_matrix.set_chart(self._left_chart)
if name == ""None"":
self._left_matrix.inactivate()
self._apply_op()
",[],0,[],/app/chartparser_app.py__select_left
1146,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__select_right,"def _select_right(self, name):
self._right_name = name
self._right_chart = self._charts[name]
self._right_matrix.set_chart(self._right_chart)
if name == ""None"":
self._right_matrix.inactivate()
self._apply_op()
",[],0,[],/app/chartparser_app.py__select_right
1147,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__apply_op,"def _apply_op(self):
if self._operator == ""-"":
self._difference()
elif self._operator == ""or"":
self._union()
elif self._operator == ""and"":
self._intersection()
",[],0,[],/app/chartparser_app.py__apply_op
1148,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_save_chart_dialog,"def save_chart_dialog(self, *args):
filename = asksaveasfilename(
filetypes=self.CHART_FILE_TYPES, defaultextension="".pickle""
)
if not filename:
return
try:
with open(filename, ""wb"") as outfile:
pickle.dump(self._out_chart, outfile)
except Exception as e:
showerror(""Error Saving Chart"", f""Unable to open file: {filename!r}\n{e}"")
",[],0,[],/app/chartparser_app.py_save_chart_dialog
1149,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_load_chart_dialog,"def load_chart_dialog(self, *args):
filename = askopenfilename(
filetypes=self.CHART_FILE_TYPES, defaultextension="".pickle""
)
if not filename:
return
try:
self.load_chart(filename)
except Exception as e:
showerror(""Error Loading Chart"", f""Unable to open file: {filename!r}\n{e}"")
",[],0,[],/app/chartparser_app.py_load_chart_dialog
1150,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_load_chart,"def load_chart(self, filename):
with open(filename, ""rb"") as infile:
chart = pickle.load(infile)
name = os.path.basename(filename)
if name.endswith("".pickle""):
name = name[:-7]
if name.endswith("".chart""):
name = name[:-6]
self._charts[name] = chart
self._left_selector.add(name)
self._right_selector.add(name)
if self._left_chart is self._emptychart:
self._left_selector.set(name)
elif self._right_chart is self._emptychart:
self._right_selector.set(name)
",[],0,[],/app/chartparser_app.py_load_chart
1151,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__update_chartviews,"def _update_chartviews(self):
self._left_matrix.update()
self._right_matrix.update()
self._out_matrix.update()
",[],0,[],/app/chartparser_app.py__update_chartviews
1152,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_select_edge,"def select_edge(self, edge):
if edge in self._left_chart:
self._left_matrix.markonly_edge(edge)
else:
self._left_matrix.unmark_edge()
if edge in self._right_chart:
self._right_matrix.markonly_edge(edge)
else:
self._right_matrix.unmark_edge()
if edge in self._out_chart:
self._out_matrix.markonly_edge(edge)
else:
self._out_matrix.unmark_edge()
",[],0,[],/app/chartparser_app.py_select_edge
1153,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_select_cell,"def select_cell(self, i, j):
self._left_matrix.select_cell(i, j)
self._right_matrix.select_cell(i, j)
self._out_matrix.select_cell(i, j)
",[],0,[],/app/chartparser_app.py_select_cell
1154,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__difference,"def _difference(self):
if not self._checkcompat():
return
out_chart = Chart(self._left_chart.tokens())
for edge in self._left_chart:
if edge not in self._right_chart:
out_chart.insert(edge, [])
self._update(""-"", out_chart)
",[],0,[],/app/chartparser_app.py__difference
1155,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__intersection,"def _intersection(self):
if not self._checkcompat():
return
out_chart = Chart(self._left_chart.tokens())
for edge in self._left_chart:
if edge in self._right_chart:
out_chart.insert(edge, [])
self._update(""and"", out_chart)
",[],0,[],/app/chartparser_app.py__intersection
1156,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__union,"def _union(self):
if not self._checkcompat():
return
out_chart = Chart(self._left_chart.tokens())
for edge in self._left_chart:
out_chart.insert(edge, [])
for edge in self._right_chart:
out_chart.insert(edge, [])
self._update(""or"", out_chart)
",[],0,[],/app/chartparser_app.py__union
1157,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__swapcharts,"def _swapcharts(self):
left, right = self._left_name, self._right_name
self._left_selector.set(right)
self._right_selector.set(left)
",[],0,[],/app/chartparser_app.py__swapcharts
1158,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__checkcompat,"def _checkcompat(self):
if (
self._left_chart.tokens() != self._right_chart.tokens()
or self._left_chart.property_names() != self._right_chart.property_names()
or self._left_chart == self._emptychart
or self._right_chart == self._emptychart
):
self._out_chart = self._emptychart
self._out_matrix.set_chart(self._out_chart)
self._out_matrix.inactivate()
self._out_label[""text""] = ""Output""
return False
else:
return True
",[],0,[],/app/chartparser_app.py__checkcompat
1159,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__update,"def _update(self, operator, out_chart):
self._operator = operator
self._op_label[""text""] = self._OPSYMBOL[operator]
self._out_chart = out_chart
self._out_matrix.set_chart(out_chart)
self._out_label[""text""] = ""{} {} {}"".format(
self._left_name,
self._operator,
self._right_name,
)
",[],0,[],/app/chartparser_app.py__update
1160,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__clear_out_chart,"def _clear_out_chart(self):
self._out_chart = self._emptychart
self._out_matrix.set_chart(self._out_chart)
self._op_label[""text""] = "" ""
self._out_matrix.inactivate()
",[],0,[],/app/chartparser_app.py__clear_out_chart
1161,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__detach_out,"def _detach_out(self):
ChartMatrixView(self._root, self._out_chart, title=self._out_label[""text""])
",[],0,[],/app/chartparser_app.py__detach_out
1162,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py___init__,"def __init__(self, chart, root=None, **kw):
""""""
Construct a new ``Chart`` display.
""""""
draw_tree = kw.get(""draw_tree"", 0)
draw_sentence = kw.get(""draw_sentence"", 1)
self._fontsize = kw.get(""fontsize"", -12)
self._chart = chart
self._callbacks = {}
self._edgelevels = []
self._edgetags = {}
self._marks = {}
self._treetoks = []
self._treetoks_edge = None
self._treetoks_index = 0
self._tree_tags = []
self._compact = 0
if root is None:
top = Tk()
top.title(""Chart View"")
",[],0,[],/app/chartparser_app.py___init__
1163,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_destroy1,"def destroy1(e, top=top):
top.destroy()
",[],0,[],/app/chartparser_app.py_destroy1
1164,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_destroy2,"def destroy2(top=top):
top.destroy()
",[],0,[],/app/chartparser_app.py_destroy2
1165,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__init_fonts,"def _init_fonts(self, root):
self._boldfont = Font(family=""helvetica"", weight=""bold"", size=self._fontsize)
self._font = Font(family=""helvetica"", size=self._fontsize)
self._sysfont = Font(font=Button()[""font""])
root.option_add(""*Font"", self._sysfont)
",[],0,[],/app/chartparser_app.py__init_fonts
1166,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__sb_canvas,"def _sb_canvas(self, root, expand=""y"", fill=""both"", side=""bottom""):
""""""
Helper for __init__: construct a canvas with a scrollbar.
""""""
cframe = Frame(root, relief=""sunk"", border=2)
cframe.pack(fill=fill, expand=expand, side=side)
canvas = Canvas(cframe, background=""#e0e0e0"")
sb = Scrollbar(cframe, orient=""vertical"")
sb.pack(side=""right"", fill=""y"")
canvas.pack(side=""left"", fill=fill, expand=""yes"")
sb[""command""] = canvas.yview
canvas[""yscrollcommand""] = sb.set
return (sb, canvas)
",[],0,[],/app/chartparser_app.py__sb_canvas
1167,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_scroll_up,"def scroll_up(self, *e):
self._chart_canvas.yview(""scroll"", -1, ""units"")
",[],0,[],/app/chartparser_app.py_scroll_up
1168,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_scroll_down,"def scroll_down(self, *e):
self._chart_canvas.yview(""scroll"", 1, ""units"")
",[],0,[],/app/chartparser_app.py_scroll_down
1169,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_page_up,"def page_up(self, *e):
self._chart_canvas.yview(""scroll"", -1, ""pages"")
",[],0,[],/app/chartparser_app.py_page_up
1170,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_page_down,"def page_down(self, *e):
self._chart_canvas.yview(""scroll"", 1, ""pages"")
",[],0,[],/app/chartparser_app.py_page_down
1171,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__grow,"def _grow(self):
""""""
Grow the window, if necessary
""""""
N = self._chart.num_leaves()
width = max(
int(self._chart_canvas[""width""]), N * self._unitsize + ChartView._MARGIN * 2
)
self._chart_canvas.configure(width=width)
self._chart_canvas.configure(height=self._chart_canvas[""height""])
self._unitsize = (width - 2 * ChartView._MARGIN) / N
if self._sentence_canvas is not None:
self._sentence_canvas[""height""] = self._sentence_height
",[],0,[],/app/chartparser_app.py__grow
1172,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_set_font_size,"def set_font_size(self, size):
self._font.configure(size=-abs(size))
self._boldfont.configure(size=-abs(size))
self._sysfont.configure(size=-abs(size))
self._analyze()
self._grow()
self.draw()
",[],0,[],/app/chartparser_app.py_set_font_size
1173,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_get_font_size,"def get_font_size(self):
return abs(self._fontsize)
",[],0,[],/app/chartparser_app.py_get_font_size
1174,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__configure,"def _configure(self, e):
""""""
The configure callback.  This is called whenever the window is
resized.  It is also called when the window is first mapped.
It figures out the unit size, and redraws the contents of each
canvas.
""""""
N = self._chart.num_leaves()
self._unitsize = (e.width - 2 * ChartView._MARGIN) / N
self.draw()
",[],0,[],/app/chartparser_app.py__configure
1175,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_update,"def update(self, chart=None):
""""""
Draw any edges that have not been drawn.  This is typically
called when a after modifies the canvas that a CanvasView is
displaying.  ``update`` will cause any edges that have been
added to the chart to be drawn.
If update is given a ``chart`` argument, then it will replace
the current chart with the given chart.
""""""
if chart is not None:
self._chart = chart
self._edgelevels = []
self._marks = {}
self._analyze()
self._grow()
self.draw()
self.erase_tree()
self._resize()
else:
for edge in self._chart:
if edge not in self._edgetags:
self._add_edge(edge)
self._resize()
",[],0,[],/app/chartparser_app.py_update
1176,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__edge_conflict,"def _edge_conflict(self, edge, lvl):
""""""
Return True if the given edge overlaps with any edge on the given
level.  This is used by _add_edge to figure out what level a
new edge should be added to.
""""""
(s1, e1) = edge.span()
for otheredge in self._edgelevels[lvl]:
(s2, e2) = otheredge.span()
if (s1 <= s2 < e1) or (s2 <= s1 < e2) or (s1 == s2 == e1 == e2):
return True
return False
",[],0,[],/app/chartparser_app.py__edge_conflict
1177,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__analyze_edge,"def _analyze_edge(self, edge):
""""""
Given a new edge, recalculate:
- _text_height
- _unitsize (if the edge text is too big for the current
_unitsize, then increase _unitsize)
""""""
c = self._chart_canvas
if isinstance(edge, TreeEdge):
lhs = edge.lhs()
rhselts = []
for elt in edge.rhs():
if isinstance(elt, Nonterminal):
rhselts.append(str(elt.symbol()))
else:
rhselts.append(repr(elt))
rhs = "" "".join(rhselts)
else:
lhs = edge.lhs()
rhs = """"
for s in (lhs, rhs):
tag = c.create_text(
0, 0, text=s, font=self._boldfont, anchor=""nw"", justify=""left""
)
bbox = c.bbox(tag)
c.delete(tag)
width = bbox[2]  # + ChartView._LEAF_SPACING
edgelen = max(edge.length(), 1)
self._unitsize = max(self._unitsize, width / edgelen)
self._text_height = max(self._text_height, bbox[3] - bbox[1])
",[],0,[],/app/chartparser_app.py__analyze_edge
1178,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__add_edge,"def _add_edge(self, edge, minlvl=0):
""""""
Add a single edge to the ChartView:
- Call analyze_edge to recalculate display parameters
- Find an available level
- Call _draw_edge
""""""
if isinstance(edge, LeafEdge):
return
if edge in self._edgetags:
return
self._analyze_edge(edge)
self._grow()
if not self._compact:
self._edgelevels.append([edge])
lvl = len(self._edgelevels) - 1
self._draw_edge(edge, lvl)
self._resize()
return
lvl = 0
while True:
while lvl >= len(self._edgelevels):
self._edgelevels.append([])
self._resize()
if lvl >= minlvl and not self._edge_conflict(edge, lvl):
self._edgelevels[lvl].append(edge)
break
lvl += 1
self._draw_edge(edge, lvl)
",[],0,[],/app/chartparser_app.py__add_edge
1179,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_view_edge,"def view_edge(self, edge):
level = None
for i in range(len(self._edgelevels)):
if edge in self._edgelevels[i]:
level = i
break
if level is None:
return
y = (level + 1) * self._chart_level_size
dy = self._text_height + 10
self._chart_canvas.yview(""moveto"", 1.0)
if self._chart_height != 0:
self._chart_canvas.yview(""moveto"", (y - dy) / self._chart_height)
",[],0,[],/app/chartparser_app.py_view_edge
1180,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__draw_edge,"def _draw_edge(self, edge, lvl):
""""""
Draw a single edge on the ChartView.
""""""
c = self._chart_canvas
x1 = edge.start() * self._unitsize + ChartView._MARGIN
x2 = edge.end() * self._unitsize + ChartView._MARGIN
if x2 == x1:
x2 += max(4, self._unitsize / 5)
y = (lvl + 1) * self._chart_level_size
linetag = c.create_line(x1, y, x2, y, arrow=""last"", width=3)
if isinstance(edge, TreeEdge):
rhs = []
for elt in edge.rhs():
if isinstance(elt, Nonterminal):
rhs.append(str(elt.symbol()))
else:
rhs.append(repr(elt))
pos = edge.dot()
else:
rhs = []
pos = 0
rhs1 = "" "".join(rhs[:pos])
rhs2 = "" "".join(rhs[pos:])
rhstag1 = c.create_text(x1 + 3, y, text=rhs1, font=self._font, anchor=""nw"")
dotx = c.bbox(rhstag1)[2] + 6
doty = (c.bbox(rhstag1)[1] + c.bbox(rhstag1)[3]) / 2
dottag = c.create_oval(dotx - 2, doty - 2, dotx + 2, doty + 2)
rhstag2 = c.create_text(dotx + 6, y, text=rhs2, font=self._font, anchor=""nw"")
lhstag = c.create_text(
(x1 + x2) / 2, y, text=str(edge.lhs()), anchor=""s"", font=self._boldfont
)
self._edgetags[edge] = (linetag, rhstag1, dottag, rhstag2, lhstag)
",[],0,[],/app/chartparser_app.py__draw_edge
1181,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_cb,"def cb(event, self=self, edge=edge):
self._fire_callbacks(""select"", edge)
",[],0,[],/app/chartparser_app.py_cb
1182,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__color_edge,"def _color_edge(self, edge, linecolor=None, textcolor=None):
""""""
Color in an edge with the given colors.
If no colors are specified, use intelligent defaults
(dependent on selection, etc.)
""""""
if edge not in self._edgetags:
return
c = self._chart_canvas
if linecolor is not None and textcolor is not None:
if edge in self._marks:
linecolor = self._marks[edge]
tags = self._edgetags[edge]
c.itemconfig(tags[0], fill=linecolor)
c.itemconfig(tags[1], fill=textcolor)
c.itemconfig(tags[2], fill=textcolor, outline=textcolor)
c.itemconfig(tags[3], fill=textcolor)
c.itemconfig(tags[4], fill=textcolor)
return
else:
N = self._chart.num_leaves()
if edge in self._marks:
self._color_edge(self._marks[edge])
if edge.is_complete() and edge.span() == (0, N):
self._color_edge(edge, ""#084"", ""#042"")
elif isinstance(edge, LeafEdge):
self._color_edge(edge, ""#48c"", ""#246"")
else:
self._color_edge(edge, ""#00f"", ""#008"")
",[],0,[],/app/chartparser_app.py__color_edge
1183,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_mark_edge,"def mark_edge(self, edge, mark=""#0df""):
""""""
Mark an edge
""""""
self._marks[edge] = mark
self._color_edge(edge)
",[],0,[],/app/chartparser_app.py_mark_edge
1184,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_unmark_edge,"def unmark_edge(self, edge=None):
""""""
Unmark an edge (or all edges)
""""""
if edge is None:
old_marked_edges = list(self._marks.keys())
self._marks = {}
for edge in old_marked_edges:
self._color_edge(edge)
else:
del self._marks[edge]
self._color_edge(edge)
",[],0,[],/app/chartparser_app.py_unmark_edge
1185,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_markonly_edge,"def markonly_edge(self, edge, mark=""#0df""):
self.unmark_edge()
self.mark_edge(edge, mark)
",[],0,[],/app/chartparser_app.py_markonly_edge
1186,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__analyze,"def _analyze(self):
""""""
Analyze the sentence string, to figure out how big a unit needs
to be, How big the tree should be, etc.
""""""
unitsize = 70  # min unitsize
text_height = 0
c = self._chart_canvas
for leaf in self._chart.leaves():
tag = c.create_text(
0, 0, text=repr(leaf), font=self._font, anchor=""nw"", justify=""left""
)
bbox = c.bbox(tag)
c.delete(tag)
width = bbox[2] + ChartView._LEAF_SPACING
unitsize = max(width, unitsize)
text_height = max(text_height, bbox[3] - bbox[1])
self._unitsize = unitsize
self._text_height = text_height
self._sentence_height = self._text_height + 2 * ChartView._MARGIN
for edge in self._chart.edges():
self._analyze_edge(edge)
self._chart_level_size = self._text_height * 2
self._tree_height = 3 * (ChartView._TREE_LEVEL_SIZE + self._text_height)
self._resize()
",[],0,[],/app/chartparser_app.py__analyze
1187,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__resize,"def _resize(self):
""""""
Update the scroll-regions for each canvas.  This ensures that
everything is within a scroll-region, so the user can use the
scrollbars to view the entire display.  This does *not*
resize the window.
""""""
c = self._chart_canvas
width = self._chart.num_leaves() * self._unitsize + ChartView._MARGIN * 2
levels = len(self._edgelevels)
self._chart_height = (levels + 2) * self._chart_level_size
c[""scrollregion""] = (0, 0, width, self._chart_height)
if self._tree_canvas:
self._tree_canvas[""scrollregion""] = (0, 0, width, self._tree_height)
",[],0,[],/app/chartparser_app.py__resize
1188,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__draw_loclines,"def _draw_loclines(self):
""""""
Draw location lines.  These are vertical gridlines used to
show where each location unit is.
""""""
BOTTOM = 50000
c1 = self._tree_canvas
c2 = self._sentence_canvas
c3 = self._chart_canvas
margin = ChartView._MARGIN
self._loclines = []
for i in range(0, self._chart.num_leaves() + 1):
x = i * self._unitsize + margin
if c1:
t1 = c1.create_line(x, 0, x, BOTTOM)
c1.tag_lower(t1)
if c2:
t2 = c2.create_line(x, 0, x, self._sentence_height)
c2.tag_lower(t2)
t3 = c3.create_line(x, 0, x, BOTTOM)
c3.tag_lower(t3)
t4 = c3.create_text(x + 2, 0, text=repr(i), anchor=""nw"", font=self._font)
c3.tag_lower(t4)
if i % 2 == 0:
if c1:
c1.itemconfig(t1, fill=""gray60"")
if c2:
c2.itemconfig(t2, fill=""gray60"")
c3.itemconfig(t3, fill=""gray60"")
else:
if c1:
c1.itemconfig(t1, fill=""gray80"")
if c2:
c2.itemconfig(t2, fill=""gray80"")
c3.itemconfig(t3, fill=""gray80"")
",[],0,[],/app/chartparser_app.py__draw_loclines
1189,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__draw_sentence,"def _draw_sentence(self):
""""""Draw the sentence string.""""""
if self._chart.num_leaves() == 0:
return
c = self._sentence_canvas
margin = ChartView._MARGIN
y = ChartView._MARGIN
for i, leaf in enumerate(self._chart.leaves()):
x1 = i * self._unitsize + margin
x2 = x1 + self._unitsize
x = (x1 + x2) / 2
tag = c.create_text(
x, y, text=repr(leaf), font=self._font, anchor=""n"", justify=""left""
)
bbox = c.bbox(tag)
rt = c.create_rectangle(
x1 + 2,
bbox[1] - (ChartView._LEAF_SPACING / 2),
x2 - 2,
bbox[3] + (ChartView._LEAF_SPACING / 2),
fill=""#f0f0f0"",
outline=""#f0f0f0"",
)
c.tag_lower(rt)
",[],0,[],/app/chartparser_app.py__draw_sentence
1190,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_erase_tree,"def erase_tree(self):
for tag in self._tree_tags:
self._tree_canvas.delete(tag)
self._treetoks = []
self._treetoks_edge = None
self._treetoks_index = 0
",[],0,[],/app/chartparser_app.py_erase_tree
1191,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_draw_tree,"def draw_tree(self, edge=None):
if edge is None and self._treetoks_edge is None:
return
if edge is None:
edge = self._treetoks_edge
if self._treetoks_edge != edge:
self._treetoks = [t for t in self._chart.trees(edge) if isinstance(t, Tree)]
self._treetoks_edge = edge
self._treetoks_index = 0
if len(self._treetoks) == 0:
return
for tag in self._tree_tags:
self._tree_canvas.delete(tag)
tree = self._treetoks[self._treetoks_index]
self._draw_treetok(tree, edge.start())
self._draw_treecycle()
w = self._chart.num_leaves() * self._unitsize + 2 * ChartView._MARGIN
h = tree.height() * (ChartView._TREE_LEVEL_SIZE + self._text_height)
self._tree_canvas[""scrollregion""] = (0, 0, w, h)
",[],0,[],/app/chartparser_app.py_draw_tree
1192,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_cycle_tree,"def cycle_tree(self):
self._treetoks_index = (self._treetoks_index + 1) % len(self._treetoks)
self.draw_tree(self._treetoks_edge)
",[],0,[],/app/chartparser_app.py_cycle_tree
1193,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__draw_treecycle,"def _draw_treecycle(self):
if len(self._treetoks) <= 1:
return
label = ""%d Trees"" % len(self._treetoks)
c = self._tree_canvas
margin = ChartView._MARGIN
right = self._chart.num_leaves() * self._unitsize + margin - 2
tag = c.create_text(right, 2, anchor=""ne"", text=label, font=self._boldfont)
self._tree_tags.append(tag)
_, _, _, y = c.bbox(tag)
for i in range(len(self._treetoks)):
x = right - 20 * (len(self._treetoks) - i - 1)
if i == self._treetoks_index:
fill = ""#084""
else:
fill = ""#fff""
tag = c.create_polygon(
x, y + 10, x - 5, y, x - 10, y + 10, fill=fill, outline=""black""
)
self._tree_tags.append(tag)
",[],0,[],/app/chartparser_app.py__draw_treecycle
1194,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_cb,"def cb(event, self=self, i=i):
self._treetoks_index = i
self.draw_tree()
",[],0,[],/app/chartparser_app.py_cb
1195,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__draw_treetok,"def _draw_treetok(self, treetok, index, depth=0):
""""""
:param index: The index of the first leaf in the tree.
:return: The index of the first leaf after the tree.
""""""
c = self._tree_canvas
margin = ChartView._MARGIN
child_xs = []
for child in treetok:
if isinstance(child, Tree):
child_x, index = self._draw_treetok(child, index, depth + 1)
child_xs.append(child_x)
else:
child_xs.append((2 * index + 1) * self._unitsize / 2 + margin)
index += 1
if child_xs:
nodex = sum(child_xs) / len(child_xs)
else:
nodex = (2 * index + 1) * self._unitsize / 2 + margin
index += 1
nodey = depth * (ChartView._TREE_LEVEL_SIZE + self._text_height)
tag = c.create_text(
nodex,
nodey,
anchor=""n"",
justify=""center"",
text=str(treetok.label()),
fill=""#042"",
font=self._boldfont,
)
self._tree_tags.append(tag)
childy = nodey + ChartView._TREE_LEVEL_SIZE + self._text_height
for childx, child in zip(child_xs, treetok):
if isinstance(child, Tree) and child:
tag = c.create_line(
nodex,
nodey + self._text_height,
childx,
childy,
width=2,
fill=""#084"",
)
self._tree_tags.append(tag)
if isinstance(child, Tree) and not child:
tag = c.create_line(
nodex,
nodey + self._text_height,
childx,
childy,
width=2,
fill=""#048"",
dash=""2 3"",
)
self._tree_tags.append(tag)
if not isinstance(child, Tree):
tag = c.create_line(
nodex,
nodey + self._text_height,
childx,
10000,
width=2,
fill=""#084"",
)
self._tree_tags.append(tag)
return nodex, index
",[],0,[],/app/chartparser_app.py__draw_treetok
1196,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_draw,"def draw(self):
""""""
Draw everything (from scratch).
""""""
if self._tree_canvas:
self._tree_canvas.delete(""all"")
self.draw_tree()
if self._sentence_canvas:
self._sentence_canvas.delete(""all"")
self._draw_sentence()
self._chart_canvas.delete(""all"")
self._edgetags = {}
for lvl in range(len(self._edgelevels)):
for edge in self._edgelevels[lvl]:
self._draw_edge(edge, lvl)
for edge in self._chart:
self._add_edge(edge)
self._draw_loclines()
",[],0,[],/app/chartparser_app.py_draw
1197,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_add_callback,"def add_callback(self, event, func):
self._callbacks.setdefault(event, {})[func] = 1
",[],0,[],/app/chartparser_app.py_add_callback
1198,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_remove_callback,"def remove_callback(self, event, func=None):
if func is None:
del self._callbacks[event]
else:
try:
del self._callbacks[event][func]
except:
pass
",[],0,[],/app/chartparser_app.py_remove_callback
1199,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__fire_callbacks,"def _fire_callbacks(self, event, *args):
if event not in self._callbacks:
return
for cb_func in list(self._callbacks[event].keys()):
cb_func(*args)
",[],0,[],/app/chartparser_app.py__fire_callbacks
1200,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py___init__,"def __init__(self, edge):
super = self.__class__.__bases__[1]
self._edge = edge
self.NUM_EDGES = super.NUM_EDGES - 1
",[],0,[],/app/chartparser_app.py___init__
1201,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_apply,"def apply(self, chart, grammar, *edges):
super = self.__class__.__bases__[1]
edges += (self._edge,)
yield from super.apply(self, chart, grammar, *edges)
",[],0,[],/app/chartparser_app.py_apply
1202,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py___str__,"def __str__(self):
super = self.__class__.__bases__[1]
return super.__str__(self)
",[],0,[],/app/chartparser_app.py___str__
1203,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py___init__,"def __init__(self, grammar, tokens, title=""Chart Parser Application""):
self._init_parser(grammar, tokens)
self._root = None
try:
self._root = Tk()
self._root.title(title)
self._root.bind(""<Control-q>"", self.destroy)
frame3 = Frame(self._root)
frame2 = Frame(self._root)
frame1 = Frame(self._root)
frame3.pack(side=""bottom"", fill=""none"")
frame2.pack(side=""bottom"", fill=""x"")
frame1.pack(side=""bottom"", fill=""both"", expand=1)
self._init_fonts(self._root)
self._init_animation()
self._init_chartview(frame1)
self._init_rulelabel(frame2)
self._init_buttons(frame3)
self._init_menubar()
self._matrix = None
self._results = None
self._init_bindings()
except:
print(""Error creating Tree View"")
self.destroy()
raise
",[],0,[],/app/chartparser_app.py___init__
1204,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_destroy,"def destroy(self, *args):
if self._root is None:
return
self._root.destroy()
self._root = None
",[],0,[],/app/chartparser_app.py_destroy
1205,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_mainloop,"def mainloop(self, *args, **kwargs):
""""""
Enter the Tkinter mainloop.  This function must be called if
this demo is created from a non-interactive program (e.g.
from a secript)
the script completes.
""""""
if in_idle():
return
self._root.mainloop(*args, **kwargs)
",[],0,[],/app/chartparser_app.py_mainloop
1206,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__init_parser,"def _init_parser(self, grammar, tokens):
self._grammar = grammar
self._tokens = tokens
self._reset_parser()
",[],0,[],/app/chartparser_app.py__init_parser
1207,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__reset_parser,"def _reset_parser(self):
self._cp = SteppingChartParser(self._grammar)
self._cp.initialize(self._tokens)
self._chart = self._cp.chart()
for _new_edge in LeafInitRule().apply(self._chart, self._grammar):
pass
self._cpstep = self._cp.step()
self._selection = None
",[],0,[],/app/chartparser_app.py__reset_parser
1208,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__init_fonts,"def _init_fonts(self, root):
self._sysfont = Font(font=Button()[""font""])
root.option_add(""*Font"", self._sysfont)
self._size = IntVar(root)
self._size.set(self._sysfont.cget(""size""))
self._boldfont = Font(family=""helvetica"", weight=""bold"", size=self._size.get())
self._font = Font(family=""helvetica"", size=self._size.get())
",[],0,[],/app/chartparser_app.py__init_fonts
1209,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__init_animation,"def _init_animation(self):
self._step = IntVar(self._root)
self._step.set(1)
self._animate = IntVar(self._root)
self._animate.set(3)  # Default speed = fast
self._animating = 0
",[],0,[],/app/chartparser_app.py__init_animation
1210,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__init_chartview,"def _init_chartview(self, parent):
self._cv = ChartView(self._chart, parent, draw_tree=1, draw_sentence=1)
self._cv.add_callback(""select"", self._click_cv_edge)
",[],0,[],/app/chartparser_app.py__init_chartview
1211,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__init_rulelabel,"def _init_rulelabel(self, parent):
ruletxt = ""Last edge generated by:""
self._rulelabel1 = Label(parent, text=ruletxt, font=self._boldfont)
self._rulelabel2 = Label(
parent, width=40, relief=""groove"", anchor=""w"", font=self._boldfont
)
self._rulelabel1.pack(side=""left"")
self._rulelabel2.pack(side=""left"")
step = Checkbutton(parent, variable=self._step, text=""Step"")
step.pack(side=""right"")
",[],0,[],/app/chartparser_app.py__init_rulelabel
1212,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__init_buttons,"def _init_buttons(self, parent):
frame1 = Frame(parent)
frame2 = Frame(parent)
frame1.pack(side=""bottom"", fill=""x"")
frame2.pack(side=""top"", fill=""none"")
Button(
frame1,
text=""Reset\nParser"",
background=""#90c0d0"",
foreground=""black"",
command=self.reset,
).pack(side=""right"")
Button(
frame1,
text=""Top Down\nStrategy"",
background=""#90c0d0"",
foreground=""black"",
command=self.top_down_strategy,
).pack(side=""left"")
Button(
frame1,
text=""Bottom Up\nStrategy"",
background=""#90c0d0"",
foreground=""black"",
command=self.bottom_up_strategy,
).pack(side=""left"")
Button(
frame1,
text=""Bottom Up\nLeft-Corner Strategy"",
background=""#90c0d0"",
foreground=""black"",
command=self.bottom_up_leftcorner_strategy,
).pack(side=""left"")
Button(
frame2,
text=""Top Down Init\nRule"",
background=""#90f090"",
foreground=""black"",
command=self.top_down_init,
).pack(side=""left"")
Button(
frame2,
text=""Top Down Predict\nRule"",
background=""#90f090"",
foreground=""black"",
command=self.top_down_predict,
).pack(side=""left"")
Frame(frame2, width=20).pack(side=""left"")
Button(
frame2,
text=""Bottom Up Predict\nRule"",
background=""#90f090"",
foreground=""black"",
command=self.bottom_up,
).pack(side=""left"")
Frame(frame2, width=20).pack(side=""left"")
Button(
frame2,
text=""Bottom Up Left-Corner\nPredict Rule"",
background=""#90f090"",
foreground=""black"",
command=self.bottom_up_leftcorner,
).pack(side=""left"")
Frame(frame2, width=20).pack(side=""left"")
Button(
frame2,
text=""Fundamental\nRule"",
background=""#90f090"",
foreground=""black"",
command=self.fundamental,
).pack(side=""left"")
",[],0,[],/app/chartparser_app.py__init_buttons
1213,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__init_menubar,"def _init_menubar(self):
menubar = Menu(self._root)
filemenu = Menu(menubar, tearoff=0)
filemenu.add_command(
label=""Save Chart"",
underline=0,
command=self.save_chart,
accelerator=""Ctrl-s"",
)
filemenu.add_command(
label=""Load Chart"",
underline=0,
command=self.load_chart,
accelerator=""Ctrl-o"",
)
filemenu.add_command(
label=""Reset Chart"", underline=0, command=self.reset, accelerator=""Ctrl-r""
)
filemenu.add_separator()
filemenu.add_command(label=""Save Grammar"", command=self.save_grammar)
filemenu.add_command(label=""Load Grammar"", command=self.load_grammar)
filemenu.add_separator()
filemenu.add_command(
label=""Exit"", underline=1, command=self.destroy, accelerator=""Ctrl-x""
)
menubar.add_cascade(label=""File"", underline=0, menu=filemenu)
editmenu = Menu(menubar, tearoff=0)
editmenu.add_command(
label=""Edit Grammar"",
underline=5,
command=self.edit_grammar,
accelerator=""Ctrl-g"",
)
editmenu.add_command(
label=""Edit Text"",
underline=5,
command=self.edit_sentence,
accelerator=""Ctrl-t"",
)
menubar.add_cascade(label=""Edit"", underline=0, menu=editmenu)
viewmenu = Menu(menubar, tearoff=0)
viewmenu.add_command(
label=""Chart Matrix"", underline=6, command=self.view_matrix
)
viewmenu.add_command(label=""Results"", underline=0, command=self.view_results)
menubar.add_cascade(label=""View"", underline=0, menu=viewmenu)
rulemenu = Menu(menubar, tearoff=0)
rulemenu.add_command(
label=""Top Down Strategy"",
underline=0,
command=self.top_down_strategy,
accelerator=""t"",
)
rulemenu.add_command(
label=""Bottom Up Strategy"",
underline=0,
command=self.bottom_up_strategy,
accelerator=""b"",
)
rulemenu.add_command(
label=""Bottom Up Left-Corner Strategy"",
underline=0,
command=self.bottom_up_leftcorner_strategy,
accelerator=""c"",
)
rulemenu.add_separator()
rulemenu.add_command(label=""Bottom Up Rule"", command=self.bottom_up)
rulemenu.add_command(
label=""Bottom Up Left-Corner Rule"", command=self.bottom_up_leftcorner
)
rulemenu.add_command(label=""Top Down Init Rule"", command=self.top_down_init)
rulemenu.add_command(
label=""Top Down Predict Rule"", command=self.top_down_predict
)
rulemenu.add_command(label=""Fundamental Rule"", command=self.fundamental)
menubar.add_cascade(label=""Apply"", underline=0, menu=rulemenu)
animatemenu = Menu(menubar, tearoff=0)
animatemenu.add_checkbutton(
label=""Step"", underline=0, variable=self._step, accelerator=""s""
)
animatemenu.add_separator()
animatemenu.add_radiobutton(
label=""No Animation"", underline=0, variable=self._animate, value=0
)
animatemenu.add_radiobutton(
label=""Slow Animation"",
underline=0,
variable=self._animate,
value=1,
accelerator=""-"",
)
animatemenu.add_radiobutton(
label=""Normal Animation"",
underline=0,
variable=self._animate,
value=2,
accelerator=""="",
)
animatemenu.add_radiobutton(
label=""Fast Animation"",
underline=0,
variable=self._animate,
value=3,
accelerator=""+"",
)
menubar.add_cascade(label=""Animate"", underline=1, menu=animatemenu)
zoommenu = Menu(menubar, tearoff=0)
zoommenu.add_radiobutton(
label=""Tiny"",
variable=self._size,
underline=0,
value=10,
command=self.resize,
)
zoommenu.add_radiobutton(
label=""Small"",
variable=self._size,
underline=0,
value=12,
command=self.resize,
)
zoommenu.add_radiobutton(
label=""Medium"",
variable=self._size,
underline=0,
value=14,
command=self.resize,
)
zoommenu.add_radiobutton(
label=""Large"",
variable=self._size,
underline=0,
value=18,
command=self.resize,
)
zoommenu.add_radiobutton(
label=""Huge"",
variable=self._size,
underline=0,
value=24,
command=self.resize,
)
menubar.add_cascade(label=""Zoom"", underline=0, menu=zoommenu)
helpmenu = Menu(menubar, tearoff=0)
helpmenu.add_command(label=""About"", underline=0, command=self.about)
helpmenu.add_command(
label=""Instructions"", underline=0, command=self.help, accelerator=""F1""
)
menubar.add_cascade(label=""Help"", underline=0, menu=helpmenu)
self._root.config(menu=menubar)
",[],0,[],/app/chartparser_app.py__init_menubar
1214,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__click_cv_edge,"def _click_cv_edge(self, edge):
if edge != self._selection:
self._select_edge(edge)
else:
self._cv.cycle_tree()
",[],0,[],/app/chartparser_app.py__click_cv_edge
1215,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__select_matrix_edge,"def _select_matrix_edge(self, edge):
self._select_edge(edge)
self._cv.view_edge(edge)
",[],0,[],/app/chartparser_app.py__select_matrix_edge
1216,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__select_edge,"def _select_edge(self, edge):
self._selection = edge
self._cv.markonly_edge(edge, ""#f00"")
self._cv.draw_tree(edge)
if self._matrix:
self._matrix.markonly_edge(edge)
if self._matrix:
self._matrix.view_edge(edge)
",[],0,[],/app/chartparser_app.py__select_edge
1217,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__deselect_edge,"def _deselect_edge(self):
self._selection = None
self._cv.unmark_edge()
self._cv.erase_tree()
if self._matrix:
self._matrix.unmark_edge()
",[],0,[],/app/chartparser_app.py__deselect_edge
1218,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__show_new_edge,"def _show_new_edge(self, edge):
self._display_rule(self._cp.current_chartrule())
self._cv.update()
self._cv.draw_tree(edge)
self._cv.markonly_edge(edge, ""#0df"")
self._cv.view_edge(edge)
if self._matrix:
self._matrix.update()
if self._matrix:
self._matrix.markonly_edge(edge)
if self._matrix:
self._matrix.view_edge(edge)
if self._results:
self._results.update(edge)
",[],0,[],/app/chartparser_app.py__show_new_edge
1219,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_help,"def help(self, *e):
self._animating = 0
try:
ShowText(
self._root,
""Help: Chart Parser Application"",
(__doc__ or """").strip(),
width=75,
font=""fixed"",
)
except:
ShowText(
self._root,
""Help: Chart Parser Application"",
(__doc__ or """").strip(),
width=75,
)
",[],0,[],/app/chartparser_app.py_help
1220,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_about,"def about(self, *e):
ABOUT = ""NLTK Chart Parser Application\n"" + ""Written by Edward Loper""
showinfo(""About: Chart Parser Application"", ABOUT)
",[],0,[],/app/chartparser_app.py_about
1221,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_load_chart,"def load_chart(self, *args):
""Load a chart from a pickle file""
filename = askopenfilename(
filetypes=self.CHART_FILE_TYPES, defaultextension="".pickle""
)
if not filename:
return
try:
with open(filename, ""rb"") as infile:
chart = pickle.load(infile)
self._chart = chart
self._cv.update(chart)
if self._matrix:
self._matrix.set_chart(chart)
if self._matrix:
self._matrix.deselect_cell()
if self._results:
self._results.set_chart(chart)
self._cp.set_chart(chart)
except Exception as e:
raise
showerror(""Error Loading Chart"", ""Unable to open file: %r"" % filename)
",[],0,[],/app/chartparser_app.py_load_chart
1222,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_save_chart,"def save_chart(self, *args):
""Save a chart to a pickle file""
filename = asksaveasfilename(
filetypes=self.CHART_FILE_TYPES, defaultextension="".pickle""
)
if not filename:
return
try:
with open(filename, ""wb"") as outfile:
pickle.dump(self._chart, outfile)
except Exception as e:
raise
showerror(""Error Saving Chart"", ""Unable to open file: %r"" % filename)
",[],0,[],/app/chartparser_app.py_save_chart
1223,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_load_grammar,"def load_grammar(self, *args):
""Load a grammar from a pickle file""
filename = askopenfilename(
filetypes=self.GRAMMAR_FILE_TYPES, defaultextension="".cfg""
)
if not filename:
return
try:
if filename.endswith("".pickle""):
with open(filename, ""rb"") as infile:
grammar = pickle.load(infile)
else:
with open(filename) as infile:
grammar = CFG.fromstring(infile.read())
self.set_grammar(grammar)
except Exception as e:
showerror(""Error Loading Grammar"", ""Unable to open file: %r"" % filename)
",[],0,[],/app/chartparser_app.py_load_grammar
1224,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_save_grammar,"def save_grammar(self, *args):
filename = asksaveasfilename(
filetypes=self.GRAMMAR_FILE_TYPES, defaultextension="".cfg""
)
if not filename:
return
try:
if filename.endswith("".pickle""):
with open(filename, ""wb"") as outfile:
pickle.dump((self._chart, self._tokens), outfile)
else:
with open(filename, ""w"") as outfile:
prods = self._grammar.productions()
start = [p for p in prods if p.lhs() == self._grammar.start()]
rest = [p for p in prods if p.lhs() != self._grammar.start()]
for prod in start:
outfile.write(""%s\n"" % prod)
for prod in rest:
outfile.write(""%s\n"" % prod)
except Exception as e:
showerror(""Error Saving Grammar"", ""Unable to open file: %r"" % filename)
",[],0,[],/app/chartparser_app.py_save_grammar
1225,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_reset,"def reset(self, *args):
self._animating = 0
self._reset_parser()
self._cv.update(self._chart)
if self._matrix:
self._matrix.set_chart(self._chart)
if self._matrix:
self._matrix.deselect_cell()
if self._results:
self._results.set_chart(self._chart)
",[],0,[],/app/chartparser_app.py_reset
1226,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_edit_grammar,"def edit_grammar(self, *e):
CFGEditor(self._root, self._grammar, self.set_grammar)
",[],0,[],/app/chartparser_app.py_edit_grammar
1227,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_set_grammar,"def set_grammar(self, grammar):
self._grammar = grammar
self._cp.set_grammar(grammar)
if self._results:
self._results.set_grammar(grammar)
",[],0,[],/app/chartparser_app.py_set_grammar
1228,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_edit_sentence,"def edit_sentence(self, *e):
sentence = "" "".join(self._tokens)
title = ""Edit Text""
instr = ""Enter a new sentence to parse.""
EntryDialog(self._root, sentence, instr, self.set_sentence, title)
",[],0,[],/app/chartparser_app.py_edit_sentence
1229,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_set_sentence,"def set_sentence(self, sentence):
self._tokens = list(sentence.split())
self.reset()
",[],0,[],/app/chartparser_app.py_set_sentence
1230,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_view_matrix,"def view_matrix(self, *e):
if self._matrix is not None:
self._matrix.destroy()
self._matrix = ChartMatrixView(self._root, self._chart)
self._matrix.add_callback(""select"", self._select_matrix_edge)
",[],0,[],/app/chartparser_app.py_view_matrix
1231,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_view_results,"def view_results(self, *e):
if self._results is not None:
self._results.destroy()
self._results = ChartResultsView(self._root, self._chart, self._grammar)
",[],0,[],/app/chartparser_app.py_view_results
1232,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_resize,"def resize(self):
self._animating = 0
self.set_font_size(self._size.get())
",[],0,[],/app/chartparser_app.py_resize
1233,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_set_font_size,"def set_font_size(self, size):
self._cv.set_font_size(size)
self._font.configure(size=-abs(size))
self._boldfont.configure(size=-abs(size))
self._sysfont.configure(size=-abs(size))
",[],0,[],/app/chartparser_app.py_set_font_size
1234,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_get_font_size,"def get_font_size(self):
return abs(self._size.get())
",[],0,[],/app/chartparser_app.py_get_font_size
1235,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_apply_strategy,"def apply_strategy(self, strategy, edge_strategy=None):
if self._animating:
self._animating = 0
return
self._display_rule(None)
if self._step.get():
selection = self._selection
if (selection is not None) and (edge_strategy is not None):
self._cp.set_strategy([edge_strategy(selection)])
newedge = self._apply_strategy()
if newedge is None:
self._cv.unmark_edge()
self._selection = None
else:
self._cp.set_strategy(strategy)
self._apply_strategy()
else:
self._cp.set_strategy(strategy)
if self._animate.get():
self._animating = 1
self._animate_strategy()
else:
for edge in self._cpstep:
if edge is None:
break
self._cv.update()
if self._matrix:
self._matrix.update()
if self._results:
self._results.update()
",[],0,[],/app/chartparser_app.py_apply_strategy
1236,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__stop_animation,"def _stop_animation(self, *e):
self._animating = 0
",[],0,[],/app/chartparser_app.py__stop_animation
1237,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__animate_strategy,"def _animate_strategy(self, speed=1):
if self._animating == 0:
return
if self._apply_strategy() is not None:
if self._animate.get() == 0 or self._step.get() == 1:
return
if self._animate.get() == 1:
self._root.after(3000, self._animate_strategy)
elif self._animate.get() == 2:
self._root.after(1000, self._animate_strategy)
else:
self._root.after(20, self._animate_strategy)
",[],0,[],/app/chartparser_app.py__animate_strategy
1238,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__apply_strategy,"def _apply_strategy(self):
new_edge = next(self._cpstep)
if new_edge is not None:
self._show_new_edge(new_edge)
return new_edge
",[],0,[],/app/chartparser_app.py__apply_strategy
1239,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py__display_rule,"def _display_rule(self, rule):
if rule is None:
self._rulelabel2[""text""] = """"
else:
name = str(rule)
self._rulelabel2[""text""] = name
size = self._cv.get_font_size()
",[],0,[],/app/chartparser_app.py__display_rule
1240,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_top_down_init,"def top_down_init(self, *e):
self.apply_strategy(self._TD_INIT, None)
",[],0,[],/app/chartparser_app.py_top_down_init
1241,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_top_down_predict,"def top_down_predict(self, *e):
self.apply_strategy(self._TD_PREDICT, TopDownPredictEdgeRule)
",[],0,[],/app/chartparser_app.py_top_down_predict
1242,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_bottom_up,"def bottom_up(self, *e):
self.apply_strategy(self._BU_RULE, BottomUpEdgeRule)
",[],0,[],/app/chartparser_app.py_bottom_up
1243,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_bottom_up_leftcorner,"def bottom_up_leftcorner(self, *e):
self.apply_strategy(self._BU_LC_RULE, BottomUpLeftCornerEdgeRule)
",[],0,[],/app/chartparser_app.py_bottom_up_leftcorner
1244,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_fundamental,"def fundamental(self, *e):
self.apply_strategy(self._FUNDAMENTAL, FundamentalEdgeRule)
",[],0,[],/app/chartparser_app.py_fundamental
1245,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_bottom_up_strategy,"def bottom_up_strategy(self, *e):
self.apply_strategy(self._BU_STRATEGY, BottomUpEdgeRule)
",[],0,[],/app/chartparser_app.py_bottom_up_strategy
1246,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_bottom_up_leftcorner_strategy,"def bottom_up_leftcorner_strategy(self, *e):
self.apply_strategy(self._BU_LC_STRATEGY, BottomUpLeftCornerEdgeRule)
",[],0,[],/app/chartparser_app.py_bottom_up_leftcorner_strategy
1247,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_top_down_strategy,"def top_down_strategy(self, *e):
self.apply_strategy(self._TD_STRATEGY, TopDownPredictEdgeRule)
",[],0,[],/app/chartparser_app.py_top_down_strategy
1248,/home/amandapotts/git/nltk/nltk/app/chartparser_app.py_app,"def app():
grammar = CFG.fromstring(
""""""
S -> NP VP
VP -> VP PP | V NP | V
NP -> Det N | NP PP
PP -> P NP
NP -> 'John' | 'I'
Det -> 'the' | 'my' | 'a'
N -> 'dog' | 'cookie' | 'table' | 'cake' | 'fork'
V -> 'ate' | 'saw'
P -> 'on' | 'under' | 'with'
""""""
)
sent = ""John ate the cake on the table with a fork""
sent = ""John ate the cake on the table""
tokens = list(sent.split())
print(""grammar= ("")
for rule in grammar.productions():
print((""    "", repr(rule) + "",""))
print("")"")
print(""tokens = %r"" % tokens)
print('Calling ""ChartParserApp(grammar, tokens)""...')
ChartParserApp(grammar, tokens).mainloop()
",[],0,[],/app/chartparser_app.py_app
1249,/home/amandapotts/git/nltk/nltk/chunk/__init__.py_ne_chunk,"def ne_chunk(tagged_tokens, binary=False):
""""""
Use NLTK's currently recommended named entity chunker to
chunk the given list of tagged tokens.
""""""
if binary:
chunker_pickle = _BINARY_NE_CHUNKER
else:
chunker_pickle = _MULTICLASS_NE_CHUNKER
chunker = load(chunker_pickle)
return chunker.parse(tagged_tokens)
",[],0,[],/chunk/__init__.py_ne_chunk
1250,/home/amandapotts/git/nltk/nltk/chunk/__init__.py_ne_chunk_sents,"def ne_chunk_sents(tagged_sentences, binary=False):
""""""
Use NLTK's currently recommended named entity chunker to chunk the
given list of tagged sentences, each consisting of a list of tagged tokens.
""""""
if binary:
chunker_pickle = _BINARY_NE_CHUNKER
else:
chunker_pickle = _MULTICLASS_NE_CHUNKER
chunker = load(chunker_pickle)
return chunker.parse_sents(tagged_sentences)
",[],0,[],/chunk/__init__.py_ne_chunk_sents
1251,/home/amandapotts/git/nltk/nltk/chunk/regexp.py___init__,"def __init__(self, chunk_struct, debug_level=1):
""""""
Construct a new ``ChunkString`` that encodes the chunking of
the text ``tagged_tokens``.
:type chunk_struct: Tree
:param chunk_struct: The chunk structure to be further chunked.
:type debug_level: int
:param debug_level: The level of debugging which should be
applied to transformations on the ``ChunkString``.  The
valid levels are:
- 0: no checks
- 1: full check on to_chunkstruct
- 2: full check on to_chunkstruct and cursory check after
each transformation.
- 3: full check on to_chunkstruct and full check after
each transformation.
We recommend you use at least level 1.  You should
probably use level 3 if you use any non-standard
subclasses of ``RegexpChunkRule``.
""""""
self._root_label = chunk_struct.label()
self._pieces = chunk_struct[:]
tags = [self._tag(tok) for tok in self._pieces]
self._str = ""<"" + ""><"".join(tags) + "">""
self._debug = debug_level
",[],0,[],/chunk/regexp.py___init__
1252,/home/amandapotts/git/nltk/nltk/chunk/regexp.py__tag,"def _tag(self, tok):
if isinstance(tok, tuple):
return tok[1]
elif isinstance(tok, Tree):
return tok.label()
else:
raise ValueError(""chunk structures must contain tagged "" ""tokens or trees"")
",[],0,[],/chunk/regexp.py__tag
1253,/home/amandapotts/git/nltk/nltk/chunk/regexp.py__verify,"def _verify(self, s, verify_tags):
""""""
Check to make sure that ``s`` still corresponds to some chunked
version of ``_pieces``.
:type verify_tags: bool
:param verify_tags: Whether the individual tags should be
checked.  If this is false, ``_verify`` will check to make
sure that ``_str`` encodes a chunked version of *some*
list of tokens.  If this is true, then ``_verify`` will
check to make sure that the tags in ``_str`` match those in
``_pieces``.
:raise ValueError: if the internal string representation of
this ``ChunkString`` is invalid or not consistent with _pieces.
""""""
if not ChunkString._VALID.match(s):
raise ValueError(
""Transformation generated invalid "" ""chunkstring:\n  %s"" % s
)
brackets = ChunkString._BRACKETS.sub("""", s)
for i in range(1 + len(brackets) // 5000):
substr = brackets[i * 5000 : i * 5000 + 5000]
if not ChunkString._BALANCED_BRACKETS.match(substr):
raise ValueError(
""Transformation generated invalid "" ""chunkstring:\n  %s"" % s
)
if verify_tags <= 0:
return
tags1 = (re.split(r""[\{\}<>]+"", s))[1:-1]
tags2 = [self._tag(piece) for piece in self._pieces]
if tags1 != tags2:
raise ValueError(
""Transformation generated invalid "" ""chunkstring: tag changed""
)
",[],0,[],/chunk/regexp.py__verify
1254,/home/amandapotts/git/nltk/nltk/chunk/regexp.py_to_chunkstruct,"def to_chunkstruct(self, chunk_label=""CHUNK""):
""""""
Return the chunk structure encoded by this ``ChunkString``.
:rtype: Tree
:raise ValueError: If a transformation has generated an
invalid chunkstring.
""""""
if self._debug > 0:
self._verify(self._str, 1)
pieces = []
index = 0
piece_in_chunk = 0
for piece in re.split(""[{}]"", self._str):
length = piece.count(""<"")
subsequence = self._pieces[index : index + length]
if piece_in_chunk:
pieces.append(Tree(chunk_label, subsequence))
else:
pieces += subsequence
index += length
piece_in_chunk = not piece_in_chunk
return Tree(self._root_label, pieces)
",[],0,[],/chunk/regexp.py_to_chunkstruct
1255,/home/amandapotts/git/nltk/nltk/chunk/regexp.py_xform,"def xform(self, regexp, repl):
""""""
Apply the given transformation to the string encoding of this
``ChunkString``.  In particular, find all occurrences that match
``regexp``, and replace them using ``repl`` (as done by
``re.sub``).
This transformation should only add and remove braces
should *not* modify the sequence of angle-bracket delimited
tags.  Furthermore, this transformation may not result in
improper bracketing.  Note, in particular, that bracketing may
not be nested.
:type regexp: str or regexp
:param regexp: A regular expression matching the substring
that should be replaced.  This will typically include a
named group, which can be used by ``repl``.
:type repl: str
:param repl: An expression specifying what should replace the
matched substring.  Typically, this will include a named
replacement group, specified by ``regexp``.
:rtype: None
:raise ValueError: If this transformation generated an
invalid chunkstring.
""""""
s = re.sub(regexp, repl, self._str)
s = re.sub(r""\{\}"", """", s)
if self._debug > 1:
self._verify(s, self._debug - 2)
self._str = s
",[],0,[],/chunk/regexp.py_xform
1256,/home/amandapotts/git/nltk/nltk/chunk/regexp.py___repr__,"def __repr__(self):
""""""
Return a string representation of this ``ChunkString``.
It has the form::
<ChunkString: '{<DT><JJ><NN>}<VBN><IN>{<DT><NN>}'>
:rtype: str
""""""
return ""<ChunkString: %s>"" % repr(self._str)
",[],0,[],/chunk/regexp.py___repr__
1257,/home/amandapotts/git/nltk/nltk/chunk/regexp.py___str__,"def __str__(self):
""""""
Return a formatted representation of this ``ChunkString``.
This representation will include extra spaces to ensure that
tags will line up with the representation of other
``ChunkStrings`` for the same text, regardless of the chunking.
:rtype: str
""""""
str = re.sub(r"">(?!\})"", r""> "", self._str)
str = re.sub(r""([^\{])<"", r""\1 <"", str)
if str[0] == ""<"":
str = "" "" + str
return str
",[],0,[],/chunk/regexp.py___str__
1258,/home/amandapotts/git/nltk/nltk/chunk/regexp.py___init__,"def __init__(self, regexp, repl, descr):
""""""
Construct a new RegexpChunkRule.
:type regexp: regexp or str
:param regexp: The regular expression for this ``RegexpChunkRule``.
When this rule is applied to a ``ChunkString``, any
substring that matches ``regexp`` will be replaced using
the replacement string ``repl``.  Note that this must be a
normal regular expression, not a tag pattern.
:type repl: str
:param repl: The replacement expression for this ``RegexpChunkRule``.
When this rule is applied to a ``ChunkString``, any substring
that matches ``regexp`` will be replaced using ``repl``.
:type descr: str
:param descr: A short description of the purpose and/or effect
of this rule.
""""""
if isinstance(regexp, str):
regexp = re.compile(regexp)
self._repl = repl
self._descr = descr
self._regexp = regexp
",[],0,[],/chunk/regexp.py___init__
1259,/home/amandapotts/git/nltk/nltk/chunk/regexp.py_apply,"def apply(self, chunkstr):
""""""
Apply this rule to the given ``ChunkString``.  See the
class reference documentation for a description of what it
means to apply a rule.
:type chunkstr: ChunkString
:param chunkstr: The chunkstring to which this rule is applied.
:rtype: None
:raise ValueError: If this transformation generated an
invalid chunkstring.
""""""
chunkstr.xform(self._regexp, self._repl)
",[],0,[],/chunk/regexp.py_apply
1260,/home/amandapotts/git/nltk/nltk/chunk/regexp.py_descr,"def descr(self):
""""""
Return a short description of the purpose and/or effect of
this rule.
:rtype: str
""""""
return self._descr
",[],0,[],/chunk/regexp.py_descr
1261,/home/amandapotts/git/nltk/nltk/chunk/regexp.py___repr__,"def __repr__(self):
""""""
Return a string representation of this rule.  It has the form::
<RegexpChunkRule: '{<IN|VB.*>}'->'<IN>'>
Note that this representation does not include the
description string
separately with the ``descr()`` method.
:rtype: str
""""""
return (
""<RegexpChunkRule: ""
+ repr(self._regexp.pattern)
+ ""->""
+ repr(self._repl)
+ "">""
)
",[],0,[],/chunk/regexp.py___repr__
1262,/home/amandapotts/git/nltk/nltk/chunk/regexp.py_fromstring,"def fromstring(s):
""""""
Create a RegexpChunkRule from a string description.
Currently, the following formats are supported::
{regexp}         # chunk rule
}regexp{         # strip rule
regexp}{regexp   # split rule
regexp{}regexp   # merge rule
Where ``regexp`` is a regular expression for the rule.  Any
text following the comment marker (``#``) will be used as
the rule's description:
>>> from nltk.chunk.regexp import RegexpChunkRule
>>> RegexpChunkRule.fromstring('{<DT>?<NN.*>+}')
<ChunkRule: '<DT>?<NN.*>+'>
""""""
m = re.match(r""(?P<rule>(\\.|[^#])*)(?P<comment>#.*)?"", s)
rule = m.group(""rule"").strip()
comment = (m.group(""comment"") or """")[1:].strip()
try:
if not rule:
raise ValueError(""Empty chunk pattern"")
if rule[0] == ""{"" and rule[-1] == ""}"":
return ChunkRule(rule[1:-1], comment)
elif rule[0] == ""}"" and rule[-1] == ""{"":
return StripRule(rule[1:-1], comment)
elif ""}{"" in rule:
left, right = rule.split(""}{"")
return SplitRule(left, right, comment)
elif ""{}"" in rule:
left, right = rule.split(""{}"")
return MergeRule(left, right, comment)
elif re.match(""[^{}]*{[^{}]*}[^{}]*"", rule):
left, chunk, right = re.split(""[{}]"", rule)
return ChunkRuleWithContext(left, chunk, right, comment)
else:
raise ValueError(""Illegal chunk pattern: %s"" % rule)
except (ValueError, re.error) as e:
raise ValueError(""Illegal chunk pattern: %s"" % rule) from e
",[],0,[],/chunk/regexp.py_fromstring
1263,/home/amandapotts/git/nltk/nltk/chunk/regexp.py___init__,"def __init__(self, tag_pattern, descr):
""""""
Construct a new ``ChunkRule``.
:type tag_pattern: str
:param tag_pattern: This rule's tag pattern.  When
applied to a ``ChunkString``, this rule will
chunk any substring that matches this tag pattern and that
is not already part of a chunk.
:type descr: str
:param descr: A short description of the purpose and/or effect
of this rule.
""""""
self._pattern = tag_pattern
regexp = re.compile(
""(?P<chunk>%s)%s""
% (tag_pattern2re_pattern(tag_pattern), ChunkString.IN_STRIP_PATTERN)
)
RegexpChunkRule.__init__(self, regexp, r""{\g<chunk>}"", descr)
",[],0,[],/chunk/regexp.py___init__
1264,/home/amandapotts/git/nltk/nltk/chunk/regexp.py___repr__,"def __repr__(self):
""""""
Return a string representation of this rule.  It has the form::
<ChunkRule: '<IN|VB.*>'>
Note that this representation does not include the
description string
separately with the ``descr()`` method.
:rtype: str
""""""
return ""<ChunkRule: "" + repr(self._pattern) + "">""
",[],0,[],/chunk/regexp.py___repr__
1265,/home/amandapotts/git/nltk/nltk/chunk/regexp.py___init__,"def __init__(self, tag_pattern, descr):
""""""
Construct a new ``StripRule``.
:type tag_pattern: str
:param tag_pattern: This rule's tag pattern.  When
applied to a ``ChunkString``, this rule will
find any substring that matches this tag pattern and that
is contained in a chunk, and remove it from that chunk,
thus creating two new chunks.
:type descr: str
:param descr: A short description of the purpose and/or effect
of this rule.
""""""
self._pattern = tag_pattern
regexp = re.compile(
""(?P<strip>%s)%s""
% (tag_pattern2re_pattern(tag_pattern), ChunkString.IN_CHUNK_PATTERN)
)
RegexpChunkRule.__init__(self, regexp, r""}\g<strip>{"", descr)
",[],0,[],/chunk/regexp.py___init__
1266,/home/amandapotts/git/nltk/nltk/chunk/regexp.py___repr__,"def __repr__(self):
""""""
Return a string representation of this rule.  It has the form::
<StripRule: '<IN|VB.*>'>
Note that this representation does not include the
description string
separately with the ``descr()`` method.
:rtype: str
""""""
return ""<StripRule: "" + repr(self._pattern) + "">""
",[],0,[],/chunk/regexp.py___repr__
1267,/home/amandapotts/git/nltk/nltk/chunk/regexp.py___init__,"def __init__(self, tag_pattern, descr):
""""""
Construct a new ``UnChunkRule``.
:type tag_pattern: str
:param tag_pattern: This rule's tag pattern.  When
applied to a ``ChunkString``, this rule will
find any complete chunk that matches this tag pattern,
and un-chunk it.
:type descr: str
:param descr: A short description of the purpose and/or effect
of this rule.
""""""
self._pattern = tag_pattern
regexp = re.compile(r""\{(?P<chunk>%s)\}"" % tag_pattern2re_pattern(tag_pattern))
RegexpChunkRule.__init__(self, regexp, r""\g<chunk>"", descr)
",[],0,[],/chunk/regexp.py___init__
1268,/home/amandapotts/git/nltk/nltk/chunk/regexp.py___repr__,"def __repr__(self):
""""""
Return a string representation of this rule.  It has the form::
<UnChunkRule: '<IN|VB.*>'>
Note that this representation does not include the
description string
separately with the ``descr()`` method.
:rtype: str
""""""
return ""<UnChunkRule: "" + repr(self._pattern) + "">""
",[],0,[],/chunk/regexp.py___repr__
1269,/home/amandapotts/git/nltk/nltk/chunk/regexp.py___init__,"def __init__(self, left_tag_pattern, right_tag_pattern, descr):
""""""
Construct a new ``MergeRule``.
:type right_tag_pattern: str
:param right_tag_pattern: This rule's right tag
pattern.  When applied to a ``ChunkString``, this
rule will find any chunk whose end matches
``left_tag_pattern``, and immediately followed by a chunk
whose beginning matches this pattern.  It will
then merge those two chunks into a single chunk.
:type left_tag_pattern: str
:param left_tag_pattern: This rule's left tag
pattern.  When applied to a ``ChunkString``, this
rule will find any chunk whose end matches
this pattern, and immediately followed by a chunk
whose beginning matches ``right_tag_pattern``.  It will
then merge those two chunks into a single chunk.
:type descr: str
:param descr: A short description of the purpose and/or effect
of this rule.
""""""
re.compile(tag_pattern2re_pattern(left_tag_pattern))
re.compile(tag_pattern2re_pattern(right_tag_pattern))
self._left_tag_pattern = left_tag_pattern
self._right_tag_pattern = right_tag_pattern
regexp = re.compile(
""(?P<left>%s)}{(?=%s)""
% (
tag_pattern2re_pattern(left_tag_pattern),
tag_pattern2re_pattern(right_tag_pattern),
)
)
RegexpChunkRule.__init__(self, regexp, r""\g<left>"", descr)
",[],0,[],/chunk/regexp.py___init__
1270,/home/amandapotts/git/nltk/nltk/chunk/regexp.py___repr__,"def __repr__(self):
""""""
Return a string representation of this rule.  It has the form::
<MergeRule: '<NN|DT|JJ>', '<NN|JJ>'>
Note that this representation does not include the
description string
separately with the ``descr()`` method.
:rtype: str
""""""
return (
""<MergeRule: ""
+ repr(self._left_tag_pattern)
+ "", ""
+ repr(self._right_tag_pattern)
+ "">""
)
",[],0,[],/chunk/regexp.py___repr__
1271,/home/amandapotts/git/nltk/nltk/chunk/regexp.py___init__,"def __init__(self, left_tag_pattern, right_tag_pattern, descr):
""""""
Construct a new ``SplitRule``.
:type right_tag_pattern: str
:param right_tag_pattern: This rule's right tag
pattern.  When applied to a ``ChunkString``, this rule will
find any chunk containing a substring that matches
``left_tag_pattern`` followed by this pattern.  It will
then split the chunk into two new chunks at the point
between these two matching patterns.
:type left_tag_pattern: str
:param left_tag_pattern: This rule's left tag
pattern.  When applied to a ``ChunkString``, this rule will
find any chunk containing a substring that matches this
pattern followed by ``right_tag_pattern``.  It will then
split the chunk into two new chunks at the point between
these two matching patterns.
:type descr: str
:param descr: A short description of the purpose and/or effect
of this rule.
""""""
re.compile(tag_pattern2re_pattern(left_tag_pattern))
re.compile(tag_pattern2re_pattern(right_tag_pattern))
self._left_tag_pattern = left_tag_pattern
self._right_tag_pattern = right_tag_pattern
regexp = re.compile(
""(?P<left>%s)(?=%s)""
% (
tag_pattern2re_pattern(left_tag_pattern),
tag_pattern2re_pattern(right_tag_pattern),
)
)
RegexpChunkRule.__init__(self, regexp, r""\g<left>}{"", descr)
",[],0,[],/chunk/regexp.py___init__
1272,/home/amandapotts/git/nltk/nltk/chunk/regexp.py___repr__,"def __repr__(self):
""""""
Return a string representation of this rule.  It has the form::
<SplitRule: '<NN>', '<DT>'>
Note that this representation does not include the
description string
separately with the ``descr()`` method.
:rtype: str
""""""
return (
""<SplitRule: ""
+ repr(self._left_tag_pattern)
+ "", ""
+ repr(self._right_tag_pattern)
+ "">""
)
",[],0,[],/chunk/regexp.py___repr__
1273,/home/amandapotts/git/nltk/nltk/chunk/regexp.py___init__,"def __init__(self, left_tag_pattern, right_tag_pattern, descr):
""""""
Construct a new ``ExpandRightRule``.
:type right_tag_pattern: str
:param right_tag_pattern: This rule's right tag
pattern.  When applied to a ``ChunkString``, this
rule will find any chunk whose beginning matches
``right_tag_pattern``, and immediately preceded by a strip
whose end matches this pattern.  It will
then merge those two chunks into a single chunk.
:type left_tag_pattern: str
:param left_tag_pattern: This rule's left tag
pattern.  When applied to a ``ChunkString``, this
rule will find any chunk whose beginning matches
this pattern, and immediately preceded by a strip
whose end matches ``left_tag_pattern``.  It will
then expand the chunk to incorporate the new material on the left.
:type descr: str
:param descr: A short description of the purpose and/or effect
of this rule.
""""""
re.compile(tag_pattern2re_pattern(left_tag_pattern))
re.compile(tag_pattern2re_pattern(right_tag_pattern))
self._left_tag_pattern = left_tag_pattern
self._right_tag_pattern = right_tag_pattern
regexp = re.compile(
r""(?P<left>%s)\{(?P<right>%s)""
% (
tag_pattern2re_pattern(left_tag_pattern),
tag_pattern2re_pattern(right_tag_pattern),
)
)
RegexpChunkRule.__init__(self, regexp, r""{\g<left>\g<right>"", descr)
",[],0,[],/chunk/regexp.py___init__
1274,/home/amandapotts/git/nltk/nltk/chunk/regexp.py___repr__,"def __repr__(self):
""""""
Return a string representation of this rule.  It has the form::
<ExpandLeftRule: '<NN|DT|JJ>', '<NN|JJ>'>
Note that this representation does not include the
description string
separately with the ``descr()`` method.
:rtype: str
""""""
return (
""<ExpandLeftRule: ""
+ repr(self._left_tag_pattern)
+ "", ""
+ repr(self._right_tag_pattern)
+ "">""
)
",[],0,[],/chunk/regexp.py___repr__
1275,/home/amandapotts/git/nltk/nltk/chunk/regexp.py___init__,"def __init__(self, left_tag_pattern, right_tag_pattern, descr):
""""""
Construct a new ``ExpandRightRule``.
:type right_tag_pattern: str
:param right_tag_pattern: This rule's right tag
pattern.  When applied to a ``ChunkString``, this
rule will find any chunk whose end matches
``left_tag_pattern``, and immediately followed by a strip
whose beginning matches this pattern.  It will
then merge those two chunks into a single chunk.
:type left_tag_pattern: str
:param left_tag_pattern: This rule's left tag
pattern.  When applied to a ``ChunkString``, this
rule will find any chunk whose end matches
this pattern, and immediately followed by a strip
whose beginning matches ``right_tag_pattern``.  It will
then expand the chunk to incorporate the new material on the right.
:type descr: str
:param descr: A short description of the purpose and/or effect
of this rule.
""""""
re.compile(tag_pattern2re_pattern(left_tag_pattern))
re.compile(tag_pattern2re_pattern(right_tag_pattern))
self._left_tag_pattern = left_tag_pattern
self._right_tag_pattern = right_tag_pattern
regexp = re.compile(
r""(?P<left>%s)\}(?P<right>%s)""
% (
tag_pattern2re_pattern(left_tag_pattern),
tag_pattern2re_pattern(right_tag_pattern),
)
)
RegexpChunkRule.__init__(self, regexp, r""\g<left>\g<right>}"", descr)
",[],0,[],/chunk/regexp.py___init__
1276,/home/amandapotts/git/nltk/nltk/chunk/regexp.py___repr__,"def __repr__(self):
""""""
Return a string representation of this rule.  It has the form::
<ExpandRightRule: '<NN|DT|JJ>', '<NN|JJ>'>
Note that this representation does not include the
description string
separately with the ``descr()`` method.
:rtype: str
""""""
return (
""<ExpandRightRule: ""
+ repr(self._left_tag_pattern)
+ "", ""
+ repr(self._right_tag_pattern)
+ "">""
)
",[],0,[],/chunk/regexp.py___repr__
1277,/home/amandapotts/git/nltk/nltk/chunk/regexp.py___init__,"def __init__(
self,
left_context_tag_pattern,
chunk_tag_pattern,
right_context_tag_pattern,
descr,
",[],0,[],/chunk/regexp.py___init__
1278,/home/amandapotts/git/nltk/nltk/chunk/regexp.py___repr__,"def __repr__(self):
""""""
Return a string representation of this rule.  It has the form::
<ChunkRuleWithContext: '<IN>', '<NN>', '<DT>'>
Note that this representation does not include the
description string
separately with the ``descr()`` method.
:rtype: str
""""""
return ""<ChunkRuleWithContext:  {!r}, {!r}, {!r}>"".format(
self._left_context_tag_pattern,
self._chunk_tag_pattern,
self._right_context_tag_pattern,
)
",[],0,[],/chunk/regexp.py___repr__
1279,/home/amandapotts/git/nltk/nltk/chunk/regexp.py_tag_pattern2re_pattern,"def tag_pattern2re_pattern(tag_pattern):
""""""
Convert a tag pattern to a regular expression pattern.  A ""tag
pattern"" is a modified version of a regular expression, designed
for matching sequences of tags.  The differences between regular
expression patterns and tag patterns are:
- In tag patterns, ``'<'`` and ``'>'`` act as parentheses
``'<NN>+'`` matches one or more repetitions of ``'<NN>'``, not
``'<NN'`` followed by one or more repetitions of ``'>'``.
- Whitespace in tag patterns is ignored.  So
``'<DT> | <NN>'`` is equivalent to ``'<DT>|<NN>'``
- In tag patterns, ``'.'`` is equivalent to ``'[^{}<>]'``
``'<NN.*>'`` matches any single tag starting with ``'NN'``.
In particular, ``tag_pattern2re_pattern`` performs the following
transformations on the given pattern:
- Replace '.' with '[^<>{}]'
- Remove any whitespace
- Add extra parens around '<' and '>', to make '<' and '>' act
like parentheses.  E.g., so that in '<NN>+', the '+' has scope
over the entire '<NN>'
scope over 'NN' and 'IN', but not '<' or '>'.
- Check to make sure the resulting pattern is valid.
:type tag_pattern: str
:param tag_pattern: The tag pattern to convert to a regular
expression pattern.
:raise ValueError: If ``tag_pattern`` is not a valid tag pattern.
In particular, ``tag_pattern`` should not include braces
should not contain nested or mismatched angle-brackets.
:rtype: str
:return: A regular expression pattern corresponding to
``tag_pattern``.
""""""
tag_pattern = re.sub(r""\s"", """", tag_pattern)
tag_pattern = re.sub(r""<"", ""(<("", tag_pattern)
tag_pattern = re.sub(r"">"", "")>)"", tag_pattern)
if not CHUNK_TAG_PATTERN.match(tag_pattern):
raise ValueError(""Bad tag pattern: %r"" % tag_pattern)
",[],0,[],/chunk/regexp.py_tag_pattern2re_pattern
1280,/home/amandapotts/git/nltk/nltk/chunk/regexp.py_reverse_str,"def reverse_str(str):
lst = list(str)
lst.reverse()
return """".join(lst)
",[],0,[],/chunk/regexp.py_reverse_str
1281,/home/amandapotts/git/nltk/nltk/chunk/regexp.py___init__,"def __init__(self, rules, chunk_label=""NP"", root_label=""S"", trace=0):
""""""
Construct a new ``RegexpChunkParser``.
:type rules: list(RegexpChunkRule)
:param rules: The sequence of rules that should be used to
generate the chunking for a tagged text.
:type chunk_label: str
:param chunk_label: The node value that should be used for
chunk subtrees.  This is typically a short string
describing the type of information contained by the chunk,
such as ``""NP""`` for base noun phrases.
:type root_label: str
:param root_label: The node value that should be used for the
top node of the chunk structure.
:type trace: int
:param trace: The level of tracing that should be used when
parsing a text.  ``0`` will generate no tracing output
``1`` will generate normal tracing output
higher will generate verbose tracing output.
""""""
self._rules = rules
self._trace = trace
self._chunk_label = chunk_label
self._root_label = root_label
",[],0,[],/chunk/regexp.py___init__
1282,/home/amandapotts/git/nltk/nltk/chunk/regexp.py__trace_apply,"def _trace_apply(self, chunkstr, verbose):
""""""
Apply each rule of this ``RegexpChunkParser`` to ``chunkstr``, in
turn.  Generate trace output between each rule.  If ``verbose``
is true, then generate verbose output.
:type chunkstr: ChunkString
:param chunkstr: The chunk string to which each rule should be
applied.
:type verbose: bool
:param verbose: Whether output should be verbose.
:rtype: None
""""""
print(""# Input:"")
print(chunkstr)
for rule in self._rules:
rule.apply(chunkstr)
if verbose:
print(""#"", rule.descr() + "" ("" + repr(rule) + ""):"")
else:
print(""#"", rule.descr() + "":"")
print(chunkstr)
",[],0,[],/chunk/regexp.py__trace_apply
1283,/home/amandapotts/git/nltk/nltk/chunk/regexp.py__notrace_apply,"def _notrace_apply(self, chunkstr):
""""""
Apply each rule of this ``RegexpChunkParser`` to ``chunkstr``, in
turn.
:param chunkstr: The chunk string to which each rule should be
applied.
:type chunkstr: ChunkString
:rtype: None
""""""
for rule in self._rules:
rule.apply(chunkstr)
",[],0,[],/chunk/regexp.py__notrace_apply
1284,/home/amandapotts/git/nltk/nltk/chunk/regexp.py_parse,"def parse(self, chunk_struct, trace=None):
""""""
:type chunk_struct: Tree
:param chunk_struct: the chunk structure to be (further) chunked
:type trace: int
:param trace: The level of tracing that should be used when
parsing a text.  ``0`` will generate no tracing output
``1`` will generate normal tracing output
higher will generate verbose tracing output.  This value
overrides the trace level value that was given to the
constructor.
:rtype: Tree
:return: a chunk structure that encodes the chunks in a given
tagged sentence.  A chunk is a non-overlapping linguistic
group, such as a noun phrase.  The set of chunks
identified in the chunk structure depends on the rules
used to define this ``RegexpChunkParser``.
""""""
if len(chunk_struct) == 0:
print(""Warning: parsing empty text"")
return Tree(self._root_label, [])
try:
chunk_struct.label()
except AttributeError:
chunk_struct = Tree(self._root_label, chunk_struct)
if trace is None:
trace = self._trace
chunkstr = ChunkString(chunk_struct)
if trace:
verbose = trace > 1
self._trace_apply(chunkstr, verbose)
else:
self._notrace_apply(chunkstr)
return chunkstr.to_chunkstruct(self._chunk_label)
",[],0,[],/chunk/regexp.py_parse
1285,/home/amandapotts/git/nltk/nltk/chunk/regexp.py_rules,"def rules(self):
""""""
:return: the sequence of rules used by ``RegexpChunkParser``.
:rtype: list(RegexpChunkRule)
""""""
return self._rules
",[],0,[],/chunk/regexp.py_rules
1286,/home/amandapotts/git/nltk/nltk/chunk/regexp.py___repr__,"def __repr__(self):
""""""
:return: a concise string representation of this
``RegexpChunkParser``.
:rtype: str
""""""
return ""<RegexpChunkParser with %d rules>"" % len(self._rules)
",[],0,[],/chunk/regexp.py___repr__
1287,/home/amandapotts/git/nltk/nltk/chunk/regexp.py___str__,"def __str__(self):
""""""
:return: a verbose string representation of this ``RegexpChunkParser``.
:rtype: str
""""""
s = ""RegexpChunkParser with %d rules:\n"" % len(self._rules)
margin = 0
for rule in self._rules:
margin = max(margin, len(rule.descr()))
if margin < 35:
format = ""    %"" + repr(-(margin + 3)) + ""s%s\n""
else:
format = ""    %s\n      %s\n""
for rule in self._rules:
s += format % (rule.descr(), repr(rule))
return s[:-1]
",[],0,[],/chunk/regexp.py___str__
1288,/home/amandapotts/git/nltk/nltk/chunk/regexp.py___init__,"def __init__(self, grammar, root_label=""S"", loop=1, trace=0):
""""""
Create a new chunk parser, from the given start state
and set of chunk patterns.
:param grammar: The grammar, or a list of RegexpChunkParser objects
:type grammar: str or list(RegexpChunkParser)
:param root_label: The top node of the tree being created
:type root_label: str or Nonterminal
:param loop: The number of times to run through the patterns
:type loop: int
:type trace: int
:param trace: The level of tracing that should be used when
parsing a text.  ``0`` will generate no tracing output
``1`` will generate normal tracing output
higher will generate verbose tracing output.
""""""
self._trace = trace
self._stages = []
self._grammar = grammar
self._loop = loop
if isinstance(grammar, str):
self._read_grammar(grammar, root_label, trace)
else:
type_err = (
""Expected string or list of RegexpChunkParsers "" ""for the grammar.""
)
try:
grammar = list(grammar)
except BaseException as e:
raise TypeError(type_err) from e
for elt in grammar:
if not isinstance(elt, RegexpChunkParser):
raise TypeError(type_err)
self._stages = grammar
",[],0,[],/chunk/regexp.py___init__
1289,/home/amandapotts/git/nltk/nltk/chunk/regexp.py__read_grammar,"def _read_grammar(self, grammar, root_label, trace):
""""""
Helper function for __init__: read the grammar if it is a
string.
""""""
rules = []
lhs = None
pattern = regex.compile(""(?P<nonterminal>(\\.|[^:])*)(:(?P<rule>.*))"")
for line in grammar.split(""\n""):
line = line.strip()
m = pattern.match(line)
if m:
self._add_stage(rules, lhs, root_label, trace)
lhs = m.group(""nonterminal"").strip()
rules = []
line = m.group(""rule"").strip()
if line == """" or line.startswith(""#""):
continue
rules.append(RegexpChunkRule.fromstring(line))
self._add_stage(rules, lhs, root_label, trace)
",[],0,[],/chunk/regexp.py__read_grammar
1290,/home/amandapotts/git/nltk/nltk/chunk/regexp.py__add_stage,"def _add_stage(self, rules, lhs, root_label, trace):
""""""
Helper function for __init__: add a new stage to the parser.
""""""
if rules != []:
if not lhs:
raise ValueError(""Expected stage marker (eg NP:)"")
parser = RegexpChunkParser(
rules, chunk_label=lhs, root_label=root_label, trace=trace
)
self._stages.append(parser)
",[],0,[],/chunk/regexp.py__add_stage
1291,/home/amandapotts/git/nltk/nltk/chunk/regexp.py_parse,"def parse(self, chunk_struct, trace=None):
""""""
Apply the chunk parser to this input.
:type chunk_struct: Tree
:param chunk_struct: the chunk structure to be (further) chunked
(this tree is modified, and is also returned)
:type trace: int
:param trace: The level of tracing that should be used when
parsing a text.  ``0`` will generate no tracing output
``1`` will generate normal tracing output
higher will generate verbose tracing output.  This value
overrides the trace level value that was given to the
constructor.
:return: the chunked output.
:rtype: Tree
""""""
if trace is None:
trace = self._trace
for i in range(self._loop):
for parser in self._stages:
chunk_struct = parser.parse(chunk_struct, trace=trace)
return chunk_struct
",[],0,[],/chunk/regexp.py_parse
1292,/home/amandapotts/git/nltk/nltk/chunk/regexp.py___repr__,"def __repr__(self):
""""""
:return: a concise string representation of this ``chunk.RegexpParser``.
:rtype: str
""""""
return ""<chunk.RegexpParser with %d stages>"" % len(self._stages)
",[],0,[],/chunk/regexp.py___repr__
1293,/home/amandapotts/git/nltk/nltk/chunk/regexp.py___str__,"def __str__(self):
""""""
:return: a verbose string representation of this
``RegexpParser``.
:rtype: str
""""""
s = ""chunk.RegexpParser with %d stages:\n"" % len(self._stages)
margin = 0
for parser in self._stages:
s += ""%s\n"" % parser
return s[:-1]
",[],0,[],/chunk/regexp.py___str__
1294,/home/amandapotts/git/nltk/nltk/chunk/regexp.py_demo_eval,"def demo_eval(chunkparser, text):
""""""
Demonstration code for evaluating a chunk parser, using a
``ChunkScore``.  This function assumes that ``text`` contains one
sentence per line, and that each sentence has the form expected by
``tree.chunk``.  It runs the given chunk parser on each sentence in
the text, and scores the result.  It prints the final score
(precision, recall, and f-measure)
that were missed and the set of chunks that were incorrect.  (At
most 10 missing chunks and 10 incorrect chunks are reported).
:param chunkparser: The chunkparser to be tested
:type chunkparser: ChunkParserI
:param text: The chunked tagged text that should be used for
evaluation.
:type text: str
""""""
from nltk import chunk
from nltk.tree import Tree
chunkscore = chunk.ChunkScore()
for sentence in text.split(""\n""):
print(sentence)
sentence = sentence.strip()
if not sentence:
continue
gold = chunk.tagstr2tree(sentence)
tokens = gold.leaves()
test = chunkparser.parse(Tree(""S"", tokens), trace=1)
chunkscore.score(gold, test)
print()
print(""/"" + (""="" * 75) + ""\\"")
print(""Scoring"", chunkparser)
print(""-"" * 77)
print(""Precision: %5.1f%%"" % (chunkscore.precision() * 100), "" "" * 4, end="" "")
print(""Recall: %5.1f%%"" % (chunkscore.recall() * 100), "" "" * 6, end="" "")
print(""F-Measure: %5.1f%%"" % (chunkscore.f_measure() * 100))
if chunkscore.missed():
print(""Missed:"")
missed = chunkscore.missed()
for chunk in missed[:10]:
print(""  "", "" "".join(map(str, chunk)))
if len(chunkscore.missed()) > 10:
print(""  ..."")
if chunkscore.incorrect():
print(""Incorrect:"")
incorrect = chunkscore.incorrect()
for chunk in incorrect[:10]:
print(""  "", "" "".join(map(str, chunk)))
if len(chunkscore.incorrect()) > 10:
print(""  ..."")
print(""\\"" + (""="" * 75) + ""/"")
print()
",[],0,[],/chunk/regexp.py_demo_eval
1295,/home/amandapotts/git/nltk/nltk/chunk/regexp.py_demo,"def demo():
""""""
A demonstration for the ``RegexpChunkParser`` class.  A single text is
parsed with four different chunk parsers, using a variety of rules
and strategies.
""""""
from nltk import Tree, chunk
text = """"""\
[ the/DT little/JJ cat/NN ] sat/VBD on/IN [ the/DT mat/NN ] ./.
[ John/NNP ] saw/VBD [the/DT cats/NNS] [the/DT dog/NN] chased/VBD ./.
[ John/NNP ] thinks/VBZ [ Mary/NN ] saw/VBD [ the/DT cat/NN ] sit/VB on/IN [ the/DT mat/NN ]./.
""""""
print(""*"" * 75)
print(""Evaluation text:"")
print(text)
print(""*"" * 75)
print()
grammar = r""""""
NP:                   # NP stage
{<DT>?<JJ>*<NN>}    # chunk determiners, adjectives and nouns
{<NNP>+}            # chunk proper nouns
""""""
cp = chunk.RegexpParser(grammar)
demo_eval(cp, text)
grammar = r""""""
NP:
{<.*>}              # start by chunking each tag
}<[\.VI].*>+{       # unchunk any verbs, prepositions or periods
<DT|JJ>{}<NN.*>     # merge det/adj with nouns
""""""
cp = chunk.RegexpParser(grammar)
demo_eval(cp, text)
grammar = r""""""
NP: {<DT>?<JJ>*<NN>}    # chunk determiners, adjectives and nouns
VP: {<TO>?<VB.*>}       # VP = verb words
""""""
cp = chunk.RegexpParser(grammar)
demo_eval(cp, text)
grammar = r""""""
NP: {<.*>*}             # start by chunking everything
}<[\.VI].*>+{       # strip any verbs, prepositions or periods
<.*>}{<DT>          # separate on determiners
PP: {<IN><NP>}          # PP = preposition + noun phrase
VP: {<VB.*><NP|PP>*}    # VP = verb words + NPs and PPs
""""""
cp = chunk.RegexpParser(grammar)
demo_eval(cp, text)
from nltk.corpus import conll2000
print()
print(""Demonstration of empty grammar:"")
cp = chunk.RegexpParser("""")
print(chunk.accuracy(cp, conll2000.chunked_sents(""test.txt"", chunk_types=(""NP"",))))
print()
print(""Demonstration of accuracy evaluation using CoNLL tags:"")
grammar = r""""""
NP:
{<.*>}              # start by chunking each tag
}<[\.VI].*>+{       # unchunk any verbs, prepositions or periods
<DT|JJ>{}<NN.*>     # merge det/adj with nouns
""""""
cp = chunk.RegexpParser(grammar)
print(chunk.accuracy(cp, conll2000.chunked_sents(""test.txt"")[:5]))
print()
print(""Demonstration of tagged token input"")
grammar = r""""""
NP: {<.*>*}             # start by chunking everything
}<[\.VI].*>+{       # strip any verbs, prepositions or periods
<.*>}{<DT>          # separate on determiners
PP: {<IN><NP>}          # PP = preposition + noun phrase
VP: {<VB.*><NP|PP>*}    # VP = verb words + NPs and PPs
""""""
cp = chunk.RegexpParser(grammar)
print(
cp.parse(
[
(""the"", ""DT""),
(""little"", ""JJ""),
(""cat"", ""NN""),
(""sat"", ""VBD""),
(""on"", ""IN""),
(""the"", ""DT""),
(""mat"", ""NN""),
(""."", "".""),
]
)
)
",[],0,[],/chunk/regexp.py_demo
1296,/home/amandapotts/git/nltk/nltk/chunk/util.py_accuracy,"def accuracy(chunker, gold):
""""""
Score the accuracy of the chunker against the gold standard.
Strip the chunk information from the gold standard and rechunk it using
the chunker, then compute the accuracy score.
:type chunker: ChunkParserI
:param chunker: The chunker being evaluated.
:type gold: tree
:param gold: The chunk structures to score the chunker on.
:rtype: float
""""""
gold_tags = []
test_tags = []
for gold_tree in gold:
test_tree = chunker.parse(gold_tree.flatten())
gold_tags += tree2conlltags(gold_tree)
test_tags += tree2conlltags(test_tree)
return _accuracy(gold_tags, test_tags)
",[],0,[],/chunk/util.py_accuracy
1297,/home/amandapotts/git/nltk/nltk/chunk/util.py___init__,"def __init__(self, **kwargs):
self._correct = set()
self._guessed = set()
self._tp = set()
self._fp = set()
self._fn = set()
self._max_tp = kwargs.get(""max_tp_examples"", 100)
self._max_fp = kwargs.get(""max_fp_examples"", 100)
self._max_fn = kwargs.get(""max_fn_examples"", 100)
self._chunk_label = kwargs.get(""chunk_label"", "".*"")
self._tp_num = 0
self._fp_num = 0
self._fn_num = 0
self._count = 0
self._tags_correct = 0.0
self._tags_total = 0.0
self._measuresNeedUpdate = False
",[],0,[],/chunk/util.py___init__
1298,/home/amandapotts/git/nltk/nltk/chunk/util.py__updateMeasures,"def _updateMeasures(self):
if self._measuresNeedUpdate:
self._tp = self._guessed & self._correct
self._fn = self._correct - self._guessed
self._fp = self._guessed - self._correct
self._tp_num = len(self._tp)
self._fp_num = len(self._fp)
self._fn_num = len(self._fn)
self._measuresNeedUpdate = False
",[],0,[],/chunk/util.py__updateMeasures
1299,/home/amandapotts/git/nltk/nltk/chunk/util.py_score,"def score(self, correct, guessed):
""""""
Given a correctly chunked sentence, score another chunked
version of the same sentence.
:type correct: chunk structure
:param correct: The known-correct (""gold standard"") chunked
sentence.
:type guessed: chunk structure
:param guessed: The chunked sentence to be scored.
""""""
self._correct |= _chunksets(correct, self._count, self._chunk_label)
self._guessed |= _chunksets(guessed, self._count, self._chunk_label)
self._count += 1
self._measuresNeedUpdate = True
try:
correct_tags = tree2conlltags(correct)
guessed_tags = tree2conlltags(guessed)
except ValueError:
correct_tags = guessed_tags = ()
self._tags_total += len(correct_tags)
self._tags_correct += sum(
1 for (t, g) in zip(guessed_tags, correct_tags) if t == g
)
",[],0,[],/chunk/util.py_score
1300,/home/amandapotts/git/nltk/nltk/chunk/util.py_accuracy,"def accuracy(self):
""""""
Return the overall tag-based accuracy for all text that have
been scored by this ``ChunkScore``, using the IOB (conll2000)
tag encoding.
:rtype: float
""""""
if self._tags_total == 0:
return 1
return self._tags_correct / self._tags_total
",[],0,[],/chunk/util.py_accuracy
1301,/home/amandapotts/git/nltk/nltk/chunk/util.py_precision,"def precision(self):
""""""
Return the overall precision for all texts that have been
scored by this ``ChunkScore``.
:rtype: float
""""""
self._updateMeasures()
div = self._tp_num + self._fp_num
if div == 0:
return 0
else:
return self._tp_num / div
",[],0,[],/chunk/util.py_precision
1302,/home/amandapotts/git/nltk/nltk/chunk/util.py_recall,"def recall(self):
""""""
Return the overall recall for all texts that have been
scored by this ``ChunkScore``.
:rtype: float
""""""
self._updateMeasures()
div = self._tp_num + self._fn_num
if div == 0:
return 0
else:
return self._tp_num / div
",[],0,[],/chunk/util.py_recall
1303,/home/amandapotts/git/nltk/nltk/chunk/util.py_f_measure,"def f_measure(self, alpha=0.5):
""""""
Return the overall F measure for all texts that have been
scored by this ``ChunkScore``.
:param alpha: the relative weighting of precision and recall.
Larger alpha biases the score towards the precision value,
while smaller alpha biases the score towards the recall
value.  ``alpha`` should have a value in the range [0,1].
:type alpha: float
:rtype: float
""""""
self._updateMeasures()
p = self.precision()
r = self.recall()
if p == 0 or r == 0:  # what if alpha is 0 or 1?
return 0
return 1 / (alpha / p + (1 - alpha) / r)
",[],0,[],/chunk/util.py_f_measure
1304,/home/amandapotts/git/nltk/nltk/chunk/util.py_missed,"def missed(self):
""""""
Return the chunks which were included in the
correct chunk structures, but not in the guessed chunk
structures, listed in input order.
:rtype: list of chunks
""""""
self._updateMeasures()
chunks = list(self._fn)
return [c[1] for c in chunks]  # discard position information
",[],0,[],/chunk/util.py_missed
1305,/home/amandapotts/git/nltk/nltk/chunk/util.py_incorrect,"def incorrect(self):
""""""
Return the chunks which were included in the guessed chunk structures,
but not in the correct chunk structures, listed in input order.
:rtype: list of chunks
""""""
self._updateMeasures()
chunks = list(self._fp)
return [c[1] for c in chunks]  # discard position information
",[],0,[],/chunk/util.py_incorrect
1306,/home/amandapotts/git/nltk/nltk/chunk/util.py_correct,"def correct(self):
""""""
Return the chunks which were included in the correct
chunk structures, listed in input order.
:rtype: list of chunks
""""""
chunks = list(self._correct)
return [c[1] for c in chunks]  # discard position information
",[],0,[],/chunk/util.py_correct
1307,/home/amandapotts/git/nltk/nltk/chunk/util.py_guessed,"def guessed(self):
""""""
Return the chunks which were included in the guessed
chunk structures, listed in input order.
:rtype: list of chunks
""""""
chunks = list(self._guessed)
return [c[1] for c in chunks]  # discard position information
",[],0,[],/chunk/util.py_guessed
1308,/home/amandapotts/git/nltk/nltk/chunk/util.py___len__,"def __len__(self):
self._updateMeasures()
return self._tp_num + self._fn_num
",[],0,[],/chunk/util.py___len__
1309,/home/amandapotts/git/nltk/nltk/chunk/util.py___repr__,"def __repr__(self):
""""""
Return a concise representation of this ``ChunkScoring``.
:rtype: str
""""""
return ""<ChunkScoring of "" + repr(len(self)) + "" chunks>""
",[],0,[],/chunk/util.py___repr__
1310,/home/amandapotts/git/nltk/nltk/chunk/util.py___str__,"def __str__(self):
""""""
Return a verbose representation of this ``ChunkScoring``.
This representation includes the precision, recall, and
f-measure scores.  For other information about the score,
use the accessor methods (e.g., ``missed()`` and ``incorrect()``).
:rtype: str
""""""
return (
""ChunkParse score:\n""
+ (f""    IOB Accuracy: {self.accuracy() * 100:5.1f}%%\n"")
+ (f""    Precision:    {self.precision() * 100:5.1f}%%\n"")
+ (f""    Recall:       {self.recall() * 100:5.1f}%%\n"")
+ (f""    F-Measure:    {self.f_measure() * 100:5.1f}%%"")
)
",[],0,[],/chunk/util.py___str__
1311,/home/amandapotts/git/nltk/nltk/chunk/util.py__chunksets,"def _chunksets(t, count, chunk_label):
pos = 0
chunks = []
for child in t:
if isinstance(child, Tree):
if re.match(chunk_label, child.label()):
chunks.append(((count, pos), child.freeze()))
pos += len(child.leaves())
else:
pos += 1
return set(chunks)
",[],0,[],/chunk/util.py__chunksets
1312,/home/amandapotts/git/nltk/nltk/chunk/util.py_tagstr2tree,"def tagstr2tree(
s, chunk_label=""NP"", root_label=""S"", sep=""/"", source_tagset=None, target_tagset=None
",[],0,[],/chunk/util.py_tagstr2tree
1313,/home/amandapotts/git/nltk/nltk/chunk/util.py_conllstr2tree,"def conllstr2tree(s, chunk_types=(""NP"", ""PP"", ""VP""), root_label=""S""):
""""""
Return a chunk structure for a single sentence
encoded in the given CONLL 2000 style string.
This function converts a CoNLL IOB string into a tree.
It uses the specified chunk types
(defaults to NP, PP and VP), and creates a tree rooted at a node
labeled S (by default).
:param s: The CoNLL string to be converted.
:type s: str
:param chunk_types: The chunk types to be converted.
:type chunk_types: tuple
:param root_label: The node label to use for the root.
:type root_label: str
:rtype: Tree
""""""
stack = [Tree(root_label, [])]
for lineno, line in enumerate(s.split(""\n"")):
if not line.strip():
continue
match = _LINE_RE.match(line)
if match is None:
raise ValueError(f""Error on line {lineno:d}"")
(word, tag, state, chunk_type) = match.groups()
if chunk_types is not None and chunk_type not in chunk_types:
state = ""O""
mismatch_I = state == ""I"" and chunk_type != stack[-1].label()
if state in ""BO"" or mismatch_I:
if len(stack) == 2:
stack.pop()
if state == ""B"" or mismatch_I:
chunk = Tree(chunk_type, [])
stack[-1].append(chunk)
stack.append(chunk)
stack[-1].append((word, tag))
return stack[0]
",[],0,[],/chunk/util.py_conllstr2tree
1314,/home/amandapotts/git/nltk/nltk/chunk/util.py_tree2conlltags,"def tree2conlltags(t):
""""""
Return a list of 3-tuples containing ``(word, tag, IOB-tag)``.
Convert a tree to the CoNLL IOB tag format.
:param t: The tree to be converted.
:type t: Tree
:rtype: list(tuple)
""""""
tags = []
for child in t:
try:
category = child.label()
prefix = ""B-""
for contents in child:
if isinstance(contents, Tree):
raise ValueError(
""Tree is too deeply nested to be printed in CoNLL format""
)
tags.append((contents[0], contents[1], prefix + category))
prefix = ""I-""
except AttributeError:
tags.append((child[0], child[1], ""O""))
return tags
",[],0,[],/chunk/util.py_tree2conlltags
1315,/home/amandapotts/git/nltk/nltk/chunk/util.py_conlltags2tree,"def conlltags2tree(
sentence, chunk_types=(""NP"", ""PP"", ""VP""), root_label=""S"", strict=False
",[],0,[],/chunk/util.py_conlltags2tree
1316,/home/amandapotts/git/nltk/nltk/chunk/util.py_tree2conllstr,"def tree2conllstr(t):
""""""
Return a multiline string where each line contains a word, tag and IOB tag.
Convert a tree to the CoNLL IOB string format
:param t: The tree to be converted.
:type t: Tree
:rtype: str
""""""
lines = ["" "".join(token) for token in tree2conlltags(t)]
return ""\n"".join(lines)
",[],0,[],/chunk/util.py_tree2conllstr
1317,/home/amandapotts/git/nltk/nltk/chunk/util.py__ieer_read_text,"def _ieer_read_text(s, root_label):
stack = [Tree(root_label, [])]
if s is None:
return []
for piece_m in re.finditer(r""<[^>]+>|[^\s<]+"", s):
piece = piece_m.group()
try:
if piece.startswith(""<b_""):
m = _IEER_TYPE_RE.match(piece)
if m is None:
print(""XXXX"", piece)
chunk = Tree(m.group(""type""), [])
stack[-1].append(chunk)
stack.append(chunk)
elif piece.startswith(""<e_""):
stack.pop()
else:
stack[-1].append(piece)
except (IndexError, ValueError) as e:
raise ValueError(
f""Bad IEER string (error at character {piece_m.start():d})""
) from e
if len(stack) != 1:
raise ValueError(""Bad IEER string"")
return stack[0]
",[],0,[],/chunk/util.py__ieer_read_text
1318,/home/amandapotts/git/nltk/nltk/chunk/util.py_ieerstr2tree,"def ieerstr2tree(
s,
chunk_types=[
""LOCATION"",
""ORGANIZATION"",
""PERSON"",
""DURATION"",
""DATE"",
""CARDINAL"",
""PERCENT"",
""MONEY"",
""MEASURE"",
],
root_label=""S"",
",[],0,[],/chunk/util.py_ieerstr2tree
1319,/home/amandapotts/git/nltk/nltk/chunk/util.py_demo,"def demo():
s = ""[ Pierre/NNP Vinken/NNP ] ,/, [ 61/CD years/NNS ] old/JJ ,/, will/MD join/VB [ the/DT board/NN ] ./.""
import nltk
t = nltk.chunk.tagstr2tree(s, chunk_label=""NP"")
t.pprint()
print()
s = """"""
",[],0,[],/chunk/util.py_demo
1320,/home/amandapotts/git/nltk/nltk/chunk/named_entity.py___init__,"def __init__(self, train):
ClassifierBasedTagger.__init__(
self, train=train, classifier_builder=self._classifier_builder
)
",[],0,[],/chunk/named_entity.py___init__
1321,/home/amandapotts/git/nltk/nltk/chunk/named_entity.py__classifier_builder,"def _classifier_builder(self, train):
return MaxentClassifier.train(
train, algorithm=""megam"", gaussian_prior_sigma=1, trace=2
)
",[],0,[],/chunk/named_entity.py__classifier_builder
1322,/home/amandapotts/git/nltk/nltk/chunk/named_entity.py__english_wordlist,"def _english_wordlist(self):
try:
wl = self._en_wordlist
except AttributeError:
from nltk.corpus import words
self._en_wordlist = set(words.words(""en-basic""))
wl = self._en_wordlist
return wl
",[],0,[],/chunk/named_entity.py__english_wordlist
1323,/home/amandapotts/git/nltk/nltk/chunk/named_entity.py__feature_detector,"def _feature_detector(self, tokens, index, history):
word = tokens[index][0]
pos = simplify_pos(tokens[index][1])
if index == 0:
prevword = prevprevword = None
prevpos = prevprevpos = None
prevshape = prevtag = prevprevtag = None
elif index == 1:
prevword = tokens[index - 1][0].lower()
prevprevword = None
prevpos = simplify_pos(tokens[index - 1][1])
prevprevpos = None
prevtag = history[index - 1][0]
prevshape = prevprevtag = None
else:
prevword = tokens[index - 1][0].lower()
prevprevword = tokens[index - 2][0].lower()
prevpos = simplify_pos(tokens[index - 1][1])
prevprevpos = simplify_pos(tokens[index - 2][1])
prevtag = history[index - 1]
prevprevtag = history[index - 2]
prevshape = shape(prevword)
if index == len(tokens) - 1:
nextword = nextnextword = None
nextpos = nextnextpos = None
elif index == len(tokens) - 2:
nextword = tokens[index + 1][0].lower()
nextpos = tokens[index + 1][1].lower()
nextnextword = None
nextnextpos = None
else:
nextword = tokens[index + 1][0].lower()
nextpos = tokens[index + 1][1].lower()
nextnextword = tokens[index + 2][0].lower()
nextnextpos = tokens[index + 2][1].lower()
features = {
""bias"": True,
""shape"": shape(word),
""wordlen"": len(word),
""prefix3"": word[:3].lower(),
""suffix3"": word[-3:].lower(),
""pos"": pos,
""word"": word,
""en-wordlist"": (word in self._english_wordlist()),
""prevtag"": prevtag,
""prevpos"": prevpos,
""nextpos"": nextpos,
""prevword"": prevword,
""nextword"": nextword,
""word+nextpos"": f""{word.lower()}+{nextpos}"",
""pos+prevtag"": f""{pos}+{prevtag}"",
""shape+prevtag"": f""{prevshape}+{prevtag}"",
}
return features
",[],0,[],/chunk/named_entity.py__feature_detector
1324,/home/amandapotts/git/nltk/nltk/chunk/named_entity.py___init__,"def __init__(self, train):
self._train(train)
",[],0,[],/chunk/named_entity.py___init__
1325,/home/amandapotts/git/nltk/nltk/chunk/named_entity.py_parse,"def parse(self, tokens):
""""""
Each token should be a pos-tagged word
""""""
tagged = self._tagger.tag(tokens)
tree = self._tagged_to_parse(tagged)
return tree
",[],0,[],/chunk/named_entity.py_parse
1326,/home/amandapotts/git/nltk/nltk/chunk/named_entity.py__train,"def _train(self, corpus):
corpus = [self._parse_to_tagged(s) for s in corpus]
self._tagger = NEChunkParserTagger(train=corpus)
",[],0,[],/chunk/named_entity.py__train
1327,/home/amandapotts/git/nltk/nltk/chunk/named_entity.py__tagged_to_parse,"def _tagged_to_parse(self, tagged_tokens):
""""""
Convert a list of tagged tokens to a chunk-parse tree.
""""""
sent = Tree(""S"", [])
for tok, tag in tagged_tokens:
if tag == ""O"":
sent.append(tok)
elif tag.startswith(""B-""):
sent.append(Tree(tag[2:], [tok]))
elif tag.startswith(""I-""):
if sent and isinstance(sent[-1], Tree) and sent[-1].label() == tag[2:]:
sent[-1].append(tok)
else:
sent.append(Tree(tag[2:], [tok]))
return sent
",[],0,[],/chunk/named_entity.py__tagged_to_parse
1328,/home/amandapotts/git/nltk/nltk/chunk/named_entity.py__parse_to_tagged,"def _parse_to_tagged(sent):
""""""
Convert a chunk-parse tree to a list of tagged tokens.
""""""
toks = []
for child in sent:
if isinstance(child, Tree):
if len(child) == 0:
print(""Warning -- empty chunk in sentence"")
continue
toks.append((child[0], f""B-{child.label()}""))
for tok in child[1:]:
toks.append((tok, f""I-{child.label()}""))
else:
toks.append((child, ""O""))
return toks
",[],0,[],/chunk/named_entity.py__parse_to_tagged
1329,/home/amandapotts/git/nltk/nltk/chunk/named_entity.py_shape,"def shape(word):
if re.match(r""[0-9]+(\.[0-9]*)?|[0-9]*\.[0-9]+$"", word, re.UNICODE):
return ""number""
elif re.match(r""\W+$"", word, re.UNICODE):
return ""punct""
elif re.match(r""\w+$"", word, re.UNICODE):
if word.istitle():
return ""upcase""
elif word.islower():
return ""downcase""
else:
return ""mixedcase""
else:
return ""other""
",[],0,[],/chunk/named_entity.py_shape
1330,/home/amandapotts/git/nltk/nltk/chunk/named_entity.py_simplify_pos,"def simplify_pos(s):
if s.startswith(""V""):
return ""V""
else:
return s.split(""-"")[0]
",[],0,[],/chunk/named_entity.py_simplify_pos
1331,/home/amandapotts/git/nltk/nltk/chunk/named_entity.py_postag_tree,"def postag_tree(tree):
words = tree.leaves()
tag_iter = (pos for (word, pos) in pos_tag(words))
newtree = Tree(""S"", [])
for child in tree:
if isinstance(child, Tree):
newtree.append(Tree(child.label(), []))
for subchild in child:
newtree[-1].append((subchild, next(tag_iter)))
else:
newtree.append((child, next(tag_iter)))
return newtree
",[],0,[],/chunk/named_entity.py_postag_tree
1332,/home/amandapotts/git/nltk/nltk/chunk/named_entity.py_load_ace_data,"def load_ace_data(roots, fmt=""binary"", skip_bnews=True):
for root in roots:
for root, dirs, files in os.walk(root):
if root.endswith(""bnews"") and skip_bnews:
continue
for f in files:
if f.endswith("".sgm""):
yield from load_ace_file(os.path.join(root, f), fmt)
",[],0,[],/chunk/named_entity.py_load_ace_data
1333,/home/amandapotts/git/nltk/nltk/chunk/named_entity.py_load_ace_file,"def load_ace_file(textfile, fmt):
print(f""  - {os.path.split(textfile)[1]}"")
annfile = textfile + "".tmx.rdc.xml""
entities = []
with open(annfile) as infile:
xml = ET.parse(infile).getroot()
for entity in xml.findall(""document/entity""):
typ = entity.find(""entity_type"").text
for mention in entity.findall(""entity_mention""):
if mention.get(""TYPE"") != ""NAME"":
continue  # only NEs
s = int(mention.find(""head/charseq/start"").text)
e = int(mention.find(""head/charseq/end"").text) + 1
entities.append((s, e, typ))
with open(textfile) as infile:
text = infile.read()
text = re.sub(""<(?!/?TEXT)[^>]+>"", """", text)
",[],0,[],/chunk/named_entity.py_load_ace_file
1334,/home/amandapotts/git/nltk/nltk/chunk/named_entity.py_subfunc,"def subfunc(m):
return "" "" * (m.end() - m.start() - 6)
",[],0,[],/chunk/named_entity.py_subfunc
1335,/home/amandapotts/git/nltk/nltk/chunk/named_entity.py_cmp_chunks,"def cmp_chunks(correct, guessed):
correct = NEChunkParser._parse_to_tagged(correct)
guessed = NEChunkParser._parse_to_tagged(guessed)
ellipsis = False
for (w, ct), (w, gt) in zip(correct, guessed):
if ct == gt == ""O"":
if not ellipsis:
print(f""  {ct:15} {gt:15} {w}"")
print(""  {:15} {:15} {2}"".format(""..."", ""..."", ""...""))
ellipsis = True
else:
ellipsis = False
print(f""  {ct:15} {gt:15} {w}"")
",[],0,[],/chunk/named_entity.py_cmp_chunks
1336,/home/amandapotts/git/nltk/nltk/chunk/named_entity.py_build_model,"def build_model(fmt=""binary""):
print(""Loading training data..."")
train_paths = [
find(""corpora/ace_data/ace.dev""),
find(""corpora/ace_data/ace.heldout""),
find(""corpora/ace_data/bbn.dev""),
find(""corpora/ace_data/muc.dev""),
]
train_trees = load_ace_data(train_paths, fmt)
train_data = [postag_tree(t) for t in train_trees]
print(""Training..."")
cp = NEChunkParser(train_data)
del train_data
print(""Loading eval data..."")
eval_paths = [find(""corpora/ace_data/ace.eval"")]
eval_trees = load_ace_data(eval_paths, fmt)
eval_data = [postag_tree(t) for t in eval_trees]
print(""Evaluating..."")
chunkscore = ChunkScore()
for i, correct in enumerate(eval_data):
guess = cp.parse(correct.leaves())
chunkscore.score(correct, guess)
if i < 3:
cmp_chunks(correct, guess)
print(chunkscore)
outfilename = f""/tmp/ne_chunker_{fmt}.pickle""
print(f""Saving chunker to {outfilename}..."")
with open(outfilename, ""wb"") as outfile:
pickle.dump(cp, outfile, -1)
return cp
",[],0,[],/chunk/named_entity.py_build_model
1337,/home/amandapotts/git/nltk/nltk/chunk/api.py_parse,"def parse(self, tokens):
""""""
Return the best chunk structure for the given tokens
and return a tree.
:param tokens: The list of (word, tag) tokens to be chunked.
:type tokens: list(tuple)
:rtype: Tree
""""""
raise NotImplementedError()
",[],0,[],/chunk/api.py_parse
1338,/home/amandapotts/git/nltk/nltk/chunk/api.py_evaluate,"def evaluate(self, gold):
return self.accuracy(gold)
",[],0,[],/chunk/api.py_evaluate
1339,/home/amandapotts/git/nltk/nltk/chunk/api.py_accuracy,"def accuracy(self, gold):
""""""
Score the accuracy of the chunker against the gold standard.
Remove the chunking the gold standard text, rechunk it using
the chunker, and return a ``ChunkScore`` object
reflecting the performance of this chunk parser.
:type gold: list(Tree)
:param gold: The list of chunked sentences to score the chunker on.
:rtype: ChunkScore
""""""
chunkscore = ChunkScore()
for correct in gold:
chunkscore.score(correct, self.parse(correct.leaves()))
return chunkscore
",[],0,[],/chunk/api.py_accuracy
1340,/home/amandapotts/git/nltk/nltk/ccg/lexicon.py___init__,"def __init__(self, token, categ, semantics=None):
self._token = token
self._categ = categ
self._semantics = semantics
",[],0,[],/ccg/lexicon.py___init__
1341,/home/amandapotts/git/nltk/nltk/ccg/lexicon.py_categ,"def categ(self):
return self._categ
",[],0,[],/ccg/lexicon.py_categ
1342,/home/amandapotts/git/nltk/nltk/ccg/lexicon.py_semantics,"def semantics(self):
return self._semantics
",[],0,[],/ccg/lexicon.py_semantics
1343,/home/amandapotts/git/nltk/nltk/ccg/lexicon.py___str__,"def __str__(self):
semantics_str = """"
if self._semantics is not None:
semantics_str = "" {"" + str(self._semantics) + ""}""
return """" + str(self._categ) + semantics_str
",[],0,[],/ccg/lexicon.py___str__
1344,/home/amandapotts/git/nltk/nltk/ccg/lexicon.py___cmp__,"def __cmp__(self, other):
if not isinstance(other, Token):
return -1
return cmp((self._categ, self._semantics), other.categ(), other.semantics())
",[],0,[],/ccg/lexicon.py___cmp__
1345,/home/amandapotts/git/nltk/nltk/ccg/lexicon.py___init__,"def __init__(self, start, primitives, families, entries):
self._start = PrimitiveCategory(start)
self._primitives = primitives
self._families = families
self._entries = entries
",[],0,[],/ccg/lexicon.py___init__
1346,/home/amandapotts/git/nltk/nltk/ccg/lexicon.py_categories,"def categories(self, word):
""""""
Returns all the possible categories for a word
""""""
return self._entries[word]
",[],0,[],/ccg/lexicon.py_categories
1347,/home/amandapotts/git/nltk/nltk/ccg/lexicon.py_start,"def start(self):
""""""
Return the target category for the parser
""""""
return self._start
",[],0,[],/ccg/lexicon.py_start
1348,/home/amandapotts/git/nltk/nltk/ccg/lexicon.py___str__,"def __str__(self):
""""""
String representation of the lexicon. Used for debugging.
""""""
string = """"
first = True
for ident in sorted(self._entries):
if not first:
string = string + ""\n""
string = string + ident + "" => ""
first = True
for cat in self._entries[ident]:
if not first:
string = string + "" | ""
else:
first = False
string = string + ""%s"" % cat
return string
",[],0,[],/ccg/lexicon.py___str__
1349,/home/amandapotts/git/nltk/nltk/ccg/lexicon.py_matchBrackets,"def matchBrackets(string):
""""""
Separate the contents matching the first set of brackets from the rest of
the input.
""""""
rest = string[1:]
inside = ""(""
while rest != """" and not rest.startswith("")""):
if rest.startswith(""(""):
(part, rest) = matchBrackets(rest)
inside = inside + part
else:
inside = inside + rest[0]
rest = rest[1:]
if rest.startswith("")""):
return (inside + "")"", rest[1:])
raise AssertionError(""Unmatched bracket in string '"" + string + ""'"")
",[],0,[],/ccg/lexicon.py_matchBrackets
1350,/home/amandapotts/git/nltk/nltk/ccg/lexicon.py_nextCategory,"def nextCategory(string):
""""""
Separate the string for the next portion of the category from the rest
of the string
""""""
if string.startswith(""(""):
return matchBrackets(string)
return NEXTPRIM_RE.match(string).groups()
",[],0,[],/ccg/lexicon.py_nextCategory
1351,/home/amandapotts/git/nltk/nltk/ccg/lexicon.py_parseApplication,"def parseApplication(app):
""""""
Parse an application operator
""""""
return Direction(app[0], app[1:])
",[],0,[],/ccg/lexicon.py_parseApplication
1352,/home/amandapotts/git/nltk/nltk/ccg/lexicon.py_parseSubscripts,"def parseSubscripts(subscr):
""""""
Parse the subscripts for a primitive category
""""""
if subscr:
return subscr[1:-1].split("","")
return []
",[],0,[],/ccg/lexicon.py_parseSubscripts
1353,/home/amandapotts/git/nltk/nltk/ccg/lexicon.py_parsePrimitiveCategory,"def parsePrimitiveCategory(chunks, primitives, families, var):
""""""
Parse a primitive category
If the primitive is the special category 'var', replace it with the
correct `CCGVar`.
""""""
if chunks[0] == ""var"":
if chunks[1] is None:
if var is None:
var = CCGVar()
return (var, var)
catstr = chunks[0]
if catstr in families:
(cat, cvar) = families[catstr]
if var is None:
var = cvar
else:
cat = cat.substitute([(cvar, var)])
return (cat, var)
if catstr in primitives:
subscrs = parseSubscripts(chunks[1])
return (PrimitiveCategory(catstr, subscrs), var)
raise AssertionError(
""String '"" + catstr + ""' is neither a family nor primitive category.""
)
",[],0,[],/ccg/lexicon.py_parsePrimitiveCategory
1354,/home/amandapotts/git/nltk/nltk/ccg/lexicon.py_augParseCategory,"def augParseCategory(line, primitives, families, var=None):
""""""
Parse a string representing a category, and returns a tuple with
(possibly) the CCG variable for the category
""""""
(cat_string, rest) = nextCategory(line)
if cat_string.startswith(""(""):
(res, var) = augParseCategory(cat_string[1:-1], primitives, families, var)
else:
(res, var) = parsePrimitiveCategory(
PRIM_RE.match(cat_string).groups(), primitives, families, var
)
while rest != """":
app = APP_RE.match(rest).groups()
direction = parseApplication(app[0:3])
rest = app[3]
(cat_string, rest) = nextCategory(rest)
if cat_string.startswith(""(""):
(arg, var) = augParseCategory(cat_string[1:-1], primitives, families, var)
else:
(arg, var) = parsePrimitiveCategory(
PRIM_RE.match(cat_string).groups(), primitives, families, var
)
res = FunctionalCategory(res, arg, direction)
return (res, var)
",[],0,[],/ccg/lexicon.py_augParseCategory
1355,/home/amandapotts/git/nltk/nltk/ccg/lexicon.py_fromstring,"def fromstring(lex_str, include_semantics=False):
""""""
Convert string representation into a lexicon for CCGs.
""""""
CCGVar.reset_id()
primitives = []
families = {}
entries = defaultdict(list)
for line in lex_str.splitlines():
line = COMMENTS_RE.match(line).groups()[0].strip()
if line == """":
continue
if line.startswith("":-""):
primitives = primitives + [
prim.strip() for prim in line[2:].strip().split("","")
]
else:
(ident, sep, rhs) = LEX_RE.match(line).groups()
(catstr, semantics_str) = RHS_RE.match(rhs).groups()
(cat, var) = augParseCategory(catstr, primitives, families)
if sep == ""::"":
families[ident] = (cat, var)
else:
semantics = None
if include_semantics is True:
if semantics_str is None:
raise AssertionError(
line
+ "" must contain semantics because include_semantics is set to True""
)
else:
semantics = Expression.fromstring(
SEMANTICS_RE.match(semantics_str).groups()[0]
)
entries[ident].append(Token(ident, cat, semantics))
return CCGLexicon(primitives[0], primitives, families, entries)
",[],0,[],/ccg/lexicon.py_fromstring
1356,/home/amandapotts/git/nltk/nltk/ccg/lexicon.py_parseLexicon,"def parseLexicon(lex_str):
return fromstring(lex_str)
",[],0,[],/ccg/lexicon.py_parseLexicon
1357,/home/amandapotts/git/nltk/nltk/ccg/logic.py_compute_type_raised_semantics,"def compute_type_raised_semantics(semantics):
core = semantics
parent = None
while isinstance(core, LambdaExpression):
parent = core
core = core.term
var = Variable(""F"")
while var in core.free():
var = unique_variable(pattern=var)
core = ApplicationExpression(FunctionVariableExpression(var), core)
if parent is not None:
parent.term = core
else:
semantics = core
return LambdaExpression(var, semantics)
",[],0,[],/ccg/logic.py_compute_type_raised_semantics
1358,/home/amandapotts/git/nltk/nltk/ccg/logic.py_compute_function_semantics,"def compute_function_semantics(function, argument):
return ApplicationExpression(function, argument).simplify()
",[],0,[],/ccg/logic.py_compute_function_semantics
1359,/home/amandapotts/git/nltk/nltk/ccg/chart.py___init__,"def __init__(self, span, categ, rule):
self._span = span
self._categ = categ
self._rule = rule
self._comparison_key = (span, categ, rule)
",[],0,[],/ccg/chart.py___init__
1360,/home/amandapotts/git/nltk/nltk/ccg/chart.py_lhs,"def lhs(self):
return self._categ
",[],0,[],/ccg/chart.py_lhs
1361,/home/amandapotts/git/nltk/nltk/ccg/chart.py_span,"def span(self):
return self._span
",[],0,[],/ccg/chart.py_span
1362,/home/amandapotts/git/nltk/nltk/ccg/chart.py_start,"def start(self):
return self._span[0]
",[],0,[],/ccg/chart.py_start
1363,/home/amandapotts/git/nltk/nltk/ccg/chart.py_end,"def end(self):
return self._span[1]
",[],0,[],/ccg/chart.py_end
1364,/home/amandapotts/git/nltk/nltk/ccg/chart.py_length,"def length(self):
return self._span[1] - self.span[0]
",[],0,[],/ccg/chart.py_length
1365,/home/amandapotts/git/nltk/nltk/ccg/chart.py_rhs,"def rhs(self):
return ()
",[],0,[],/ccg/chart.py_rhs
1366,/home/amandapotts/git/nltk/nltk/ccg/chart.py_dot,"def dot(self):
return 0
",[],0,[],/ccg/chart.py_dot
1367,/home/amandapotts/git/nltk/nltk/ccg/chart.py_is_complete,"def is_complete(self):
return True
",[],0,[],/ccg/chart.py_is_complete
1368,/home/amandapotts/git/nltk/nltk/ccg/chart.py_is_incomplete,"def is_incomplete(self):
return False
",[],0,[],/ccg/chart.py_is_incomplete
1369,/home/amandapotts/git/nltk/nltk/ccg/chart.py_nextsym,"def nextsym(self):
return None
",[],0,[],/ccg/chart.py_nextsym
1370,/home/amandapotts/git/nltk/nltk/ccg/chart.py_categ,"def categ(self):
return self._categ
",[],0,[],/ccg/chart.py_categ
1371,/home/amandapotts/git/nltk/nltk/ccg/chart.py_rule,"def rule(self):
return self._rule
",[],0,[],/ccg/chart.py_rule
1372,/home/amandapotts/git/nltk/nltk/ccg/chart.py___init__,"def __init__(self, pos, token, leaf):
self._pos = pos
self._token = token
self._leaf = leaf
self._comparison_key = (pos, token.categ(), leaf)
",[],0,[],/ccg/chart.py___init__
1373,/home/amandapotts/git/nltk/nltk/ccg/chart.py_lhs,"def lhs(self):
return self._token.categ()
",[],0,[],/ccg/chart.py_lhs
1374,/home/amandapotts/git/nltk/nltk/ccg/chart.py_span,"def span(self):
return (self._pos, self._pos + 1)
",[],0,[],/ccg/chart.py_span
1375,/home/amandapotts/git/nltk/nltk/ccg/chart.py_start,"def start(self):
return self._pos
",[],0,[],/ccg/chart.py_start
1376,/home/amandapotts/git/nltk/nltk/ccg/chart.py_end,"def end(self):
return self._pos + 1
",[],0,[],/ccg/chart.py_end
1377,/home/amandapotts/git/nltk/nltk/ccg/chart.py_length,"def length(self):
return 1
",[],0,[],/ccg/chart.py_length
1378,/home/amandapotts/git/nltk/nltk/ccg/chart.py_rhs,"def rhs(self):
return self._leaf
",[],0,[],/ccg/chart.py_rhs
1379,/home/amandapotts/git/nltk/nltk/ccg/chart.py_dot,"def dot(self):
return 0
",[],0,[],/ccg/chart.py_dot
1380,/home/amandapotts/git/nltk/nltk/ccg/chart.py_is_complete,"def is_complete(self):
return True
",[],0,[],/ccg/chart.py_is_complete
1381,/home/amandapotts/git/nltk/nltk/ccg/chart.py_is_incomplete,"def is_incomplete(self):
return False
",[],0,[],/ccg/chart.py_is_incomplete
1382,/home/amandapotts/git/nltk/nltk/ccg/chart.py_nextsym,"def nextsym(self):
return None
",[],0,[],/ccg/chart.py_nextsym
1383,/home/amandapotts/git/nltk/nltk/ccg/chart.py_token,"def token(self):
return self._token
",[],0,[],/ccg/chart.py_token
1384,/home/amandapotts/git/nltk/nltk/ccg/chart.py_categ,"def categ(self):
return self._token.categ()
",[],0,[],/ccg/chart.py_categ
1385,/home/amandapotts/git/nltk/nltk/ccg/chart.py_leaf,"def leaf(self):
return self._leaf
",[],0,[],/ccg/chart.py_leaf
1386,/home/amandapotts/git/nltk/nltk/ccg/chart.py___init__,"def __init__(self, combinator):
self._combinator = combinator
",[],0,[],/ccg/chart.py___init__
1387,/home/amandapotts/git/nltk/nltk/ccg/chart.py_apply,"def apply(self, chart, grammar, left_edge, right_edge):
if not (left_edge.end() == right_edge.start()):
return
if self._combinator.can_combine(left_edge.categ(), right_edge.categ()):
for res in self._combinator.combine(left_edge.categ(), right_edge.categ()):
new_edge = CCGEdge(
span=(left_edge.start(), right_edge.end()),
categ=res,
rule=self._combinator,
)
if chart.insert(new_edge, (left_edge, right_edge)):
yield new_edge
",[],0,[],/ccg/chart.py_apply
1388,/home/amandapotts/git/nltk/nltk/ccg/chart.py___str__,"def __str__(self):
return ""%s"" % self._combinator
",[],0,[],/ccg/chart.py___str__
1389,/home/amandapotts/git/nltk/nltk/ccg/chart.py___init__,"def __init__(self):
self._combinator = ForwardT
",[],0,[],/ccg/chart.py___init__
1390,/home/amandapotts/git/nltk/nltk/ccg/chart.py_apply,"def apply(self, chart, grammar, left_edge, right_edge):
if not (left_edge.end() == right_edge.start()):
return
for res in self._combinator.combine(left_edge.categ(), right_edge.categ()):
new_edge = CCGEdge(span=left_edge.span(), categ=res, rule=self._combinator)
if chart.insert(new_edge, (left_edge,)):
yield new_edge
",[],0,[],/ccg/chart.py_apply
1391,/home/amandapotts/git/nltk/nltk/ccg/chart.py___str__,"def __str__(self):
return ""%s"" % self._combinator
",[],0,[],/ccg/chart.py___str__
1392,/home/amandapotts/git/nltk/nltk/ccg/chart.py___init__,"def __init__(self):
self._combinator = BackwardT
",[],0,[],/ccg/chart.py___init__
1393,/home/amandapotts/git/nltk/nltk/ccg/chart.py_apply,"def apply(self, chart, grammar, left_edge, right_edge):
if not (left_edge.end() == right_edge.start()):
return
for res in self._combinator.combine(left_edge.categ(), right_edge.categ()):
new_edge = CCGEdge(span=right_edge.span(), categ=res, rule=self._combinator)
if chart.insert(new_edge, (right_edge,)):
yield new_edge
",[],0,[],/ccg/chart.py_apply
1394,/home/amandapotts/git/nltk/nltk/ccg/chart.py___str__,"def __str__(self):
return ""%s"" % self._combinator
",[],0,[],/ccg/chart.py___str__
1395,/home/amandapotts/git/nltk/nltk/ccg/chart.py___init__,"def __init__(self, lexicon, rules, trace=0):
self._lexicon = lexicon
self._rules = rules
self._trace = trace
",[],0,[],/ccg/chart.py___init__
1396,/home/amandapotts/git/nltk/nltk/ccg/chart.py_lexicon,"def lexicon(self):
return self._lexicon
",[],0,[],/ccg/chart.py_lexicon
1397,/home/amandapotts/git/nltk/nltk/ccg/chart.py_parse,"def parse(self, tokens):
tokens = list(tokens)
chart = CCGChart(list(tokens))
lex = self._lexicon
for index in range(chart.num_leaves()):
for token in lex.categories(chart.leaf(index)):
new_edge = CCGLeafEdge(index, token, chart.leaf(index))
chart.insert(new_edge, ())
for span in range(2, chart.num_leaves() + 1):
for start in range(0, chart.num_leaves() - span + 1):
for part in range(1, span):
lstart = start
mid = start + part
rend = start + span
for left in chart.select(span=(lstart, mid)):
for right in chart.select(span=(mid, rend)):
for rule in self._rules:
edges_added_by_rule = 0
for newedge in rule.apply(chart, lex, left, right):
edges_added_by_rule += 1
return chart.parses(lex.start())
",[],0,[],/ccg/chart.py_parse
1398,/home/amandapotts/git/nltk/nltk/ccg/chart.py___init__,"def __init__(self, tokens):
Chart.__init__(self, tokens)
",[],0,[],/ccg/chart.py___init__
1399,/home/amandapotts/git/nltk/nltk/ccg/chart.py__trees,"def _trees(self, edge, complete, memo, tree_class):
assert complete, ""CCGChart cannot build incomplete trees""
if edge in memo:
return memo[edge]
if isinstance(edge, CCGLeafEdge):
word = tree_class(edge.token(), [self._tokens[edge.start()]])
leaf = tree_class((edge.token(), ""Leaf""), [word])
memo[edge] = [leaf]
return [leaf]
memo[edge] = []
trees = []
for cpl in self.child_pointer_lists(edge):
child_choices = [self._trees(cp, complete, memo, tree_class) for cp in cpl]
for children in itertools.product(*child_choices):
lhs = (
Token(
self._tokens[edge.start() : edge.end()],
edge.lhs(),
compute_semantics(children, edge),
),
str(edge.rule()),
)
trees.append(tree_class(lhs, children))
memo[edge] = trees
return trees
",[],0,[],/ccg/chart.py__trees
1400,/home/amandapotts/git/nltk/nltk/ccg/chart.py_compute_semantics,"def compute_semantics(children, edge):
if children[0].label()[0].semantics() is None:
return None
if len(children) == 2:
if isinstance(edge.rule(), BackwardCombinator):
children = [children[1], children[0]]
combinator = edge.rule()._combinator
function = children[0].label()[0].semantics()
argument = children[1].label()[0].semantics()
if isinstance(combinator, UndirectedFunctionApplication):
return compute_function_semantics(function, argument)
elif isinstance(combinator, UndirectedComposition):
return compute_composition_semantics(function, argument)
elif isinstance(combinator, UndirectedSubstitution):
return compute_substitution_semantics(function, argument)
else:
raise AssertionError(""Unsupported combinator '"" + combinator + ""'"")
else:
return compute_type_raised_semantics(children[0].label()[0].semantics())
",[],0,[],/ccg/chart.py_compute_semantics
1401,/home/amandapotts/git/nltk/nltk/ccg/chart.py_printCCGDerivation,"def printCCGDerivation(tree):
leafcats = tree.pos()
leafstr = """"
catstr = """"
for leaf, cat in leafcats:
str_cat = ""%s"" % cat
nextlen = 2 + max(len(leaf), len(str_cat))
lcatlen = (nextlen - len(str_cat)) // 2
rcatlen = lcatlen + (nextlen - len(str_cat)) % 2
catstr += "" "" * lcatlen + str_cat + "" "" * rcatlen
lleaflen = (nextlen - len(leaf)) // 2
rleaflen = lleaflen + (nextlen - len(leaf)) % 2
leafstr += "" "" * lleaflen + leaf + "" "" * rleaflen
print(leafstr.rstrip())
print(catstr.rstrip())
printCCGTree(0, tree)
",[],0,[],/ccg/chart.py_printCCGDerivation
1402,/home/amandapotts/git/nltk/nltk/ccg/chart.py_printCCGTree,"def printCCGTree(lwidth, tree):
rwidth = lwidth
if not isinstance(tree, Tree):
return 2 + lwidth + len(tree)
for child in tree:
rwidth = max(rwidth, printCCGTree(rwidth, child))
if not isinstance(tree.label(), tuple):
return max(
rwidth, 2 + lwidth + len(""%s"" % tree.label()), 2 + lwidth + len(tree[0])
)
(token, op) = tree.label()
if op == ""Leaf"":
return rwidth
print(lwidth * "" "" + (rwidth - lwidth) * ""-"" + ""%s"" % op)
str_res = ""%s"" % (token.categ())
if token.semantics() is not None:
str_res += "" {"" + str(token.semantics()) + ""}""
respadlen = (rwidth - lwidth - len(str_res)) // 2 + lwidth
print(respadlen * "" "" + str_res)
return rwidth
",[],0,[],/ccg/chart.py_printCCGTree
1403,/home/amandapotts/git/nltk/nltk/ccg/chart.py_demo,"def demo():
parser = CCGChartParser(lex, DefaultRuleSet)
for parse in parser.parse(""I might cook and eat the bacon"".split()):
printCCGDerivation(parse)
",[],0,[],/ccg/chart.py_demo
1404,/home/amandapotts/git/nltk/nltk/ccg/api.py_is_primitive,"def is_primitive(self):
""""""
Returns true if the category is primitive.
""""""
",[],0,[],/ccg/api.py_is_primitive
1405,/home/amandapotts/git/nltk/nltk/ccg/api.py_is_function,"def is_function(self):
""""""
Returns true if the category is a function application.
""""""
",[],0,[],/ccg/api.py_is_function
1406,/home/amandapotts/git/nltk/nltk/ccg/api.py_is_var,"def is_var(self):
""""""
Returns true if the category is a variable.
""""""
",[],0,[],/ccg/api.py_is_var
1407,/home/amandapotts/git/nltk/nltk/ccg/api.py_substitute,"def substitute(self, substitutions):
""""""
Takes a set of (var, category) substitutions, and replaces every
occurrence of the variable with the corresponding category.
""""""
",[],0,[],/ccg/api.py_substitute
1408,/home/amandapotts/git/nltk/nltk/ccg/api.py_can_unify,"def can_unify(self, other):
""""""
Determines whether two categories can be unified.
- Returns None if they cannot be unified
- Returns a list of necessary substitutions if they can.
""""""
",[],0,[],/ccg/api.py_can_unify
1409,/home/amandapotts/git/nltk/nltk/ccg/api.py___str__,"def __str__(self):
pass
",[],0,[],/ccg/api.py___str__
1410,/home/amandapotts/git/nltk/nltk/ccg/api.py___eq__,"def __eq__(self, other):
return (
self.__class__ is other.__class__
and self._comparison_key == other._comparison_key
)
",[],0,[],/ccg/api.py___eq__
1411,/home/amandapotts/git/nltk/nltk/ccg/api.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/ccg/api.py___ne__
1412,/home/amandapotts/git/nltk/nltk/ccg/api.py___lt__,"def __lt__(self, other):
if not isinstance(other, AbstractCCGCategory):
raise_unorderable_types(""<"", self, other)
if self.__class__ is other.__class__:
return self._comparison_key < other._comparison_key
else:
return self.__class__.__name__ < other.__class__.__name__
",[],0,[],/ccg/api.py___lt__
1413,/home/amandapotts/git/nltk/nltk/ccg/api.py___hash__,"def __hash__(self):
try:
return self._hash
except AttributeError:
self._hash = hash(self._comparison_key)
return self._hash
",[],0,[],/ccg/api.py___hash__
1414,/home/amandapotts/git/nltk/nltk/ccg/api.py___init__,"def __init__(self, prim_only=False):
""""""Initialize a variable (selects a new identifier)
:param prim_only: a boolean that determines whether the variable is
restricted to primitives
:type prim_only: bool
""""""
self._id = self.new_id()
self._prim_only = prim_only
self._comparison_key = self._id
",[],0,[],/ccg/api.py___init__
1415,/home/amandapotts/git/nltk/nltk/ccg/api.py_new_id,"def new_id(cls):
""""""
A class method allowing generation of unique variable identifiers.
""""""
cls._maxID = cls._maxID + 1
return cls._maxID - 1
",[],0,[],/ccg/api.py_new_id
1416,/home/amandapotts/git/nltk/nltk/ccg/api.py_reset_id,"def reset_id(cls):
cls._maxID = 0
",[],0,[],/ccg/api.py_reset_id
1417,/home/amandapotts/git/nltk/nltk/ccg/api.py_is_primitive,"def is_primitive(self):
return False
",[],0,[],/ccg/api.py_is_primitive
1418,/home/amandapotts/git/nltk/nltk/ccg/api.py_is_function,"def is_function(self):
return False
",[],0,[],/ccg/api.py_is_function
1419,/home/amandapotts/git/nltk/nltk/ccg/api.py_is_var,"def is_var(self):
return True
",[],0,[],/ccg/api.py_is_var
1420,/home/amandapotts/git/nltk/nltk/ccg/api.py_substitute,"def substitute(self, substitutions):
""""""If there is a substitution corresponding to this variable,
return the substituted category.
""""""
for var, cat in substitutions:
if var == self:
return cat
return self
",[],0,[],/ccg/api.py_substitute
1421,/home/amandapotts/git/nltk/nltk/ccg/api.py_can_unify,"def can_unify(self, other):
""""""If the variable can be replaced with other
a substitution is returned.
""""""
if other.is_primitive() or not self._prim_only:
return [(self, other)]
return None
",[],0,[],/ccg/api.py_can_unify
1422,/home/amandapotts/git/nltk/nltk/ccg/api.py_id,"def id(self):
return self._id
",[],0,[],/ccg/api.py_id
1423,/home/amandapotts/git/nltk/nltk/ccg/api.py___str__,"def __str__(self):
return ""_var"" + str(self._id)
",[],0,[],/ccg/api.py___str__
1424,/home/amandapotts/git/nltk/nltk/ccg/api.py___init__,"def __init__(self, dir, restrictions):
self._dir = dir
self._restrs = restrictions
self._comparison_key = (dir, tuple(restrictions))
",[],0,[],/ccg/api.py___init__
1425,/home/amandapotts/git/nltk/nltk/ccg/api.py_is_forward,"def is_forward(self):
return self._dir == ""/""
",[],0,[],/ccg/api.py_is_forward
1426,/home/amandapotts/git/nltk/nltk/ccg/api.py_is_backward,"def is_backward(self):
return self._dir == ""\\""
",[],0,[],/ccg/api.py_is_backward
1427,/home/amandapotts/git/nltk/nltk/ccg/api.py_dir,"def dir(self):
return self._dir
",[],0,[],/ccg/api.py_dir
1428,/home/amandapotts/git/nltk/nltk/ccg/api.py_restrs,"def restrs(self):
""""""A list of restrictions on the combinators.
'.' denotes that permuting operations are disallowed
',' denotes that function composition is disallowed
'_' denotes that the direction has variable restrictions.
(This is redundant in the current implementation of type-raising)
""""""
return self._restrs
",[],0,[],/ccg/api.py_restrs
1429,/home/amandapotts/git/nltk/nltk/ccg/api.py_is_variable,"def is_variable(self):
return self._restrs == ""_""
",[],0,[],/ccg/api.py_is_variable
1430,/home/amandapotts/git/nltk/nltk/ccg/api.py_can_unify,"def can_unify(self, other):
if other.is_variable():
return [(""_"", self.restrs())]
elif self.is_variable():
return [(""_"", other.restrs())]
else:
if self.restrs() == other.restrs():
return []
return None
",[],0,[],/ccg/api.py_can_unify
1431,/home/amandapotts/git/nltk/nltk/ccg/api.py_substitute,"def substitute(self, subs):
if not self.is_variable():
return self
for var, restrs in subs:
if var == ""_"":
return Direction(self._dir, restrs)
return self
",[],0,[],/ccg/api.py_substitute
1432,/home/amandapotts/git/nltk/nltk/ccg/api.py_can_compose,"def can_compose(self):
return "","" not in self._restrs
",[],0,[],/ccg/api.py_can_compose
1433,/home/amandapotts/git/nltk/nltk/ccg/api.py_can_cross,"def can_cross(self):
return ""."" not in self._restrs
",[],0,[],/ccg/api.py_can_cross
1434,/home/amandapotts/git/nltk/nltk/ccg/api.py___eq__,"def __eq__(self, other):
return (
self.__class__ is other.__class__
and self._comparison_key == other._comparison_key
)
",[],0,[],/ccg/api.py___eq__
1435,/home/amandapotts/git/nltk/nltk/ccg/api.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/ccg/api.py___ne__
1436,/home/amandapotts/git/nltk/nltk/ccg/api.py___lt__,"def __lt__(self, other):
if not isinstance(other, Direction):
raise_unorderable_types(""<"", self, other)
if self.__class__ is other.__class__:
return self._comparison_key < other._comparison_key
else:
return self.__class__.__name__ < other.__class__.__name__
",[],0,[],/ccg/api.py___lt__
1437,/home/amandapotts/git/nltk/nltk/ccg/api.py___hash__,"def __hash__(self):
try:
return self._hash
except AttributeError:
self._hash = hash(self._comparison_key)
return self._hash
",[],0,[],/ccg/api.py___hash__
1438,/home/amandapotts/git/nltk/nltk/ccg/api.py___str__,"def __str__(self):
r_str = """"
for r in self._restrs:
r_str = r_str + ""%s"" % r
return f""{self._dir}{r_str}""
",[],0,[],/ccg/api.py___str__
1439,/home/amandapotts/git/nltk/nltk/ccg/api.py___neg__,"def __neg__(self):
if self._dir == ""/"":
return Direction(""\\"", self._restrs)
else:
return Direction(""/"", self._restrs)
",[],0,[],/ccg/api.py___neg__
1440,/home/amandapotts/git/nltk/nltk/ccg/api.py___init__,"def __init__(self, categ, restrictions=[]):
self._categ = categ
self._restrs = restrictions
self._comparison_key = (categ, tuple(restrictions))
",[],0,[],/ccg/api.py___init__
1441,/home/amandapotts/git/nltk/nltk/ccg/api.py_is_primitive,"def is_primitive(self):
return True
",[],0,[],/ccg/api.py_is_primitive
1442,/home/amandapotts/git/nltk/nltk/ccg/api.py_is_function,"def is_function(self):
return False
",[],0,[],/ccg/api.py_is_function
1443,/home/amandapotts/git/nltk/nltk/ccg/api.py_is_var,"def is_var(self):
return False
",[],0,[],/ccg/api.py_is_var
1444,/home/amandapotts/git/nltk/nltk/ccg/api.py_restrs,"def restrs(self):
return self._restrs
",[],0,[],/ccg/api.py_restrs
1445,/home/amandapotts/git/nltk/nltk/ccg/api.py_categ,"def categ(self):
return self._categ
",[],0,[],/ccg/api.py_categ
1446,/home/amandapotts/git/nltk/nltk/ccg/api.py_substitute,"def substitute(self, subs):
return self
",[],0,[],/ccg/api.py_substitute
1447,/home/amandapotts/git/nltk/nltk/ccg/api.py_can_unify,"def can_unify(self, other):
if not other.is_primitive():
return None
if other.is_var():
return [(other, self)]
if other.categ() == self.categ():
for restr in self._restrs:
if restr not in other.restrs():
return None
return []
return None
",[],0,[],/ccg/api.py_can_unify
1448,/home/amandapotts/git/nltk/nltk/ccg/api.py___str__,"def __str__(self):
if self._restrs == []:
return ""%s"" % self._categ
restrictions = ""[%s]"" % "","".join(repr(r) for r in self._restrs)
return f""{self._categ}{restrictions}""
",[],0,[],/ccg/api.py___str__
1449,/home/amandapotts/git/nltk/nltk/ccg/api.py___init__,"def __init__(self, res, arg, dir):
self._res = res
self._arg = arg
self._dir = dir
self._comparison_key = (arg, dir, res)
",[],0,[],/ccg/api.py___init__
1450,/home/amandapotts/git/nltk/nltk/ccg/api.py_is_primitive,"def is_primitive(self):
return False
",[],0,[],/ccg/api.py_is_primitive
1451,/home/amandapotts/git/nltk/nltk/ccg/api.py_is_function,"def is_function(self):
return True
",[],0,[],/ccg/api.py_is_function
1452,/home/amandapotts/git/nltk/nltk/ccg/api.py_is_var,"def is_var(self):
return False
",[],0,[],/ccg/api.py_is_var
1453,/home/amandapotts/git/nltk/nltk/ccg/api.py_substitute,"def substitute(self, subs):
sub_res = self._res.substitute(subs)
sub_dir = self._dir.substitute(subs)
sub_arg = self._arg.substitute(subs)
return FunctionalCategory(sub_res, sub_arg, self._dir)
",[],0,[],/ccg/api.py_substitute
1454,/home/amandapotts/git/nltk/nltk/ccg/api.py_can_unify,"def can_unify(self, other):
if other.is_var():
return [(other, self)]
if other.is_function():
sa = self._res.can_unify(other.res())
sd = self._dir.can_unify(other.dir())
if sa is not None and sd is not None:
sb = self._arg.substitute(sa).can_unify(other.arg().substitute(sa))
if sb is not None:
return sa + sb
return None
",[],0,[],/ccg/api.py_can_unify
1455,/home/amandapotts/git/nltk/nltk/ccg/api.py_arg,"def arg(self):
return self._arg
",[],0,[],/ccg/api.py_arg
1456,/home/amandapotts/git/nltk/nltk/ccg/api.py_res,"def res(self):
return self._res
",[],0,[],/ccg/api.py_res
1457,/home/amandapotts/git/nltk/nltk/ccg/api.py_dir,"def dir(self):
return self._dir
",[],0,[],/ccg/api.py_dir
1458,/home/amandapotts/git/nltk/nltk/ccg/api.py___str__,"def __str__(self):
return f""({self._res}{self._dir}{self._arg})""
",[],0,[],/ccg/api.py___str__
1459,/home/amandapotts/git/nltk/nltk/ccg/combinator.py_can_combine,"def can_combine(self, function, argument):
pass
",[],0,[],/ccg/combinator.py_can_combine
1460,/home/amandapotts/git/nltk/nltk/ccg/combinator.py_combine,"def combine(self, function, argument):
pass
",[],0,[],/ccg/combinator.py_combine
1461,/home/amandapotts/git/nltk/nltk/ccg/combinator.py_can_combine,"def can_combine(self, left, right):
pass
",[],0,[],/ccg/combinator.py_can_combine
1462,/home/amandapotts/git/nltk/nltk/ccg/combinator.py_combine,"def combine(self, left, right):
pass
",[],0,[],/ccg/combinator.py_combine
1463,/home/amandapotts/git/nltk/nltk/ccg/combinator.py___init__,"def __init__(self, combinator, predicate, suffix=""""):
self._combinator = combinator
self._predicate = predicate
self._suffix = suffix
",[],0,[],/ccg/combinator.py___init__
1464,/home/amandapotts/git/nltk/nltk/ccg/combinator.py_can_combine,"def can_combine(self, left, right):
return self._combinator.can_combine(left, right) and self._predicate(
left, right
)
",[],0,[],/ccg/combinator.py_can_combine
1465,/home/amandapotts/git/nltk/nltk/ccg/combinator.py_combine,"def combine(self, left, right):
yield from self._combinator.combine(left, right)
",[],0,[],/ccg/combinator.py_combine
1466,/home/amandapotts/git/nltk/nltk/ccg/combinator.py___str__,"def __str__(self):
return f"">{self._combinator}{self._suffix}""
",[],0,[],/ccg/combinator.py___str__
1467,/home/amandapotts/git/nltk/nltk/ccg/combinator.py___init__,"def __init__(self, combinator, predicate, suffix=""""):
self._combinator = combinator
self._predicate = predicate
self._suffix = suffix
",[],0,[],/ccg/combinator.py___init__
1468,/home/amandapotts/git/nltk/nltk/ccg/combinator.py_can_combine,"def can_combine(self, left, right):
return self._combinator.can_combine(right, left) and self._predicate(
left, right
)
",[],0,[],/ccg/combinator.py_can_combine
1469,/home/amandapotts/git/nltk/nltk/ccg/combinator.py_combine,"def combine(self, left, right):
yield from self._combinator.combine(right, left)
",[],0,[],/ccg/combinator.py_combine
1470,/home/amandapotts/git/nltk/nltk/ccg/combinator.py___str__,"def __str__(self):
return f""<{self._combinator}{self._suffix}""
",[],0,[],/ccg/combinator.py___str__
1471,/home/amandapotts/git/nltk/nltk/ccg/combinator.py_can_combine,"def can_combine(self, function, argument):
if not function.is_function():
return False
return not function.arg().can_unify(argument) is None
",[],0,[],/ccg/combinator.py_can_combine
1472,/home/amandapotts/git/nltk/nltk/ccg/combinator.py_combine,"def combine(self, function, argument):
if not function.is_function():
return
subs = function.arg().can_unify(argument)
if subs is None:
return
yield function.res().substitute(subs)
",[],0,[],/ccg/combinator.py_combine
1473,/home/amandapotts/git/nltk/nltk/ccg/combinator.py___str__,"def __str__(self):
return """"
",[],0,[],/ccg/combinator.py___str__
1474,/home/amandapotts/git/nltk/nltk/ccg/combinator.py_forwardOnly,"def forwardOnly(left, right):
return left.dir().is_forward()
",[],0,[],/ccg/combinator.py_forwardOnly
1475,/home/amandapotts/git/nltk/nltk/ccg/combinator.py_backwardOnly,"def backwardOnly(left, right):
return right.dir().is_backward()
",[],0,[],/ccg/combinator.py_backwardOnly
1476,/home/amandapotts/git/nltk/nltk/ccg/combinator.py_can_combine,"def can_combine(self, function, argument):
if not (function.is_function() and argument.is_function()):
return False
if function.dir().can_compose() and argument.dir().can_compose():
return not function.arg().can_unify(argument.res()) is None
return False
",[],0,[],/ccg/combinator.py_can_combine
1477,/home/amandapotts/git/nltk/nltk/ccg/combinator.py_combine,"def combine(self, function, argument):
if not (function.is_function() and argument.is_function()):
return
if function.dir().can_compose() and argument.dir().can_compose():
subs = function.arg().can_unify(argument.res())
if subs is not None:
yield FunctionalCategory(
function.res().substitute(subs),
argument.arg().substitute(subs),
argument.dir(),
)
",[],0,[],/ccg/combinator.py_combine
1478,/home/amandapotts/git/nltk/nltk/ccg/combinator.py___str__,"def __str__(self):
return ""B""
",[],0,[],/ccg/combinator.py___str__
1479,/home/amandapotts/git/nltk/nltk/ccg/combinator.py_bothForward,"def bothForward(left, right):
return left.dir().is_forward() and right.dir().is_forward()
",[],0,[],/ccg/combinator.py_bothForward
1480,/home/amandapotts/git/nltk/nltk/ccg/combinator.py_bothBackward,"def bothBackward(left, right):
return left.dir().is_backward() and right.dir().is_backward()
",[],0,[],/ccg/combinator.py_bothBackward
1481,/home/amandapotts/git/nltk/nltk/ccg/combinator.py_crossedDirs,"def crossedDirs(left, right):
return left.dir().is_forward() and right.dir().is_backward()
",[],0,[],/ccg/combinator.py_crossedDirs
1482,/home/amandapotts/git/nltk/nltk/ccg/combinator.py_backwardBxConstraint,"def backwardBxConstraint(left, right):
if not crossedDirs(left, right):
return False
if not left.dir().can_cross() and right.dir().can_cross():
return False
return left.arg().is_primitive()
",[],0,[],/ccg/combinator.py_backwardBxConstraint
1483,/home/amandapotts/git/nltk/nltk/ccg/combinator.py_can_combine,"def can_combine(self, function, argument):
if function.is_primitive() or argument.is_primitive():
return False
if function.res().is_primitive():
return False
if not function.arg().is_primitive():
return False
if not (function.dir().can_compose() and argument.dir().can_compose()):
return False
return (function.res().arg() == argument.res()) and (
function.arg() == argument.arg()
)
",[],0,[],/ccg/combinator.py_can_combine
1484,/home/amandapotts/git/nltk/nltk/ccg/combinator.py_combine,"def combine(self, function, argument):
if self.can_combine(function, argument):
yield FunctionalCategory(
function.res().res(), argument.arg(), argument.dir()
)
",[],0,[],/ccg/combinator.py_combine
1485,/home/amandapotts/git/nltk/nltk/ccg/combinator.py___str__,"def __str__(self):
return ""S""
",[],0,[],/ccg/combinator.py___str__
1486,/home/amandapotts/git/nltk/nltk/ccg/combinator.py_forwardSConstraint,"def forwardSConstraint(left, right):
if not bothForward(left, right):
return False
return left.res().dir().is_forward() and left.arg().is_primitive()
",[],0,[],/ccg/combinator.py_forwardSConstraint
1487,/home/amandapotts/git/nltk/nltk/ccg/combinator.py_backwardSxConstraint,"def backwardSxConstraint(left, right):
if not left.dir().can_cross() and right.dir().can_cross():
return False
if not bothForward(left, right):
return False
return right.res().dir().is_backward() and right.arg().is_primitive()
",[],0,[],/ccg/combinator.py_backwardSxConstraint
1488,/home/amandapotts/git/nltk/nltk/ccg/combinator.py_innermostFunction,"def innermostFunction(categ):
while categ.res().is_function():
categ = categ.res()
return categ
",[],0,[],/ccg/combinator.py_innermostFunction
1489,/home/amandapotts/git/nltk/nltk/ccg/combinator.py_can_combine,"def can_combine(self, function, arg):
if not (arg.is_function() and arg.res().is_function()):
return False
arg = innermostFunction(arg)
subs = left.can_unify(arg_categ.arg())
if subs is not None:
return True
return False
",[],0,[],/ccg/combinator.py_can_combine
1490,/home/amandapotts/git/nltk/nltk/ccg/combinator.py_combine,"def combine(self, function, arg):
if not (
function.is_primitive() and arg.is_function() and arg.res().is_function()
):
return
arg = innermostFunction(arg)
subs = function.can_unify(arg.arg())
if subs is not None:
xcat = arg.res().substitute(subs)
yield FunctionalCategory(
xcat, FunctionalCategory(xcat, function, arg.dir()), -(arg.dir())
)
",[],0,[],/ccg/combinator.py_combine
1491,/home/amandapotts/git/nltk/nltk/ccg/combinator.py___str__,"def __str__(self):
return ""T""
",[],0,[],/ccg/combinator.py___str__
1492,/home/amandapotts/git/nltk/nltk/ccg/combinator.py_forwardTConstraint,"def forwardTConstraint(left, right):
arg = innermostFunction(right)
return arg.dir().is_backward() and arg.res().is_primitive()
",[],0,[],/ccg/combinator.py_forwardTConstraint
1493,/home/amandapotts/git/nltk/nltk/ccg/combinator.py_backwardTConstraint,"def backwardTConstraint(left, right):
arg = innermostFunction(left)
return arg.dir().is_forward() and arg.res().is_primitive()
",[],0,[],/ccg/combinator.py_backwardTConstraint
1494,/home/amandapotts/git/nltk/nltk/parse/transitionparser.py___init__,"def __init__(self, dep_graph):
""""""
:param dep_graph: the representation of an input in the form of dependency graph.
:type dep_graph: DependencyGraph where the dependencies are not specified.
""""""
self.stack = [0]  # The root element
self.buffer = list(range(1, len(dep_graph.nodes)))  # The rest is in the buffer
self.arcs = []  # empty set of arc
self._tokens = dep_graph.nodes
self._max_address = len(self.buffer)
",[],0,[],/parse/transitionparser.py___init__
1495,/home/amandapotts/git/nltk/nltk/parse/transitionparser.py___str__,"def __str__(self):
return (
""Stack : ""
+ str(self.stack)
+ ""  Buffer : ""
+ str(self.buffer)
+ ""   Arcs : ""
+ str(self.arcs)
)
",[],0,[],/parse/transitionparser.py___str__
1496,/home/amandapotts/git/nltk/nltk/parse/transitionparser.py__check_informative,"def _check_informative(self, feat, flag=False):
""""""
Check whether a feature is informative
The flag control whether ""_"" is informative or not
""""""
if feat is None:
return False
if feat == """":
return False
if flag is False:
if feat == ""_"":
return False
return True
",[],0,[],/parse/transitionparser.py__check_informative
1497,/home/amandapotts/git/nltk/nltk/parse/transitionparser.py_extract_features,"def extract_features(self):
""""""
Extract the set of features for the current configuration. Implement standard features as describe in
Table 3.2 (page 31) in Dependency Parsing book by Sandra Kubler, Ryan McDonal, Joakim Nivre.
Please note that these features are very basic.
:return: list(str)
""""""
result = []
if len(self.stack) > 0:
stack_idx0 = self.stack[len(self.stack) - 1]
token = self._tokens[stack_idx0]
if self._check_informative(token[""word""], True):
result.append(""STK_0_FORM_"" + token[""word""])
if ""lemma"" in token and self._check_informative(token[""lemma""]):
result.append(""STK_0_LEMMA_"" + token[""lemma""])
if self._check_informative(token[""tag""]):
result.append(""STK_0_POS_"" + token[""tag""])
if ""feats"" in token and self._check_informative(token[""feats""]):
feats = token[""feats""].split(""|"")
for feat in feats:
result.append(""STK_0_FEATS_"" + feat)
if len(self.stack) > 1:
stack_idx1 = self.stack[len(self.stack) - 2]
token = self._tokens[stack_idx1]
if self._check_informative(token[""tag""]):
result.append(""STK_1_POS_"" + token[""tag""])
left_most = 1000000
right_most = -1
dep_left_most = """"
dep_right_most = """"
for wi, r, wj in self.arcs:
if wi == stack_idx0:
if (wj > wi) and (wj > right_most):
right_most = wj
dep_right_most = r
if (wj < wi) and (wj < left_most):
left_most = wj
dep_left_most = r
if self._check_informative(dep_left_most):
result.append(""STK_0_LDEP_"" + dep_left_most)
if self._check_informative(dep_right_most):
result.append(""STK_0_RDEP_"" + dep_right_most)
if len(self.buffer) > 0:
buffer_idx0 = self.buffer[0]
token = self._tokens[buffer_idx0]
if self._check_informative(token[""word""], True):
result.append(""BUF_0_FORM_"" + token[""word""])
if ""lemma"" in token and self._check_informative(token[""lemma""]):
result.append(""BUF_0_LEMMA_"" + token[""lemma""])
if self._check_informative(token[""tag""]):
result.append(""BUF_0_POS_"" + token[""tag""])
if ""feats"" in token and self._check_informative(token[""feats""]):
feats = token[""feats""].split(""|"")
for feat in feats:
result.append(""BUF_0_FEATS_"" + feat)
if len(self.buffer) > 1:
buffer_idx1 = self.buffer[1]
token = self._tokens[buffer_idx1]
if self._check_informative(token[""word""], True):
result.append(""BUF_1_FORM_"" + token[""word""])
if self._check_informative(token[""tag""]):
result.append(""BUF_1_POS_"" + token[""tag""])
if len(self.buffer) > 2:
buffer_idx2 = self.buffer[2]
token = self._tokens[buffer_idx2]
if self._check_informative(token[""tag""]):
result.append(""BUF_2_POS_"" + token[""tag""])
if len(self.buffer) > 3:
buffer_idx3 = self.buffer[3]
token = self._tokens[buffer_idx3]
if self._check_informative(token[""tag""]):
result.append(""BUF_3_POS_"" + token[""tag""])
left_most = 1000000
right_most = -1
dep_left_most = """"
dep_right_most = """"
for wi, r, wj in self.arcs:
if wi == buffer_idx0:
if (wj > wi) and (wj > right_most):
right_most = wj
dep_right_most = r
if (wj < wi) and (wj < left_most):
left_most = wj
dep_left_most = r
if self._check_informative(dep_left_most):
result.append(""BUF_0_LDEP_"" + dep_left_most)
if self._check_informative(dep_right_most):
result.append(""BUF_0_RDEP_"" + dep_right_most)
return result
",[],0,[],/parse/transitionparser.py_extract_features
1498,/home/amandapotts/git/nltk/nltk/parse/transitionparser.py___init__,"def __init__(self, alg_option):
""""""
:param alg_option: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm
:type alg_option: str
""""""
self._algo = alg_option
if alg_option not in [
TransitionParser.ARC_STANDARD,
TransitionParser.ARC_EAGER,
]:
raise ValueError(
"" Currently we only support %s and %s ""
% (TransitionParser.ARC_STANDARD, TransitionParser.ARC_EAGER)
)
",[],0,[],/parse/transitionparser.py___init__
1499,/home/amandapotts/git/nltk/nltk/parse/transitionparser.py_left_arc,"def left_arc(self, conf, relation):
""""""
Note that the algorithm for left-arc is quite similar except for precondition for both arc-standard and arc-eager
:param configuration: is the current configuration
:return: A new configuration or -1 if the pre-condition is not satisfied
""""""
if (len(conf.buffer) <= 0) or (len(conf.stack) <= 0):
return -1
if conf.buffer[0] == 0:
return -1
idx_wi = conf.stack[len(conf.stack) - 1]
flag = True
if self._algo == TransitionParser.ARC_EAGER:
for idx_parent, r, idx_child in conf.arcs:
if idx_child == idx_wi:
flag = False
if flag:
conf.stack.pop()
idx_wj = conf.buffer[0]
conf.arcs.append((idx_wj, relation, idx_wi))
else:
return -1
",[],0,[],/parse/transitionparser.py_left_arc
1500,/home/amandapotts/git/nltk/nltk/parse/transitionparser.py_right_arc,"def right_arc(self, conf, relation):
""""""
Note that the algorithm for right-arc is DIFFERENT for arc-standard and arc-eager
:param configuration: is the current configuration
:return: A new configuration or -1 if the pre-condition is not satisfied
""""""
if (len(conf.buffer) <= 0) or (len(conf.stack) <= 0):
return -1
if self._algo == TransitionParser.ARC_STANDARD:
idx_wi = conf.stack.pop()
idx_wj = conf.buffer[0]
conf.buffer[0] = idx_wi
conf.arcs.append((idx_wi, relation, idx_wj))
else:  # arc-eager
idx_wi = conf.stack[len(conf.stack) - 1]
idx_wj = conf.buffer.pop(0)
conf.stack.append(idx_wj)
conf.arcs.append((idx_wi, relation, idx_wj))
",[],0,[],/parse/transitionparser.py_right_arc
1501,/home/amandapotts/git/nltk/nltk/parse/transitionparser.py_reduce,"def reduce(self, conf):
""""""
Note that the algorithm for reduce is only available for arc-eager
:param configuration: is the current configuration
:return: A new configuration or -1 if the pre-condition is not satisfied
""""""
if self._algo != TransitionParser.ARC_EAGER:
return -1
if len(conf.stack) <= 0:
return -1
idx_wi = conf.stack[len(conf.stack) - 1]
flag = False
for idx_parent, r, idx_child in conf.arcs:
if idx_child == idx_wi:
flag = True
if flag:
conf.stack.pop()  # reduce it
else:
return -1
",[],0,[],/parse/transitionparser.py_reduce
1502,/home/amandapotts/git/nltk/nltk/parse/transitionparser.py_shift,"def shift(self, conf):
""""""
Note that the algorithm for shift is the SAME for arc-standard and arc-eager
:param configuration: is the current configuration
:return: A new configuration or -1 if the pre-condition is not satisfied
""""""
if len(conf.buffer) <= 0:
return -1
idx_wi = conf.buffer.pop(0)
conf.stack.append(idx_wi)
",[],0,[],/parse/transitionparser.py_shift
1503,/home/amandapotts/git/nltk/nltk/parse/transitionparser.py___init__,"def __init__(self, algorithm):
""""""
:param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm
:type algorithm: str
""""""
if not (algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):
raise ValueError(
"" Currently we only support %s and %s ""
% (self.ARC_STANDARD, self.ARC_EAGER)
)
self._algorithm = algorithm
self._dictionary = {}
self._transition = {}
self._match_transition = {}
",[],0,[],/parse/transitionparser.py___init__
1504,/home/amandapotts/git/nltk/nltk/parse/transitionparser.py__get_dep_relation,"def _get_dep_relation(self, idx_parent, idx_child, depgraph):
p_node = depgraph.nodes[idx_parent]
c_node = depgraph.nodes[idx_child]
if c_node[""word""] is None:
return None  # Root word
if c_node[""head""] == p_node[""address""]:
return c_node[""rel""]
else:
return None
",[],0,[],/parse/transitionparser.py__get_dep_relation
1505,/home/amandapotts/git/nltk/nltk/parse/transitionparser.py__convert_to_binary_features,"def _convert_to_binary_features(self, features):
""""""
:param features: list of feature string which is needed to convert to binary features
:type features: list(str)
:return : string of binary features in libsvm format  which is 'featureID:value' pairs
""""""
unsorted_result = []
for feature in features:
self._dictionary.setdefault(feature, len(self._dictionary))
unsorted_result.append(self._dictionary[feature])
return "" "".join(
str(featureID) + "":1.0"" for featureID in sorted(unsorted_result)
)
",[],0,[],/parse/transitionparser.py__convert_to_binary_features
1506,/home/amandapotts/git/nltk/nltk/parse/transitionparser.py__is_projective,"def _is_projective(self, depgraph):
arc_list = []
for key in depgraph.nodes:
node = depgraph.nodes[key]
if ""head"" in node:
childIdx = node[""address""]
parentIdx = node[""head""]
if parentIdx is not None:
arc_list.append((parentIdx, childIdx))
for parentIdx, childIdx in arc_list:
if childIdx > parentIdx:
temp = childIdx
childIdx = parentIdx
parentIdx = temp
for k in range(childIdx + 1, parentIdx):
for m in range(len(depgraph.nodes)):
if (m < childIdx) or (m > parentIdx):
if (k, m) in arc_list:
return False
if (m, k) in arc_list:
return False
return True
",[],0,[],/parse/transitionparser.py__is_projective
1507,/home/amandapotts/git/nltk/nltk/parse/transitionparser.py__write_to_file,"def _write_to_file(self, key, binary_features, input_file):
""""""
write the binary features to input file and update the transition dictionary
""""""
self._transition.setdefault(key, len(self._transition) + 1)
self._match_transition[self._transition[key]] = key
input_str = str(self._transition[key]) + "" "" + binary_features + ""\n""
input_file.write(input_str.encode(""utf-8""))
",[],0,[],/parse/transitionparser.py__write_to_file
1508,/home/amandapotts/git/nltk/nltk/parse/transitionparser.py__create_training_examples_arc_std,"def _create_training_examples_arc_std(self, depgraphs, input_file):
""""""
Create the training example in the libsvm format and write it to the input_file.
Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)
""""""
operation = Transition(self.ARC_STANDARD)
count_proj = 0
training_seq = []
for depgraph in depgraphs:
if not self._is_projective(depgraph):
continue
count_proj += 1
conf = Configuration(depgraph)
while len(conf.buffer) > 0:
b0 = conf.buffer[0]
features = conf.extract_features()
binary_features = self._convert_to_binary_features(features)
if len(conf.stack) > 0:
s0 = conf.stack[len(conf.stack) - 1]
rel = self._get_dep_relation(b0, s0, depgraph)
if rel is not None:
key = Transition.LEFT_ARC + "":"" + rel
self._write_to_file(key, binary_features, input_file)
operation.left_arc(conf, rel)
training_seq.append(key)
continue
rel = self._get_dep_relation(s0, b0, depgraph)
if rel is not None:
precondition = True
maxID = conf._max_address
for w in range(maxID + 1):
if w != b0:
relw = self._get_dep_relation(b0, w, depgraph)
if relw is not None:
if (b0, relw, w) not in conf.arcs:
precondition = False
if precondition:
key = Transition.RIGHT_ARC + "":"" + rel
self._write_to_file(key, binary_features, input_file)
operation.right_arc(conf, rel)
training_seq.append(key)
continue
key = Transition.SHIFT
self._write_to_file(key, binary_features, input_file)
operation.shift(conf)
training_seq.append(key)
print("" Number of training examples : "" + str(len(depgraphs)))
print("" Number of valid (projective) examples : "" + str(count_proj))
return training_seq
",[],0,[],/parse/transitionparser.py__create_training_examples_arc_std
1509,/home/amandapotts/git/nltk/nltk/parse/transitionparser.py__create_training_examples_arc_eager,"def _create_training_examples_arc_eager(self, depgraphs, input_file):
""""""
Create the training example in the libsvm format and write it to the input_file.
Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre
""""""
operation = Transition(self.ARC_EAGER)
countProj = 0
training_seq = []
for depgraph in depgraphs:
if not self._is_projective(depgraph):
continue
countProj += 1
conf = Configuration(depgraph)
while len(conf.buffer) > 0:
b0 = conf.buffer[0]
features = conf.extract_features()
binary_features = self._convert_to_binary_features(features)
if len(conf.stack) > 0:
s0 = conf.stack[len(conf.stack) - 1]
rel = self._get_dep_relation(b0, s0, depgraph)
if rel is not None:
key = Transition.LEFT_ARC + "":"" + rel
self._write_to_file(key, binary_features, input_file)
operation.left_arc(conf, rel)
training_seq.append(key)
continue
rel = self._get_dep_relation(s0, b0, depgraph)
if rel is not None:
key = Transition.RIGHT_ARC + "":"" + rel
self._write_to_file(key, binary_features, input_file)
operation.right_arc(conf, rel)
training_seq.append(key)
continue
flag = False
for k in range(s0):
if self._get_dep_relation(k, b0, depgraph) is not None:
flag = True
if self._get_dep_relation(b0, k, depgraph) is not None:
flag = True
if flag:
key = Transition.REDUCE
self._write_to_file(key, binary_features, input_file)
operation.reduce(conf)
training_seq.append(key)
continue
key = Transition.SHIFT
self._write_to_file(key, binary_features, input_file)
operation.shift(conf)
training_seq.append(key)
print("" Number of training examples : "" + str(len(depgraphs)))
print("" Number of valid (projective) examples : "" + str(countProj))
return training_seq
",[],0,[],/parse/transitionparser.py__create_training_examples_arc_eager
1510,/home/amandapotts/git/nltk/nltk/parse/transitionparser.py_train,"def train(self, depgraphs, modelfile, verbose=True):
""""""
:param depgraphs : list of DependencyGraph as the training data
:type depgraphs : DependencyGraph
:param modelfile : file name to save the trained model
:type modelfile : str
""""""
try:
input_file = tempfile.NamedTemporaryFile(
prefix=""transition_parse.train"", dir=tempfile.gettempdir(), delete=False
)
if self._algorithm == self.ARC_STANDARD:
self._create_training_examples_arc_std(depgraphs, input_file)
else:
self._create_training_examples_arc_eager(depgraphs, input_file)
input_file.close()
x_train, y_train = load_svmlight_file(input_file.name)
model = svm.SVC(
kernel=""poly"",
degree=2,
coef0=0,
gamma=0.2,
C=0.5,
verbose=verbose,
probability=True,
)
model.fit(x_train, y_train)
pickle.dump(model, open(modelfile, ""wb""))
finally:
remove(input_file.name)
",[],0,[],/parse/transitionparser.py_train
1511,/home/amandapotts/git/nltk/nltk/parse/transitionparser.py_parse,"def parse(self, depgraphs, modelFile):
""""""
:param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy
:type depgraphs: list(DependencyGraph)
:param modelfile: the model file
:type modelfile: str
:return: list (DependencyGraph) with the 'head' and 'rel' information
""""""
result = []
model = pickle.load(open(modelFile, ""rb""))
operation = Transition(self._algorithm)
for depgraph in depgraphs:
conf = Configuration(depgraph)
while len(conf.buffer) > 0:
features = conf.extract_features()
col = []
row = []
data = []
for feature in features:
if feature in self._dictionary:
col.append(self._dictionary[feature])
row.append(0)
data.append(1.0)
np_col = array(sorted(col))  # NB : index must be sorted
np_row = array(row)
np_data = array(data)
x_test = sparse.csr_matrix(
(np_data, (np_row, np_col)), shape=(1, len(self._dictionary))
)
prob_dict = {}
pred_prob = model.predict_proba(x_test)[0]
for i in range(len(pred_prob)):
prob_dict[i] = pred_prob[i]
sorted_Prob = sorted(prob_dict.items(), key=itemgetter(1), reverse=True)
for y_pred_idx, confidence in sorted_Prob:
y_pred = model.classes_[y_pred_idx]
if y_pred in self._match_transition:
strTransition = self._match_transition[y_pred]
baseTransition = strTransition.split("":"")[0]
if baseTransition == Transition.LEFT_ARC:
if (
operation.left_arc(conf, strTransition.split("":"")[1])
!= -1
):
break
elif baseTransition == Transition.RIGHT_ARC:
if (
operation.right_arc(conf, strTransition.split("":"")[1])
!= -1
):
break
elif baseTransition == Transition.REDUCE:
if operation.reduce(conf) != -1:
break
elif baseTransition == Transition.SHIFT:
if operation.shift(conf) != -1:
break
else:
raise ValueError(
""The predicted transition is not recognized, expected errors""
)
new_depgraph = deepcopy(depgraph)
for key in new_depgraph.nodes:
node = new_depgraph.nodes[key]
node[""rel""] = """"
node[""head""] = 0
for head, rel, child in conf.arcs:
c_node = new_depgraph.nodes[child]
c_node[""head""] = head
c_node[""rel""] = rel
result.append(new_depgraph)
return result
",[],0,[],/parse/transitionparser.py_parse
1512,/home/amandapotts/git/nltk/nltk/parse/transitionparser.py_demo,"def demo():
""""""
>>> from nltk.parse import DependencyGraph, DependencyEvaluator
>>> from nltk.parse.transitionparser import TransitionParser, Configuration, Transition
>>> gold_sent = DependencyGraph(\""""""
... Economic  JJ     2      ATT
... news  NN     3       SBJ
... has       VBD       0       ROOT
... little      JJ      5       ATT
... effect   NN     3       OBJ
... on     IN      5       ATT
... financial       JJ       8       ATT
... markets    NNS      6       PC
... .    .      3       PU
... \"""""")
>>> conf = Configuration(gold_sent)
>>> print(', '.join(conf.extract_features()))
STK_0_POS_TOP, BUF_0_FORM_Economic, BUF_0_LEMMA_Economic, BUF_0_POS_JJ, BUF_1_FORM_news, BUF_1_POS_NN, BUF_2_POS_VBD, BUF_3_POS_JJ
Check the Initialized Configuration
>>> print(conf)
Stack : [0]  Buffer : [1, 2, 3, 4, 5, 6, 7, 8, 9]   Arcs : []
A. Do some transition checks for ARC-STANDARD
>>> operation = Transition('arc-standard')
>>> operation.shift(conf)
>>> operation.left_arc(conf, ""ATT"")
>>> operation.shift(conf)
>>> operation.left_arc(conf,""SBJ"")
>>> operation.shift(conf)
>>> operation.shift(conf)
>>> operation.left_arc(conf, ""ATT"")
>>> operation.shift(conf)
>>> operation.shift(conf)
>>> operation.shift(conf)
>>> operation.left_arc(conf, ""ATT"")
Middle Configuration and Features Check
>>> print(conf)
Stack : [0, 3, 5, 6]  Buffer : [8, 9]   Arcs : [(2, 'ATT', 1), (3, 'SBJ', 2), (5, 'ATT', 4), (8, 'ATT', 7)]
>>> print(', '.join(conf.extract_features()))
STK_0_FORM_on, STK_0_LEMMA_on, STK_0_POS_IN, STK_1_POS_NN, BUF_0_FORM_markets, BUF_0_LEMMA_markets, BUF_0_POS_NNS, BUF_1_FORM_., BUF_1_POS_., BUF_0_LDEP_ATT
>>> operation.right_arc(conf, ""PC"")
>>> operation.right_arc(conf, ""ATT"")
>>> operation.right_arc(conf, ""OBJ"")
>>> operation.shift(conf)
>>> operation.right_arc(conf, ""PU"")
>>> operation.right_arc(conf, ""ROOT"")
>>> operation.shift(conf)
Terminated Configuration Check
>>> print(conf)
Stack : [0]  Buffer : []   Arcs : [(2, 'ATT', 1), (3, 'SBJ', 2), (5, 'ATT', 4), (8, 'ATT', 7), (6, 'PC', 8), (5, 'ATT', 6), (3, 'OBJ', 5), (3, 'PU', 9), (0, 'ROOT', 3)]
B. Do some transition checks for ARC-EAGER
>>> conf = Configuration(gold_sent)
>>> operation = Transition('arc-eager')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'ATT')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'SBJ')
>>> operation.right_arc(conf,'ROOT')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'ATT')
>>> operation.right_arc(conf,'OBJ')
>>> operation.right_arc(conf,'ATT')
>>> operation.shift(conf)
>>> operation.left_arc(conf,'ATT')
>>> operation.right_arc(conf,'PC')
>>> operation.reduce(conf)
>>> operation.reduce(conf)
>>> operation.reduce(conf)
>>> operation.right_arc(conf,'PU')
>>> print(conf)
Stack : [0, 3, 9]  Buffer : []   Arcs : [(2, 'ATT', 1), (3, 'SBJ', 2), (0, 'ROOT', 3), (5, 'ATT', 4), (3, 'OBJ', 5), (5, 'ATT', 6), (8, 'ATT', 7), (6, 'PC', 8), (3, 'PU', 9)]
A. Check the ARC-STANDARD training
>>> import tempfile
>>> import os
>>> input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)
>>> parser_std = TransitionParser('arc-standard')
>>> print(', '.join(parser_std._create_training_examples_arc_std([gold_sent], input_file)))
Number of training examples : 1
Number of valid (projective) examples : 1
SHIFT, LEFTARC:ATT, SHIFT, LEFTARC:SBJ, SHIFT, SHIFT, LEFTARC:ATT, SHIFT, SHIFT, SHIFT, LEFTARC:ATT, RIGHTARC:PC, RIGHTARC:ATT, RIGHTARC:OBJ, SHIFT, RIGHTARC:PU, RIGHTARC:ROOT, SHIFT
>>> parser_std.train([gold_sent],'temp.arcstd.model', verbose=False)
Number of training examples : 1
Number of valid (projective) examples : 1
>>> input_file.close()
>>> remove(input_file.name)
B. Check the ARC-EAGER training
>>> input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(),delete=False)
>>> parser_eager = TransitionParser('arc-eager')
>>> print(', '.join(parser_eager._create_training_examples_arc_eager([gold_sent], input_file)))
Number of training examples : 1
Number of valid (projective) examples : 1
SHIFT, LEFTARC:ATT, SHIFT, LEFTARC:SBJ, RIGHTARC:ROOT, SHIFT, LEFTARC:ATT, RIGHTARC:OBJ, RIGHTARC:ATT, SHIFT, LEFTARC:ATT, RIGHTARC:PC, REDUCE, REDUCE, REDUCE, RIGHTARC:PU
>>> parser_eager.train([gold_sent],'temp.arceager.model', verbose=False)
Number of training examples : 1
Number of valid (projective) examples : 1
>>> input_file.close()
>>> remove(input_file.name)
A. Check the ARC-STANDARD parser
>>> result = parser_std.parse([gold_sent], 'temp.arcstd.model')
>>> de = DependencyEvaluator(result, [gold_sent])
>>> de.eval() >= (0, 0)
True
B. Check the ARC-EAGER parser
>>> result = parser_eager.parse([gold_sent], 'temp.arceager.model')
>>> de = DependencyEvaluator(result, [gold_sent])
>>> de.eval() >= (0, 0)
True
Remove test temporary files
>>> remove('temp.arceager.model')
>>> remove('temp.arcstd.model')
Note that result is very poor because of only one training example.
""""""
",[],0,[],/parse/transitionparser.py_demo
1513,/home/amandapotts/git/nltk/nltk/parse/earleychart.py_initialize,"def initialize(self):
self._edgelists = tuple([] for x in self._positions())
self._edge_to_cpls = {}
self._indexes = {}
",[],0,[],/parse/earleychart.py_initialize
1514,/home/amandapotts/git/nltk/nltk/parse/earleychart.py_edges,"def edges(self):
return list(self.iteredges())
",[],0,[],/parse/earleychart.py_edges
1515,/home/amandapotts/git/nltk/nltk/parse/earleychart.py_iteredges,"def iteredges(self):
return (edge for edgelist in self._edgelists for edge in edgelist)
",[],0,[],/parse/earleychart.py_iteredges
1516,/home/amandapotts/git/nltk/nltk/parse/earleychart.py_select,"def select(self, end, **restrictions):
edgelist = self._edgelists[end]
if restrictions == {}:
return iter(edgelist)
restr_keys = sorted(restrictions.keys())
restr_keys = tuple(restr_keys)
if restr_keys not in self._indexes:
self._add_index(restr_keys)
vals = tuple(restrictions[key] for key in restr_keys)
return iter(self._indexes[restr_keys][end].get(vals, []))
",[],0,[],/parse/earleychart.py_select
1517,/home/amandapotts/git/nltk/nltk/parse/earleychart.py__add_index,"def _add_index(self, restr_keys):
for key in restr_keys:
if not hasattr(EdgeI, key):
raise ValueError(""Bad restriction: %s"" % key)
index = self._indexes[restr_keys] = tuple({} for x in self._positions())
for end, edgelist in enumerate(self._edgelists):
this_index = index[end]
for edge in edgelist:
vals = tuple(getattr(edge, key)() for key in restr_keys)
this_index.setdefault(vals, []).append(edge)
",[],0,[],/parse/earleychart.py__add_index
1518,/home/amandapotts/git/nltk/nltk/parse/earleychart.py__register_with_indexes,"def _register_with_indexes(self, edge):
end = edge.end()
for restr_keys, index in self._indexes.items():
vals = tuple(getattr(edge, key)() for key in restr_keys)
index[end].setdefault(vals, []).append(edge)
",[],0,[],/parse/earleychart.py__register_with_indexes
1519,/home/amandapotts/git/nltk/nltk/parse/earleychart.py__append_edge,"def _append_edge(self, edge):
self._edgelists[edge.end()].append(edge)
",[],0,[],/parse/earleychart.py__append_edge
1520,/home/amandapotts/git/nltk/nltk/parse/earleychart.py__positions,"def _positions(self):
return range(self.num_leaves() + 1)
",[],0,[],/parse/earleychart.py__positions
1521,/home/amandapotts/git/nltk/nltk/parse/earleychart.py_select,"def select(self, end, **restrictions):
edgelist = self._edgelists[end]
if restrictions == {}:
return iter(edgelist)
restr_keys = sorted(restrictions.keys())
restr_keys = tuple(restr_keys)
if restr_keys not in self._indexes:
self._add_index(restr_keys)
vals = tuple(
self._get_type_if_possible(restrictions[key]) for key in restr_keys
)
return iter(self._indexes[restr_keys][end].get(vals, []))
",[],0,[],/parse/earleychart.py_select
1522,/home/amandapotts/git/nltk/nltk/parse/earleychart.py__add_index,"def _add_index(self, restr_keys):
for key in restr_keys:
if not hasattr(EdgeI, key):
raise ValueError(""Bad restriction: %s"" % key)
index = self._indexes[restr_keys] = tuple({} for x in self._positions())
for end, edgelist in enumerate(self._edgelists):
this_index = index[end]
for edge in edgelist:
vals = tuple(
self._get_type_if_possible(getattr(edge, key)())
for key in restr_keys
)
this_index.setdefault(vals, []).append(edge)
",[],0,[],/parse/earleychart.py__add_index
1523,/home/amandapotts/git/nltk/nltk/parse/earleychart.py__register_with_indexes,"def _register_with_indexes(self, edge):
end = edge.end()
for restr_keys, index in self._indexes.items():
vals = tuple(
self._get_type_if_possible(getattr(edge, key)()) for key in restr_keys
)
index[end].setdefault(vals, []).append(edge)
",[],0,[],/parse/earleychart.py__register_with_indexes
1524,/home/amandapotts/git/nltk/nltk/parse/earleychart.py__apply_incomplete,"def _apply_incomplete(self, chart, grammar, left_edge):
end = left_edge.end()
for right_edge in chart.select(
start=end, end=end, is_complete=True, lhs=left_edge.nextsym()
):
new_edge = left_edge.move_dot_forward(right_edge.end())
if chart.insert_with_backpointer(new_edge, left_edge, right_edge):
yield new_edge
",[],0,[],/parse/earleychart.py__apply_incomplete
1525,/home/amandapotts/git/nltk/nltk/parse/earleychart.py_apply,"def apply(self, chart, grammar, edge):
if not isinstance(edge, LeafEdge):
yield from self._fundamental_rule.apply(chart, grammar, edge)
",[],0,[],/parse/earleychart.py_apply
1526,/home/amandapotts/git/nltk/nltk/parse/earleychart.py_apply,"def apply(self, chart, grammar, edge):
if isinstance(edge, LeafEdge):
yield from self._fundamental_rule.apply(chart, grammar, edge)
",[],0,[],/parse/earleychart.py_apply
1527,/home/amandapotts/git/nltk/nltk/parse/earleychart.py_apply,"def apply(self, chart, grammar, edge):
if edge.is_complete():
yield from self._apply_complete(chart, grammar, edge)
",[],0,[],/parse/earleychart.py_apply
1528,/home/amandapotts/git/nltk/nltk/parse/earleychart.py__apply_incomplete,"def _apply_incomplete(self, chart, grammar, left_edge):
fr = self._fundamental_rule
end = left_edge.end()
for right_edge in chart.select(
start=end, end=end, is_complete=True, lhs=left_edge.nextsym()
):
yield from fr.apply(chart, grammar, left_edge, right_edge)
",[],0,[],/parse/earleychart.py__apply_incomplete
1529,/home/amandapotts/git/nltk/nltk/parse/earleychart.py___init__,"def __init__(
self,
grammar,
strategy=BU_LC_INCREMENTAL_STRATEGY,
trace=0,
trace_chart_width=50,
chart_class=IncrementalChart,
",[],0,[],/parse/earleychart.py___init__
1530,/home/amandapotts/git/nltk/nltk/parse/earleychart.py_chart_parse,"def chart_parse(self, tokens, trace=None):
if trace is None:
trace = self._trace
trace_new_edges = self._trace_new_edges
tokens = list(tokens)
self._grammar.check_coverage(tokens)
chart = self._chart_class(tokens)
grammar = self._grammar
trace_edge_width = self._trace_chart_width // (chart.num_leaves() + 1)
if trace:
print(chart.pretty_format_leaves(trace_edge_width))
for axiom in self._axioms:
new_edges = list(axiom.apply(chart, grammar))
trace_new_edges(chart, axiom, new_edges, trace, trace_edge_width)
inference_rules = self._inference_rules
for end in range(chart.num_leaves() + 1):
if trace > 1:
print(""\n* Processing queue:"", end, ""\n"")
agenda = list(chart.select(end=end))
while agenda:
edge = agenda.pop()
for rule in inference_rules:
new_edges = list(rule.apply(chart, grammar, edge))
trace_new_edges(chart, rule, new_edges, trace, trace_edge_width)
for new_edge in new_edges:
if new_edge.end() == end:
agenda.append(new_edge)
return chart
",[],0,[],/parse/earleychart.py_chart_parse
1531,/home/amandapotts/git/nltk/nltk/parse/earleychart.py___init__,"def __init__(self, grammar, **parser_args):
IncrementalChartParser.__init__(self, grammar, EARLEY_STRATEGY, **parser_args)
",[],0,[],/parse/earleychart.py___init__
1532,/home/amandapotts/git/nltk/nltk/parse/earleychart.py___init__,"def __init__(self, grammar, **parser_args):
IncrementalChartParser.__init__(
self, grammar, TD_INCREMENTAL_STRATEGY, **parser_args
)
",[],0,[],/parse/earleychart.py___init__
1533,/home/amandapotts/git/nltk/nltk/parse/earleychart.py___init__,"def __init__(self, grammar, **parser_args):
IncrementalChartParser.__init__(
self, grammar, BU_INCREMENTAL_STRATEGY, **parser_args
)
",[],0,[],/parse/earleychart.py___init__
1534,/home/amandapotts/git/nltk/nltk/parse/earleychart.py___init__,"def __init__(self, grammar, **parser_args):
IncrementalChartParser.__init__(
self, grammar, BU_LC_INCREMENTAL_STRATEGY, **parser_args
)
",[],0,[],/parse/earleychart.py___init__
1535,/home/amandapotts/git/nltk/nltk/parse/earleychart.py___init__,"def __init__(self, grammar, **parser_args):
if not grammar.is_nonempty():
raise ValueError(
""IncrementalLeftCornerParser only works for grammars ""
""without empty productions.""
)
IncrementalChartParser.__init__(
self, grammar, LC_INCREMENTAL_STRATEGY, **parser_args
)
",[],0,[],/parse/earleychart.py___init__
1536,/home/amandapotts/git/nltk/nltk/parse/earleychart.py___init__,"def __init__(
self,
grammar,
strategy=BU_LC_INCREMENTAL_FEATURE_STRATEGY,
trace_chart_width=20,
chart_class=FeatureIncrementalChart,
",[],0,[],/parse/earleychart.py___init__
1537,/home/amandapotts/git/nltk/nltk/parse/earleychart.py___init__,"def __init__(self, grammar, **parser_args):
FeatureIncrementalChartParser.__init__(
self, grammar, EARLEY_FEATURE_STRATEGY, **parser_args
)
",[],0,[],/parse/earleychart.py___init__
1538,/home/amandapotts/git/nltk/nltk/parse/earleychart.py___init__,"def __init__(self, grammar, **parser_args):
FeatureIncrementalChartParser.__init__(
self, grammar, TD_INCREMENTAL_FEATURE_STRATEGY, **parser_args
)
",[],0,[],/parse/earleychart.py___init__
1539,/home/amandapotts/git/nltk/nltk/parse/earleychart.py___init__,"def __init__(self, grammar, **parser_args):
FeatureIncrementalChartParser.__init__(
self, grammar, BU_INCREMENTAL_FEATURE_STRATEGY, **parser_args
)
",[],0,[],/parse/earleychart.py___init__
1540,/home/amandapotts/git/nltk/nltk/parse/earleychart.py___init__,"def __init__(self, grammar, **parser_args):
FeatureIncrementalChartParser.__init__(
self, grammar, BU_LC_INCREMENTAL_FEATURE_STRATEGY, **parser_args
)
",[],0,[],/parse/earleychart.py___init__
1541,/home/amandapotts/git/nltk/nltk/parse/earleychart.py_demo,"def demo(
print_times=True,
print_grammar=False,
print_trees=True,
trace=2,
sent=""I saw John with a dog with my cookie"",
numparses=5,
",[],0,[],/parse/earleychart.py_demo
1542,/home/amandapotts/git/nltk/nltk/parse/nonprojectivedependencyparser.py___init__,"def __init__(self):
if self.__class__ == DependencyScorerI:
raise TypeError(""DependencyScorerI is an abstract interface"")
",[],0,[],/parse/nonprojectivedependencyparser.py___init__
1543,/home/amandapotts/git/nltk/nltk/parse/nonprojectivedependencyparser.py_train,"def train(self, graphs):
""""""
:type graphs: list(DependencyGraph)
:param graphs: A list of dependency graphs to train the scorer.
Typically the edges present in the graphs can be used as
positive training examples, and the edges not present as negative
examples.
""""""
raise NotImplementedError()
",[],0,[],/parse/nonprojectivedependencyparser.py_train
1544,/home/amandapotts/git/nltk/nltk/parse/nonprojectivedependencyparser.py_score,"def score(self, graph):
""""""
:type graph: DependencyGraph
:param graph: A dependency graph whose set of edges need to be
scored.
:rtype: A three-dimensional list of numbers.
:return: The score is returned in a multidimensional(3) list, such
that the outer-dimension refers to the head, and the
inner-dimension refers to the dependencies.  For instance,
scores[0][1] would reference the list of scores corresponding to
arcs from node 0 to node 1.  The node's 'address' field can be used
to determine its number identification.
For further illustration, a score list corresponding to Fig.2 of
Keith Hall's 'K-best Spanning Tree Parsing' paper::
scores = [[[], [5],  [1],  [1]],
[[], [],   [11], [4]],
[[], [10], [],   [5]],
[[], [8],  [8],  []]]
When used in conjunction with a MaxEntClassifier, each score would
correspond to the confidence of a particular edge being classified
with the positive training examples.
""""""
raise NotImplementedError()
",[],0,[],/parse/nonprojectivedependencyparser.py_score
1545,/home/amandapotts/git/nltk/nltk/parse/nonprojectivedependencyparser.py___init__,"def __init__(self):
pass  # Do nothing without throwing error
",[],0,[],/parse/nonprojectivedependencyparser.py___init__
1546,/home/amandapotts/git/nltk/nltk/parse/nonprojectivedependencyparser.py_train,"def train(self, graphs):
""""""
Trains a ``NaiveBayesClassifier`` using the edges present in
graphs list as positive examples, the edges not present as
negative examples.  Uses a feature vector of head-word,
head-tag, child-word, and child-tag.
:type graphs: list(DependencyGraph)
:param graphs: A list of dependency graphs to train the scorer.
""""""
from nltk.classify import NaiveBayesClassifier
labeled_examples = []
for graph in graphs:
for head_node in graph.nodes.values():
for child_index, child_node in graph.nodes.items():
if child_index in head_node[""deps""]:
label = ""T""
else:
label = ""F""
labeled_examples.append(
(
dict(
a=head_node[""word""],
b=head_node[""tag""],
c=child_node[""word""],
d=child_node[""tag""],
),
label,
)
)
self.classifier = NaiveBayesClassifier.train(labeled_examples)
",[],0,[],/parse/nonprojectivedependencyparser.py_train
1547,/home/amandapotts/git/nltk/nltk/parse/nonprojectivedependencyparser.py_score,"def score(self, graph):
""""""
Converts the graph into a feature-based representation of
each edge, and then assigns a score to each based on the
confidence of the classifier in assigning it to the
positive label.  Scores are returned in a multidimensional list.
:type graph: DependencyGraph
:param graph: A dependency graph to score.
:rtype: 3 dimensional list
:return: Edge scores for the graph parameter.
""""""
edges = []
for head_node in graph.nodes.values():
for child_node in graph.nodes.values():
edges.append(
dict(
a=head_node[""word""],
b=head_node[""tag""],
c=child_node[""word""],
d=child_node[""tag""],
)
)
edge_scores = []
row = []
count = 0
for pdist in self.classifier.prob_classify_many(edges):
logger.debug(""%.4f %.4f"", pdist.prob(""T""), pdist.prob(""F""))
row.append([math.log(pdist.prob(""T"") + 0.00000000001)])
count += 1
if count == len(graph.nodes):
edge_scores.append(row)
row = []
count = 0
return edge_scores
",[],0,[],/parse/nonprojectivedependencyparser.py_score
1548,/home/amandapotts/git/nltk/nltk/parse/nonprojectivedependencyparser.py_train,"def train(self, graphs):
print(""Training..."")
",[],0,[],/parse/nonprojectivedependencyparser.py_train
1549,/home/amandapotts/git/nltk/nltk/parse/nonprojectivedependencyparser.py_score,"def score(self, graph):
return [
[[], [5], [1], [1]],
[[], [], [11], [4]],
[[], [10], [], [5]],
[[], [8], [8], []],
]
",[],0,[],/parse/nonprojectivedependencyparser.py_score
1550,/home/amandapotts/git/nltk/nltk/parse/nonprojectivedependencyparser.py___init__,"def __init__(self):
""""""
Creates a new non-projective parser.
""""""
logging.debug(""initializing prob. nonprojective..."")
",[],0,[],/parse/nonprojectivedependencyparser.py___init__
1551,/home/amandapotts/git/nltk/nltk/parse/nonprojectivedependencyparser.py_train,"def train(self, graphs, dependency_scorer):
""""""
Trains a ``DependencyScorerI`` from a set of ``DependencyGraph`` objects,
and establishes this as the parser's scorer.  This is used to
initialize the scores on a ``DependencyGraph`` during the parsing
procedure.
:type graphs: list(DependencyGraph)
:param graphs: A list of dependency graphs to train the scorer.
:type dependency_scorer: DependencyScorerI
:param dependency_scorer: A scorer which implements the
``DependencyScorerI`` interface.
""""""
self._scorer = dependency_scorer
self._scorer.train(graphs)
",[],0,[],/parse/nonprojectivedependencyparser.py_train
1552,/home/amandapotts/git/nltk/nltk/parse/nonprojectivedependencyparser.py_initialize_edge_scores,"def initialize_edge_scores(self, graph):
""""""
Assigns a score to every edge in the ``DependencyGraph`` graph.
These scores are generated via the parser's scorer which
was assigned during the training process.
:type graph: DependencyGraph
:param graph: A dependency graph to assign scores to.
""""""
self.scores = self._scorer.score(graph)
",[],0,[],/parse/nonprojectivedependencyparser.py_initialize_edge_scores
1553,/home/amandapotts/git/nltk/nltk/parse/nonprojectivedependencyparser.py_collapse_nodes,"def collapse_nodes(self, new_node, cycle_path, g_graph, b_graph, c_graph):
""""""
Takes a list of nodes that have been identified to belong to a cycle,
and collapses them into on larger node.  The arcs of all nodes in
the graph must be updated to account for this.
:type new_node: Node.
:param new_node: A Node (Dictionary) to collapse the cycle nodes into.
:type cycle_path: A list of integers.
:param cycle_path: A list of node addresses, each of which is in the cycle.
:type g_graph, b_graph, c_graph: DependencyGraph
:param g_graph, b_graph, c_graph: Graphs which need to be updated.
""""""
logger.debug(""Collapsing nodes..."")
for cycle_node_index in cycle_path:
g_graph.remove_by_address(cycle_node_index)
g_graph.add_node(new_node)
g_graph.redirect_arcs(cycle_path, new_node[""address""])
",[],0,[],/parse/nonprojectivedependencyparser.py_collapse_nodes
1554,/home/amandapotts/git/nltk/nltk/parse/nonprojectivedependencyparser.py_update_edge_scores,"def update_edge_scores(self, new_node, cycle_path):
""""""
Updates the edge scores to reflect a collapse operation into
new_node.
:type new_node: A Node.
:param new_node: The node which cycle nodes are collapsed into.
:type cycle_path: A list of integers.
:param cycle_path: A list of node addresses that belong to the cycle.
""""""
logger.debug(""cycle %s"", cycle_path)
cycle_path = self.compute_original_indexes(cycle_path)
logger.debug(""old cycle %s"", cycle_path)
logger.debug(""Prior to update: %s"", self.scores)
for i, row in enumerate(self.scores):
for j, column in enumerate(self.scores[i]):
logger.debug(self.scores[i][j])
if j in cycle_path and i not in cycle_path and self.scores[i][j]:
subtract_val = self.compute_max_subtract_score(j, cycle_path)
logger.debug(""%s - %s"", self.scores[i][j], subtract_val)
new_vals = []
for cur_val in self.scores[i][j]:
new_vals.append(cur_val - subtract_val)
self.scores[i][j] = new_vals
for i, row in enumerate(self.scores):
for j, cell in enumerate(self.scores[i]):
if i in cycle_path and j in cycle_path:
self.scores[i][j] = []
logger.debug(""After update: %s"", self.scores)
",[],0,[],/parse/nonprojectivedependencyparser.py_update_edge_scores
1555,/home/amandapotts/git/nltk/nltk/parse/nonprojectivedependencyparser.py_compute_original_indexes,"def compute_original_indexes(self, new_indexes):
""""""
As nodes are collapsed into others, they are replaced
by the new node in the graph, but it's still necessary
to keep track of what these original nodes were.  This
takes a list of node addresses and replaces any collapsed
node addresses with their original addresses.
:type new_indexes: A list of integers.
:param new_indexes: A list of node addresses to check for
subsumed nodes.
""""""
swapped = True
while swapped:
originals = []
swapped = False
for new_index in new_indexes:
if new_index in self.inner_nodes:
for old_val in self.inner_nodes[new_index]:
if old_val not in originals:
originals.append(old_val)
swapped = True
else:
originals.append(new_index)
new_indexes = originals
return new_indexes
",[],0,[],/parse/nonprojectivedependencyparser.py_compute_original_indexes
1556,/home/amandapotts/git/nltk/nltk/parse/nonprojectivedependencyparser.py_compute_max_subtract_score,"def compute_max_subtract_score(self, column_index, cycle_indexes):
""""""
When updating scores the score of the highest-weighted incoming
arc is subtracted upon collapse.  This returns the correct
amount to subtract from that edge.
:type column_index: integer.
:param column_index: A index representing the column of incoming arcs
to a particular node being updated
:type cycle_indexes: A list of integers.
:param cycle_indexes: Only arcs from cycle nodes are considered.  This
is a list of such nodes addresses.
""""""
max_score = -100000
for row_index in cycle_indexes:
for subtract_val in self.scores[row_index][column_index]:
if subtract_val > max_score:
max_score = subtract_val
return max_score
",[],0,[],/parse/nonprojectivedependencyparser.py_compute_max_subtract_score
1557,/home/amandapotts/git/nltk/nltk/parse/nonprojectivedependencyparser.py_best_incoming_arc,"def best_incoming_arc(self, node_index):
""""""
Returns the source of the best incoming arc to the
node with address: node_index
:type node_index: integer.
:param node_index: The address of the 'destination' node,
the node that is arced to.
""""""
originals = self.compute_original_indexes([node_index])
logger.debug(""originals: %s"", originals)
max_arc = None
max_score = None
for row_index in range(len(self.scores)):
for col_index in range(len(self.scores[row_index])):
if col_index in originals and (
max_score is None or self.scores[row_index][col_index] > max_score
):
max_score = self.scores[row_index][col_index]
max_arc = row_index
logger.debug(""%s, %s"", row_index, col_index)
logger.debug(max_score)
for key in self.inner_nodes:
replaced_nodes = self.inner_nodes[key]
if max_arc in replaced_nodes:
return key
return max_arc
",[],0,[],/parse/nonprojectivedependencyparser.py_best_incoming_arc
1558,/home/amandapotts/git/nltk/nltk/parse/nonprojectivedependencyparser.py_original_best_arc,"def original_best_arc(self, node_index):
originals = self.compute_original_indexes([node_index])
max_arc = None
max_score = None
max_orig = None
for row_index in range(len(self.scores)):
for col_index in range(len(self.scores[row_index])):
if col_index in originals and (
max_score is None or self.scores[row_index][col_index] > max_score
):
max_score = self.scores[row_index][col_index]
max_arc = row_index
max_orig = col_index
return [max_arc, max_orig]
",[],0,[],/parse/nonprojectivedependencyparser.py_original_best_arc
1559,/home/amandapotts/git/nltk/nltk/parse/nonprojectivedependencyparser.py_parse,"def parse(self, tokens, tags):
""""""
Parses a list of tokens in accordance to the MST parsing algorithm
for non-projective dependency parses.  Assumes that the tokens to
be parsed have already been tagged and those tags are provided.  Various
scoring methods can be used by implementing the ``DependencyScorerI``
interface and passing it to the training algorithm.
:type tokens: list(str)
:param tokens: A list of words or punctuation to be parsed.
:type tags: list(str)
:param tags: A list of tags corresponding by index to the words in the tokens list.
:return: An iterator of non-projective parses.
:rtype: iter(DependencyGraph)
""""""
self.inner_nodes = {}
g_graph = DependencyGraph()
for index, token in enumerate(tokens):
g_graph.nodes[index + 1].update(
{""word"": token, ""tag"": tags[index], ""rel"": ""NTOP"", ""address"": index + 1}
)
g_graph.connect_graph()
original_graph = DependencyGraph()
for index, token in enumerate(tokens):
original_graph.nodes[index + 1].update(
{""word"": token, ""tag"": tags[index], ""rel"": ""NTOP"", ""address"": index + 1}
)
b_graph = DependencyGraph()
c_graph = DependencyGraph()
for index, token in enumerate(tokens):
c_graph.nodes[index + 1].update(
{""word"": token, ""tag"": tags[index], ""rel"": ""NTOP"", ""address"": index + 1}
)
self.initialize_edge_scores(g_graph)
logger.debug(self.scores)
unvisited_vertices = [vertex[""address""] for vertex in c_graph.nodes.values()]
nr_vertices = len(tokens)
betas = {}
while unvisited_vertices:
current_vertex = unvisited_vertices.pop(0)
logger.debug(""current_vertex: %s"", current_vertex)
current_node = g_graph.get_by_address(current_vertex)
logger.debug(""current_node: %s"", current_node)
best_in_edge = self.best_incoming_arc(current_vertex)
betas[current_vertex] = self.original_best_arc(current_vertex)
logger.debug(""best in arc: %s --> %s"", best_in_edge, current_vertex)
for new_vertex in [current_vertex, best_in_edge]:
b_graph.nodes[new_vertex].update(
{""word"": ""TEMP"", ""rel"": ""NTOP"", ""address"": new_vertex}
)
b_graph.add_arc(best_in_edge, current_vertex)
cycle_path = b_graph.contains_cycle()
if cycle_path:
new_node = {""word"": ""NONE"", ""rel"": ""NTOP"", ""address"": nr_vertices + 1}
c_graph.add_node(new_node)
self.update_edge_scores(new_node, cycle_path)
self.collapse_nodes(new_node, cycle_path, g_graph, b_graph, c_graph)
for cycle_index in cycle_path:
c_graph.add_arc(new_node[""address""], cycle_index)
self.inner_nodes[new_node[""address""]] = cycle_path
unvisited_vertices.insert(0, nr_vertices + 1)
nr_vertices += 1
for cycle_node_address in cycle_path:
b_graph.remove_by_address(cycle_node_address)
logger.debug(""g_graph: %s"", g_graph)
logger.debug(""b_graph: %s"", b_graph)
logger.debug(""c_graph: %s"", c_graph)
logger.debug(""Betas: %s"", betas)
logger.debug(""replaced nodes %s"", self.inner_nodes)
logger.debug(""Final scores: %s"", self.scores)
logger.debug(""Recovering parse..."")
for i in range(len(tokens) + 1, nr_vertices + 1):
betas[betas[i][1]] = betas[i]
logger.debug(""Betas: %s"", betas)
for node in original_graph.nodes.values():
node[""deps""] = {}
for i in range(1, len(tokens) + 1):
original_graph.add_arc(betas[i][0], betas[i][1])
logger.debug(""Done."")
yield original_graph
",[],0,[],/parse/nonprojectivedependencyparser.py_parse
1560,/home/amandapotts/git/nltk/nltk/parse/nonprojectivedependencyparser.py___init__,"def __init__(self, dependency_grammar):
""""""
Creates a new ``NonprojectiveDependencyParser``.
:param dependency_grammar: a grammar of word-to-word relations.
:type dependency_grammar: DependencyGrammar
""""""
self._grammar = dependency_grammar
",[],0,[],/parse/nonprojectivedependencyparser.py___init__
1561,/home/amandapotts/git/nltk/nltk/parse/nonprojectivedependencyparser.py_parse,"def parse(self, tokens):
""""""
Parses the input tokens with respect to the parser's grammar.  Parsing
is accomplished by representing the search-space of possible parses as
a fully-connected directed graph.  Arcs that would lead to ungrammatical
parses are removed and a lattice is constructed of length n, where n is
the number of input tokens, to represent all possible grammatical
traversals.  All possible paths through the lattice are then enumerated
to produce the set of non-projective parses.
param tokens: A list of tokens to parse.
type tokens: list(str)
return: An iterator of non-projective parses.
rtype: iter(DependencyGraph)
""""""
self._graph = DependencyGraph()
for index, token in enumerate(tokens):
self._graph.nodes[index] = {
""word"": token,
""deps"": [],
""rel"": ""NTOP"",
""address"": index,
}
for head_node in self._graph.nodes.values():
deps = []
for dep_node in self._graph.nodes.values():
if (
self._grammar.contains(head_node[""word""], dep_node[""word""])
and head_node[""word""] != dep_node[""word""]
):
deps.append(dep_node[""address""])
head_node[""deps""] = deps
roots = []
possible_heads = []
for i, word in enumerate(tokens):
heads = []
for j, head in enumerate(tokens):
if (i != j) and self._grammar.contains(head, word):
heads.append(j)
if len(heads) == 0:
roots.append(i)
possible_heads.append(heads)
if len(roots) < 2:
if len(roots) == 0:
for i in range(len(tokens)):
roots.append(i)
analyses = []
for _ in roots:
stack = []
analysis = [[] for i in range(len(possible_heads))]
i = 0
forward = True
while i >= 0:
if forward:
if len(possible_heads[i]) == 1:
analysis[i] = possible_heads[i][0]
elif len(possible_heads[i]) == 0:
analysis[i] = -1
else:
head = possible_heads[i].pop()
analysis[i] = head
stack.append([i, head])
if not forward:
index_on_stack = False
for stack_item in stack:
if stack_item[0] == i:
index_on_stack = True
orig_length = len(possible_heads[i])
if index_on_stack and orig_length == 0:
for j in range(len(stack) - 1, -1, -1):
stack_item = stack[j]
if stack_item[0] == i:
possible_heads[i].append(stack.pop(j)[1])
elif index_on_stack and orig_length > 0:
head = possible_heads[i].pop()
analysis[i] = head
stack.append([i, head])
forward = True
if i + 1 == len(possible_heads):
analyses.append(analysis[:])
forward = False
if forward:
i += 1
else:
i -= 1
for analysis in analyses:
if analysis.count(-1) > 1:
continue
graph = DependencyGraph()
graph.root = graph.nodes[analysis.index(-1) + 1]
for address, (token, head_index) in enumerate(
zip(tokens, analysis), start=1
):
head_address = head_index + 1
node = graph.nodes[address]
node.update({""word"": token, ""address"": address})
if head_address == 0:
rel = ""ROOT""
else:
rel = """"
graph.nodes[head_index + 1][""deps""][rel].append(address)
yield graph
",[],0,[],/parse/nonprojectivedependencyparser.py_parse
1562,/home/amandapotts/git/nltk/nltk/parse/nonprojectivedependencyparser.py_demo,"def demo():
nonprojective_conll_parse_demo()
rule_based_demo()
",[],0,[],/parse/nonprojectivedependencyparser.py_demo
1563,/home/amandapotts/git/nltk/nltk/parse/nonprojectivedependencyparser.py_hall_demo,"def hall_demo():
npp = ProbabilisticNonprojectiveParser()
npp.train([], DemoScorer())
for parse_graph in npp.parse([""v1"", ""v2"", ""v3""], [None, None, None]):
print(parse_graph)
",[],0,[],/parse/nonprojectivedependencyparser.py_hall_demo
1564,/home/amandapotts/git/nltk/nltk/parse/nonprojectivedependencyparser.py_nonprojective_conll_parse_demo,"def nonprojective_conll_parse_demo():
from nltk.parse.dependencygraph import conll_data2
graphs = [DependencyGraph(entry) for entry in conll_data2.split(""\n\n"") if entry]
npp = ProbabilisticNonprojectiveParser()
npp.train(graphs, NaiveBayesDependencyScorer())
for parse_graph in npp.parse(
[""Cathy"", ""zag"", ""hen"", ""zwaaien"", "".""], [""N"", ""V"", ""Pron"", ""Adj"", ""N"", ""Punc""]
):
print(parse_graph)
",[],0,[],/parse/nonprojectivedependencyparser.py_nonprojective_conll_parse_demo
1565,/home/amandapotts/git/nltk/nltk/parse/nonprojectivedependencyparser.py_rule_based_demo,"def rule_based_demo():
from nltk.grammar import DependencyGrammar
grammar = DependencyGrammar.fromstring(
""""""
'taught' -> 'play' | 'man'
'man' -> 'the' | 'in'
'in' -> 'corner'
'corner' -> 'the'
'play' -> 'golf' | 'dachshund' | 'to'
'dachshund' -> 'his'
""""""
)
print(grammar)
ndp = NonprojectiveDependencyParser(grammar)
graphs = ndp.parse(
[
""the"",
""man"",
""in"",
""the"",
""corner"",
""taught"",
""his"",
""dachshund"",
""to"",
""play"",
""golf"",
]
)
print(""Graphs:"")
for graph in graphs:
print(graph)
",[],0,[],/parse/nonprojectivedependencyparser.py_rule_based_demo
1566,/home/amandapotts/git/nltk/nltk/parse/malt.py_malt_regex_tagger,"def malt_regex_tagger():
from nltk.tag import RegexpTagger
_tagger = RegexpTagger(
[
(r""\.$"", "".""),
(r""\,$"", "",""),
(r""\?$"", ""?""),  # fullstop, comma, Qmark
(r""\($"", ""(""),
(r""\)$"", "")""),  # round brackets
(r""\[$"", ""[""),
(r""\]$"", ""]""),  # square brackets
(r""^-?[0-9]+(\.[0-9]+)?$"", ""CD""),  # cardinal numbers
(r""(The|the|A|a|An|an)$"", ""DT""),  # articles
(r""(He|he|She|she|It|it|I|me|Me|You|you)$"", ""PRP""),  # pronouns
(r""(His|his|Her|her|Its|its)$"", ""PRP$""),  # possessive
(r""(my|Your|your|Yours|yours)$"", ""PRP$""),  # possessive
(r""(on|On|in|In|at|At|since|Since)$"", ""IN""),  # time prepopsitions
(r""(for|For|ago|Ago|before|Before)$"", ""IN""),  # time prepopsitions
(r""(till|Till|until|Until)$"", ""IN""),  # time prepopsitions
(r""(by|By|beside|Beside)$"", ""IN""),  # space prepopsitions
(r""(under|Under|below|Below)$"", ""IN""),  # space prepopsitions
(r""(over|Over|above|Above)$"", ""IN""),  # space prepopsitions
(r""(across|Across|through|Through)$"", ""IN""),  # space prepopsitions
(r""(into|Into|towards|Towards)$"", ""IN""),  # space prepopsitions
(r""(onto|Onto|from|From)$"", ""IN""),  # space prepopsitions
(r"".*able$"", ""JJ""),  # adjectives
(r"".*ness$"", ""NN""),  # nouns formed from adjectives
(r"".*ly$"", ""RB""),  # adverbs
(r"".*s$"", ""NNS""),  # plural nouns
(r"".*ing$"", ""VBG""),  # gerunds
(r"".*ed$"", ""VBD""),  # past tense verbs
(r"".*"", ""NN""),  # nouns (default)
]
)
return _tagger.tag
",[],0,[],/parse/malt.py_malt_regex_tagger
1567,/home/amandapotts/git/nltk/nltk/parse/malt.py_find_malt_model,"def find_malt_model(model_filename):
""""""
A module to find pre-trained MaltParser model.
""""""
if model_filename is None:
return ""malt_temp.mco""
elif os.path.exists(model_filename):  # If a full path is given.
return model_filename
else:  # Try to find path to malt model in environment variables.
return find_file(model_filename, env_vars=(""MALT_MODEL"",), verbose=False)
",[],0,[],/parse/malt.py_find_malt_model
1568,/home/amandapotts/git/nltk/nltk/parse/malt.py___init__,"def __init__(
self,
parser_dirname="""",
model_filename=None,
tagger=None,
additional_java_args=None,
",[],0,[],/parse/malt.py___init__
1569,/home/amandapotts/git/nltk/nltk/parse/malt.py_parse_tagged_sents,"def parse_tagged_sents(self, sentences, verbose=False, top_relation_label=""null""):
""""""
Use MaltParser to parse multiple POS tagged sentences. Takes multiple
sentences where each sentence is a list of (word, tag) tuples.
The sentences must have already been tokenized and tagged.
:param sentences: Input sentences to parse
:type sentence: list(list(tuple(str, str)))
:return: iter(iter(``DependencyGraph``)) the dependency graph
representation of each sentence
""""""
if not self._trained:
raise Exception(""Parser has not been trained. Call train() first."")
with tempfile.NamedTemporaryFile(
prefix=""malt_input.conll."", dir=self.working_dir, mode=""w"", delete=False
) as input_file:
with tempfile.NamedTemporaryFile(
prefix=""malt_output.conll."",
dir=self.working_dir,
mode=""w"",
delete=False,
) as output_file:
for line in taggedsents_to_conll(sentences):
input_file.write(str(line))
input_file.close()
cmd = self.generate_malt_command(
input_file.name, output_file.name, mode=""parse""
)
_current_path = os.getcwd()  # Remembers the current path.
try:  # Change to modelfile path
os.chdir(os.path.split(self.model)[0])
except:
pass
ret = self._execute(cmd, verbose)  # Run command.
os.chdir(_current_path)  # Change back to current path.
if ret != 0:
raise Exception(
""MaltParser parsing (%s) failed with exit ""
""code %d"" % ("" "".join(cmd), ret)
)
with open(output_file.name) as infile:
for tree_str in infile.read().split(""\n\n""):
yield (
iter(
[
DependencyGraph(
tree_str, top_relation_label=top_relation_label
)
]
)
)
os.remove(input_file.name)
os.remove(output_file.name)
",[],0,[],/parse/malt.py_parse_tagged_sents
1570,/home/amandapotts/git/nltk/nltk/parse/malt.py_parse_sents,"def parse_sents(self, sentences, verbose=False, top_relation_label=""null""):
""""""
Use MaltParser to parse multiple sentences.
Takes a list of sentences, where each sentence is a list of words.
Each sentence will be automatically tagged with this
MaltParser instance's tagger.
:param sentences: Input sentences to parse
:type sentence: list(list(str))
:return: iter(DependencyGraph)
""""""
tagged_sentences = (self.tagger(sentence) for sentence in sentences)
return self.parse_tagged_sents(
tagged_sentences, verbose, top_relation_label=top_relation_label
)
",[],0,[],/parse/malt.py_parse_sents
1571,/home/amandapotts/git/nltk/nltk/parse/malt.py_generate_malt_command,"def generate_malt_command(self, inputfilename, outputfilename=None, mode=None):
""""""
This function generates the maltparser command use at the terminal.
:param inputfilename: path to the input file
:type inputfilename: str
:param outputfilename: path to the output file
:type outputfilename: str
""""""
cmd = [""java""]
cmd += self.additional_java_args  # Adds additional java arguments
classpaths_separator = ""
cmd += [
""-cp"",
classpaths_separator.join(self.malt_jars),
]  # Adds classpaths for jars
cmd += [""org.maltparser.Malt""]  # Adds the main function.
if os.path.exists(self.model):  # when parsing
cmd += [""-c"", os.path.split(self.model)[-1]]
else:  # when learning
cmd += [""-c"", self.model]
cmd += [""-i"", inputfilename]
if mode == ""parse"":
cmd += [""-o"", outputfilename]
cmd += [""-m"", mode]  # mode use to generate parses.
return cmd
",[],0,[],/parse/malt.py_generate_malt_command
1572,/home/amandapotts/git/nltk/nltk/parse/malt.py__execute,"def _execute(cmd, verbose=False):
output = None if verbose else subprocess.PIPE
p = subprocess.Popen(cmd, stdout=output, stderr=output)
return p.wait()
",[],0,[],/parse/malt.py__execute
1573,/home/amandapotts/git/nltk/nltk/parse/malt.py_train,"def train(self, depgraphs, verbose=False):
""""""
Train MaltParser from a list of ``DependencyGraph`` objects
:param depgraphs: list of ``DependencyGraph`` objects for training input data
:type depgraphs: DependencyGraph
""""""
with tempfile.NamedTemporaryFile(
prefix=""malt_train.conll."", dir=self.working_dir, mode=""w"", delete=False
) as input_file:
input_str = ""\n"".join(dg.to_conll(10) for dg in depgraphs)
input_file.write(str(input_str))
self.train_from_file(input_file.name, verbose=verbose)
os.remove(input_file.name)
",[],0,[],/parse/malt.py_train
1574,/home/amandapotts/git/nltk/nltk/parse/malt.py_train_from_file,"def train_from_file(self, conll_file, verbose=False):
""""""
Train MaltParser from a file
:param conll_file: str for the filename of the training input data
:type conll_file: str
""""""
if isinstance(conll_file, ZipFilePathPointer):
with tempfile.NamedTemporaryFile(
prefix=""malt_train.conll."", dir=self.working_dir, mode=""w"", delete=False
) as input_file:
with conll_file.open() as conll_input_file:
conll_str = conll_input_file.read()
input_file.write(str(conll_str))
return self.train_from_file(input_file.name, verbose=verbose)
cmd = self.generate_malt_command(conll_file, mode=""learn"")
ret = self._execute(cmd, verbose)
if ret != 0:
raise Exception(
""MaltParser training (%s) failed with exit ""
""code %d"" % ("" "".join(cmd), ret)
)
self._trained = True
",[],0,[],/parse/malt.py_train_from_file
1575,/home/amandapotts/git/nltk/nltk/parse/projectivedependencyparser.py___init__,"def __init__(self, start_index, end_index, head_index, arcs, tags):
self._start_index = start_index
self._end_index = end_index
self._head_index = head_index
self._arcs = arcs
self._tags = tags
self._comparison_key = (start_index, end_index, head_index, tuple(arcs))
self._hash = hash(self._comparison_key)
",[],0,[],/parse/projectivedependencyparser.py___init__
1576,/home/amandapotts/git/nltk/nltk/parse/projectivedependencyparser.py_head_index,"def head_index(self):
""""""
:return: An value indexing the head of the entire ``DependencySpan``.
:rtype: int
""""""
return self._head_index
",[],0,[],/parse/projectivedependencyparser.py_head_index
1577,/home/amandapotts/git/nltk/nltk/parse/projectivedependencyparser.py___repr__,"def __repr__(self):
""""""
:return: A concise string representatino of the ``DependencySpan``.
:rtype: str.
""""""
return ""Span %d-%d
self._start_index,
self._end_index,
self._head_index,
)
",[],0,[],/parse/projectivedependencyparser.py___repr__
1578,/home/amandapotts/git/nltk/nltk/parse/projectivedependencyparser.py___str__,"def __str__(self):
""""""
:return: A verbose string representation of the ``DependencySpan``.
:rtype: str
""""""
str = ""Span %d-%d
self._start_index,
self._end_index,
self._head_index,
)
for i in range(len(self._arcs)):
str += ""\n%d <- %d, %s"" % (i, self._arcs[i], self._tags[i])
return str
",[],0,[],/parse/projectivedependencyparser.py___str__
1579,/home/amandapotts/git/nltk/nltk/parse/projectivedependencyparser.py___eq__,"def __eq__(self, other):
return (
type(self) == type(other) and self._comparison_key == other._comparison_key
)
",[],0,[],/parse/projectivedependencyparser.py___eq__
1580,/home/amandapotts/git/nltk/nltk/parse/projectivedependencyparser.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/parse/projectivedependencyparser.py___ne__
1581,/home/amandapotts/git/nltk/nltk/parse/projectivedependencyparser.py___lt__,"def __lt__(self, other):
if not isinstance(other, DependencySpan):
raise_unorderable_types(""<"", self, other)
return self._comparison_key < other._comparison_key
",[],0,[],/parse/projectivedependencyparser.py___lt__
1582,/home/amandapotts/git/nltk/nltk/parse/projectivedependencyparser.py___hash__,"def __hash__(self):
""""""
:return: The hash value of this ``DependencySpan``.
""""""
return self._hash
",[],0,[],/parse/projectivedependencyparser.py___hash__
1583,/home/amandapotts/git/nltk/nltk/parse/projectivedependencyparser.py___init__,"def __init__(self, x, y):
""""""
:param x: This cell's x coordinate.
:type x: int.
:param y: This cell's y coordinate.
:type y: int.
""""""
self._x = x
self._y = y
self._entries = set()
",[],0,[],/parse/projectivedependencyparser.py___init__
1584,/home/amandapotts/git/nltk/nltk/parse/projectivedependencyparser.py_add,"def add(self, span):
""""""
Appends the given span to the list of spans
representing the chart cell's entries.
:param span: The span to add.
:type span: DependencySpan
""""""
self._entries.add(span)
",[],0,[],/parse/projectivedependencyparser.py_add
1585,/home/amandapotts/git/nltk/nltk/parse/projectivedependencyparser.py___str__,"def __str__(self):
""""""
:return: A verbose string representation of this ``ChartCell``.
:rtype: str.
""""""
return ""CC[%d,%d]: %s"" % (self._x, self._y, self._entries)
",[],0,[],/parse/projectivedependencyparser.py___str__
1586,/home/amandapotts/git/nltk/nltk/parse/projectivedependencyparser.py___repr__,"def __repr__(self):
""""""
:return: A concise string representation of this ``ChartCell``.
:rtype: str.
""""""
return ""%s"" % self
",[],0,[],/parse/projectivedependencyparser.py___repr__
1587,/home/amandapotts/git/nltk/nltk/parse/projectivedependencyparser.py___init__,"def __init__(self, dependency_grammar):
""""""
Create a new ProjectiveDependencyParser, from a word-to-word
dependency grammar ``DependencyGrammar``.
:param dependency_grammar: A word-to-word relation dependencygrammar.
:type dependency_grammar: DependencyGrammar
""""""
self._grammar = dependency_grammar
",[],0,[],/parse/projectivedependencyparser.py___init__
1588,/home/amandapotts/git/nltk/nltk/parse/projectivedependencyparser.py_parse,"def parse(self, tokens):
""""""
Performs a projective dependency parse on the list of tokens using
a chart-based, span-concatenation algorithm similar to Eisner (1996).
:param tokens: The list of input tokens.
:type tokens: list(str)
:return: An iterator over parse trees.
:rtype: iter(Tree)
""""""
self._tokens = list(tokens)
chart = []
for i in range(0, len(self._tokens) + 1):
chart.append([])
for j in range(0, len(self._tokens) + 1):
chart[i].append(ChartCell(i, j))
if i == j + 1:
chart[i][j].add(DependencySpan(i - 1, i, i - 1, [-1], [""null""]))
for i in range(1, len(self._tokens) + 1):
for j in range(i - 2, -1, -1):
for k in range(i - 1, j, -1):
for span1 in chart[k][j]._entries:
for span2 in chart[i][k]._entries:
for newspan in self.concatenate(span1, span2):
chart[i][j].add(newspan)
for parse in chart[len(self._tokens)][0]._entries:
conll_format = """"
for i in range(len(tokens)):
conll_format += ""\t%d\t%s\t%s\t%s\t%s\t%s\t%d\t%s\t%s\t%s\n"" % (
i + 1,
tokens[i],
tokens[i],
""null"",
""null"",
""null"",
parse._arcs[i] + 1,
""ROOT"",
""-"",
""-"",
)
dg = DependencyGraph(conll_format)
yield dg.tree()
",[],0,[],/parse/projectivedependencyparser.py_parse
1589,/home/amandapotts/git/nltk/nltk/parse/projectivedependencyparser.py_concatenate,"def concatenate(self, span1, span2):
""""""
Concatenates the two spans in whichever way possible.  This
includes rightward concatenation (from the leftmost word of the
leftmost span to the rightmost word of the rightmost span) and
leftward concatenation (vice-versa) between adjacent spans.  Unlike
Eisner's presentation of span concatenation, these spans do not
share or pivot on a particular word/word-index.
:return: A list of new spans formed through concatenation.
:rtype: list(DependencySpan)
""""""
spans = []
if span1._start_index == span2._start_index:
print(""Error: Mismatched spans - replace this with thrown error"")
if span1._start_index > span2._start_index:
temp_span = span1
span1 = span2
span2 = temp_span
new_arcs = span1._arcs + span2._arcs
new_tags = span1._tags + span2._tags
if self._grammar.contains(
self._tokens[span1._head_index], self._tokens[span2._head_index]
):
new_arcs[span2._head_index - span1._start_index] = span1._head_index
spans.append(
DependencySpan(
span1._start_index,
span2._end_index,
span1._head_index,
new_arcs,
new_tags,
)
)
new_arcs = span1._arcs + span2._arcs
if self._grammar.contains(
self._tokens[span2._head_index], self._tokens[span1._head_index]
):
new_arcs[span1._head_index - span1._start_index] = span2._head_index
spans.append(
DependencySpan(
span1._start_index,
span2._end_index,
span2._head_index,
new_arcs,
new_tags,
)
)
return spans
",[],0,[],/parse/projectivedependencyparser.py_concatenate
1590,/home/amandapotts/git/nltk/nltk/parse/projectivedependencyparser.py___init__,"def __init__(self):
""""""
Create a new probabilistic dependency parser.  No additional
operations are necessary.
""""""
",[],0,[],/parse/projectivedependencyparser.py___init__
1591,/home/amandapotts/git/nltk/nltk/parse/projectivedependencyparser.py_parse,"def parse(self, tokens):
""""""
Parses the list of tokens subject to the projectivity constraint
and the productions in the parser's grammar.  This uses a method
similar to the span-concatenation algorithm defined in Eisner (1996).
It returns the most probable parse derived from the parser's
probabilistic dependency grammar.
""""""
self._tokens = list(tokens)
chart = []
for i in range(0, len(self._tokens) + 1):
chart.append([])
for j in range(0, len(self._tokens) + 1):
chart[i].append(ChartCell(i, j))
if i == j + 1:
if tokens[i - 1] in self._grammar._tags:
for tag in self._grammar._tags[tokens[i - 1]]:
chart[i][j].add(
DependencySpan(i - 1, i, i - 1, [-1], [tag])
)
else:
print(
""No tag found for input token '%s', parse is impossible.""
% tokens[i - 1]
)
return []
for i in range(1, len(self._tokens) + 1):
for j in range(i - 2, -1, -1):
for k in range(i - 1, j, -1):
for span1 in chart[k][j]._entries:
for span2 in chart[i][k]._entries:
for newspan in self.concatenate(span1, span2):
chart[i][j].add(newspan)
trees = []
max_parse = None
max_score = 0
for parse in chart[len(self._tokens)][0]._entries:
conll_format = """"
malt_format = """"
for i in range(len(tokens)):
malt_format += ""%s\t%s\t%d\t%s\n"" % (
tokens[i],
""null"",
parse._arcs[i] + 1,
""null"",
)
conll_format += ""\t%d\t%s\t%s\t%s\t%s\t%s\t%d\t%s\t%s\t%s\n"" % (
i + 1,
tokens[i],
tokens[i],
parse._tags[i],
parse._tags[i],
""null"",
parse._arcs[i] + 1,
""ROOT"",
""-"",
""-"",
)
dg = DependencyGraph(conll_format)
score = self.compute_prob(dg)
trees.append((score, dg.tree()))
trees.sort()
return (tree for (score, tree) in trees)
",[],0,[],/parse/projectivedependencyparser.py_parse
1592,/home/amandapotts/git/nltk/nltk/parse/projectivedependencyparser.py_concatenate,"def concatenate(self, span1, span2):
""""""
Concatenates the two spans in whichever way possible.  This
includes rightward concatenation (from the leftmost word of the
leftmost span to the rightmost word of the rightmost span) and
leftward concatenation (vice-versa) between adjacent spans.  Unlike
Eisner's presentation of span concatenation, these spans do not
share or pivot on a particular word/word-index.
:return: A list of new spans formed through concatenation.
:rtype: list(DependencySpan)
""""""
spans = []
if span1._start_index == span2._start_index:
print(""Error: Mismatched spans - replace this with thrown error"")
if span1._start_index > span2._start_index:
temp_span = span1
span1 = span2
span2 = temp_span
new_arcs = span1._arcs + span2._arcs
new_tags = span1._tags + span2._tags
if self._grammar.contains(
self._tokens[span1._head_index], self._tokens[span2._head_index]
):
new_arcs[span2._head_index - span1._start_index] = span1._head_index
spans.append(
DependencySpan(
span1._start_index,
span2._end_index,
span1._head_index,
new_arcs,
new_tags,
)
)
new_arcs = span1._arcs + span2._arcs
new_tags = span1._tags + span2._tags
if self._grammar.contains(
self._tokens[span2._head_index], self._tokens[span1._head_index]
):
new_arcs[span1._head_index - span1._start_index] = span2._head_index
spans.append(
DependencySpan(
span1._start_index,
span2._end_index,
span2._head_index,
new_arcs,
new_tags,
)
)
return spans
",[],0,[],/parse/projectivedependencyparser.py_concatenate
1593,/home/amandapotts/git/nltk/nltk/parse/projectivedependencyparser.py_train,"def train(self, graphs):
""""""
Trains a ProbabilisticDependencyGrammar based on the list of input
DependencyGraphs.  This model is an implementation of Eisner's (1996)
Model C, which derives its statistics from head-word, head-tag,
child-word, and child-tag relationships.
:param graphs: A list of dependency graphs to train from.
:type: list(DependencyGraph)
""""""
productions = []
events = defaultdict(int)
tags = {}
for dg in graphs:
for node_index in range(1, len(dg.nodes)):
children = list(
chain.from_iterable(dg.nodes[node_index][""deps""].values())
)
nr_left_children = dg.left_children(node_index)
nr_right_children = dg.right_children(node_index)
nr_children = nr_left_children + nr_right_children
for child_index in range(
0 - (nr_left_children + 1), nr_right_children + 2
):
head_word = dg.nodes[node_index][""word""]
head_tag = dg.nodes[node_index][""tag""]
if head_word in tags:
tags[head_word].add(head_tag)
else:
tags[head_word] = {head_tag}
child = ""STOP""
child_tag = ""STOP""
prev_word = ""START""
prev_tag = ""START""
if child_index < 0:
array_index = child_index + nr_left_children
if array_index >= 0:
child = dg.nodes[children[array_index]][""word""]
child_tag = dg.nodes[children[array_index]][""tag""]
if child_index != -1:
prev_word = dg.nodes[children[array_index + 1]][""word""]
prev_tag = dg.nodes[children[array_index + 1]][""tag""]
if child != ""STOP"":
productions.append(DependencyProduction(head_word, [child]))
head_event = ""(head ({} {}) (mods ({}, {}, {}) left))"".format(
child,
child_tag,
prev_tag,
head_word,
head_tag,
)
mod_event = ""(mods ({}, {}, {}) left))"".format(
prev_tag,
head_word,
head_tag,
)
events[head_event] += 1
events[mod_event] += 1
elif child_index > 0:
array_index = child_index + nr_left_children - 1
if array_index < nr_children:
child = dg.nodes[children[array_index]][""word""]
child_tag = dg.nodes[children[array_index]][""tag""]
if child_index != 1:
prev_word = dg.nodes[children[array_index - 1]][""word""]
prev_tag = dg.nodes[children[array_index - 1]][""tag""]
if child != ""STOP"":
productions.append(DependencyProduction(head_word, [child]))
head_event = ""(head ({} {}) (mods ({}, {}, {}) right))"".format(
child,
child_tag,
prev_tag,
head_word,
head_tag,
)
mod_event = ""(mods ({}, {}, {}) right))"".format(
prev_tag,
head_word,
head_tag,
)
events[head_event] += 1
events[mod_event] += 1
self._grammar = ProbabilisticDependencyGrammar(productions, events, tags)
",[],0,[],/parse/projectivedependencyparser.py_train
1594,/home/amandapotts/git/nltk/nltk/parse/projectivedependencyparser.py_compute_prob,"def compute_prob(self, dg):
""""""
Computes the probability of a dependency graph based
on the parser's probability model (defined by the parser's
statistical dependency grammar).
:param dg: A dependency graph to score.
:type dg: DependencyGraph
:return: The probability of the dependency graph.
:rtype: int
""""""
prob = 1.0
for node_index in range(1, len(dg.nodes)):
children = list(chain.from_iterable(dg.nodes[node_index][""deps""].values()))
nr_left_children = dg.left_children(node_index)
nr_right_children = dg.right_children(node_index)
nr_children = nr_left_children + nr_right_children
for child_index in range(0 - (nr_left_children + 1), nr_right_children + 2):
head_word = dg.nodes[node_index][""word""]
head_tag = dg.nodes[node_index][""tag""]
child = ""STOP""
child_tag = ""STOP""
prev_word = ""START""
prev_tag = ""START""
if child_index < 0:
array_index = child_index + nr_left_children
if array_index >= 0:
child = dg.nodes[children[array_index]][""word""]
child_tag = dg.nodes[children[array_index]][""tag""]
if child_index != -1:
prev_word = dg.nodes[children[array_index + 1]][""word""]
prev_tag = dg.nodes[children[array_index + 1]][""tag""]
head_event = ""(head ({} {}) (mods ({}, {}, {}) left))"".format(
child,
child_tag,
prev_tag,
head_word,
head_tag,
)
mod_event = ""(mods ({}, {}, {}) left))"".format(
prev_tag,
head_word,
head_tag,
)
h_count = self._grammar._events[head_event]
m_count = self._grammar._events[mod_event]
if m_count != 0:
prob *= h_count / m_count
else:
prob = 0.00000001  # Very small number
elif child_index > 0:
array_index = child_index + nr_left_children - 1
if array_index < nr_children:
child = dg.nodes[children[array_index]][""word""]
child_tag = dg.nodes[children[array_index]][""tag""]
if child_index != 1:
prev_word = dg.nodes[children[array_index - 1]][""word""]
prev_tag = dg.nodes[children[array_index - 1]][""tag""]
head_event = ""(head ({} {}) (mods ({}, {}, {}) right))"".format(
child,
child_tag,
prev_tag,
head_word,
head_tag,
)
mod_event = ""(mods ({}, {}, {}) right))"".format(
prev_tag,
head_word,
head_tag,
)
h_count = self._grammar._events[head_event]
m_count = self._grammar._events[mod_event]
if m_count != 0:
prob *= h_count / m_count
else:
prob = 0.00000001  # Very small number
return prob
",[],0,[],/parse/projectivedependencyparser.py_compute_prob
1595,/home/amandapotts/git/nltk/nltk/parse/projectivedependencyparser.py_demo,"def demo():
projective_rule_parse_demo()
projective_prob_parse_demo()
",[],0,[],/parse/projectivedependencyparser.py_demo
1596,/home/amandapotts/git/nltk/nltk/parse/projectivedependencyparser.py_projective_rule_parse_demo,"def projective_rule_parse_demo():
""""""
A demonstration showing the creation and use of a
``DependencyGrammar`` to perform a projective dependency
parse.
""""""
grammar = DependencyGrammar.fromstring(
""""""
'scratch' -> 'cats' | 'walls'
'walls' -> 'the'
'cats' -> 'the'
""""""
)
print(grammar)
pdp = ProjectiveDependencyParser(grammar)
trees = pdp.parse([""the"", ""cats"", ""scratch"", ""the"", ""walls""])
for tree in trees:
print(tree)
",[],0,[],/parse/projectivedependencyparser.py_projective_rule_parse_demo
1597,/home/amandapotts/git/nltk/nltk/parse/projectivedependencyparser.py_arity_parse_demo,"def arity_parse_demo():
""""""
A demonstration showing the creation of a ``DependencyGrammar``
in which a specific number of modifiers is listed for a given
head.  This can further constrain the number of possible parses
created by a ``ProjectiveDependencyParser``.
""""""
print()
print(""A grammar with no arity constraints. Each DependencyProduction"")
print(""specifies a relationship between one head word and only one"")
print(""modifier word."")
grammar = DependencyGrammar.fromstring(
""""""
'fell' -> 'price' | 'stock'
'price' -> 'of' | 'the'
'of' -> 'stock'
'stock' -> 'the'
""""""
)
print(grammar)
print()
print(""For the sentence 'The price of the stock fell', this grammar"")
print(""will produce the following three parses:"")
pdp = ProjectiveDependencyParser(grammar)
trees = pdp.parse([""the"", ""price"", ""of"", ""the"", ""stock"", ""fell""])
for tree in trees:
print(tree)
print()
print(""By contrast, the following grammar contains a "")
print(""DependencyProduction that specifies a relationship"")
print(""between a single head word, 'price', and two modifier"")
print(""words, 'of' and 'the'."")
grammar = DependencyGrammar.fromstring(
""""""
'fell' -> 'price' | 'stock'
'price' -> 'of' 'the'
'of' -> 'stock'
'stock' -> 'the'
""""""
)
print(grammar)
print()
print(
""This constrains the number of possible parses to just one:""
)  # unimplemented, soon to replace
pdp = ProjectiveDependencyParser(grammar)
trees = pdp.parse([""the"", ""price"", ""of"", ""the"", ""stock"", ""fell""])
for tree in trees:
print(tree)
",[],0,[],/parse/projectivedependencyparser.py_arity_parse_demo
1598,/home/amandapotts/git/nltk/nltk/parse/projectivedependencyparser.py_projective_prob_parse_demo,"def projective_prob_parse_demo():
""""""
A demo showing the training and use of a projective
dependency parser.
""""""
from nltk.parse.dependencygraph import conll_data2
graphs = [DependencyGraph(entry) for entry in conll_data2.split(""\n\n"") if entry]
ppdp = ProbabilisticProjectiveDependencyParser()
print(""Training Probabilistic Projective Dependency Parser..."")
ppdp.train(graphs)
sent = [""Cathy"", ""zag"", ""hen"", ""wild"", ""zwaaien"", "".""]
print(""Parsing '"", "" "".join(sent), ""'..."")
print(""Parse:"")
for tree in ppdp.parse(sent):
print(tree)
",[],0,[],/parse/projectivedependencyparser.py_projective_prob_parse_demo
1599,/home/amandapotts/git/nltk/nltk/parse/dependencygraph.py_remove_by_address,"def remove_by_address(self, address):
""""""
Removes the node with the given address.  References
to this node in others will still exist.
""""""
del self.nodes[address]
",[],0,[],/parse/dependencygraph.py_remove_by_address
1600,/home/amandapotts/git/nltk/nltk/parse/dependencygraph.py_redirect_arcs,"def redirect_arcs(self, originals, redirect):
""""""
Redirects arcs to any of the nodes in the originals list
to the redirect node address.
""""""
for node in self.nodes.values():
new_deps = []
for dep in node[""deps""]:
if dep in originals:
new_deps.append(redirect)
else:
new_deps.append(dep)
node[""deps""] = new_deps
",[],0,[],/parse/dependencygraph.py_redirect_arcs
1601,/home/amandapotts/git/nltk/nltk/parse/dependencygraph.py_add_arc,"def add_arc(self, head_address, mod_address):
""""""
Adds an arc from the node specified by head_address to the
node specified by the mod address.
""""""
relation = self.nodes[mod_address][""rel""]
self.nodes[head_address][""deps""].setdefault(relation, [])
self.nodes[head_address][""deps""][relation].append(mod_address)
",[],0,[],/parse/dependencygraph.py_add_arc
1602,/home/amandapotts/git/nltk/nltk/parse/dependencygraph.py_connect_graph,"def connect_graph(self):
""""""
Fully connects all non-root nodes.  All nodes are set to be dependents
of the root node.
""""""
for node1 in self.nodes.values():
for node2 in self.nodes.values():
if node1[""address""] != node2[""address""] and node2[""rel""] != ""TOP"":
relation = node2[""rel""]
node1[""deps""].setdefault(relation, [])
node1[""deps""][relation].append(node2[""address""])
",[],0,[],/parse/dependencygraph.py_connect_graph
1603,/home/amandapotts/git/nltk/nltk/parse/dependencygraph.py_get_by_address,"def get_by_address(self, node_address):
""""""Return the node with the given address.""""""
return self.nodes[node_address]
",[],0,[],/parse/dependencygraph.py_get_by_address
1604,/home/amandapotts/git/nltk/nltk/parse/dependencygraph.py_contains_address,"def contains_address(self, node_address):
""""""
Returns true if the graph contains a node with the given node
address, false otherwise.
""""""
return node_address in self.nodes
",[],0,[],/parse/dependencygraph.py_contains_address
1605,/home/amandapotts/git/nltk/nltk/parse/dependencygraph.py__repr_svg_,"def _repr_svg_(self):
""""""Show SVG representation of the transducer (IPython magic).
>>> from nltk.test.setup_fixt import check_binary
>>> check_binary('dot')
>>> dg = DependencyGraph(
...     'John N 2\\n'
...     'loves V 0\\n'
...     'Mary N 2'
... )
>>> dg._repr_svg_().split('\\n')[0]
'<?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?>'
""""""
dot_string = self.to_dot()
return dot2img(dot_string)
",[],0,[],/parse/dependencygraph.py__repr_svg_
1606,/home/amandapotts/git/nltk/nltk/parse/dependencygraph.py___str__,"def __str__(self):
return pformat(self.nodes)
",[],0,[],/parse/dependencygraph.py___str__
1607,/home/amandapotts/git/nltk/nltk/parse/dependencygraph.py___repr__,"def __repr__(self):
return f""<DependencyGraph with {len(self.nodes)} nodes>""
",[],0,[],/parse/dependencygraph.py___repr__
1608,/home/amandapotts/git/nltk/nltk/parse/dependencygraph.py_load,"def load(
filename, zero_based=False, cell_separator=None, top_relation_label=""ROOT""
",[],0,[],/parse/dependencygraph.py_load
1609,/home/amandapotts/git/nltk/nltk/parse/dependencygraph.py_left_children,"def left_children(self, node_index):
""""""
Returns the number of left children under the node specified
by the given address.
""""""
children = chain.from_iterable(self.nodes[node_index][""deps""].values())
index = self.nodes[node_index][""address""]
return sum(1 for c in children if c < index)
",[],0,[],/parse/dependencygraph.py_left_children
1610,/home/amandapotts/git/nltk/nltk/parse/dependencygraph.py_right_children,"def right_children(self, node_index):
""""""
Returns the number of right children under the node specified
by the given address.
""""""
children = chain.from_iterable(self.nodes[node_index][""deps""].values())
index = self.nodes[node_index][""address""]
return sum(1 for c in children if c > index)
",[],0,[],/parse/dependencygraph.py_right_children
1611,/home/amandapotts/git/nltk/nltk/parse/dependencygraph.py_add_node,"def add_node(self, node):
if not self.contains_address(node[""address""]):
self.nodes[node[""address""]].update(node)
",[],0,[],/parse/dependencygraph.py_add_node
1612,/home/amandapotts/git/nltk/nltk/parse/dependencygraph.py__parse,"def _parse(
self,
input_,
cell_extractor=None,
zero_based=False,
cell_separator=None,
top_relation_label=""ROOT"",
",[],0,[],/parse/dependencygraph.py__parse
1613,/home/amandapotts/git/nltk/nltk/parse/dependencygraph.py_extract_3_cells,"def extract_3_cells(cells, index):
word, tag, head = cells
return index, word, word, tag, tag, """", head, """"
",[],0,[],/parse/dependencygraph.py_extract_3_cells
1614,/home/amandapotts/git/nltk/nltk/parse/dependencygraph.py_extract_4_cells,"def extract_4_cells(cells, index):
word, tag, head, rel = cells
return index, word, word, tag, tag, """", head, rel
",[],0,[],/parse/dependencygraph.py_extract_4_cells
1615,/home/amandapotts/git/nltk/nltk/parse/dependencygraph.py_extract_7_cells,"def extract_7_cells(cells, index):
line_index, word, lemma, tag, _, head, rel = cells
try:
index = int(line_index)
except ValueError:
pass
return index, word, lemma, tag, tag, """", head, rel
",[],0,[],/parse/dependencygraph.py_extract_7_cells
1616,/home/amandapotts/git/nltk/nltk/parse/dependencygraph.py_extract_10_cells,"def extract_10_cells(cells, index):
line_index, word, lemma, ctag, tag, feats, head, rel, _, _ = cells
try:
index = int(line_index)
except ValueError:
pass
return index, word, lemma, ctag, tag, feats, head, rel
",[],0,[],/parse/dependencygraph.py_extract_10_cells
1617,/home/amandapotts/git/nltk/nltk/parse/dependencygraph.py__word,"def _word(self, node, filter=True):
w = node[""word""]
if filter:
if w != "","":
return w
return w
",[],0,[],/parse/dependencygraph.py__word
1618,/home/amandapotts/git/nltk/nltk/parse/dependencygraph.py__tree,"def _tree(self, i):
""""""Turn dependency graphs into NLTK trees.
:param int i: index of a node
:return: either a word (if the indexed node is a leaf) or a ``Tree``.
""""""
node = self.get_by_address(i)
word = node[""word""]
deps = sorted(chain.from_iterable(node[""deps""].values()))
if deps:
return Tree(word, [self._tree(dep) for dep in deps])
else:
return word
",[],0,[],/parse/dependencygraph.py__tree
1619,/home/amandapotts/git/nltk/nltk/parse/dependencygraph.py_tree,"def tree(self):
""""""
Starting with the ``root`` node, build a dependency tree using the NLTK
``Tree`` constructor. Dependency labels are omitted.
""""""
node = self.root
word = node[""word""]
deps = sorted(chain.from_iterable(node[""deps""].values()))
return Tree(word, [self._tree(dep) for dep in deps])
",[],0,[],/parse/dependencygraph.py_tree
1620,/home/amandapotts/git/nltk/nltk/parse/dependencygraph.py_triples,"def triples(self, node=None):
""""""
Extract dependency triples of the form:
((head word, head tag), rel, (dep word, dep tag))
""""""
if not node:
node = self.root
head = (node[""word""], node[""ctag""])
for i in sorted(chain.from_iterable(node[""deps""].values())):
dep = self.get_by_address(i)
yield (head, dep[""rel""], (dep[""word""], dep[""ctag""]))
yield from self.triples(node=dep)
",[],0,[],/parse/dependencygraph.py_triples
1621,/home/amandapotts/git/nltk/nltk/parse/dependencygraph.py__hd,"def _hd(self, i):
try:
return self.nodes[i][""head""]
except IndexError:
return None
",[],0,[],/parse/dependencygraph.py__hd
1622,/home/amandapotts/git/nltk/nltk/parse/dependencygraph.py__rel,"def _rel(self, i):
try:
return self.nodes[i][""rel""]
except IndexError:
return None
",[],0,[],/parse/dependencygraph.py__rel
1623,/home/amandapotts/git/nltk/nltk/parse/dependencygraph.py_contains_cycle,"def contains_cycle(self):
""""""Check whether there are cycles.
>>> dg = DependencyGraph(treebank_data)
>>> dg.contains_cycle()
False
>>> cyclic_dg = DependencyGraph()
>>> top = {'word': None, 'deps': [1], 'rel': 'TOP', 'address': 0}
>>> child1 = {'word': None, 'deps': [2], 'rel': 'NTOP', 'address': 1}
>>> child2 = {'word': None, 'deps': [4], 'rel': 'NTOP', 'address': 2}
>>> child3 = {'word': None, 'deps': [1], 'rel': 'NTOP', 'address': 3}
>>> child4 = {'word': None, 'deps': [3], 'rel': 'NTOP', 'address': 4}
>>> cyclic_dg.nodes = {
...     0: top,
...     1: child1,
...     2: child2,
...     3: child3,
...     4: child4,
... }
>>> cyclic_dg.root = top
>>> cyclic_dg.contains_cycle()
[1, 2, 4, 3]
""""""
distances = {}
for node in self.nodes.values():
for dep in node[""deps""]:
key = tuple([node[""address""], dep])
distances[key] = 1
for _ in self.nodes:
new_entries = {}
for pair1 in distances:
for pair2 in distances:
if pair1[1] == pair2[0]:
key = tuple([pair1[0], pair2[1]])
new_entries[key] = distances[pair1] + distances[pair2]
for pair in new_entries:
distances[pair] = new_entries[pair]
if pair[0] == pair[1]:
path = self.get_cycle_path(self.get_by_address(pair[0]), pair[0])
return path
return False  # return []?
",[],0,[],/parse/dependencygraph.py_contains_cycle
1624,/home/amandapotts/git/nltk/nltk/parse/dependencygraph.py_get_cycle_path,"def get_cycle_path(self, curr_node, goal_node_index):
for dep in curr_node[""deps""]:
if dep == goal_node_index:
return [curr_node[""address""]]
for dep in curr_node[""deps""]:
path = self.get_cycle_path(self.get_by_address(dep), goal_node_index)
if len(path) > 0:
path.insert(0, curr_node[""address""])
return path
return []
",[],0,[],/parse/dependencygraph.py_get_cycle_path
1625,/home/amandapotts/git/nltk/nltk/parse/dependencygraph.py_to_conll,"def to_conll(self, style):
""""""
The dependency graph in CoNLL format.
:param style: the style to use for the format (3, 4, 10 columns)
:type style: int
:rtype: str
""""""
if style == 3:
template = ""{word}\t{tag}\t{head}\n""
elif style == 4:
template = ""{word}\t{tag}\t{head}\t{rel}\n""
elif style == 10:
template = (
""{i}\t{word}\t{lemma}\t{ctag}\t{tag}\t{feats}\t{head}\t{rel}\t_\t_\n""
)
else:
raise ValueError(
""Number of tab-delimited fields ({}) not supported by ""
""CoNLL(10) or Malt-Tab(4) format"".format(style)
)
return """".join(
template.format(i=i, **node)
for i, node in sorted(self.nodes.items())
if node[""tag""] != ""TOP""
)
",[],0,[],/parse/dependencygraph.py_to_conll
1626,/home/amandapotts/git/nltk/nltk/parse/dependencygraph.py_nx_graph,"def nx_graph(self):
""""""Convert the data in a ``nodelist`` into a networkx labeled directed graph.""""""
import networkx
nx_nodelist = list(range(1, len(self.nodes)))
nx_edgelist = [
(n, self._hd(n), self._rel(n)) for n in nx_nodelist if self._hd(n)
]
self.nx_labels = {}
for n in nx_nodelist:
self.nx_labels[n] = self.nodes[n][""word""]
g = networkx.MultiDiGraph()
g.add_nodes_from(nx_nodelist)
g.add_edges_from(nx_edgelist)
return g
",[],0,[],/parse/dependencygraph.py_nx_graph
1627,/home/amandapotts/git/nltk/nltk/parse/dependencygraph.py_dot2img,"def dot2img(dot_string, t=""svg""):
""""""
Create image representation fom dot_string, using the 'dot' program
from the Graphviz package.
Use the 't' argument to specify the image file format, for ex. 'jpeg', 'eps',
'json', 'png' or 'webp' (Running 'dot -T:' lists all available formats).
Note that the ""capture_output"" option of subprocess.run() is only available
with text formats (like svg), but not with binary image formats (like png).
""""""
try:
find_binary(""dot"")
try:
if t in [""dot"", ""dot_json"", ""json"", ""svg""]:
proc = subprocess.run(
[""dot"", ""-T%s"" % t],
capture_output=True,
input=dot_string,
text=True,
)
else:
proc = subprocess.run(
[""dot"", ""-T%s"" % t],
input=bytes(dot_string, encoding=""utf8""),
)
return proc.stdout
except:
raise Exception(
""Cannot create image representation by running dot from string: {}""
"""".format(dot_string)
)
except OSError as e:
raise Exception(""Cannot find the dot binary from Graphviz package"") from e
",[],0,[],/parse/dependencygraph.py_dot2img
1628,/home/amandapotts/git/nltk/nltk/parse/dependencygraph.py_demo,"def demo():
malt_demo()
conll_demo()
conll_file_demo()
cycle_finding_demo()
",[],0,[],/parse/dependencygraph.py_demo
1629,/home/amandapotts/git/nltk/nltk/parse/dependencygraph.py_malt_demo,"def malt_demo(nx=False):
""""""
A demonstration of the result of reading a dependency
version of the first sentence of the Penn Treebank.
""""""
dg = DependencyGraph(
""""""Pierre  NNP     2       NMOD
",[],0,[],/parse/dependencygraph.py_malt_demo
1630,/home/amandapotts/git/nltk/nltk/parse/dependencygraph.py_conll_demo,"def conll_demo():
""""""
A demonstration of how to read a string representation of
a CoNLL format dependency tree.
""""""
dg = DependencyGraph(conll_data1)
tree = dg.tree()
tree.pprint()
print(dg)
print(dg.to_conll(4))
",[],0,[],/parse/dependencygraph.py_conll_demo
1631,/home/amandapotts/git/nltk/nltk/parse/dependencygraph.py_conll_file_demo,"def conll_file_demo():
print(""Mass conll_read demo..."")
graphs = [DependencyGraph(entry) for entry in conll_data2.split(""\n\n"") if entry]
for graph in graphs:
tree = graph.tree()
print(""\n"")
tree.pprint()
",[],0,[],/parse/dependencygraph.py_conll_file_demo
1632,/home/amandapotts/git/nltk/nltk/parse/dependencygraph.py_cycle_finding_demo,"def cycle_finding_demo():
dg = DependencyGraph(treebank_data)
print(dg.contains_cycle())
cyclic_dg = DependencyGraph()
cyclic_dg.add_node({""word"": None, ""deps"": [1], ""rel"": ""TOP"", ""address"": 0})
cyclic_dg.add_node({""word"": None, ""deps"": [2], ""rel"": ""NTOP"", ""address"": 1})
cyclic_dg.add_node({""word"": None, ""deps"": [4], ""rel"": ""NTOP"", ""address"": 2})
cyclic_dg.add_node({""word"": None, ""deps"": [1], ""rel"": ""NTOP"", ""address"": 3})
cyclic_dg.add_node({""word"": None, ""deps"": [3], ""rel"": ""NTOP"", ""address"": 4})
print(cyclic_dg.contains_cycle())
",[],0,[],/parse/dependencygraph.py_cycle_finding_demo
1633,/home/amandapotts/git/nltk/nltk/parse/evaluate.py___init__,"def __init__(self, parsed_sents, gold_sents):
""""""
:param parsed_sents: the list of parsed_sents as the output of parser
:type parsed_sents: list(DependencyGraph)
""""""
self._parsed_sents = parsed_sents
self._gold_sents = gold_sents
",[],0,[],/parse/evaluate.py___init__
1634,/home/amandapotts/git/nltk/nltk/parse/evaluate.py__remove_punct,"def _remove_punct(self, inStr):
""""""
Function to remove punctuation from Unicode string.
:param input: the input string
:return: Unicode string after remove all punctuation
""""""
punc_cat = {""Pc"", ""Pd"", ""Ps"", ""Pe"", ""Pi"", ""Pf"", ""Po""}
return """".join(x for x in inStr if unicodedata.category(x) not in punc_cat)
",[],0,[],/parse/evaluate.py__remove_punct
1635,/home/amandapotts/git/nltk/nltk/parse/evaluate.py_eval,"def eval(self):
""""""
Return the Labeled Attachment Score (LAS) and Unlabeled Attachment Score (UAS)
:return : tuple(float,float)
""""""
if len(self._parsed_sents) != len(self._gold_sents):
raise ValueError(
"" Number of parsed sentence is different with number of gold sentence.""
)
corr = 0
corrL = 0
total = 0
for i in range(len(self._parsed_sents)):
parsed_sent_nodes = self._parsed_sents[i].nodes
gold_sent_nodes = self._gold_sents[i].nodes
if len(parsed_sent_nodes) != len(gold_sent_nodes):
raise ValueError(""Sentences must have equal length."")
for parsed_node_address, parsed_node in parsed_sent_nodes.items():
gold_node = gold_sent_nodes[parsed_node_address]
if parsed_node[""word""] is None:
continue
if parsed_node[""word""] != gold_node[""word""]:
raise ValueError(""Sentence sequence is not matched."")
if self._remove_punct(parsed_node[""word""]) == """":
continue
total += 1
if parsed_node[""head""] == gold_node[""head""]:
corr += 1
if parsed_node[""rel""] == gold_node[""rel""]:
corrL += 1
return corrL / total, corr / total
",[],0,[],/parse/evaluate.py_eval
1636,/home/amandapotts/git/nltk/nltk/parse/util.py_load_parser,"def load_parser(
grammar_url, trace=0, parser=None, chart_class=None, beam_size=0, **load_args
",[],0,[],/parse/util.py_load_parser
1637,/home/amandapotts/git/nltk/nltk/parse/util.py_taggedsent_to_conll,"def taggedsent_to_conll(sentence):
""""""
A module to convert a single POS tagged sentence into CONLL format.
>>> from nltk import word_tokenize, pos_tag
>>> text = ""This is a foobar sentence.""
>>> for line in taggedsent_to_conll(pos_tag(word_tokenize(text))): # doctest: +NORMALIZE_WHITESPACE
... 	print(line, end="""")
1	This	_	DT	DT	_	0	a	_	_
2	is	_	VBZ	VBZ	_	0	a	_	_
3	a	_	DT	DT	_	0	a	_	_
4	foobar	_	JJ	JJ	_	0	a	_	_
5	sentence	_	NN	NN	_	0	a	_	_
6	.		_	.	.	_	0	a	_	_
:param sentence: A single input sentence to parse
:type sentence: list(tuple(str, str))
:rtype: iter(str)
:return: a generator yielding a single sentence in CONLL format.
""""""
for i, (word, tag) in enumerate(sentence, start=1):
input_str = [str(i), word, ""_"", tag, tag, ""_"", ""0"", ""a"", ""_"", ""_""]
input_str = ""\t"".join(input_str) + ""\n""
yield input_str
",[],0,[],/parse/util.py_taggedsent_to_conll
1638,/home/amandapotts/git/nltk/nltk/parse/util.py_taggedsents_to_conll,"def taggedsents_to_conll(sentences):
""""""
A module to convert the a POS tagged document stream
(i.e. list of list of tuples, a list of sentences) and yield lines
in CONLL format. This module yields one line per word and two newlines
for end of sentence.
>>> from nltk import word_tokenize, sent_tokenize, pos_tag
>>> text = ""This is a foobar sentence. Is that right?""
>>> sentences = [pos_tag(word_tokenize(sent)) for sent in sent_tokenize(text)]
>>> for line in taggedsents_to_conll(sentences): # doctest: +NORMALIZE_WHITESPACE
...     if line:
...         print(line, end="""")
1	This	_	DT	DT	_	0	a	_	_
2	is	_	VBZ	VBZ	_	0	a	_	_
3	a	_	DT	DT	_	0	a	_	_
4	foobar	_	JJ	JJ	_	0	a	_	_
5	sentence	_	NN	NN	_	0	a	_	_
6	.		_	.	.	_	0	a	_	_
<BLANKLINE>
<BLANKLINE>
1	Is	_	VBZ	VBZ	_	0	a	_	_
2	that	_	IN	IN	_	0	a	_	_
3	right	_	NN	NN	_	0	a	_	_
4	?	_	.	.	_	0	a	_	_
<BLANKLINE>
<BLANKLINE>
:param sentences: Input sentences to parse
:type sentence: list(list(tuple(str, str)))
:rtype: iter(str)
:return: a generator yielding sentences in CONLL format.
""""""
for sentence in sentences:
yield from taggedsent_to_conll(sentence)
yield ""\n\n""
",[],0,[],/parse/util.py_taggedsents_to_conll
1639,/home/amandapotts/git/nltk/nltk/parse/util.py___init__,"def __init__(self, grammar, suite, accept=None, reject=None):
self.test_grammar = grammar
self.cp = load_parser(grammar, trace=0)
self.suite = suite
self._accept = accept
self._reject = reject
",[],0,[],/parse/util.py___init__
1640,/home/amandapotts/git/nltk/nltk/parse/util.py_run,"def run(self, show_trees=False):
""""""
Sentences in the test suite are divided into two classes:
- grammatical (``accept``) and
- ungrammatical (``reject``).
If a sentence should parse according to the grammar, the value of
``trees`` will be a non-empty list. If a sentence should be rejected
according to the grammar, then the value of ``trees`` will be None.
""""""
for test in self.suite:
print(test[""doc""] + "":"", end="" "")
for key in [""accept"", ""reject""]:
for sent in test[key]:
tokens = sent.split()
trees = list(self.cp.parse(tokens))
if show_trees and trees:
print()
print(sent)
for tree in trees:
print(tree)
if key == ""accept"":
if trees == []:
raise ValueError(""Sentence '%s' failed to parse'"" % sent)
else:
accepted = True
else:
if trees:
raise ValueError(""Sentence '%s' received a parse'"" % sent)
else:
rejected = True
if accepted and rejected:
print(""All tests passed!"")
",[],0,[],/parse/util.py_run
1641,/home/amandapotts/git/nltk/nltk/parse/util.py_extract_test_sentences,"def extract_test_sentences(string, comment_chars=""#%
""""""
Parses a string with one test sentence per line.
Lines can optionally begin with:
- a bool, saying if the sentence is grammatical or not, or
- an int, giving the number of parse trees is should have,
The result information is followed by a colon, and then the sentence.
Empty lines and lines beginning with a comment char are ignored.
:return: a list of tuple of sentences and expected results,
where a sentence is a list of str,
and a result is None, or bool, or int
:param comment_chars: ``str`` of possible comment characters.
:param encoding: the encoding of the string, if it is binary
""""""
if encoding is not None:
string = string.decode(encoding)
sentences = []
for sentence in string.split(""\n""):
if sentence == """" or sentence[0] in comment_chars:
continue
split_info = sentence.split("":"", 1)
result = None
if len(split_info) == 2:
if split_info[0] in [""True"", ""true"", ""False"", ""false""]:
result = split_info[0] in [""True"", ""true""]
sentence = split_info[1]
else:
result = int(split_info[0])
sentence = split_info[1]
tokens = sentence.split()
if tokens == []:
continue
sentences += [(tokens, result)]
return sentences
",[],0,[],/parse/util.py_extract_test_sentences
1642,/home/amandapotts/git/nltk/nltk/parse/bllip.py__ensure_bllip_import_or_error,"def _ensure_bllip_import_or_error():
pass
",[],0,[],/parse/bllip.py__ensure_bllip_import_or_error
1643,/home/amandapotts/git/nltk/nltk/parse/bllip.py__ensure_bllip_import_or_error,"def _ensure_bllip_import_or_error(ie=ie):
raise ImportError(""Couldn't import bllipparser module: %s"" % ie)
",[],0,[],/parse/bllip.py__ensure_bllip_import_or_error
1644,/home/amandapotts/git/nltk/nltk/parse/bllip.py__ensure_ascii,"def _ensure_ascii(words):
try:
for i, word in enumerate(words):
word.encode(""ascii"")
except UnicodeEncodeError as e:
raise ValueError(
f""Token {i} ({word!r}) is non-ASCII. BLLIP Parser ""
""currently doesn't support non-ASCII inputs.""
) from e
",[],0,[],/parse/bllip.py__ensure_ascii
1645,/home/amandapotts/git/nltk/nltk/parse/bllip.py__scored_parse_to_nltk_tree,"def _scored_parse_to_nltk_tree(scored_parse):
return Tree.fromstring(str(scored_parse.ptb_parse))
",[],0,[],/parse/bllip.py__scored_parse_to_nltk_tree
1646,/home/amandapotts/git/nltk/nltk/parse/bllip.py___init__,"def __init__(
self,
parser_model=None,
reranker_features=None,
reranker_weights=None,
parser_options=None,
reranker_options=None,
",[],0,[],/parse/bllip.py___init__
1647,/home/amandapotts/git/nltk/nltk/parse/bllip.py_parse,"def parse(self, sentence):
""""""
Use BLLIP Parser to parse a sentence. Takes a sentence as a list
of words
instance's tagger.
:return: An iterator that generates parse trees for the sentence
from most likely to least likely.
:param sentence: The sentence to be parsed
:type sentence: list(str)
:rtype: iter(Tree)
""""""
_ensure_ascii(sentence)
nbest_list = self.rrp.parse(sentence)
for scored_parse in nbest_list:
yield _scored_parse_to_nltk_tree(scored_parse)
",[],0,[],/parse/bllip.py_parse
1648,/home/amandapotts/git/nltk/nltk/parse/bllip.py_tagged_parse,"def tagged_parse(self, word_and_tag_pairs):
""""""
Use BLLIP to parse a sentence. Takes a sentence as a list of
(word, tag) tuples
and tagged. BLLIP will attempt to use the tags provided but may
use others if it can't come up with a complete parse subject
to those constraints. You may also specify a tag as ``None``
to leave a token's tag unconstrained.
:return: An iterator that generates parse trees for the sentence
from most likely to least likely.
:param sentence: Input sentence to parse as (word, tag) pairs
:type sentence: list(tuple(str, str))
:rtype: iter(Tree)
""""""
words = []
tag_map = {}
for i, (word, tag) in enumerate(word_and_tag_pairs):
words.append(word)
if tag is not None:
tag_map[i] = tag
_ensure_ascii(words)
nbest_list = self.rrp.parse_tagged(words, tag_map)
for scored_parse in nbest_list:
yield _scored_parse_to_nltk_tree(scored_parse)
",[],0,[],/parse/bllip.py_tagged_parse
1649,/home/amandapotts/git/nltk/nltk/parse/bllip.py_from_unified_model_dir,"def from_unified_model_dir(
cls, model_dir, parser_options=None, reranker_options=None
",[],0,[],/parse/bllip.py_from_unified_model_dir
1650,/home/amandapotts/git/nltk/nltk/parse/bllip.py_demo,"def demo():
""""""This assumes the Python module bllipparser is installed.""""""
from nltk.data import find
model_dir = find(""models/bllip_wsj_no_aux"").path
print(""Loading BLLIP Parsing models..."")
bllip = BllipParser.from_unified_model_dir(model_dir)
print(""Done."")
sentence1 = ""British left waffles on Falklands ."".split()
sentence2 = ""I saw the man with the telescope ."".split()
fail1 = ""# ! ? : -"".split()
for sentence in (sentence1, sentence2, fail1):
print(""Sentence: %r"" % "" "".join(sentence))
try:
tree = next(bllip.parse(sentence))
print(tree)
except StopIteration:
print(""(parse failed)"")
for i, parse in enumerate(bllip.parse(sentence1)):
print(""parse %d:\n%s"" % (i, parse))
print(
""forcing 'tree' to be 'NN':"",
next(bllip.tagged_parse([(""A"", None), (""tree"", ""NN"")])),
)
print(
""forcing 'A' to be 'DT' and 'tree' to be 'NNP':"",
next(bllip.tagged_parse([(""A"", ""DT""), (""tree"", ""NNP"")])),
)
print(
""forcing 'A' to be 'NNP':"",
next(bllip.tagged_parse([(""A"", ""NNP""), (""tree"", None)])),
)
",[],0,[],/parse/bllip.py_demo
1651,/home/amandapotts/git/nltk/nltk/parse/shiftreduce.py___init__,"def __init__(self, grammar, trace=0):
""""""
Create a new ``ShiftReduceParser``, that uses ``grammar`` to
parse texts.
:type grammar: Grammar
:param grammar: The grammar used to parse texts.
:type trace: int
:param trace: The level of tracing that should be used when
parsing a text.  ``0`` will generate no tracing output
and higher numbers will produce more verbose tracing
output.
""""""
self._grammar = grammar
self._trace = trace
self._check_grammar()
",[],0,[],/parse/shiftreduce.py___init__
1652,/home/amandapotts/git/nltk/nltk/parse/shiftreduce.py_grammar,"def grammar(self):
return self._grammar
",[],0,[],/parse/shiftreduce.py_grammar
1653,/home/amandapotts/git/nltk/nltk/parse/shiftreduce.py_parse,"def parse(self, tokens):
tokens = list(tokens)
self._grammar.check_coverage(tokens)
stack = []
remaining_text = tokens
if self._trace:
print(""Parsing %r"" % "" "".join(tokens))
self._trace_stack(stack, remaining_text)
while len(remaining_text) > 0:
self._shift(stack, remaining_text)
while self._reduce(stack, remaining_text):
pass
if len(stack) == 1:
if stack[0].label() == self._grammar.start().symbol():
yield stack[0]
",[],0,[],/parse/shiftreduce.py_parse
1654,/home/amandapotts/git/nltk/nltk/parse/shiftreduce.py__shift,"def _shift(self, stack, remaining_text):
""""""
Move a token from the beginning of ``remaining_text`` to the
end of ``stack``.
:type stack: list(str and Tree)
:param stack: A list of strings and Trees, encoding
the structure of the text that has been parsed so far.
:type remaining_text: list(str)
:param remaining_text: The portion of the text that is not yet
covered by ``stack``.
:rtype: None
""""""
stack.append(remaining_text[0])
remaining_text.remove(remaining_text[0])
if self._trace:
self._trace_shift(stack, remaining_text)
",[],0,[],/parse/shiftreduce.py__shift
1655,/home/amandapotts/git/nltk/nltk/parse/shiftreduce.py__match_rhs,"def _match_rhs(self, rhs, rightmost_stack):
""""""
:rtype: bool
:return: true if the right hand side of a CFG production
matches the rightmost elements of the stack.  ``rhs``
matches ``rightmost_stack`` if they are the same length,
and each element of ``rhs`` matches the corresponding
element of ``rightmost_stack``.  A nonterminal element of
``rhs`` matches any Tree whose node value is equal
to the nonterminal's symbol.  A terminal element of ``rhs``
matches any string whose type is equal to the terminal.
:type rhs: list(terminal and Nonterminal)
:param rhs: The right hand side of a CFG production.
:type rightmost_stack: list(string and Tree)
:param rightmost_stack: The rightmost elements of the parser's
stack.
""""""
if len(rightmost_stack) != len(rhs):
return False
for i in range(len(rightmost_stack)):
if isinstance(rightmost_stack[i], Tree):
if not isinstance(rhs[i], Nonterminal):
return False
if rightmost_stack[i].label() != rhs[i].symbol():
return False
else:
if isinstance(rhs[i], Nonterminal):
return False
if rightmost_stack[i] != rhs[i]:
return False
return True
",[],0,[],/parse/shiftreduce.py__match_rhs
1656,/home/amandapotts/git/nltk/nltk/parse/shiftreduce.py__reduce,"def _reduce(self, stack, remaining_text, production=None):
""""""
Find a CFG production whose right hand side matches the
rightmost stack elements
into a single Tree, with the node specified by the
production's left-hand side.  If more than one CFG production
matches the stack, then use the production that is listed
earliest in the grammar.  The new Tree replaces the
elements in the stack.
:rtype: Production or None
:return: If a reduction is performed, then return the CFG
production that the reduction is based on
return false.
:type stack: list(string and Tree)
:param stack: A list of strings and Trees, encoding
the structure of the text that has been parsed so far.
:type remaining_text: list(str)
:param remaining_text: The portion of the text that is not yet
covered by ``stack``.
""""""
if production is None:
productions = self._grammar.productions()
else:
productions = [production]
for production in productions:
rhslen = len(production.rhs())
if self._match_rhs(production.rhs(), stack[-rhslen:]):
tree = Tree(production.lhs().symbol(), stack[-rhslen:])
stack[-rhslen:] = [tree]
if self._trace:
self._trace_reduce(stack, production, remaining_text)
return production
return None
",[],0,[],/parse/shiftreduce.py__reduce
1657,/home/amandapotts/git/nltk/nltk/parse/shiftreduce.py_trace,"def trace(self, trace=2):
""""""
Set the level of tracing output that should be generated when
parsing a text.
:type trace: int
:param trace: The trace level.  A trace level of ``0`` will
generate no tracing output
produce more verbose tracing output.
:rtype: None
""""""
self._trace = trace
",[],0,[],/parse/shiftreduce.py_trace
1658,/home/amandapotts/git/nltk/nltk/parse/shiftreduce.py__trace_stack,"def _trace_stack(self, stack, remaining_text, marker="" ""):
""""""
Print trace output displaying the given stack and text.
:rtype: None
:param marker: A character that is printed to the left of the
stack.  This is used with trace level 2 to print 'S'
before shifted stacks and 'R' before reduced stacks.
""""""
s = ""  "" + marker + "" [ ""
for elt in stack:
if isinstance(elt, Tree):
s += repr(Nonterminal(elt.label())) + "" ""
else:
s += repr(elt) + "" ""
s += ""* "" + "" "".join(remaining_text) + ""]""
print(s)
",[],0,[],/parse/shiftreduce.py__trace_stack
1659,/home/amandapotts/git/nltk/nltk/parse/shiftreduce.py__trace_shift,"def _trace_shift(self, stack, remaining_text):
""""""
Print trace output displaying that a token has been shifted.
:rtype: None
""""""
if self._trace > 2:
print(""Shift %r:"" % stack[-1])
if self._trace == 2:
self._trace_stack(stack, remaining_text, ""S"")
elif self._trace > 0:
self._trace_stack(stack, remaining_text)
",[],0,[],/parse/shiftreduce.py__trace_shift
1660,/home/amandapotts/git/nltk/nltk/parse/shiftreduce.py__trace_reduce,"def _trace_reduce(self, stack, production, remaining_text):
""""""
Print trace output displaying that ``production`` was used to
reduce ``stack``.
:rtype: None
""""""
if self._trace > 2:
rhs = "" "".join(production.rhs())
print(f""Reduce {production.lhs()!r} <- {rhs}"")
if self._trace == 2:
self._trace_stack(stack, remaining_text, ""R"")
elif self._trace > 1:
self._trace_stack(stack, remaining_text)
",[],0,[],/parse/shiftreduce.py__trace_reduce
1661,/home/amandapotts/git/nltk/nltk/parse/shiftreduce.py__check_grammar,"def _check_grammar(self):
""""""
Check to make sure that all of the CFG productions are
potentially useful.  If any productions can never be used,
then print a warning.
:rtype: None
""""""
productions = self._grammar.productions()
for i in range(len(productions)):
for j in range(i + 1, len(productions)):
rhs1 = productions[i].rhs()
rhs2 = productions[j].rhs()
if rhs1[: len(rhs2)] == rhs2:
print(""Warning: %r will never be used"" % productions[i])
",[],0,[],/parse/shiftreduce.py__check_grammar
1662,/home/amandapotts/git/nltk/nltk/parse/shiftreduce.py___init__,"def __init__(self, grammar, trace=0):
super().__init__(grammar, trace)
self._stack = None
self._remaining_text = None
self._history = []
",[],0,[],/parse/shiftreduce.py___init__
1663,/home/amandapotts/git/nltk/nltk/parse/shiftreduce.py_parse,"def parse(self, tokens):
tokens = list(tokens)
self.initialize(tokens)
while self.step():
pass
return self.parses()
",[],0,[],/parse/shiftreduce.py_parse
1664,/home/amandapotts/git/nltk/nltk/parse/shiftreduce.py_stack,"def stack(self):
""""""
:return: The parser's stack.
:rtype: list(str and Tree)
""""""
return self._stack
",[],0,[],/parse/shiftreduce.py_stack
1665,/home/amandapotts/git/nltk/nltk/parse/shiftreduce.py_remaining_text,"def remaining_text(self):
""""""
:return: The portion of the text that is not yet covered by the
stack.
:rtype: list(str)
""""""
return self._remaining_text
",[],0,[],/parse/shiftreduce.py_remaining_text
1666,/home/amandapotts/git/nltk/nltk/parse/shiftreduce.py_initialize,"def initialize(self, tokens):
""""""
Start parsing a given text.  This sets the parser's stack to
``[]`` and sets its remaining text to ``tokens``.
""""""
self._stack = []
self._remaining_text = tokens
self._history = []
",[],0,[],/parse/shiftreduce.py_initialize
1667,/home/amandapotts/git/nltk/nltk/parse/shiftreduce.py_step,"def step(self):
""""""
Perform a single parsing operation.  If a reduction is
possible, then perform that reduction, and return the
production that it is based on.  Otherwise, if a shift is
possible, then perform it, and return True.  Otherwise,
return False.
:return: False if no operation was performed
performed
reduction was performed.
:rtype: Production or bool
""""""
return self.reduce() or self.shift()
",[],0,[],/parse/shiftreduce.py_step
1668,/home/amandapotts/git/nltk/nltk/parse/shiftreduce.py_shift,"def shift(self):
""""""
Move a token from the beginning of the remaining text to the
end of the stack.  If there are no more tokens in the
remaining text, then do nothing.
:return: True if the shift operation was successful.
:rtype: bool
""""""
if len(self._remaining_text) == 0:
return False
self._history.append((self._stack[:], self._remaining_text[:]))
self._shift(self._stack, self._remaining_text)
return True
",[],0,[],/parse/shiftreduce.py_shift
1669,/home/amandapotts/git/nltk/nltk/parse/shiftreduce.py_reduce,"def reduce(self, production=None):
""""""
Use ``production`` to combine the rightmost stack elements into
a single Tree.  If ``production`` does not match the
rightmost stack elements, then do nothing.
:return: The production used to reduce the stack, if a
reduction was performed.  If no reduction was performed,
return None.
:rtype: Production or None
""""""
self._history.append((self._stack[:], self._remaining_text[:]))
return_val = self._reduce(self._stack, self._remaining_text, production)
if not return_val:
self._history.pop()
return return_val
",[],0,[],/parse/shiftreduce.py_reduce
1670,/home/amandapotts/git/nltk/nltk/parse/shiftreduce.py_undo,"def undo(self):
""""""
Return the parser to its state before the most recent
shift or reduce operation.  Calling ``undo`` repeatedly return
the parser to successively earlier states.  If no shift or
reduce operations have been performed, ``undo`` will make no
changes.
:return: true if an operation was successfully undone.
:rtype: bool
""""""
if len(self._history) == 0:
return False
(self._stack, self._remaining_text) = self._history.pop()
return True
",[],0,[],/parse/shiftreduce.py_undo
1671,/home/amandapotts/git/nltk/nltk/parse/shiftreduce.py_reducible_productions,"def reducible_productions(self):
""""""
:return: A list of the productions for which reductions are
available for the current parser state.
:rtype: list(Production)
""""""
productions = []
for production in self._grammar.productions():
rhslen = len(production.rhs())
if self._match_rhs(production.rhs(), self._stack[-rhslen:]):
productions.append(production)
return productions
",[],0,[],/parse/shiftreduce.py_reducible_productions
1672,/home/amandapotts/git/nltk/nltk/parse/shiftreduce.py_parses,"def parses(self):
""""""
:return: An iterator of the parses that have been found by this
parser so far.
:rtype: iter(Tree)
""""""
if (
len(self._remaining_text) == 0
and len(self._stack) == 1
and self._stack[0].label() == self._grammar.start().symbol()
):
yield self._stack[0]
",[],0,[],/parse/shiftreduce.py_parses
1673,/home/amandapotts/git/nltk/nltk/parse/shiftreduce.py_set_grammar,"def set_grammar(self, grammar):
""""""
Change the grammar used to parse texts.
:param grammar: The new grammar.
:type grammar: CFG
""""""
self._grammar = grammar
",[],0,[],/parse/shiftreduce.py_set_grammar
1674,/home/amandapotts/git/nltk/nltk/parse/shiftreduce.py_demo,"def demo():
""""""
A demonstration of the shift-reduce parser.
""""""
from nltk import CFG, parse
grammar = CFG.fromstring(
""""""
S -> NP VP
NP -> Det N | Det N PP
VP -> V NP | V NP PP
PP -> P NP
NP -> 'I'
N -> 'man' | 'park' | 'telescope' | 'dog'
Det -> 'the' | 'a'
P -> 'in' | 'with'
V -> 'saw'
""""""
)
sent = ""I saw a man in the park"".split()
parser = parse.ShiftReduceParser(grammar, trace=2)
for p in parser.parse(sent):
print(p)
",[],0,[],/parse/shiftreduce.py_demo
1675,/home/amandapotts/git/nltk/nltk/parse/corenlp.py_try_port,"def try_port(port=0):
sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
sock.bind(("""", port))
p = sock.getsockname()[1]
sock.close()
return p
",[],0,[],/parse/corenlp.py_try_port
1676,/home/amandapotts/git/nltk/nltk/parse/corenlp.py_start,"def start(self, stdout=""devnull"", stderr=""devnull""):
""""""Starts the CoreNLP server
:param stdout, stderr: Specifies where CoreNLP output is redirected. Valid values are 'devnull', 'stdout', 'pipe'
""""""
import requests
cmd = [""edu.stanford.nlp.pipeline.StanfordCoreNLPServer""]
if self.corenlp_options:
cmd.extend(self.corenlp_options)
default_options = "" "".join(_java_options)
config_java(options=self.java_options, verbose=self.verbose)
try:
self.popen = java(
cmd,
classpath=self._classpath,
blocking=False,
stdout=stdout,
stderr=stderr,
)
finally:
config_java(options=default_options, verbose=self.verbose)
returncode = self.popen.poll()
if returncode is not None:
_, stderrdata = self.popen.communicate()
raise CoreNLPServerError(
returncode,
""Could not start the server. ""
""The error was: {}"".format(stderrdata.decode(""ascii"")),
)
for i in range(30):
try:
response = requests.get(requests.compat.urljoin(self.url, ""live""))
except requests.exceptions.ConnectionError:
time.sleep(1)
else:
if response.ok:
break
else:
raise CoreNLPServerError(""Could not connect to the server."")
for i in range(60):
try:
response = requests.get(requests.compat.urljoin(self.url, ""ready""))
except requests.exceptions.ConnectionError:
time.sleep(1)
else:
if response.ok:
break
else:
raise CoreNLPServerError(""The server is not ready."")
",[],0,[],/parse/corenlp.py_start
1677,/home/amandapotts/git/nltk/nltk/parse/corenlp.py_stop,"def stop(self):
self.popen.terminate()
self.popen.wait()
",[],0,[],/parse/corenlp.py_stop
1678,/home/amandapotts/git/nltk/nltk/parse/corenlp.py___enter__,"def __enter__(self):
self.start()
return self
",[],0,[],/parse/corenlp.py___enter__
1679,/home/amandapotts/git/nltk/nltk/parse/corenlp.py___exit__,"def __exit__(self, exc_type, exc_val, exc_tb):
self.stop()
return False
",[],0,[],/parse/corenlp.py___exit__
1680,/home/amandapotts/git/nltk/nltk/parse/corenlp.py___init__,"def __init__(
self,
url=""http://localhost:9000"",
encoding=""utf8"",
tagtype=None,
strict_json=True,
",[],0,[],/parse/corenlp.py___init__
1681,/home/amandapotts/git/nltk/nltk/parse/corenlp.py_parse_sents,"def parse_sents(self, sentences, *args, **kwargs):
""""""Parse multiple sentences.
Takes multiple sentences as a list where each sentence is a list of
words. Each sentence will be automatically tagged with this
CoreNLPParser instance's tagger.
If a whitespace exists inside a token, then the token will be treated as
several tokens.
:param sentences: Input sentences to parse
:type sentences: list(list(str))
:rtype: iter(iter(Tree))
""""""
sentences = ("" "".join(words) for words in sentences)
return self.raw_parse_sents(sentences, *args, **kwargs)
",[],0,[],/parse/corenlp.py_parse_sents
1682,/home/amandapotts/git/nltk/nltk/parse/corenlp.py_raw_parse,"def raw_parse(self, sentence, properties=None, *args, **kwargs):
""""""Parse a sentence.
Takes a sentence as a string
tokenized and tagged by the CoreNLP Parser.
:param sentence: Input sentence to parse
:type sentence: str
:rtype: iter(Tree)
""""""
default_properties = {""tokenize.whitespace"": ""false""}
default_properties.update(properties or {})
return next(
self.raw_parse_sents(
[sentence], properties=default_properties, *args, **kwargs
)
)
",[],0,[],/parse/corenlp.py_raw_parse
1683,/home/amandapotts/git/nltk/nltk/parse/corenlp.py_api_call,"def api_call(self, data, properties=None, timeout=60):
default_properties = {
""outputFormat"": ""json"",
""annotators"": ""tokenize,pos,lemma,ssplit,{parser_annotator}"".format(
parser_annotator=self.parser_annotator
),
}
default_properties.update(properties or {})
response = self.session.post(
self.url,
params={""properties"": json.dumps(default_properties)},
data=data.encode(self.encoding),
headers={""Content-Type"": f""text/plain
timeout=timeout,
)
response.raise_for_status()
return response.json(strict=self.strict_json)
",[],0,[],/parse/corenlp.py_api_call
1684,/home/amandapotts/git/nltk/nltk/parse/corenlp.py_raw_parse_sents,"def raw_parse_sents(
self, sentences, verbose=False, properties=None, *args, **kwargs
",[],0,[],/parse/corenlp.py_raw_parse_sents
1685,/home/amandapotts/git/nltk/nltk/parse/corenlp.py_parse_text,"def parse_text(self, text, *args, **kwargs):
""""""Parse a piece of text.
The text might contain several sentences which will be split by CoreNLP.
:param str text: text to be split.
:returns: an iterable of syntactic structures.  # TODO: should it be an iterable of iterables?
""""""
parsed_data = self.api_call(text, *args, **kwargs)
for parse in parsed_data[""sentences""]:
yield self.make_tree(parse)
",[],0,[],/parse/corenlp.py_parse_text
1686,/home/amandapotts/git/nltk/nltk/parse/corenlp.py_tokenize,"def tokenize(self, text, properties=None):
""""""Tokenize a string of text.
Skip these tests if CoreNLP is likely not ready.
>>> from nltk.test.setup_fixt import check_jar
>>> check_jar(CoreNLPServer._JAR, env_vars=(""CORENLP"",), is_regex=True)
The CoreNLP server can be started using the following notation, although
we recommend the `with CoreNLPServer() as server:` context manager notation
to ensure that the server is always stopped.
>>> server = CoreNLPServer()
>>> server.start()
>>> parser = CoreNLPParser(url=server.url)
>>> text = 'Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\nThanks.'
>>> list(parser.tokenize(text))
['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']
>>> s = ""The colour of the wall is blue.""
>>> list(
...     parser.tokenize(
...         'The colour of the wall is blue.',
...             properties={'tokenize.options': 'americanize=true'},
...     )
... )
['The', 'colour', 'of', 'the', 'wall', 'is', 'blue', '.']
>>> server.stop()
""""""
default_properties = {""annotators"": ""tokenize,ssplit""}
default_properties.update(properties or {})
result = self.api_call(text, properties=default_properties)
for sentence in result[""sentences""]:
for token in sentence[""tokens""]:
yield token[""originalText""] or token[""word""]
",[],0,[],/parse/corenlp.py_tokenize
1687,/home/amandapotts/git/nltk/nltk/parse/corenlp.py_tag_sents,"def tag_sents(self, sentences):
""""""
Tag multiple sentences.
Takes multiple sentences as a list where each sentence is a list of
tokens.
:param sentences: Input sentences to tag
:type sentences: list(list(str))
:rtype: list(list(tuple(str, str))
""""""
sentences = ("" "".join(words) for words in sentences)
return [sentences[0] for sentences in self.raw_tag_sents(sentences)]
",[],0,[],/parse/corenlp.py_tag_sents
1688,/home/amandapotts/git/nltk/nltk/parse/corenlp.py_tag,"def tag(self, sentence: str) -> List[Tuple[str, str]]:
""""""
Tag a list of tokens.
:rtype: list(tuple(str, str))
Skip these tests if CoreNLP is likely not ready.
>>> from nltk.test.setup_fixt import check_jar
>>> check_jar(CoreNLPServer._JAR, env_vars=(""CORENLP"",), is_regex=True)
The CoreNLP server can be started using the following notation, although
we recommend the `with CoreNLPServer() as server:` context manager notation
to ensure that the server is always stopped.
>>> server = CoreNLPServer()
>>> server.start()
>>> parser = CoreNLPParser(url=server.url, tagtype='ner')
>>> tokens = 'Rami Eid is studying at Stony Brook University in NY'.split()
>>> parser.tag(tokens)  # doctest: +NORMALIZE_WHITESPACE
[('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'), ('at', 'O'), ('Stony', 'ORGANIZATION'),
('Brook', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'STATE_OR_PROVINCE')]
>>> parser = CoreNLPParser(url=server.url, tagtype='pos')
>>> tokens = ""What is the airspeed of an unladen swallow ?"".split()
>>> parser.tag(tokens)  # doctest: +NORMALIZE_WHITESPACE
[('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'),
('airspeed', 'NN'), ('of', 'IN'), ('an', 'DT'),
('unladen', 'JJ'), ('swallow', 'VB'), ('?', '.')]
>>> server.stop()
""""""
return self.tag_sents([sentence])[0]
",[],0,[],/parse/corenlp.py_tag
1689,/home/amandapotts/git/nltk/nltk/parse/corenlp.py_raw_tag_sents,"def raw_tag_sents(self, sentences):
""""""
Tag multiple sentences.
Takes multiple sentences as a list where each sentence is a string.
:param sentences: Input sentences to tag
:type sentences: list(str)
:rtype: list(list(list(tuple(str, str)))
""""""
default_properties = {
""ssplit.isOneSentence"": ""true"",
""annotators"": ""tokenize,ssplit,"",
}
assert self.tagtype in [""pos"", ""ner""]
default_properties[""annotators""] += self.tagtype
for sentence in sentences:
tagged_data = self.api_call(sentence, properties=default_properties)
yield [
[
(token[""word""], token[self.tagtype])
for token in tagged_sentence[""tokens""]
]
for tagged_sentence in tagged_data[""sentences""]
]
",[],0,[],/parse/corenlp.py_raw_tag_sents
1690,/home/amandapotts/git/nltk/nltk/parse/corenlp.py_make_tree,"def make_tree(self, result):
return Tree.fromstring(result[""parse""])
",[],0,[],/parse/corenlp.py_make_tree
1691,/home/amandapotts/git/nltk/nltk/parse/corenlp.py_make_tree,"def make_tree(self, result):
return DependencyGraph(
(
"" "".join(n_items[1:])  # NLTK expects an iterable of strings...
for n_items in sorted(transform(result))
),
cell_separator="" "",  # To make sure that a non-breaking space is kept inside of a token.
)
",[],0,[],/parse/corenlp.py_make_tree
1692,/home/amandapotts/git/nltk/nltk/parse/corenlp.py_transform,"def transform(sentence):
for dependency in sentence[""basicDependencies""]:
dependent_index = dependency[""dependent""]
token = sentence[""tokens""][dependent_index - 1]
yield (
dependent_index,
""_"",
token[""word""],
token[""lemma""],
token[""pos""],
token[""pos""],
""_"",
str(dependency[""governor""]),
dependency[""dep""],
""_"",
""_"",
)
",[],0,[],/parse/corenlp.py_transform
1693,/home/amandapotts/git/nltk/nltk/parse/stanford.py__parse_trees_output,"def _parse_trees_output(self, output_):
res = []
cur_lines = []
cur_trees = []
blank = False
for line in output_.splitlines(False):
if line == """":
if blank:
res.append(iter(cur_trees))
cur_trees = []
blank = False
elif self._DOUBLE_SPACED_OUTPUT:
cur_trees.append(self._make_tree(""\n"".join(cur_lines)))
cur_lines = []
blank = True
else:
res.append(iter([self._make_tree(""\n"".join(cur_lines))]))
cur_lines = []
else:
cur_lines.append(line)
blank = False
return iter(res)
",[],0,[],/parse/stanford.py__parse_trees_output
1694,/home/amandapotts/git/nltk/nltk/parse/stanford.py_parse_sents,"def parse_sents(self, sentences, verbose=False):
""""""
Use StanfordParser to parse multiple sentences. Takes multiple sentences as a
list where each sentence is a list of words.
Each sentence will be automatically tagged with this StanfordParser instance's
tagger.
If whitespaces exists inside a token, then the token will be treated as
separate tokens.
:param sentences: Input sentences to parse
:type sentences: list(list(str))
:rtype: iter(iter(Tree))
""""""
cmd = [
self._MAIN_CLASS,
""-model"",
self.model_path,
""-sentences"",
""newline"",
""-outputFormat"",
self._OUTPUT_FORMAT,
""-tokenized"",
""-escaper"",
""edu.stanford.nlp.process.PTBEscapingProcessor"",
]
return self._parse_trees_output(
self._execute(
cmd, ""\n"".join("" "".join(sentence) for sentence in sentences), verbose
)
)
",[],0,[],/parse/stanford.py_parse_sents
1695,/home/amandapotts/git/nltk/nltk/parse/stanford.py_raw_parse,"def raw_parse(self, sentence, verbose=False):
""""""
Use StanfordParser to parse a sentence. Takes a sentence as a string
before parsing, it will be automatically tokenized and tagged by
the Stanford Parser.
:param sentence: Input sentence to parse
:type sentence: str
:rtype: iter(Tree)
""""""
return next(self.raw_parse_sents([sentence], verbose))
",[],0,[],/parse/stanford.py_raw_parse
1696,/home/amandapotts/git/nltk/nltk/parse/stanford.py_raw_parse_sents,"def raw_parse_sents(self, sentences, verbose=False):
""""""
Use StanfordParser to parse multiple sentences. Takes multiple sentences as a
list of strings.
Each sentence will be automatically tokenized and tagged by the Stanford Parser.
:param sentences: Input sentences to parse
:type sentences: list(str)
:rtype: iter(iter(Tree))
""""""
cmd = [
self._MAIN_CLASS,
""-model"",
self.model_path,
""-sentences"",
""newline"",
""-outputFormat"",
self._OUTPUT_FORMAT,
]
return self._parse_trees_output(
self._execute(cmd, ""\n"".join(sentences), verbose)
)
",[],0,[],/parse/stanford.py_raw_parse_sents
1697,/home/amandapotts/git/nltk/nltk/parse/stanford.py_tagged_parse,"def tagged_parse(self, sentence, verbose=False):
""""""
Use StanfordParser to parse a sentence. Takes a sentence as a list of
(word, tag) tuples
tagged.
:param sentence: Input sentence to parse
:type sentence: list(tuple(str, str))
:rtype: iter(Tree)
""""""
return next(self.tagged_parse_sents([sentence], verbose))
",[],0,[],/parse/stanford.py_tagged_parse
1698,/home/amandapotts/git/nltk/nltk/parse/stanford.py_tagged_parse_sents,"def tagged_parse_sents(self, sentences, verbose=False):
""""""
Use StanfordParser to parse multiple sentences. Takes multiple sentences
where each sentence is a list of (word, tag) tuples.
The sentences must have already been tokenized and tagged.
:param sentences: Input sentences to parse
:type sentences: list(list(tuple(str, str)))
:rtype: iter(iter(Tree))
""""""
tag_separator = ""/""
cmd = [
self._MAIN_CLASS,
""-model"",
self.model_path,
""-sentences"",
""newline"",
""-outputFormat"",
self._OUTPUT_FORMAT,
""-tokenized"",
""-tagSeparator"",
tag_separator,
""-tokenizerFactory"",
""edu.stanford.nlp.process.WhitespaceTokenizer"",
""-tokenizerMethod"",
""newCoreLabelTokenizerFactory"",
]
return self._parse_trees_output(
self._execute(
cmd,
""\n"".join(
"" "".join(tag_separator.join(tagged) for tagged in sentence)
for sentence in sentences
),
verbose,
)
)
",[],0,[],/parse/stanford.py_tagged_parse_sents
1699,/home/amandapotts/git/nltk/nltk/parse/stanford.py__execute,"def _execute(self, cmd, input_, verbose=False):
encoding = self._encoding
cmd.extend([""-encoding"", encoding])
if self.corenlp_options:
cmd.extend(self.corenlp_options.split())
default_options = "" "".join(_java_options)
config_java(options=self.java_options, verbose=verbose)
with tempfile.NamedTemporaryFile(mode=""wb"", delete=False) as input_file:
if isinstance(input_, str) and encoding:
input_ = input_.encode(encoding)
input_file.write(input_)
input_file.flush()
if self._USE_STDIN:
input_file.seek(0)
stdout, stderr = java(
cmd,
classpath=self._classpath,
stdin=input_file,
stdout=PIPE,
stderr=PIPE,
)
else:
cmd.append(input_file.name)
stdout, stderr = java(
cmd, classpath=self._classpath, stdout=PIPE, stderr=PIPE
)
stdout = stdout.replace(b""\xc2\xa0"", b"" "")
stdout = stdout.replace(b""\x00\xa0"", b"" "")
stdout = stdout.decode(encoding)
os.unlink(input_file.name)
config_java(options=default_options, verbose=False)
return stdout
",[],0,[],/parse/stanford.py__execute
1700,/home/amandapotts/git/nltk/nltk/parse/stanford.py___init__,"def __init__(self, *args, **kwargs):
warnings.warn(
""The StanfordParser will be deprecated\n""
""Please use \033[91mnltk.parse.corenlp.CoreNLPParser\033[0m instead."",
DeprecationWarning,
stacklevel=2,
)
super().__init__(*args, **kwargs)
",[],0,[],/parse/stanford.py___init__
1701,/home/amandapotts/git/nltk/nltk/parse/stanford.py__make_tree,"def _make_tree(self, result):
return Tree.fromstring(result)
",[],0,[],/parse/stanford.py__make_tree
1702,/home/amandapotts/git/nltk/nltk/parse/stanford.py___init__,"def __init__(self, *args, **kwargs):
warnings.warn(
""The StanfordDependencyParser will be deprecated\n""
""Please use \033[91mnltk.parse.corenlp.CoreNLPDependencyParser\033[0m instead."",
DeprecationWarning,
stacklevel=2,
)
super().__init__(*args, **kwargs)
",[],0,[],/parse/stanford.py___init__
1703,/home/amandapotts/git/nltk/nltk/parse/stanford.py__make_tree,"def _make_tree(self, result):
return DependencyGraph(result, top_relation_label=""root"")
",[],0,[],/parse/stanford.py__make_tree
1704,/home/amandapotts/git/nltk/nltk/parse/stanford.py___init__,"def __init__(self, *args, **kwargs):
warnings.warn(
""The StanfordNeuralDependencyParser will be deprecated\n""
""Please use \033[91mnltk.parse.corenlp.CoreNLPDependencyParser\033[0m instead."",
DeprecationWarning,
stacklevel=2,
)
super().__init__(*args, **kwargs)
self.corenlp_options += ""-annotators tokenize,ssplit,pos,depparse""
",[],0,[],/parse/stanford.py___init__
1705,/home/amandapotts/git/nltk/nltk/parse/stanford.py_tagged_parse_sents,"def tagged_parse_sents(self, sentences, verbose=False):
""""""
Currently unimplemented because the neural dependency parser (and
the StanfordCoreNLP pipeline class) doesn't support passing in pre-
tagged tokens.
""""""
raise NotImplementedError(
""tagged_parse[_sents] is not supported by ""
""StanfordNeuralDependencyParser
""parse[_sents] or raw_parse[_sents] instead.""
)
",[],0,[],/parse/stanford.py_tagged_parse_sents
1706,/home/amandapotts/git/nltk/nltk/parse/stanford.py__make_tree,"def _make_tree(self, result):
return DependencyGraph(result, top_relation_label=""ROOT"")
",[],0,[],/parse/stanford.py__make_tree
1707,/home/amandapotts/git/nltk/nltk/parse/chart.py___init__,"def __init__(self):
if self.__class__ == EdgeI:
raise TypeError(""Edge is an abstract interface"")
",[],0,[],/parse/chart.py___init__
1708,/home/amandapotts/git/nltk/nltk/parse/chart.py_span,"def span(self):
""""""
Return a tuple ``(s, e)``, where ``tokens[s:e]`` is the
portion of the sentence that is consistent with this
edge's structure.
:rtype: tuple(int, int)
""""""
raise NotImplementedError()
",[],0,[],/parse/chart.py_span
1709,/home/amandapotts/git/nltk/nltk/parse/chart.py_start,"def start(self):
""""""
Return the start index of this edge's span.
:rtype: int
""""""
raise NotImplementedError()
",[],0,[],/parse/chart.py_start
1710,/home/amandapotts/git/nltk/nltk/parse/chart.py_end,"def end(self):
""""""
Return the end index of this edge's span.
:rtype: int
""""""
raise NotImplementedError()
",[],0,[],/parse/chart.py_end
1711,/home/amandapotts/git/nltk/nltk/parse/chart.py_length,"def length(self):
""""""
Return the length of this edge's span.
:rtype: int
""""""
raise NotImplementedError()
",[],0,[],/parse/chart.py_length
1712,/home/amandapotts/git/nltk/nltk/parse/chart.py_lhs,"def lhs(self):
""""""
Return this edge's left-hand side, which specifies what kind
of structure is hypothesized by this edge.
:see: ``TreeEdge`` and ``LeafEdge`` for a description of
the left-hand side values for each edge type.
""""""
raise NotImplementedError()
",[],0,[],/parse/chart.py_lhs
1713,/home/amandapotts/git/nltk/nltk/parse/chart.py_rhs,"def rhs(self):
""""""
Return this edge's right-hand side, which specifies
the content of the structure hypothesized by this edge.
:see: ``TreeEdge`` and ``LeafEdge`` for a description of
the right-hand side values for each edge type.
""""""
raise NotImplementedError()
",[],0,[],/parse/chart.py_rhs
1714,/home/amandapotts/git/nltk/nltk/parse/chart.py_dot,"def dot(self):
""""""
Return this edge's dot position, which indicates how much of
the hypothesized structure is consistent with the
sentence.  In particular, ``self.rhs[:dot]`` is consistent
with ``tokens[self.start():self.end()]``.
:rtype: int
""""""
raise NotImplementedError()
",[],0,[],/parse/chart.py_dot
1715,/home/amandapotts/git/nltk/nltk/parse/chart.py_nextsym,"def nextsym(self):
""""""
Return the element of this edge's right-hand side that
immediately follows its dot.
:rtype: Nonterminal or terminal or None
""""""
raise NotImplementedError()
",[],0,[],/parse/chart.py_nextsym
1716,/home/amandapotts/git/nltk/nltk/parse/chart.py_is_complete,"def is_complete(self):
""""""
Return True if this edge's structure is fully consistent
with the text.
:rtype: bool
""""""
raise NotImplementedError()
",[],0,[],/parse/chart.py_is_complete
1717,/home/amandapotts/git/nltk/nltk/parse/chart.py_is_incomplete,"def is_incomplete(self):
""""""
Return True if this edge's structure is partially consistent
with the text.
:rtype: bool
""""""
raise NotImplementedError()
",[],0,[],/parse/chart.py_is_incomplete
1718,/home/amandapotts/git/nltk/nltk/parse/chart.py___eq__,"def __eq__(self, other):
return (
self.__class__ is other.__class__
and self._comparison_key == other._comparison_key
)
",[],0,[],/parse/chart.py___eq__
1719,/home/amandapotts/git/nltk/nltk/parse/chart.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/parse/chart.py___ne__
1720,/home/amandapotts/git/nltk/nltk/parse/chart.py___lt__,"def __lt__(self, other):
if not isinstance(other, EdgeI):
raise_unorderable_types(""<"", self, other)
if self.__class__ is other.__class__:
return self._comparison_key < other._comparison_key
else:
return self.__class__.__name__ < other.__class__.__name__
",[],0,[],/parse/chart.py___lt__
1721,/home/amandapotts/git/nltk/nltk/parse/chart.py___hash__,"def __hash__(self):
try:
return self._hash
except AttributeError:
self._hash = hash(self._comparison_key)
return self._hash
",[],0,[],/parse/chart.py___hash__
1722,/home/amandapotts/git/nltk/nltk/parse/chart.py___init__,"def __init__(self, span, lhs, rhs, dot=0):
""""""
Construct a new ``TreeEdge``.
:type span: tuple(int, int)
:param span: A tuple ``(s, e)``, where ``tokens[s:e]`` is the
portion of the sentence that is consistent with the new
edge's structure.
:type lhs: Nonterminal
:param lhs: The new edge's left-hand side, specifying the
hypothesized tree's node value.
:type rhs: list(Nonterminal and str)
:param rhs: The new edge's right-hand side, specifying the
hypothesized tree's children.
:type dot: int
:param dot: The position of the new edge's dot.  This position
specifies what prefix of the production's right hand side
is consistent with the text.  In particular, if
``sentence`` is the list of tokens in the sentence, then
``okens[span[0]:span[1]]`` can be spanned by the
children specified by ``rhs[:dot]``.
""""""
self._span = span
self._lhs = lhs
rhs = tuple(rhs)
self._rhs = rhs
self._dot = dot
self._comparison_key = (span, lhs, rhs, dot)
",[],0,[],/parse/chart.py___init__
1723,/home/amandapotts/git/nltk/nltk/parse/chart.py_from_production,"def from_production(production, index):
""""""
Return a new ``TreeEdge`` formed from the given production.
The new edge's left-hand side and right-hand side will
be taken from ``production``
``(index,index)``
:rtype: TreeEdge
""""""
return TreeEdge(
span=(index, index), lhs=production.lhs(), rhs=production.rhs(), dot=0
)
",[],0,[],/parse/chart.py_from_production
1724,/home/amandapotts/git/nltk/nltk/parse/chart.py_move_dot_forward,"def move_dot_forward(self, new_end):
""""""
Return a new ``TreeEdge`` formed from this edge.
The new edge's dot position is increased by ``1``,
and its end index will be replaced by ``new_end``.
:param new_end: The new end index.
:type new_end: int
:rtype: TreeEdge
""""""
return TreeEdge(
span=(self._span[0], new_end),
lhs=self._lhs,
rhs=self._rhs,
dot=self._dot + 1,
)
",[],0,[],/parse/chart.py_move_dot_forward
1725,/home/amandapotts/git/nltk/nltk/parse/chart.py_lhs,"def lhs(self):
return self._lhs
",[],0,[],/parse/chart.py_lhs
1726,/home/amandapotts/git/nltk/nltk/parse/chart.py_span,"def span(self):
return self._span
",[],0,[],/parse/chart.py_span
1727,/home/amandapotts/git/nltk/nltk/parse/chart.py_start,"def start(self):
return self._span[0]
",[],0,[],/parse/chart.py_start
1728,/home/amandapotts/git/nltk/nltk/parse/chart.py_end,"def end(self):
return self._span[1]
",[],0,[],/parse/chart.py_end
1729,/home/amandapotts/git/nltk/nltk/parse/chart.py_length,"def length(self):
return self._span[1] - self._span[0]
",[],0,[],/parse/chart.py_length
1730,/home/amandapotts/git/nltk/nltk/parse/chart.py_rhs,"def rhs(self):
return self._rhs
",[],0,[],/parse/chart.py_rhs
1731,/home/amandapotts/git/nltk/nltk/parse/chart.py_dot,"def dot(self):
return self._dot
",[],0,[],/parse/chart.py_dot
1732,/home/amandapotts/git/nltk/nltk/parse/chart.py_is_complete,"def is_complete(self):
return self._dot == len(self._rhs)
",[],0,[],/parse/chart.py_is_complete
1733,/home/amandapotts/git/nltk/nltk/parse/chart.py_is_incomplete,"def is_incomplete(self):
return self._dot != len(self._rhs)
",[],0,[],/parse/chart.py_is_incomplete
1734,/home/amandapotts/git/nltk/nltk/parse/chart.py_nextsym,"def nextsym(self):
if self._dot >= len(self._rhs):
return None
else:
return self._rhs[self._dot]
",[],0,[],/parse/chart.py_nextsym
1735,/home/amandapotts/git/nltk/nltk/parse/chart.py___str__,"def __str__(self):
str = f""[{self._span[0]}:{self._span[1]}] ""
str += ""%-2r ->"" % (self._lhs,)
for i in range(len(self._rhs)):
if i == self._dot:
str += "" *""
str += "" %s"" % repr(self._rhs[i])
if len(self._rhs) == self._dot:
str += "" *""
return str
",[],0,[],/parse/chart.py___str__
1736,/home/amandapotts/git/nltk/nltk/parse/chart.py___repr__,"def __repr__(self):
return ""[Edge: %s]"" % self
",[],0,[],/parse/chart.py___repr__
1737,/home/amandapotts/git/nltk/nltk/parse/chart.py___init__,"def __init__(self, leaf, index):
""""""
Construct a new ``LeafEdge``.
:param leaf: The new edge's leaf value, specifying the word
that is recorded by this edge.
:param index: The new edge's index, specifying the position of
the word that is recorded by this edge.
""""""
self._leaf = leaf
self._index = index
self._comparison_key = (leaf, index)
",[],0,[],/parse/chart.py___init__
1738,/home/amandapotts/git/nltk/nltk/parse/chart.py_lhs,"def lhs(self):
return self._leaf
",[],0,[],/parse/chart.py_lhs
1739,/home/amandapotts/git/nltk/nltk/parse/chart.py_span,"def span(self):
return (self._index, self._index + 1)
",[],0,[],/parse/chart.py_span
1740,/home/amandapotts/git/nltk/nltk/parse/chart.py_start,"def start(self):
return self._index
",[],0,[],/parse/chart.py_start
1741,/home/amandapotts/git/nltk/nltk/parse/chart.py_end,"def end(self):
return self._index + 1
",[],0,[],/parse/chart.py_end
1742,/home/amandapotts/git/nltk/nltk/parse/chart.py_length,"def length(self):
return 1
",[],0,[],/parse/chart.py_length
1743,/home/amandapotts/git/nltk/nltk/parse/chart.py_rhs,"def rhs(self):
return ()
",[],0,[],/parse/chart.py_rhs
1744,/home/amandapotts/git/nltk/nltk/parse/chart.py_dot,"def dot(self):
return 0
",[],0,[],/parse/chart.py_dot
1745,/home/amandapotts/git/nltk/nltk/parse/chart.py_is_complete,"def is_complete(self):
return True
",[],0,[],/parse/chart.py_is_complete
1746,/home/amandapotts/git/nltk/nltk/parse/chart.py_is_incomplete,"def is_incomplete(self):
return False
",[],0,[],/parse/chart.py_is_incomplete
1747,/home/amandapotts/git/nltk/nltk/parse/chart.py_nextsym,"def nextsym(self):
return None
",[],0,[],/parse/chart.py_nextsym
1748,/home/amandapotts/git/nltk/nltk/parse/chart.py___str__,"def __str__(self):
return f""[{self._index}:{self._index + 1}] {repr(self._leaf)}""
",[],0,[],/parse/chart.py___str__
1749,/home/amandapotts/git/nltk/nltk/parse/chart.py___repr__,"def __repr__(self):
return ""[Edge: %s]"" % (self)
",[],0,[],/parse/chart.py___repr__
1750,/home/amandapotts/git/nltk/nltk/parse/chart.py___init__,"def __init__(self, tokens):
""""""
Construct a new chart. The chart is initialized with the
leaf edges corresponding to the terminal leaves.
:type tokens: list
:param tokens: The sentence that this chart will be used to parse.
""""""
self._tokens = tuple(tokens)
self._num_leaves = len(self._tokens)
self.initialize()
",[],0,[],/parse/chart.py___init__
1751,/home/amandapotts/git/nltk/nltk/parse/chart.py_initialize,"def initialize(self):
""""""
Clear the chart.
""""""
self._edges = []
self._edge_to_cpls = {}
self._indexes = {}
",[],0,[],/parse/chart.py_initialize
1752,/home/amandapotts/git/nltk/nltk/parse/chart.py_num_leaves,"def num_leaves(self):
""""""
Return the number of words in this chart's sentence.
:rtype: int
""""""
return self._num_leaves
",[],0,[],/parse/chart.py_num_leaves
1753,/home/amandapotts/git/nltk/nltk/parse/chart.py_leaf,"def leaf(self, index):
""""""
Return the leaf value of the word at the given index.
:rtype: str
""""""
return self._tokens[index]
",[],0,[],/parse/chart.py_leaf
1754,/home/amandapotts/git/nltk/nltk/parse/chart.py_leaves,"def leaves(self):
""""""
Return a list of the leaf values of each word in the
chart's sentence.
:rtype: list(str)
""""""
return self._tokens
",[],0,[],/parse/chart.py_leaves
1755,/home/amandapotts/git/nltk/nltk/parse/chart.py_edges,"def edges(self):
""""""
Return a list of all edges in this chart.  New edges
that are added to the chart after the call to edges()
will *not* be contained in this list.
:rtype: list(EdgeI)
:see: ``iteredges``, ``select``
""""""
return self._edges[:]
",[],0,[],/parse/chart.py_edges
1756,/home/amandapotts/git/nltk/nltk/parse/chart.py_iteredges,"def iteredges(self):
""""""
Return an iterator over the edges in this chart.  It is
not guaranteed that new edges which are added to the
chart before the iterator is exhausted will also be generated.
:rtype: iter(EdgeI)
:see: ``edges``, ``select``
""""""
return iter(self._edges)
",[],0,[],/parse/chart.py_iteredges
1757,/home/amandapotts/git/nltk/nltk/parse/chart.py_num_edges,"def num_edges(self):
""""""
Return the number of edges contained in this chart.
:rtype: int
""""""
return len(self._edge_to_cpls)
",[],0,[],/parse/chart.py_num_edges
1758,/home/amandapotts/git/nltk/nltk/parse/chart.py_select,"def select(self, **restrictions):
""""""
Return an iterator over the edges in this chart.  Any
new edges that are added to the chart before the iterator
is exahusted will also be generated.  ``restrictions``
can be used to restrict the set of edges that will be
generated.
:param span: Only generate edges ``e`` where ``e.span()==span``
:param start: Only generate edges ``e`` where ``e.start()==start``
:param end: Only generate edges ``e`` where ``e.end()==end``
:param length: Only generate edges ``e`` where ``e.length()==length``
:param lhs: Only generate edges ``e`` where ``e.lhs()==lhs``
:param rhs: Only generate edges ``e`` where ``e.rhs()==rhs``
:param nextsym: Only generate edges ``e`` where
``e.nextsym()==nextsym``
:param dot: Only generate edges ``e`` where ``e.dot()==dot``
:param is_complete: Only generate edges ``e`` where
``e.is_complete()==is_complete``
:param is_incomplete: Only generate edges ``e`` where
``e.is_incomplete()==is_incomplete``
:rtype: iter(EdgeI)
""""""
if restrictions == {}:
return iter(self._edges)
restr_keys = sorted(restrictions.keys())
restr_keys = tuple(restr_keys)
if restr_keys not in self._indexes:
self._add_index(restr_keys)
vals = tuple(restrictions[key] for key in restr_keys)
return iter(self._indexes[restr_keys].get(vals, []))
",[],0,[],/parse/chart.py_select
1759,/home/amandapotts/git/nltk/nltk/parse/chart.py__add_index,"def _add_index(self, restr_keys):
""""""
A helper function for ``select``, which creates a new index for
a given set of attributes (aka restriction keys).
""""""
for key in restr_keys:
if not hasattr(EdgeI, key):
raise ValueError(""Bad restriction: %s"" % key)
index = self._indexes[restr_keys] = {}
for edge in self._edges:
vals = tuple(getattr(edge, key)() for key in restr_keys)
index.setdefault(vals, []).append(edge)
",[],0,[],/parse/chart.py__add_index
1760,/home/amandapotts/git/nltk/nltk/parse/chart.py__register_with_indexes,"def _register_with_indexes(self, edge):
""""""
A helper function for ``insert``, which registers the new
edge with all existing indexes.
""""""
for restr_keys, index in self._indexes.items():
vals = tuple(getattr(edge, key)() for key in restr_keys)
index.setdefault(vals, []).append(edge)
",[],0,[],/parse/chart.py__register_with_indexes
1761,/home/amandapotts/git/nltk/nltk/parse/chart.py_insert_with_backpointer,"def insert_with_backpointer(self, new_edge, previous_edge, child_edge):
""""""
Add a new edge to the chart, using a pointer to the previous edge.
""""""
cpls = self.child_pointer_lists(previous_edge)
new_cpls = [cpl + (child_edge,) for cpl in cpls]
return self.insert(new_edge, *new_cpls)
",[],0,[],/parse/chart.py_insert_with_backpointer
1762,/home/amandapotts/git/nltk/nltk/parse/chart.py_insert,"def insert(self, edge, *child_pointer_lists):
""""""
Add a new edge to the chart, and return True if this operation
modified the chart.  In particular, return true iff the chart
did not already contain ``edge``, or if it did not already associate
``child_pointer_lists`` with ``edge``.
:type edge: EdgeI
:param edge: The new edge
:type child_pointer_lists: sequence of tuple(EdgeI)
:param child_pointer_lists: A sequence of lists of the edges that
were used to form this edge.  This list is used to reconstruct
the trees (or partial trees) that are associated with ``edge``.
:rtype: bool
""""""
if edge not in self._edge_to_cpls:
self._append_edge(edge)
self._register_with_indexes(edge)
cpls = self._edge_to_cpls.setdefault(edge, OrderedDict())
chart_was_modified = False
for child_pointer_list in child_pointer_lists:
child_pointer_list = tuple(child_pointer_list)
if child_pointer_list not in cpls:
cpls[child_pointer_list] = True
chart_was_modified = True
return chart_was_modified
",[],0,[],/parse/chart.py_insert
1763,/home/amandapotts/git/nltk/nltk/parse/chart.py__append_edge,"def _append_edge(self, edge):
self._edges.append(edge)
",[],0,[],/parse/chart.py__append_edge
1764,/home/amandapotts/git/nltk/nltk/parse/chart.py_parses,"def parses(self, root, tree_class=Tree):
""""""
Return an iterator of the complete tree structures that span
the entire chart, and whose root node is ``root``.
""""""
for edge in self.select(start=0, end=self._num_leaves, lhs=root):
yield from self.trees(edge, tree_class=tree_class, complete=True)
",[],0,[],/parse/chart.py_parses
1765,/home/amandapotts/git/nltk/nltk/parse/chart.py_trees,"def trees(self, edge, tree_class=Tree, complete=False):
""""""
Return an iterator of the tree structures that are associated
with ``edge``.
If ``edge`` is incomplete, then the unexpanded children will be
encoded as childless subtrees, whose node value is the
corresponding terminal or nonterminal.
:rtype: list(Tree)
:note: If two trees share a common subtree, then the same
Tree may be used to encode that subtree in
both trees.  If you need to eliminate this subtree
sharing, then create a deep copy of each tree.
""""""
return iter(self._trees(edge, complete, memo={}, tree_class=tree_class))
",[],0,[],/parse/chart.py_trees
1766,/home/amandapotts/git/nltk/nltk/parse/chart.py__trees,"def _trees(self, edge, complete, memo, tree_class):
""""""
A helper function for ``trees``.
:param memo: A dictionary used to record the trees that we've
generated for each edge, so that when we see an edge more
than once, we can reuse the same trees.
""""""
if edge in memo:
return memo[edge]
if complete and edge.is_incomplete():
return []
if isinstance(edge, LeafEdge):
leaf = self._tokens[edge.start()]
memo[edge] = [leaf]
return [leaf]
memo[edge] = []
trees = []
lhs = edge.lhs().symbol()
for cpl in self.child_pointer_lists(edge):
child_choices = [self._trees(cp, complete, memo, tree_class) for cp in cpl]
for children in itertools.product(*child_choices):
trees.append(tree_class(lhs, children))
if edge.is_incomplete():
unexpanded = [tree_class(elt, []) for elt in edge.rhs()[edge.dot() :]]
for tree in trees:
tree.extend(unexpanded)
memo[edge] = trees
return trees
",[],0,[],/parse/chart.py__trees
1767,/home/amandapotts/git/nltk/nltk/parse/chart.py_child_pointer_lists,"def child_pointer_lists(self, edge):
""""""
Return the set of child pointer lists for the given edge.
Each child pointer list is a list of edges that have
been used to form this edge.
:rtype: list(list(EdgeI))
""""""
return self._edge_to_cpls.get(edge, {}).keys()
",[],0,[],/parse/chart.py_child_pointer_lists
1768,/home/amandapotts/git/nltk/nltk/parse/chart.py_pretty_format_edge,"def pretty_format_edge(self, edge, width=None):
""""""
Return a pretty-printed string representation of a given edge
in this chart.
:rtype: str
:param width: The number of characters allotted to each
index in the sentence.
""""""
if width is None:
width = 50 // (self.num_leaves() + 1)
(start, end) = (edge.start(), edge.end())
str = ""|"" + (""."" + "" "" * (width - 1)) * start
if start == end:
if edge.is_complete():
str += ""#""
else:
str += "">""
elif edge.is_complete() and edge.span() == (0, self._num_leaves):
str += ""["" + (""="" * width) * (end - start - 1) + ""="" * (width - 1) + ""]""
elif edge.is_complete():
str += ""["" + (""-"" * width) * (end - start - 1) + ""-"" * (width - 1) + ""]""
else:
str += ""["" + (""-"" * width) * (end - start - 1) + ""-"" * (width - 1) + "">""
str += ("" "" * (width - 1) + ""."") * (self._num_leaves - end)
return str + ""| %s"" % edge
",[],0,[],/parse/chart.py_pretty_format_edge
1769,/home/amandapotts/git/nltk/nltk/parse/chart.py_pretty_format_leaves,"def pretty_format_leaves(self, width=None):
""""""
Return a pretty-printed string representation of this
chart's leaves.  This string can be used as a header
for calls to ``pretty_format_edge``.
""""""
if width is None:
width = 50 // (self.num_leaves() + 1)
if self._tokens is not None and width > 1:
header = ""|.""
for tok in self._tokens:
header += tok[: width - 1].center(width - 1) + "".""
header += ""|""
else:
header = """"
return header
",[],0,[],/parse/chart.py_pretty_format_leaves
1770,/home/amandapotts/git/nltk/nltk/parse/chart.py_pretty_format,"def pretty_format(self, width=None):
""""""
Return a pretty-printed string representation of this chart.
:param width: The number of characters allotted to each
index in the sentence.
:rtype: str
""""""
if width is None:
width = 50 // (self.num_leaves() + 1)
edges = sorted((e.length(), e.start(), e) for e in self)
edges = [e for (_, _, e) in edges]
return (
self.pretty_format_leaves(width)
+ ""\n""
+ ""\n"".join(self.pretty_format_edge(edge, width) for edge in edges)
)
",[],0,[],/parse/chart.py_pretty_format
1771,/home/amandapotts/git/nltk/nltk/parse/chart.py_dot_digraph,"def dot_digraph(self):
s = ""digraph nltk_chart {\n""
s += ""  rankdir=LR
s += ""  node [height=0.1,width=0.1]
s += '  node [style=filled, color=""lightgray""]
for y in range(self.num_edges(), -1, -1):
if y == 0:
s += '  node [style=filled, color=""black""]
for x in range(self.num_leaves() + 1):
if y == 0 or (
x <= self._edges[y - 1].start() or x >= self._edges[y - 1].end()
):
s += '  %04d.%04d [label=""""]
s += ""  x [style=invis]
for x in range(self.num_leaves() + 1):
s += ""  {rank=same
for y in range(self.num_edges() + 1):
if y == 0 or (
x <= self._edges[y - 1].start() or x >= self._edges[y - 1].end()
):
s += "" %04d.%04d"" % (x, y)
s += ""}\n""
s += ""  edge [style=invis, weight=100]
s += ""  node [shape=plaintext]\n""
s += ""  0000.0000""
for x in range(self.num_leaves()):
s += ""->%s->%04d.0000"" % (self.leaf(x), x + 1)
s += ""
s += ""  edge [style=solid, weight=1]
for y, edge in enumerate(self):
for x in range(edge.start()):
s += '  %04d.%04d -> %04d.%04d [style=""invis""]
x,
y + 1,
x + 1,
y + 1,
)
s += '  %04d.%04d -> %04d.%04d [label=""%s""]
edge.start(),
y + 1,
edge.end(),
y + 1,
edge,
)
for x in range(edge.end(), self.num_leaves()):
s += '  %04d.%04d -> %04d.%04d [style=""invis""]
x,
y + 1,
x + 1,
y + 1,
)
s += ""}\n""
return s
",[],0,[],/parse/chart.py_dot_digraph
1772,/home/amandapotts/git/nltk/nltk/parse/chart.py_apply,"def apply(self, chart, grammar, *edges):
""""""
Return a generator that will add edges licensed by this rule
and the given edges to the chart, one at a time.  Each
time the generator is resumed, it will either add a new
edge and yield that edge
:type edges: list(EdgeI)
:param edges: A set of existing edges.  The number of edges
that should be passed to ``apply()`` is specified by the
``NUM_EDGES`` class variable.
:rtype: iter(EdgeI)
""""""
raise NotImplementedError()
",[],0,[],/parse/chart.py_apply
1773,/home/amandapotts/git/nltk/nltk/parse/chart.py_apply_everywhere,"def apply_everywhere(self, chart, grammar):
""""""
Return a generator that will add all edges licensed by
this rule, given the edges that are currently in the
chart, one at a time.  Each time the generator is resumed,
it will either add a new edge and yield that edge
:rtype: iter(EdgeI)
""""""
raise NotImplementedError()
",[],0,[],/parse/chart.py_apply_everywhere
1774,/home/amandapotts/git/nltk/nltk/parse/chart.py_apply,"def apply(self, chart, grammar, *edges):
raise NotImplementedError()
",[],0,[],/parse/chart.py_apply
1775,/home/amandapotts/git/nltk/nltk/parse/chart.py_apply_everywhere,"def apply_everywhere(self, chart, grammar):
if self.NUM_EDGES == 0:
yield from self.apply(chart, grammar)
elif self.NUM_EDGES == 1:
for e1 in chart:
yield from self.apply(chart, grammar, e1)
elif self.NUM_EDGES == 2:
for e1 in chart:
for e2 in chart:
yield from self.apply(chart, grammar, e1, e2)
elif self.NUM_EDGES == 3:
for e1 in chart:
for e2 in chart:
for e3 in chart:
yield from self.apply(chart, grammar, e1, e2, e3)
else:
raise AssertionError(""NUM_EDGES>3 is not currently supported"")
",[],0,[],/parse/chart.py_apply_everywhere
1776,/home/amandapotts/git/nltk/nltk/parse/chart.py___str__,"def __str__(self):
return re.sub(""([a-z])([A-Z])"", r""\1 \2"", self.__class__.__name__)
",[],0,[],/parse/chart.py___str__
1777,/home/amandapotts/git/nltk/nltk/parse/chart.py_apply,"def apply(self, chart, grammar, left_edge, right_edge):
if not (
left_edge.is_incomplete()
and right_edge.is_complete()
and left_edge.end() == right_edge.start()
and left_edge.nextsym() == right_edge.lhs()
):
return
new_edge = left_edge.move_dot_forward(right_edge.end())
if chart.insert_with_backpointer(new_edge, left_edge, right_edge):
yield new_edge
",[],0,[],/parse/chart.py_apply
1778,/home/amandapotts/git/nltk/nltk/parse/chart.py_apply,"def apply(self, chart, grammar, edge):
if edge.is_incomplete():
yield from self._apply_incomplete(chart, grammar, edge)
else:
yield from self._apply_complete(chart, grammar, edge)
",[],0,[],/parse/chart.py_apply
1779,/home/amandapotts/git/nltk/nltk/parse/chart.py__apply_complete,"def _apply_complete(self, chart, grammar, right_edge):
for left_edge in chart.select(
end=right_edge.start(), is_complete=False, nextsym=right_edge.lhs()
):
new_edge = left_edge.move_dot_forward(right_edge.end())
if chart.insert_with_backpointer(new_edge, left_edge, right_edge):
yield new_edge
",[],0,[],/parse/chart.py__apply_complete
1780,/home/amandapotts/git/nltk/nltk/parse/chart.py__apply_incomplete,"def _apply_incomplete(self, chart, grammar, left_edge):
for right_edge in chart.select(
start=left_edge.end(), is_complete=True, lhs=left_edge.nextsym()
):
new_edge = left_edge.move_dot_forward(right_edge.end())
if chart.insert_with_backpointer(new_edge, left_edge, right_edge):
yield new_edge
",[],0,[],/parse/chart.py__apply_incomplete
1781,/home/amandapotts/git/nltk/nltk/parse/chart.py_apply,"def apply(self, chart, grammar):
for index in range(chart.num_leaves()):
new_edge = LeafEdge(chart.leaf(index), index)
if chart.insert(new_edge, ()):
yield new_edge
",[],0,[],/parse/chart.py_apply
1782,/home/amandapotts/git/nltk/nltk/parse/chart.py_apply,"def apply(self, chart, grammar):
for prod in grammar.productions(lhs=grammar.start()):
new_edge = TreeEdge.from_production(prod, 0)
if chart.insert(new_edge, ()):
yield new_edge
",[],0,[],/parse/chart.py_apply
1783,/home/amandapotts/git/nltk/nltk/parse/chart.py_apply,"def apply(self, chart, grammar, edge):
if edge.is_complete():
return
for prod in grammar.productions(lhs=edge.nextsym()):
new_edge = TreeEdge.from_production(prod, edge.end())
if chart.insert(new_edge, ()):
yield new_edge
",[],0,[],/parse/chart.py_apply
1784,/home/amandapotts/git/nltk/nltk/parse/chart.py___init__,"def __init__(self):
TopDownPredictRule.__init__(self)
self._done = {}
",[],0,[],/parse/chart.py___init__
1785,/home/amandapotts/git/nltk/nltk/parse/chart.py_apply,"def apply(self, chart, grammar, edge):
if edge.is_complete():
return
nextsym, index = edge.nextsym(), edge.end()
if not is_nonterminal(nextsym):
return
done = self._done.get((nextsym, index), (None, None))
if done[0] is chart and done[1] is grammar:
return
for prod in grammar.productions(lhs=nextsym):
if prod.rhs():
first = prod.rhs()[0]
if is_terminal(first):
if index >= chart.num_leaves() or first != chart.leaf(index):
continue
new_edge = TreeEdge.from_production(prod, index)
if chart.insert(new_edge, ()):
yield new_edge
self._done[nextsym, index] = (chart, grammar)
",[],0,[],/parse/chart.py_apply
1786,/home/amandapotts/git/nltk/nltk/parse/chart.py_apply,"def apply(self, chart, grammar, edge):
if edge.is_incomplete():
return
for prod in grammar.productions(rhs=edge.lhs()):
new_edge = TreeEdge.from_production(prod, edge.start())
if chart.insert(new_edge, ()):
yield new_edge
",[],0,[],/parse/chart.py_apply
1787,/home/amandapotts/git/nltk/nltk/parse/chart.py_apply,"def apply(self, chart, grammar, edge):
if edge.is_incomplete():
return
for prod in grammar.productions(rhs=edge.lhs()):
new_edge = TreeEdge(edge.span(), prod.lhs(), prod.rhs(), 1)
if chart.insert(new_edge, (edge,)):
yield new_edge
",[],0,[],/parse/chart.py_apply
1788,/home/amandapotts/git/nltk/nltk/parse/chart.py_apply,"def apply(self, chart, grammar):
for prod in grammar.productions(empty=True):
for index in range(chart.num_leaves() + 1):
new_edge = TreeEdge.from_production(prod, index)
if chart.insert(new_edge, ()):
yield new_edge
",[],0,[],/parse/chart.py_apply
1789,/home/amandapotts/git/nltk/nltk/parse/chart.py__apply_complete,"def _apply_complete(self, chart, grammar, right_edge):
end = right_edge.end()
nexttoken = end < chart.num_leaves() and chart.leaf(end)
for left_edge in chart.select(
end=right_edge.start(), is_complete=False, nextsym=right_edge.lhs()
):
if _bottomup_filter(grammar, nexttoken, left_edge.rhs(), left_edge.dot()):
new_edge = left_edge.move_dot_forward(right_edge.end())
if chart.insert_with_backpointer(new_edge, left_edge, right_edge):
yield new_edge
",[],0,[],/parse/chart.py__apply_complete
1790,/home/amandapotts/git/nltk/nltk/parse/chart.py__apply_incomplete,"def _apply_incomplete(self, chart, grammar, left_edge):
for right_edge in chart.select(
start=left_edge.end(), is_complete=True, lhs=left_edge.nextsym()
):
end = right_edge.end()
nexttoken = end < chart.num_leaves() and chart.leaf(end)
if _bottomup_filter(grammar, nexttoken, left_edge.rhs(), left_edge.dot()):
new_edge = left_edge.move_dot_forward(right_edge.end())
if chart.insert_with_backpointer(new_edge, left_edge, right_edge):
yield new_edge
",[],0,[],/parse/chart.py__apply_incomplete
1791,/home/amandapotts/git/nltk/nltk/parse/chart.py_apply,"def apply(self, chart, grammar, edge):
if edge.is_incomplete():
return
end = edge.end()
nexttoken = end < chart.num_leaves() and chart.leaf(end)
for prod in grammar.productions(rhs=edge.lhs()):
if _bottomup_filter(grammar, nexttoken, prod.rhs()):
new_edge = TreeEdge(edge.span(), prod.lhs(), prod.rhs(), 1)
if chart.insert(new_edge, (edge,)):
yield new_edge
",[],0,[],/parse/chart.py_apply
1792,/home/amandapotts/git/nltk/nltk/parse/chart.py__bottomup_filter,"def _bottomup_filter(grammar, nexttoken, rhs, dot=0):
if len(rhs) <= dot + 1:
return True
_next = rhs[dot + 1]
if is_terminal(_next):
return nexttoken == _next
else:
return grammar.is_leftcorner(_next, nexttoken)
",[],0,[],/parse/chart.py__bottomup_filter
1793,/home/amandapotts/git/nltk/nltk/parse/chart.py___init__,"def __init__(
self,
grammar,
strategy=BU_LC_STRATEGY,
trace=0,
trace_chart_width=50,
use_agenda=True,
chart_class=Chart,
",[],0,[],/parse/chart.py___init__
1794,/home/amandapotts/git/nltk/nltk/parse/chart.py_grammar,"def grammar(self):
return self._grammar
",[],0,[],/parse/chart.py_grammar
1795,/home/amandapotts/git/nltk/nltk/parse/chart.py__trace_new_edges,"def _trace_new_edges(self, chart, rule, new_edges, trace, edge_width):
if not trace:
return
print_rule_header = trace > 1
for edge in new_edges:
if print_rule_header:
print(""%s:"" % rule)
print_rule_header = False
print(chart.pretty_format_edge(edge, edge_width))
",[],0,[],/parse/chart.py__trace_new_edges
1796,/home/amandapotts/git/nltk/nltk/parse/chart.py_chart_parse,"def chart_parse(self, tokens, trace=None):
""""""
Return the final parse ``Chart`` from which all possible
parse trees can be extracted.
:param tokens: The sentence to be parsed
:type tokens: list(str)
:rtype: Chart
""""""
if trace is None:
trace = self._trace
trace_new_edges = self._trace_new_edges
tokens = list(tokens)
self._grammar.check_coverage(tokens)
chart = self._chart_class(tokens)
grammar = self._grammar
trace_edge_width = self._trace_chart_width // (chart.num_leaves() + 1)
if trace:
print(chart.pretty_format_leaves(trace_edge_width))
if self._use_agenda:
for axiom in self._axioms:
new_edges = list(axiom.apply(chart, grammar))
trace_new_edges(chart, axiom, new_edges, trace, trace_edge_width)
inference_rules = self._inference_rules
agenda = chart.edges()
agenda.reverse()
while agenda:
edge = agenda.pop()
for rule in inference_rules:
new_edges = list(rule.apply(chart, grammar, edge))
if trace:
trace_new_edges(chart, rule, new_edges, trace, trace_edge_width)
agenda += new_edges
else:
edges_added = True
while edges_added:
edges_added = False
for rule in self._strategy:
new_edges = list(rule.apply_everywhere(chart, grammar))
edges_added = len(new_edges)
trace_new_edges(chart, rule, new_edges, trace, trace_edge_width)
return chart
",[],0,[],/parse/chart.py_chart_parse
1797,/home/amandapotts/git/nltk/nltk/parse/chart.py_parse,"def parse(self, tokens, tree_class=Tree):
chart = self.chart_parse(tokens)
return iter(chart.parses(self._grammar.start(), tree_class=tree_class))
",[],0,[],/parse/chart.py_parse
1798,/home/amandapotts/git/nltk/nltk/parse/chart.py___init__,"def __init__(self, grammar, **parser_args):
ChartParser.__init__(self, grammar, TD_STRATEGY, **parser_args)
",[],0,[],/parse/chart.py___init__
1799,/home/amandapotts/git/nltk/nltk/parse/chart.py___init__,"def __init__(self, grammar, **parser_args):
if isinstance(grammar, PCFG):
warnings.warn(
""BottomUpChartParser only works for CFG, ""
""use BottomUpProbabilisticChartParser instead"",
category=DeprecationWarning,
)
ChartParser.__init__(self, grammar, BU_STRATEGY, **parser_args)
",[],0,[],/parse/chart.py___init__
1800,/home/amandapotts/git/nltk/nltk/parse/chart.py___init__,"def __init__(self, grammar, **parser_args):
ChartParser.__init__(self, grammar, BU_LC_STRATEGY, **parser_args)
",[],0,[],/parse/chart.py___init__
1801,/home/amandapotts/git/nltk/nltk/parse/chart.py___init__,"def __init__(self, grammar, **parser_args):
if not grammar.is_nonempty():
raise ValueError(
""LeftCornerParser only works for grammars "" ""without empty productions.""
)
ChartParser.__init__(self, grammar, LC_STRATEGY, **parser_args)
",[],0,[],/parse/chart.py___init__
1802,/home/amandapotts/git/nltk/nltk/parse/chart.py___init__,"def __init__(self, grammar, strategy=[], trace=0):
self._chart = None
self._current_chartrule = None
self._restart = False
ChartParser.__init__(self, grammar, strategy, trace)
",[],0,[],/parse/chart.py___init__
1803,/home/amandapotts/git/nltk/nltk/parse/chart.py_initialize,"def initialize(self, tokens):
""Begin parsing the given tokens.""
self._chart = Chart(list(tokens))
self._restart = True
",[],0,[],/parse/chart.py_initialize
1804,/home/amandapotts/git/nltk/nltk/parse/chart.py_step,"def step(self):
""""""
Return a generator that adds edges to the chart, one at a
time.  Each time the generator is resumed, it adds a single
edge and yields that edge.  If no more edges can be added,
then it yields None.
If the parser's strategy, grammar, or chart is changed, then
the generator will continue adding edges using the new
strategy, grammar, or chart.
Note that this generator never terminates, since the grammar
or strategy might be changed to values that would add new
edges.  Instead, it yields None when no more edges can be
added with the current strategy and grammar.
""""""
if self._chart is None:
raise ValueError(""Parser must be initialized first"")
while True:
self._restart = False
w = 50 // (self._chart.num_leaves() + 1)
for e in self._parse():
if self._trace > 1:
print(self._current_chartrule)
if self._trace > 0:
print(self._chart.pretty_format_edge(e, w))
yield e
if self._restart:
break
else:
yield None  # No more edges.
",[],0,[],/parse/chart.py_step
1805,/home/amandapotts/git/nltk/nltk/parse/chart.py__parse,"def _parse(self):
""""""
A generator that implements the actual parsing algorithm.
``step`` iterates through this generator, and restarts it
whenever the parser's strategy, grammar, or chart is modified.
""""""
chart = self._chart
grammar = self._grammar
edges_added = 1
while edges_added > 0:
edges_added = 0
for rule in self._strategy:
self._current_chartrule = rule
for e in rule.apply_everywhere(chart, grammar):
edges_added += 1
yield e
",[],0,[],/parse/chart.py__parse
1806,/home/amandapotts/git/nltk/nltk/parse/chart.py_strategy,"def strategy(self):
""Return the strategy used by this parser.""
return self._strategy
",[],0,[],/parse/chart.py_strategy
1807,/home/amandapotts/git/nltk/nltk/parse/chart.py_grammar,"def grammar(self):
""Return the grammar used by this parser.""
return self._grammar
",[],0,[],/parse/chart.py_grammar
1808,/home/amandapotts/git/nltk/nltk/parse/chart.py_chart,"def chart(self):
""Return the chart that is used by this parser.""
return self._chart
",[],0,[],/parse/chart.py_chart
1809,/home/amandapotts/git/nltk/nltk/parse/chart.py_current_chartrule,"def current_chartrule(self):
""Return the chart rule used to generate the most recent edge.""
return self._current_chartrule
",[],0,[],/parse/chart.py_current_chartrule
1810,/home/amandapotts/git/nltk/nltk/parse/chart.py_parses,"def parses(self, tree_class=Tree):
""Return the parse trees currently contained in the chart.""
return self._chart.parses(self._grammar.start(), tree_class)
",[],0,[],/parse/chart.py_parses
1811,/home/amandapotts/git/nltk/nltk/parse/chart.py_set_strategy,"def set_strategy(self, strategy):
""""""
Change the strategy that the parser uses to decide which edges
to add to the chart.
:type strategy: list(ChartRuleI)
:param strategy: A list of rules that should be used to decide
what edges to add to the chart.
""""""
if strategy == self._strategy:
return
self._strategy = strategy[:]  # Make a copy.
self._restart = True
",[],0,[],/parse/chart.py_set_strategy
1812,/home/amandapotts/git/nltk/nltk/parse/chart.py_set_grammar,"def set_grammar(self, grammar):
""Change the grammar used by the parser.""
if grammar is self._grammar:
return
self._grammar = grammar
self._restart = True
",[],0,[],/parse/chart.py_set_grammar
1813,/home/amandapotts/git/nltk/nltk/parse/chart.py_set_chart,"def set_chart(self, chart):
""Load a given chart into the chart parser.""
if chart is self._chart:
return
self._chart = chart
self._restart = True
",[],0,[],/parse/chart.py_set_chart
1814,/home/amandapotts/git/nltk/nltk/parse/chart.py_parse,"def parse(self, tokens, tree_class=Tree):
tokens = list(tokens)
self._grammar.check_coverage(tokens)
self.initialize(tokens)
for e in self.step():
if e is None:
break
return self.parses(tree_class=tree_class)
",[],0,[],/parse/chart.py_parse
1815,/home/amandapotts/git/nltk/nltk/parse/chart.py_demo_grammar,"def demo_grammar():
from nltk.grammar import CFG
return CFG.fromstring(
""""""
",[],0,[],/parse/chart.py_demo_grammar
1816,/home/amandapotts/git/nltk/nltk/parse/generate.py_generate,"def generate(grammar, start=None, depth=None, n=None):
""""""
Generates an iterator of all sentences from a CFG.
:param grammar: The Grammar used to generate sentences.
:param start: The Nonterminal from which to start generate sentences.
:param depth: The maximal depth of the generated tree.
:param n: The maximum number of sentences to return.
:return: An iterator of lists of terminal tokens.
""""""
if not start:
start = grammar.start()
if depth is None:
depth = sys.maxsize
iter = _generate_all(grammar, [start], depth)
if n:
iter = itertools.islice(iter, n)
return iter
",[],0,[],/parse/generate.py_generate
1817,/home/amandapotts/git/nltk/nltk/parse/generate.py__generate_all,"def _generate_all(grammar, items, depth):
if items:
try:
for frag1 in _generate_one(grammar, items[0], depth):
for frag2 in _generate_all(grammar, items[1:], depth):
yield frag1 + frag2
except RecursionError as error:
raise RuntimeError(
""The grammar has rule(s) that yield infinite recursion!""
) from error
else:
yield []
",[],0,[],/parse/generate.py__generate_all
1818,/home/amandapotts/git/nltk/nltk/parse/generate.py__generate_one,"def _generate_one(grammar, item, depth):
if depth > 0:
if isinstance(item, Nonterminal):
for prod in grammar.productions(lhs=item):
yield from _generate_all(grammar, prod.rhs(), depth - 1)
else:
yield [item]
",[],0,[],/parse/generate.py__generate_one
1819,/home/amandapotts/git/nltk/nltk/parse/generate.py_demo,"def demo(N=23):
from nltk.grammar import CFG
print(""Generating the first %d sentences for demo grammar:"" % (N,))
print(demo_grammar)
grammar = CFG.fromstring(demo_grammar)
for n, sent in enumerate(generate(grammar, n=N), 1):
print(""%3d. %s"" % (n, "" "".join(sent)))
",[],0,[],/parse/generate.py_demo
1820,/home/amandapotts/git/nltk/nltk/parse/recursivedescent.py___init__,"def __init__(self, grammar, trace=0):
""""""
Create a new ``RecursiveDescentParser``, that uses ``grammar``
to parse texts.
:type grammar: CFG
:param grammar: The grammar used to parse texts.
:type trace: int
:param trace: The level of tracing that should be used when
parsing a text.  ``0`` will generate no tracing output
and higher numbers will produce more verbose tracing
output.
""""""
self._grammar = grammar
self._trace = trace
",[],0,[],/parse/recursivedescent.py___init__
1821,/home/amandapotts/git/nltk/nltk/parse/recursivedescent.py_grammar,"def grammar(self):
return self._grammar
",[],0,[],/parse/recursivedescent.py_grammar
1822,/home/amandapotts/git/nltk/nltk/parse/recursivedescent.py_parse,"def parse(self, tokens):
tokens = list(tokens)
self._grammar.check_coverage(tokens)
start = self._grammar.start().symbol()
initial_tree = Tree(start, [])
frontier = [()]
if self._trace:
self._trace_start(initial_tree, frontier, tokens)
return self._parse(tokens, initial_tree, frontier)
",[],0,[],/parse/recursivedescent.py_parse
1823,/home/amandapotts/git/nltk/nltk/parse/recursivedescent.py__parse,"def _parse(self, remaining_text, tree, frontier):
""""""
Recursively expand and match each elements of ``tree``
specified by ``frontier``, to cover ``remaining_text``.  Return
a list of all parses found.
:return: An iterator of all parses that can be generated by
matching and expanding the elements of ``tree``
specified by ``frontier``.
:rtype: iter(Tree)
:type tree: Tree
:param tree: A partial structure for the text that is
currently being parsed.  The elements of ``tree``
that are specified by ``frontier`` have not yet been
expanded or matched.
:type remaining_text: list(str)
:param remaining_text: The portion of the text that is not yet
covered by ``tree``.
:type frontier: list(tuple(int))
:param frontier: A list of the locations within ``tree`` of
all subtrees that have not yet been expanded, and all
leaves that have not yet been matched.  This list sorted
in left-to-right order of location within the tree.
""""""
if len(remaining_text) == 0 and len(frontier) == 0:
if self._trace:
self._trace_succeed(tree, frontier)
yield tree
elif len(frontier) == 0:
if self._trace:
self._trace_backtrack(tree, frontier)
elif isinstance(tree[frontier[0]], Tree):
yield from self._expand(remaining_text, tree, frontier)
else:
yield from self._match(remaining_text, tree, frontier)
",[],0,[],/parse/recursivedescent.py__parse
1824,/home/amandapotts/git/nltk/nltk/parse/recursivedescent.py__match,"def _match(self, rtext, tree, frontier):
""""""
:rtype: iter(Tree)
:return: an iterator of all parses that can be generated by
matching the first element of ``frontier`` against the
first token in ``rtext``.  In particular, if the first
element of ``frontier`` has the same type as the first
token in ``rtext``, then substitute the token into
``tree``
matching and expanding the remaining elements of
``frontier``.  If the first element of ``frontier`` does not
have the same type as the first token in ``rtext``, then
return empty list.
:type tree: Tree
:param tree: A partial structure for the text that is
currently being parsed.  The elements of ``tree``
that are specified by ``frontier`` have not yet been
expanded or matched.
:type rtext: list(str)
:param rtext: The portion of the text that is not yet
covered by ``tree``.
:type frontier: list of tuple of int
:param frontier: A list of the locations within ``tree`` of
all subtrees that have not yet been expanded, and all
leaves that have not yet been matched.
""""""
tree_leaf = tree[frontier[0]]
if len(rtext) > 0 and tree_leaf == rtext[0]:
newtree = tree.copy(deep=True)
newtree[frontier[0]] = rtext[0]
if self._trace:
self._trace_match(newtree, frontier[1:], rtext[0])
yield from self._parse(rtext[1:], newtree, frontier[1:])
else:
if self._trace:
self._trace_backtrack(tree, frontier, rtext[:1])
",[],0,[],/parse/recursivedescent.py__match
1825,/home/amandapotts/git/nltk/nltk/parse/recursivedescent.py__expand,"def _expand(self, remaining_text, tree, frontier, production=None):
""""""
:rtype: iter(Tree)
:return: An iterator of all parses that can be generated by
expanding the first element of ``frontier`` with
``production``.  In particular, if the first element of
``frontier`` is a subtree whose node type is equal to
``production``'s left hand side, then add a child to that
subtree for each element of ``production``'s right hand
side
matching and expanding the remaining elements of
``frontier``.  If the first element of ``frontier`` is not a
subtree whose node type is equal to ``production``'s left
hand side, then return an empty list.  If ``production`` is
not specified, then return a list of all parses that can
be generated by expanding the first element of ``frontier``
with *any* CFG production.
:type tree: Tree
:param tree: A partial structure for the text that is
currently being parsed.  The elements of ``tree``
that are specified by ``frontier`` have not yet been
expanded or matched.
:type remaining_text: list(str)
:param remaining_text: The portion of the text that is not yet
covered by ``tree``.
:type frontier: list(tuple(int))
:param frontier: A list of the locations within ``tree`` of
all subtrees that have not yet been expanded, and all
leaves that have not yet been matched.
""""""
if production is None:
productions = self._grammar.productions()
else:
productions = [production]
for production in productions:
lhs = production.lhs().symbol()
if lhs == tree[frontier[0]].label():
subtree = self._production_to_tree(production)
if frontier[0] == ():
newtree = subtree
else:
newtree = tree.copy(deep=True)
newtree[frontier[0]] = subtree
new_frontier = [
frontier[0] + (i,) for i in range(len(production.rhs()))
]
if self._trace:
self._trace_expand(newtree, new_frontier, production)
yield from self._parse(
remaining_text, newtree, new_frontier + frontier[1:]
)
",[],0,[],/parse/recursivedescent.py__expand
1826,/home/amandapotts/git/nltk/nltk/parse/recursivedescent.py__production_to_tree,"def _production_to_tree(self, production):
""""""
:rtype: Tree
:return: The Tree that is licensed by ``production``.
In particular, given the production ``[lhs -> elt[1] ... elt[n]]``
return a tree that has a node ``lhs.symbol``, and
``n`` children.  For each nonterminal element
``elt[i]`` in the production, the tree token has a
childless subtree with node value ``elt[i].symbol``
for each terminal element ``elt[j]``, the tree token has
a leaf token with type ``elt[j]``.
:param production: The CFG production that licenses the tree
token that should be returned.
:type production: Production
""""""
children = []
for elt in production.rhs():
if isinstance(elt, Nonterminal):
children.append(Tree(elt.symbol(), []))
else:
children.append(elt)
return Tree(production.lhs().symbol(), children)
",[],0,[],/parse/recursivedescent.py__production_to_tree
1827,/home/amandapotts/git/nltk/nltk/parse/recursivedescent.py_trace,"def trace(self, trace=2):
""""""
Set the level of tracing output that should be generated when
parsing a text.
:type trace: int
:param trace: The trace level.  A trace level of ``0`` will
generate no tracing output
produce more verbose tracing output.
:rtype: None
""""""
self._trace = trace
",[],0,[],/parse/recursivedescent.py_trace
1828,/home/amandapotts/git/nltk/nltk/parse/recursivedescent.py__trace_fringe,"def _trace_fringe(self, tree, treeloc=None):
""""""
Print trace output displaying the fringe of ``tree``.  The
fringe of ``tree`` consists of all of its leaves and all of
its childless subtrees.
:rtype: None
""""""
if treeloc == ():
print(""*"", end="" "")
if isinstance(tree, Tree):
if len(tree) == 0:
print(repr(Nonterminal(tree.label())), end="" "")
for i in range(len(tree)):
if treeloc is not None and i == treeloc[0]:
self._trace_fringe(tree[i], treeloc[1:])
else:
self._trace_fringe(tree[i])
else:
print(repr(tree), end="" "")
",[],0,[],/parse/recursivedescent.py__trace_fringe
1829,/home/amandapotts/git/nltk/nltk/parse/recursivedescent.py__trace_tree,"def _trace_tree(self, tree, frontier, operation):
""""""
Print trace output displaying the parser's current state.
:param operation: A character identifying the operation that
generated the current state.
:rtype: None
""""""
if self._trace == 2:
print(""  %c ["" % operation, end="" "")
else:
print(""    ["", end="" "")
if len(frontier) > 0:
self._trace_fringe(tree, frontier[0])
else:
self._trace_fringe(tree)
print(""]"")
",[],0,[],/parse/recursivedescent.py__trace_tree
1830,/home/amandapotts/git/nltk/nltk/parse/recursivedescent.py__trace_start,"def _trace_start(self, tree, frontier, text):
print(""Parsing %r"" % "" "".join(text))
if self._trace > 2:
print(""Start:"")
if self._trace > 1:
self._trace_tree(tree, frontier, "" "")
",[],0,[],/parse/recursivedescent.py__trace_start
1831,/home/amandapotts/git/nltk/nltk/parse/recursivedescent.py__trace_expand,"def _trace_expand(self, tree, frontier, production):
if self._trace > 2:
print(""Expand: %s"" % production)
if self._trace > 1:
self._trace_tree(tree, frontier, ""E"")
",[],0,[],/parse/recursivedescent.py__trace_expand
1832,/home/amandapotts/git/nltk/nltk/parse/recursivedescent.py__trace_match,"def _trace_match(self, tree, frontier, tok):
if self._trace > 2:
print(""Match: %r"" % tok)
if self._trace > 1:
self._trace_tree(tree, frontier, ""M"")
",[],0,[],/parse/recursivedescent.py__trace_match
1833,/home/amandapotts/git/nltk/nltk/parse/recursivedescent.py__trace_succeed,"def _trace_succeed(self, tree, frontier):
if self._trace > 2:
print(""GOOD PARSE:"")
if self._trace == 1:
print(""Found a parse:\n%s"" % tree)
if self._trace > 1:
self._trace_tree(tree, frontier, ""+"")
",[],0,[],/parse/recursivedescent.py__trace_succeed
1834,/home/amandapotts/git/nltk/nltk/parse/recursivedescent.py__trace_backtrack,"def _trace_backtrack(self, tree, frontier, toks=None):
if self._trace > 2:
if toks:
print(""Backtrack: %r match failed"" % toks[0])
else:
print(""Backtrack"")
",[],0,[],/parse/recursivedescent.py__trace_backtrack
1835,/home/amandapotts/git/nltk/nltk/parse/recursivedescent.py___init__,"def __init__(self, grammar, trace=0):
super().__init__(grammar, trace)
self._rtext = None
self._tree = None
self._frontier = [()]
self._tried_e = {}
self._tried_m = {}
self._history = []
self._parses = []
",[],0,[],/parse/recursivedescent.py___init__
1836,/home/amandapotts/git/nltk/nltk/parse/recursivedescent.py__freeze,"def _freeze(self, tree):
c = tree.copy()
return ImmutableTree.convert(c)
",[],0,[],/parse/recursivedescent.py__freeze
1837,/home/amandapotts/git/nltk/nltk/parse/recursivedescent.py_parse,"def parse(self, tokens):
tokens = list(tokens)
self.initialize(tokens)
while self.step() is not None:
pass
return self.parses()
",[],0,[],/parse/recursivedescent.py_parse
1838,/home/amandapotts/git/nltk/nltk/parse/recursivedescent.py_initialize,"def initialize(self, tokens):
""""""
Start parsing a given text.  This sets the parser's tree to
the start symbol, its frontier to the root node, and its
remaining text to ``token['SUBTOKENS']``.
""""""
self._rtext = tokens
start = self._grammar.start().symbol()
self._tree = Tree(start, [])
self._frontier = [()]
self._tried_e = {}
self._tried_m = {}
self._history = []
self._parses = []
if self._trace:
self._trace_start(self._tree, self._frontier, self._rtext)
",[],0,[],/parse/recursivedescent.py_initialize
1839,/home/amandapotts/git/nltk/nltk/parse/recursivedescent.py_remaining_text,"def remaining_text(self):
""""""
:return: The portion of the text that is not yet covered by the
tree.
:rtype: list(str)
""""""
return self._rtext
",[],0,[],/parse/recursivedescent.py_remaining_text
1840,/home/amandapotts/git/nltk/nltk/parse/recursivedescent.py_frontier,"def frontier(self):
""""""
:return: A list of the tree locations of all subtrees that
have not yet been expanded, and all leaves that have not
yet been matched.
:rtype: list(tuple(int))
""""""
return self._frontier
",[],0,[],/parse/recursivedescent.py_frontier
1841,/home/amandapotts/git/nltk/nltk/parse/recursivedescent.py_tree,"def tree(self):
""""""
:return: A partial structure for the text that is
currently being parsed.  The elements specified by the
frontier have not yet been expanded or matched.
:rtype: Tree
""""""
return self._tree
",[],0,[],/parse/recursivedescent.py_tree
1842,/home/amandapotts/git/nltk/nltk/parse/recursivedescent.py_step,"def step(self):
""""""
Perform a single parsing operation.  If an untried match is
possible, then perform the match, and return the matched
token.  If an untried expansion is possible, then perform the
expansion, and return the production that it is based on.  If
backtracking is possible, then backtrack, and return True.
Otherwise, return None.
:return: None if no operation was performed
was performed
and True if a backtrack operation was performed.
:rtype: Production or String or bool
""""""
if self.untried_match():
token = self.match()
if token is not None:
return token
production = self.expand()
if production is not None:
return production
if self.backtrack():
self._trace_backtrack(self._tree, self._frontier)
return True
return None
",[],0,[],/parse/recursivedescent.py_step
1843,/home/amandapotts/git/nltk/nltk/parse/recursivedescent.py_expand,"def expand(self, production=None):
""""""
Expand the first element of the frontier.  In particular, if
the first element of the frontier is a subtree whose node type
is equal to ``production``'s left hand side, then add a child
to that subtree for each element of ``production``'s right hand
side.  If ``production`` is not specified, then use the first
untried expandable production.  If all expandable productions
have been tried, do nothing.
:return: The production used to expand the frontier, if an
expansion was performed.  If no expansion was performed,
return None.
:rtype: Production or None
""""""
if len(self._frontier) == 0:
return None
if not isinstance(self._tree[self._frontier[0]], Tree):
return None
if production is None:
productions = self.untried_expandable_productions()
else:
productions = [production]
parses = []
for prod in productions:
self._tried_e.setdefault(self._freeze(self._tree), []).append(prod)
for _result in self._expand(self._rtext, self._tree, self._frontier, prod):
return prod
return None
",[],0,[],/parse/recursivedescent.py_expand
1844,/home/amandapotts/git/nltk/nltk/parse/recursivedescent.py_match,"def match(self):
""""""
Match the first element of the frontier.  In particular, if
the first element of the frontier has the same type as the
next text token, then substitute the text token into the tree.
:return: The token matched, if a match operation was
performed.  If no match was performed, return None
:rtype: str or None
""""""
tok = self._rtext[0]
self._tried_m.setdefault(self._freeze(self._tree), []).append(tok)
if len(self._frontier) == 0:
return None
if isinstance(self._tree[self._frontier[0]], Tree):
return None
for _result in self._match(self._rtext, self._tree, self._frontier):
return self._history[-1][0][0]
return None
",[],0,[],/parse/recursivedescent.py_match
1845,/home/amandapotts/git/nltk/nltk/parse/recursivedescent.py_backtrack,"def backtrack(self):
""""""
Return the parser to its state before the most recent
match or expand operation.  Calling ``undo`` repeatedly return
the parser to successively earlier states.  If no match or
expand operations have been performed, ``undo`` will make no
changes.
:return: true if an operation was successfully undone.
:rtype: bool
""""""
if len(self._history) == 0:
return False
(self._rtext, self._tree, self._frontier) = self._history.pop()
return True
",[],0,[],/parse/recursivedescent.py_backtrack
1846,/home/amandapotts/git/nltk/nltk/parse/recursivedescent.py_expandable_productions,"def expandable_productions(self):
""""""
:return: A list of all the productions for which expansions
are available for the current parser state.
:rtype: list(Production)
""""""
if len(self._frontier) == 0:
return []
frontier_child = self._tree[self._frontier[0]]
if len(self._frontier) == 0 or not isinstance(frontier_child, Tree):
return []
return [
p
for p in self._grammar.productions()
if p.lhs().symbol() == frontier_child.label()
]
",[],0,[],/parse/recursivedescent.py_expandable_productions
1847,/home/amandapotts/git/nltk/nltk/parse/recursivedescent.py_untried_expandable_productions,"def untried_expandable_productions(self):
""""""
:return: A list of all the untried productions for which
expansions are available for the current parser state.
:rtype: list(Production)
""""""
tried_expansions = self._tried_e.get(self._freeze(self._tree), [])
return [p for p in self.expandable_productions() if p not in tried_expansions]
",[],0,[],/parse/recursivedescent.py_untried_expandable_productions
1848,/home/amandapotts/git/nltk/nltk/parse/recursivedescent.py_untried_match,"def untried_match(self):
""""""
:return: Whether the first element of the frontier is a token
that has not yet been matched.
:rtype: bool
""""""
if len(self._rtext) == 0:
return False
tried_matches = self._tried_m.get(self._freeze(self._tree), [])
return self._rtext[0] not in tried_matches
",[],0,[],/parse/recursivedescent.py_untried_match
1849,/home/amandapotts/git/nltk/nltk/parse/recursivedescent.py_currently_complete,"def currently_complete(self):
""""""
:return: Whether the parser's current state represents a
complete parse.
:rtype: bool
""""""
return len(self._frontier) == 0 and len(self._rtext) == 0
",[],0,[],/parse/recursivedescent.py_currently_complete
1850,/home/amandapotts/git/nltk/nltk/parse/recursivedescent.py__parse,"def _parse(self, remaining_text, tree, frontier):
""""""
A stub version of ``_parse`` that sets the parsers current
state to the given arguments.  In ``RecursiveDescentParser``,
the ``_parse`` method is used to recursively continue parsing a
text.  ``SteppingRecursiveDescentParser`` overrides it to
capture these recursive calls.  It records the parser's old
state in the history (to allow for backtracking), and updates
the parser's new state using the given arguments.  Finally, it
returns ``[1]``, which is used by ``match`` and ``expand`` to
detect whether their operations were successful.
:return: ``[1]``
:rtype: list of int
""""""
self._history.append((self._rtext, self._tree, self._frontier))
self._rtext = remaining_text
self._tree = tree
self._frontier = frontier
if len(frontier) == 0 and len(remaining_text) == 0:
self._parses.append(tree)
self._trace_succeed(self._tree, self._frontier)
return [1]
",[],0,[],/parse/recursivedescent.py__parse
1851,/home/amandapotts/git/nltk/nltk/parse/recursivedescent.py_parses,"def parses(self):
""""""
:return: An iterator of the parses that have been found by this
parser so far.
:rtype: list of Tree
""""""
return iter(self._parses)
",[],0,[],/parse/recursivedescent.py_parses
1852,/home/amandapotts/git/nltk/nltk/parse/recursivedescent.py_set_grammar,"def set_grammar(self, grammar):
""""""
Change the grammar used to parse texts.
:param grammar: The new grammar.
:type grammar: CFG
""""""
self._grammar = grammar
",[],0,[],/parse/recursivedescent.py_set_grammar
1853,/home/amandapotts/git/nltk/nltk/parse/recursivedescent.py_demo,"def demo():
""""""
A demonstration of the recursive descent parser.
""""""
from nltk import CFG, parse
grammar = CFG.fromstring(
""""""
S -> NP VP
NP -> Det N | Det N PP
VP -> V NP | V NP PP
PP -> P NP
NP -> 'I'
N -> 'man' | 'park' | 'telescope' | 'dog'
Det -> 'the' | 'a'
P -> 'in' | 'with'
V -> 'saw'
""""""
)
for prod in grammar.productions():
print(prod)
sent = ""I saw a man in the park"".split()
parser = parse.RecursiveDescentParser(grammar, trace=2)
for p in parser.parse(sent):
print(p)
",[],0,[],/parse/recursivedescent.py_demo
1854,/home/amandapotts/git/nltk/nltk/parse/api.py_grammar,"def grammar(self):
""""""
:return: The grammar used by this parser.
""""""
raise NotImplementedError()
",[],0,[],/parse/api.py_grammar
1855,/home/amandapotts/git/nltk/nltk/parse/api.py_parse,"def parse(self, sent, *args, **kwargs):
""""""
:return: An iterator that generates parse trees for the sentence.
When possible this list is sorted from most likely to least likely.
:param sent: The sentence to be parsed
:type sent: list(str)
:rtype: iter(Tree)
""""""
if overridden(self.parse_sents):
return next(self.parse_sents([sent], *args, **kwargs))
elif overridden(self.parse_one):
return (
tree
for tree in [self.parse_one(sent, *args, **kwargs)]
if tree is not None
)
elif overridden(self.parse_all):
return iter(self.parse_all(sent, *args, **kwargs))
else:
raise NotImplementedError()
",[],0,[],/parse/api.py_parse
1856,/home/amandapotts/git/nltk/nltk/parse/api.py_parse_sents,"def parse_sents(self, sents, *args, **kwargs):
""""""
Apply ``self.parse()`` to each element of ``sents``.
:rtype: iter(iter(Tree))
""""""
return (self.parse(sent, *args, **kwargs) for sent in sents)
",[],0,[],/parse/api.py_parse_sents
1857,/home/amandapotts/git/nltk/nltk/parse/api.py_parse_all,"def parse_all(self, sent, *args, **kwargs):
"""""":rtype: list(Tree)""""""
return list(self.parse(sent, *args, **kwargs))
",[],0,[],/parse/api.py_parse_all
1858,/home/amandapotts/git/nltk/nltk/parse/api.py_parse_one,"def parse_one(self, sent, *args, **kwargs):
"""""":rtype: Tree or None""""""
return next(self.parse(sent, *args, **kwargs), None)
",[],0,[],/parse/api.py_parse_one
1859,/home/amandapotts/git/nltk/nltk/parse/pchart.py_prob,"def prob(self):
return 1.0
",[],0,[],/parse/pchart.py_prob
1860,/home/amandapotts/git/nltk/nltk/parse/pchart.py___init__,"def __init__(self, prob, *args, **kwargs):
TreeEdge.__init__(self, *args, **kwargs)
self._prob = prob
self._comparison_key = (self._comparison_key, prob)
",[],0,[],/parse/pchart.py___init__
1861,/home/amandapotts/git/nltk/nltk/parse/pchart.py_prob,"def prob(self):
return self._prob
",[],0,[],/parse/pchart.py_prob
1862,/home/amandapotts/git/nltk/nltk/parse/pchart.py_from_production,"def from_production(production, index, p):
return ProbabilisticTreeEdge(
p, (index, index), production.lhs(), production.rhs(), 0
)
",[],0,[],/parse/pchart.py_from_production
1863,/home/amandapotts/git/nltk/nltk/parse/pchart.py_apply,"def apply(self, chart, grammar):
for index in range(chart.num_leaves()):
new_edge = ProbabilisticLeafEdge(chart.leaf(index), index)
if chart.insert(new_edge, ()):
yield new_edge
",[],0,[],/parse/pchart.py_apply
1864,/home/amandapotts/git/nltk/nltk/parse/pchart.py_apply,"def apply(self, chart, grammar, edge):
if edge.is_incomplete():
return
for prod in grammar.productions():
if edge.lhs() == prod.rhs()[0]:
new_edge = ProbabilisticTreeEdge.from_production(
prod, edge.start(), prod.prob()
)
if chart.insert(new_edge, ()):
yield new_edge
",[],0,[],/parse/pchart.py_apply
1865,/home/amandapotts/git/nltk/nltk/parse/pchart.py_apply,"def apply(self, chart, grammar, left_edge, right_edge):
if not (
left_edge.end() == right_edge.start()
and left_edge.nextsym() == right_edge.lhs()
and left_edge.is_incomplete()
and right_edge.is_complete()
):
return
p = left_edge.prob() * right_edge.prob()
new_edge = ProbabilisticTreeEdge(
p,
span=(left_edge.start(), right_edge.end()),
lhs=left_edge.lhs(),
rhs=left_edge.rhs(),
dot=left_edge.dot() + 1,
)
changed_chart = False
for cpl1 in chart.child_pointer_lists(left_edge):
if chart.insert(new_edge, cpl1 + (right_edge,)):
changed_chart = True
if changed_chart:
yield new_edge
",[],0,[],/parse/pchart.py_apply
1866,/home/amandapotts/git/nltk/nltk/parse/pchart.py_apply,"def apply(self, chart, grammar, edge1):
fr = self._fundamental_rule
if edge1.is_incomplete():
for edge2 in chart.select(
start=edge1.end(), is_complete=True, lhs=edge1.nextsym()
):
yield from fr.apply(chart, grammar, edge1, edge2)
else:
for edge2 in chart.select(
end=edge1.start(), is_complete=False, nextsym=edge1.lhs()
):
yield from fr.apply(chart, grammar, edge2, edge1)
",[],0,[],/parse/pchart.py_apply
1867,/home/amandapotts/git/nltk/nltk/parse/pchart.py___str__,"def __str__(self):
return ""Fundamental Rule""
",[],0,[],/parse/pchart.py___str__
1868,/home/amandapotts/git/nltk/nltk/parse/pchart.py___init__,"def __init__(self, grammar, beam_size=0, trace=0):
""""""
Create a new ``BottomUpProbabilisticChartParser``, that uses
``grammar`` to parse texts.
:type grammar: PCFG
:param grammar: The grammar used to parse texts.
:type beam_size: int
:param beam_size: The maximum length for the parser's edge queue.
:type trace: int
:param trace: The level of tracing that should be used when
parsing a text.  ``0`` will generate no tracing output
and higher numbers will produce more verbose tracing
output.
""""""
if not isinstance(grammar, PCFG):
raise ValueError(""The grammar must be probabilistic PCFG"")
self._grammar = grammar
self.beam_size = beam_size
self._trace = trace
",[],0,[],/parse/pchart.py___init__
1869,/home/amandapotts/git/nltk/nltk/parse/pchart.py_grammar,"def grammar(self):
return self._grammar
",[],0,[],/parse/pchart.py_grammar
1870,/home/amandapotts/git/nltk/nltk/parse/pchart.py_trace,"def trace(self, trace=2):
""""""
Set the level of tracing output that should be generated when
parsing a text.
:type trace: int
:param trace: The trace level.  A trace level of ``0`` will
generate no tracing output
produce more verbose tracing output.
:rtype: None
""""""
self._trace = trace
",[],0,[],/parse/pchart.py_trace
1871,/home/amandapotts/git/nltk/nltk/parse/pchart.py__setprob,"def _setprob(self, tree, prod_probs):
if tree.prob() is not None:
return
lhs = Nonterminal(tree.label())
rhs = []
for child in tree:
if isinstance(child, Tree):
rhs.append(Nonterminal(child.label()))
else:
rhs.append(child)
prob = prod_probs[lhs, tuple(rhs)]
for child in tree:
if isinstance(child, Tree):
self._setprob(child, prod_probs)
prob *= child.prob()
tree.set_prob(prob)
",[],0,[],/parse/pchart.py__setprob
1872,/home/amandapotts/git/nltk/nltk/parse/pchart.py_sort_queue,"def sort_queue(self, queue, chart):
""""""
Sort the given queue of ``Edge`` objects, placing the edge that should
be tried first at the beginning of the queue.  This method
will be called after each ``Edge`` is added to the queue.
:param queue: The queue of ``Edge`` objects to sort.  Each edge in
this queue is an edge that could be added to the chart by
the fundamental rule
:type queue: list(Edge)
:param chart: The chart being used to parse the text.  This
chart can be used to provide extra information for sorting
the queue.
:type chart: Chart
:rtype: None
""""""
raise NotImplementedError()
",[],0,[],/parse/pchart.py_sort_queue
1873,/home/amandapotts/git/nltk/nltk/parse/pchart.py__prune,"def _prune(self, queue, chart):
""""""Discard items in the queue if the queue is longer than the beam.""""""
if len(queue) > self.beam_size:
split = len(queue) - self.beam_size
if self._trace > 2:
for edge in queue[:split]:
print(""  %-50s [DISCARDED]"" % chart.pretty_format_edge(edge, 2))
del queue[:split]
",[],0,[],/parse/pchart.py__prune
1874,/home/amandapotts/git/nltk/nltk/parse/pchart.py_sort_queue,"def sort_queue(self, queue, chart):
i = random.randint(0, len(queue) - 1)
(queue[-1], queue[i]) = (queue[i], queue[-1])
",[],0,[],/parse/pchart.py_sort_queue
1875,/home/amandapotts/git/nltk/nltk/parse/pchart.py_sort_queue,"def sort_queue(self, queue, chart):
return
",[],0,[],/parse/pchart.py_sort_queue
1876,/home/amandapotts/git/nltk/nltk/parse/viterbi.py___init__,"def __init__(self, grammar, trace=0):
""""""
Create a new ``ViterbiParser`` parser, that uses ``grammar`` to
parse texts.
:type grammar: PCFG
:param grammar: The grammar used to parse texts.
:type trace: int
:param trace: The level of tracing that should be used when
parsing a text.  ``0`` will generate no tracing output
and higher numbers will produce more verbose tracing
output.
""""""
self._grammar = grammar
self._trace = trace
",[],0,[],/parse/viterbi.py___init__
1877,/home/amandapotts/git/nltk/nltk/parse/viterbi.py_grammar,"def grammar(self):
return self._grammar
",[],0,[],/parse/viterbi.py_grammar
1878,/home/amandapotts/git/nltk/nltk/parse/viterbi.py_trace,"def trace(self, trace=2):
""""""
Set the level of tracing output that should be generated when
parsing a text.
:type trace: int
:param trace: The trace level.  A trace level of ``0`` will
generate no tracing output
produce more verbose tracing output.
:rtype: None
""""""
self._trace = trace
",[],0,[],/parse/viterbi.py_trace
1879,/home/amandapotts/git/nltk/nltk/parse/viterbi.py_parse,"def parse(self, tokens):
tokens = list(tokens)
self._grammar.check_coverage(tokens)
constituents = {}
if self._trace:
print(""Inserting tokens into the most likely"" + "" constituents table..."")
for index in range(len(tokens)):
token = tokens[index]
constituents[index, index + 1, token] = token
if self._trace > 1:
self._trace_lexical_insertion(token, index, len(tokens))
for length in range(1, len(tokens) + 1):
if self._trace:
print(
""Finding the most likely constituents""
+ "" spanning %d text elements..."" % length
)
for start in range(len(tokens) - length + 1):
span = (start, start + length)
self._add_constituents_spanning(span, constituents, tokens)
tree = constituents.get((0, len(tokens), self._grammar.start()))
if tree is not None:
yield tree
",[],0,[],/parse/viterbi.py_parse
1880,/home/amandapotts/git/nltk/nltk/parse/viterbi.py__find_instantiations,"def _find_instantiations(self, span, constituents):
""""""
:return: a list of the production instantiations that cover a
given span of the text.  A ""production instantiation"" is
a tuple containing a production and a list of children,
where the production's right hand side matches the list of
children
of ``pair`` of ``Production``, (list of
(``ProbabilisticTree`` or token.
:type span: tuple(int, int)
:param span: The section of the text for which we are
trying to find production instantiations.  The span is
specified as a pair of integers, where the first integer
is the index of the first token that should be covered by
the production instantiation
the index of the first token that should not be covered by
the production instantiation.
:type constituents: dict(tuple(int,int,Nonterminal) -> ProbabilisticToken or ProbabilisticTree)
:param constituents: The most likely constituents table.  This
table records the most probable tree representation for
any given span and node value.  See the module
documentation for more information.
""""""
rv = []
for production in self._grammar.productions():
childlists = self._match_rhs(production.rhs(), span, constituents)
for childlist in childlists:
rv.append((production, childlist))
return rv
",[],0,[],/parse/viterbi.py__find_instantiations
1881,/home/amandapotts/git/nltk/nltk/parse/viterbi.py__match_rhs,"def _match_rhs(self, rhs, span, constituents):
""""""
:return: a set of all the lists of children that cover ``span``
and that match ``rhs``.
:rtype: list(list(ProbabilisticTree or token)
:type rhs: list(Nonterminal or any)
:param rhs: The list specifying what kinds of children need to
cover ``span``.  Each nonterminal in ``rhs`` specifies
that the corresponding child should be a tree whose node
value is that nonterminal's symbol.  Each terminal in ``rhs``
specifies that the corresponding child should be a token
whose type is that terminal.
:type span: tuple(int, int)
:param span: The section of the text for which we are
trying to find child lists.  The span is specified as a
pair of integers, where the first integer is the index of
the first token that should be covered by the child list
and the second integer is the index of the first token
that should not be covered by the child list.
:type constituents: dict(tuple(int,int,Nonterminal) -> ProbabilisticToken or ProbabilisticTree)
:param constituents: The most likely constituents table.  This
table records the most probable tree representation for
any given span and node value.  See the module
documentation for more information.
""""""
(start, end) = span
if start >= end and rhs == ():
return [[]]
if start >= end or rhs == ():
return []
childlists = []
for split in range(start, end + 1):
l = constituents.get((start, split, rhs[0]))
if l is not None:
rights = self._match_rhs(rhs[1:], (split, end), constituents)
childlists += [[l] + r for r in rights]
return childlists
",[],0,[],/parse/viterbi.py__match_rhs
1882,/home/amandapotts/git/nltk/nltk/parse/viterbi.py__trace_production,"def _trace_production(self, production, p, span, width):
""""""
Print trace output indicating that a given production has been
applied at a given location.
:param production: The production that has been applied
:type production: Production
:param p: The probability of the tree produced by the production.
:type p: float
:param span: The span of the production
:type span: tuple
:rtype: None
""""""
str = ""|"" + ""."" * span[0]
str += ""="" * (span[1] - span[0])
str += ""."" * (width - span[1]) + ""| ""
str += ""%s"" % production
if self._trace > 2:
str = f""{str:<40} {p:12.10f} ""
print(str)
",[],0,[],/parse/viterbi.py__trace_production
1883,/home/amandapotts/git/nltk/nltk/parse/viterbi.py__trace_lexical_insertion,"def _trace_lexical_insertion(self, token, index, width):
str = ""   Insert: |"" + ""."" * index + ""="" + ""."" * (width - index - 1) + ""| ""
str += f""{token}""
print(str)
",[],0,[],/parse/viterbi.py__trace_lexical_insertion
1884,/home/amandapotts/git/nltk/nltk/parse/viterbi.py___repr__,"def __repr__(self):
return ""<ViterbiParser for %r>"" % self._grammar
",[],0,[],/parse/viterbi.py___repr__
1885,/home/amandapotts/git/nltk/nltk/parse/featurechart.py___init__,"def __init__(self, span, lhs, rhs, dot=0, bindings=None):
""""""
Construct a new edge.  If the edge is incomplete (i.e., if
``dot<len(rhs)``), then store the bindings as-is.  If the edge
is complete (i.e., if ``dot==len(rhs)``), then apply the
bindings to all nonterminals in ``lhs`` and ``rhs``, and then
clear the bindings.  See ``TreeEdge`` for a description of
the other arguments.
""""""
if bindings is None:
bindings = {}
if dot == len(rhs) and bindings:
lhs = self._bind(lhs, bindings)
rhs = [self._bind(elt, bindings) for elt in rhs]
bindings = {}
TreeEdge.__init__(self, span, lhs, rhs, dot)
self._bindings = bindings
self._comparison_key = (self._comparison_key, tuple(sorted(bindings.items())))
",[],0,[],/parse/featurechart.py___init__
1886,/home/amandapotts/git/nltk/nltk/parse/featurechart.py_from_production,"def from_production(production, index):
""""""
:return: A new ``TreeEdge`` formed from the given production.
The new edge's left-hand side and right-hand side will
be taken from ``production``
``(index,index)``
:rtype: TreeEdge
""""""
return FeatureTreeEdge(
span=(index, index), lhs=production.lhs(), rhs=production.rhs(), dot=0
)
",[],0,[],/parse/featurechart.py_from_production
1887,/home/amandapotts/git/nltk/nltk/parse/featurechart.py_move_dot_forward,"def move_dot_forward(self, new_end, bindings=None):
""""""
:return: A new ``FeatureTreeEdge`` formed from this edge.
The new edge's dot position is increased by ``1``,
and its end index will be replaced by ``new_end``.
:rtype: FeatureTreeEdge
:param new_end: The new end index.
:type new_end: int
:param bindings: Bindings for the new edge.
:type bindings: dict
""""""
return FeatureTreeEdge(
span=(self._span[0], new_end),
lhs=self._lhs,
rhs=self._rhs,
dot=self._dot + 1,
bindings=bindings,
)
",[],0,[],/parse/featurechart.py_move_dot_forward
1888,/home/amandapotts/git/nltk/nltk/parse/featurechart.py__bind,"def _bind(self, nt, bindings):
if not isinstance(nt, FeatStructNonterminal):
return nt
return nt.substitute_bindings(bindings)
",[],0,[],/parse/featurechart.py__bind
1889,/home/amandapotts/git/nltk/nltk/parse/featurechart.py_next_with_bindings,"def next_with_bindings(self):
return self._bind(self.nextsym(), self._bindings)
",[],0,[],/parse/featurechart.py_next_with_bindings
1890,/home/amandapotts/git/nltk/nltk/parse/featurechart.py_bindings,"def bindings(self):
""""""
Return a copy of this edge's bindings dictionary.
""""""
return self._bindings.copy()
",[],0,[],/parse/featurechart.py_bindings
1891,/home/amandapotts/git/nltk/nltk/parse/featurechart.py_variables,"def variables(self):
""""""
:return: The set of variables used by this edge.
:rtype: set(Variable)
""""""
return find_variables(
[self._lhs]
+ list(self._rhs)
+ list(self._bindings.keys())
+ list(self._bindings.values()),
fs_class=FeatStruct,
)
",[],0,[],/parse/featurechart.py_variables
1892,/home/amandapotts/git/nltk/nltk/parse/featurechart.py___str__,"def __str__(self):
if self.is_complete():
return super().__str__()
else:
bindings = ""{%s}"" % "", "".join(
""%s: %r"" % item for item in sorted(self._bindings.items())
)
return f""{super().__str__()} {bindings}""
",[],0,[],/parse/featurechart.py___str__
1893,/home/amandapotts/git/nltk/nltk/parse/featurechart.py_select,"def select(self, **restrictions):
""""""
Returns an iterator over the edges in this chart.
See ``Chart.select`` for more information about the
``restrictions`` on the edges.
""""""
if restrictions == {}:
return iter(self._edges)
restr_keys = sorted(restrictions.keys())
restr_keys = tuple(restr_keys)
if restr_keys not in self._indexes:
self._add_index(restr_keys)
vals = tuple(
self._get_type_if_possible(restrictions[key]) for key in restr_keys
)
return iter(self._indexes[restr_keys].get(vals, []))
",[],0,[],/parse/featurechart.py_select
1894,/home/amandapotts/git/nltk/nltk/parse/featurechart.py__add_index,"def _add_index(self, restr_keys):
""""""
A helper function for ``select``, which creates a new index for
a given set of attributes (aka restriction keys).
""""""
for key in restr_keys:
if not hasattr(EdgeI, key):
raise ValueError(""Bad restriction: %s"" % key)
index = self._indexes[restr_keys] = {}
for edge in self._edges:
vals = tuple(
self._get_type_if_possible(getattr(edge, key)()) for key in restr_keys
)
index.setdefault(vals, []).append(edge)
",[],0,[],/parse/featurechart.py__add_index
1895,/home/amandapotts/git/nltk/nltk/parse/featurechart.py__register_with_indexes,"def _register_with_indexes(self, edge):
""""""
A helper function for ``insert``, which registers the new
edge with all existing indexes.
""""""
for restr_keys, index in self._indexes.items():
vals = tuple(
self._get_type_if_possible(getattr(edge, key)()) for key in restr_keys
)
index.setdefault(vals, []).append(edge)
",[],0,[],/parse/featurechart.py__register_with_indexes
1896,/home/amandapotts/git/nltk/nltk/parse/featurechart.py__get_type_if_possible,"def _get_type_if_possible(self, item):
""""""
Helper function which returns the ``TYPE`` feature of the ``item``,
if it exists, otherwise it returns the ``item`` itself
""""""
if isinstance(item, dict) and TYPE in item:
return item[TYPE]
else:
return item
",[],0,[],/parse/featurechart.py__get_type_if_possible
1897,/home/amandapotts/git/nltk/nltk/parse/featurechart.py_parses,"def parses(self, start, tree_class=Tree):
for edge in self.select(start=0, end=self._num_leaves):
if (
(isinstance(edge, FeatureTreeEdge))
and (edge.lhs()[TYPE] == start[TYPE])
and (unify(edge.lhs(), start, rename_vars=True))
):
yield from self.trees(edge, complete=True, tree_class=tree_class)
",[],0,[],/parse/featurechart.py_parses
1898,/home/amandapotts/git/nltk/nltk/parse/featurechart.py_apply,"def apply(self, chart, grammar, left_edge, right_edge):
if not (
left_edge.end() == right_edge.start()
and left_edge.is_incomplete()
and right_edge.is_complete()
and isinstance(left_edge, FeatureTreeEdge)
):
return
found = right_edge.lhs()
nextsym = left_edge.nextsym()
if isinstance(right_edge, FeatureTreeEdge):
if not is_nonterminal(nextsym):
return
if left_edge.nextsym()[TYPE] != right_edge.lhs()[TYPE]:
return
bindings = left_edge.bindings()
found = found.rename_variables(used_vars=left_edge.variables())
result = unify(nextsym, found, bindings, rename_vars=False)
if result is None:
return
else:
if nextsym != found:
return
bindings = left_edge.bindings()
new_edge = left_edge.move_dot_forward(right_edge.end(), bindings)
if chart.insert_with_backpointer(new_edge, left_edge, right_edge):
yield new_edge
",[],0,[],/parse/featurechart.py_apply
1899,/home/amandapotts/git/nltk/nltk/parse/featurechart.py__apply_complete,"def _apply_complete(self, chart, grammar, right_edge):
fr = self._fundamental_rule
for left_edge in chart.select(
end=right_edge.start(), is_complete=False, nextsym=right_edge.lhs()
):
yield from fr.apply(chart, grammar, left_edge, right_edge)
",[],0,[],/parse/featurechart.py__apply_complete
1900,/home/amandapotts/git/nltk/nltk/parse/featurechart.py__apply_incomplete,"def _apply_incomplete(self, chart, grammar, left_edge):
fr = self._fundamental_rule
for right_edge in chart.select(
start=left_edge.end(), is_complete=True, lhs=left_edge.nextsym()
):
yield from fr.apply(chart, grammar, left_edge, right_edge)
",[],0,[],/parse/featurechart.py__apply_incomplete
1901,/home/amandapotts/git/nltk/nltk/parse/featurechart.py_apply,"def apply(self, chart, grammar):
for prod in grammar.productions(lhs=grammar.start()):
new_edge = FeatureTreeEdge.from_production(prod, 0)
if chart.insert(new_edge, ()):
yield new_edge
",[],0,[],/parse/featurechart.py_apply
1902,/home/amandapotts/git/nltk/nltk/parse/featurechart.py_apply,"def apply(self, chart, grammar, edge):
if edge.is_complete():
return
nextsym, index = edge.nextsym(), edge.end()
if not is_nonterminal(nextsym):
return
nextsym_with_bindings = edge.next_with_bindings()
done = self._done.get((nextsym_with_bindings, index), (None, None))
if done[0] is chart and done[1] is grammar:
return
for prod in grammar.productions(lhs=nextsym):
if prod.rhs():
first = prod.rhs()[0]
if is_terminal(first):
if index >= chart.num_leaves():
continue
if first != chart.leaf(index):
continue
if unify(prod.lhs(), nextsym_with_bindings, rename_vars=True):
new_edge = FeatureTreeEdge.from_production(prod, edge.end())
if chart.insert(new_edge, ()):
yield new_edge
self._done[nextsym_with_bindings, index] = (chart, grammar)
",[],0,[],/parse/featurechart.py_apply
1903,/home/amandapotts/git/nltk/nltk/parse/featurechart.py_apply,"def apply(self, chart, grammar, edge):
if edge.is_incomplete():
return
for prod in grammar.productions(rhs=edge.lhs()):
if isinstance(edge, FeatureTreeEdge):
_next = prod.rhs()[0]
if not is_nonterminal(_next):
continue
new_edge = FeatureTreeEdge.from_production(prod, edge.start())
if chart.insert(new_edge, ()):
yield new_edge
",[],0,[],/parse/featurechart.py_apply
1904,/home/amandapotts/git/nltk/nltk/parse/featurechart.py_apply,"def apply(self, chart, grammar, edge):
if edge.is_incomplete():
return
found = edge.lhs()
for prod in grammar.productions(rhs=found):
bindings = {}
if isinstance(edge, FeatureTreeEdge):
_next = prod.rhs()[0]
if not is_nonterminal(_next):
continue
used_vars = find_variables(
(prod.lhs(),) + prod.rhs(), fs_class=FeatStruct
)
found = found.rename_variables(used_vars=used_vars)
result = unify(_next, found, bindings, rename_vars=False)
if result is None:
continue
new_edge = FeatureTreeEdge.from_production(
prod, edge.start()
).move_dot_forward(edge.end(), bindings)
if chart.insert(new_edge, (edge,)):
yield new_edge
",[],0,[],/parse/featurechart.py_apply
1905,/home/amandapotts/git/nltk/nltk/parse/featurechart.py_apply,"def apply(self, chart, grammar):
for prod in grammar.productions(empty=True):
for index in range(chart.num_leaves() + 1):
new_edge = FeatureTreeEdge.from_production(prod, index)
if chart.insert(new_edge, ()):
yield new_edge
",[],0,[],/parse/featurechart.py_apply
1906,/home/amandapotts/git/nltk/nltk/parse/featurechart.py___init__,"def __init__(
self,
grammar,
strategy=BU_LC_FEATURE_STRATEGY,
trace_chart_width=20,
chart_class=FeatureChart,
",[],0,[],/parse/featurechart.py___init__
1907,/home/amandapotts/git/nltk/nltk/parse/featurechart.py___init__,"def __init__(self, grammar, **parser_args):
FeatureChartParser.__init__(self, grammar, TD_FEATURE_STRATEGY, **parser_args)
",[],0,[],/parse/featurechart.py___init__
1908,/home/amandapotts/git/nltk/nltk/parse/featurechart.py___init__,"def __init__(self, grammar, **parser_args):
FeatureChartParser.__init__(self, grammar, BU_FEATURE_STRATEGY, **parser_args)
",[],0,[],/parse/featurechart.py___init__
1909,/home/amandapotts/git/nltk/nltk/parse/featurechart.py___init__,"def __init__(self, grammar, **parser_args):
FeatureChartParser.__init__(
self, grammar, BU_LC_FEATURE_STRATEGY, **parser_args
)
",[],0,[],/parse/featurechart.py___init__
1910,/home/amandapotts/git/nltk/nltk/parse/featurechart.py___init__,"def __init__(self, tokens):
FeatureChart.__init__(self, tokens)
",[],0,[],/parse/featurechart.py___init__
1911,/home/amandapotts/git/nltk/nltk/parse/featurechart.py_initialize,"def initialize(self):
self._instantiated = set()
FeatureChart.initialize(self)
",[],0,[],/parse/featurechart.py_initialize
1912,/home/amandapotts/git/nltk/nltk/parse/featurechart.py_insert,"def insert(self, edge, child_pointer_list):
if edge in self._instantiated:
return False
self.instantiate_edge(edge)
return FeatureChart.insert(self, edge, child_pointer_list)
",[],0,[],/parse/featurechart.py_insert
1913,/home/amandapotts/git/nltk/nltk/parse/featurechart.py_instantiate_edge,"def instantiate_edge(self, edge):
""""""
If the edge is a ``FeatureTreeEdge``, and it is complete,
then instantiate all variables whose names start with '@',
by replacing them with unique new variables.
Note that instantiation is done in-place, since the
parsing algorithms might already hold a reference to
the edge for future use.
""""""
if not isinstance(edge, FeatureTreeEdge):
return
if not edge.is_complete():
return
if edge in self._edge_to_cpls:
return
inst_vars = self.inst_vars(edge)
if not inst_vars:
return
self._instantiated.add(edge)
edge._lhs = edge.lhs().substitute_bindings(inst_vars)
",[],0,[],/parse/featurechart.py_instantiate_edge
1914,/home/amandapotts/git/nltk/nltk/parse/featurechart.py_inst_vars,"def inst_vars(self, edge):
return {
var: logic.unique_variable()
for var in edge.lhs().variables()
if var.name.startswith(""@"")
}
",[],0,[],/parse/featurechart.py_inst_vars
1915,/home/amandapotts/git/nltk/nltk/parse/featurechart.py_demo_grammar,"def demo_grammar():
from nltk.grammar import FeatureGrammar
return FeatureGrammar.fromstring(
""""""
",[],0,[],/parse/featurechart.py_demo_grammar
1916,/home/amandapotts/git/nltk/nltk/parse/featurechart.py_demo,"def demo(
print_times=True,
print_grammar=True,
print_trees=True,
print_sentence=True,
trace=1,
parser=FeatureChartParser,
sent=""I saw John with a dog with my cookie"",
",[],0,[],/parse/featurechart.py_demo
1917,/home/amandapotts/git/nltk/nltk/parse/featurechart.py_run_profile,"def run_profile():
import profile
profile.run(""for i in range(1): demo()"", ""/tmp/profile.out"")
import pstats
p = pstats.Stats(""/tmp/profile.out"")
p.strip_dirs().sort_stats(""time"", ""cum"").print_stats(60)
p.strip_dirs().sort_stats(""cum"", ""time"").print_stats(60)
",[],0,[],/parse/featurechart.py_run_profile
1918,/home/amandapotts/git/nltk/nltk/metrics/distance.py__edit_dist_init,"def _edit_dist_init(len1, len2):
lev = []
for i in range(len1):
lev.append([0] * len2)  # initialize 2D array to zero
for i in range(len1):
lev[i][0] = i  # column 0: 0,1,2,3,4,...
for j in range(len2):
lev[0][j] = j  # row 0: 0,1,2,3,4,...
return lev
",[],0,[],/metrics/distance.py__edit_dist_init
1919,/home/amandapotts/git/nltk/nltk/metrics/distance.py__last_left_t_init,"def _last_left_t_init(sigma):
return {c: 0 for c in sigma}
",[],0,[],/metrics/distance.py__last_left_t_init
1920,/home/amandapotts/git/nltk/nltk/metrics/distance.py__edit_dist_step,"def _edit_dist_step(
lev, i, j, s1, s2, last_left, last_right, substitution_cost=1, transpositions=False
",[],0,[],/metrics/distance.py__edit_dist_step
1921,/home/amandapotts/git/nltk/nltk/metrics/distance.py_edit_distance,"def edit_distance(s1, s2, substitution_cost=1, transpositions=False):
""""""
Calculate the Levenshtein edit-distance between two strings.
The edit distance is the number of characters that need to be
substituted, inserted, or deleted, to transform s1 into s2.  For
example, transforming ""rain"" to ""shine"" requires three steps,
consisting of two substitutions and one insertion:
""rain"" -> ""sain"" -> ""shin"" -> ""shine"".  These operations could have
been done in other orders, but at least three steps are needed.
Allows specifying the cost of substitution edits (e.g., ""a"" -> ""b""),
because sometimes it makes sense to assign greater penalties to
substitutions.
This also optionally allows transposition edits (e.g., ""ab"" -> ""ba""),
though this is disabled by default.
:param s1, s2: The strings to be analysed
:param transpositions: Whether to allow transposition edits
:type s1: str
:type s2: str
:type substitution_cost: int
:type transpositions: bool
:rtype: int
""""""
len1 = len(s1)
len2 = len(s2)
lev = _edit_dist_init(len1 + 1, len2 + 1)
sigma = set()
sigma.update(s1)
sigma.update(s2)
last_left_t = _last_left_t_init(sigma)
for i in range(1, len1 + 1):
last_right_buf = 0
for j in range(1, len2 + 1):
last_left = last_left_t[s2[j - 1]]
last_right = last_right_buf
if s1[i - 1] == s2[j - 1]:
last_right_buf = j
_edit_dist_step(
lev,
i,
j,
s1,
s2,
last_left,
last_right,
substitution_cost=substitution_cost,
transpositions=transpositions,
)
last_left_t[s1[i - 1]] = i
return lev[len1][len2]
",[],0,[],/metrics/distance.py_edit_distance
1922,/home/amandapotts/git/nltk/nltk/metrics/distance.py__edit_dist_backtrace,"def _edit_dist_backtrace(lev):
i, j = len(lev) - 1, len(lev[0]) - 1
alignment = [(i, j)]
while (i, j) != (0, 0):
directions = [
(i - 1, j - 1),  # substitution
(i - 1, j),  # skip s1
(i, j - 1),  # skip s2
]
direction_costs = (
(lev[i][j] if (i >= 0 and j >= 0) else float(""inf""), (i, j))
for i, j in directions
)
_, (i, j) = min(direction_costs, key=operator.itemgetter(0))
alignment.append((i, j))
return list(reversed(alignment))
",[],0,[],/metrics/distance.py__edit_dist_backtrace
1923,/home/amandapotts/git/nltk/nltk/metrics/distance.py_edit_distance_align,"def edit_distance_align(s1, s2, substitution_cost=1):
""""""
Calculate the minimum Levenshtein edit-distance based alignment
mapping between two strings. The alignment finds the mapping
from string s1 to s2 that minimizes the edit distance cost.
For example, mapping ""rain"" to ""shine"" would involve 2
substitutions, 2 matches and an insertion resulting in
the following mapping:
[(0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (4, 5)]
NB: (0, 0) is the start state without any letters associated
See more: https://web.stanford.edu/class/cs124/lec/med.pdf
In case of multiple valid minimum-distance alignments, the
backtrace has the following operation precedence:
1. Substitute s1 and s2 characters
2. Skip s1 character
3. Skip s2 character
The backtrace is carried out in reverse string order.
This function does not support transposition.
:param s1, s2: The strings to be aligned
:type s1: str
:type s2: str
:type substitution_cost: int
:rtype: List[Tuple(int, int)]
""""""
len1 = len(s1)
len2 = len(s2)
lev = _edit_dist_init(len1 + 1, len2 + 1)
for i in range(len1):
for j in range(len2):
_edit_dist_step(
lev,
i + 1,
j + 1,
s1,
s2,
0,
0,
substitution_cost=substitution_cost,
transpositions=False,
)
alignment = _edit_dist_backtrace(lev)
return alignment
",[],0,[],/metrics/distance.py_edit_distance_align
1924,/home/amandapotts/git/nltk/nltk/metrics/distance.py_binary_distance,"def binary_distance(label1, label2):
""""""Simple equality test.
0.0 if the labels are identical, 1.0 if they are different.
>>> from nltk.metrics import binary_distance
>>> binary_distance(1,1)
0.0
>>> binary_distance(1,3)
1.0
""""""
return 0.0 if label1 == label2 else 1.0
",[],0,[],/metrics/distance.py_binary_distance
1925,/home/amandapotts/git/nltk/nltk/metrics/distance.py_jaccard_distance,"def jaccard_distance(label1, label2):
""""""Distance metric comparing set-similarity.""""""
return (len(label1.union(label2)) - len(label1.intersection(label2))) / len(
label1.union(label2)
)
",[],0,[],/metrics/distance.py_jaccard_distance
1926,/home/amandapotts/git/nltk/nltk/metrics/distance.py_masi_distance,"def masi_distance(label1, label2):
""""""Distance metric that takes into account partial agreement when multiple
labels are assigned.
>>> from nltk.metrics import masi_distance
>>> masi_distance(set([1, 2]), set([1, 2, 3, 4]))
0.665
Passonneau 2006, Measuring Agreement on Set-Valued Items (MASI)
for Semantic and Pragmatic Annotation.
""""""
len_intersection = len(label1.intersection(label2))
len_union = len(label1.union(label2))
len_label1 = len(label1)
len_label2 = len(label2)
if len_label1 == len_label2 and len_label1 == len_intersection:
m = 1
elif len_intersection == min(len_label1, len_label2):
m = 0.67
elif len_intersection > 0:
m = 0.33
else:
m = 0
return 1 - len_intersection / len_union * m
",[],0,[],/metrics/distance.py_masi_distance
1927,/home/amandapotts/git/nltk/nltk/metrics/distance.py_interval_distance,"def interval_distance(label1, label2):
""""""Krippendorff's interval distance metric
>>> from nltk.metrics import interval_distance
>>> interval_distance(1,10)
81
Krippendorff 1980, Content Analysis: An Introduction to its Methodology
""""""
try:
return pow(label1 - label2, 2)
except:
print(""non-numeric labels not supported with interval distance"")
",[],0,[],/metrics/distance.py_interval_distance
1928,/home/amandapotts/git/nltk/nltk/metrics/distance.py_jaro_similarity,"def jaro_similarity(s1, s2):
""""""
Computes the Jaro similarity between 2 sequences from:
Matthew A. Jaro (1989). Advances in record linkage methodology
as applied to the 1985 census of Tampa Florida. Journal of the
American Statistical Association. 84 (406): 414-20.
The Jaro distance between is the min no. of single-character transpositions
required to change one word into another. The Jaro similarity formula from
https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance :
``jaro_sim = 0 if m = 0 else 1/3 * (m/|s_1| + m/s_2 + (m-t)/m)``
where
- `|s_i|` is the length of string `s_i`
- `m` is the no. of matching characters
- `t` is the half no. of possible transpositions.
""""""
len_s1, len_s2 = len(s1), len(s2)
match_bound = max(len_s1, len_s2) // 2 - 1
matches = 0  # no.of matched characters in s1 and s2
transpositions = 0  # no. of transpositions between s1 and s2
flagged_1 = []  # positions in s1 which are matches to some character in s2
flagged_2 = []  # positions in s2 which are matches to some character in s1
for i in range(len_s1):  # Iterate through each character.
upperbound = min(i + match_bound, len_s2 - 1)
lowerbound = max(0, i - match_bound)
for j in range(lowerbound, upperbound + 1):
if s1[i] == s2[j] and j not in flagged_2:
matches += 1
flagged_1.append(i)
flagged_2.append(j)
break
flagged_2.sort()
for i, j in zip(flagged_1, flagged_2):
if s1[i] != s2[j]:
transpositions += 1
if matches == 0:
return 0
else:
return (
1
/ 3
matches / len_s1
+ matches / len_s2
+ (matches - transpositions // 2) / matches
)
)
",[],0,[],/metrics/distance.py_jaro_similarity
1929,/home/amandapotts/git/nltk/nltk/metrics/distance.py_jaro_winkler_similarity,"def jaro_winkler_similarity(s1, s2, p=0.1, max_l=4):
""""""
The Jaro Winkler distance is an extension of the Jaro similarity in:
William E. Winkler. 1990. String Comparator Metrics and Enhanced
Decision Rules in the Fellegi-Sunter Model of Record Linkage.
Proceedings of the Section on Survey Research Methods.
American Statistical Association: 354-359.
such that:
jaro_winkler_sim = jaro_sim + ( l * p * (1 - jaro_sim) )
where,
- jaro_sim is the output from the Jaro Similarity,
see jaro_similarity()
- l is the length of common prefix at the start of the string
- this implementation provides an upperbound for the l value
to keep the prefixes.A common value of this upperbound is 4.
- p is the constant scaling factor to overweigh common prefixes.
The Jaro-Winkler similarity will fall within the [0, 1] bound,
given that max(p)<=0.25 , default is p=0.1 in Winkler (1990)
Test using outputs from https://www.census.gov/srd/papers/pdf/rr93-8.pdf
from ""Table 5 Comparison of String Comparators Rescaled between 0 and 1""
>>> winkler_examples = [(""billy"", ""billy""), (""billy"", ""bill""), (""billy"", ""blily""),
... (""massie"", ""massey""), (""yvette"", ""yevett""), (""billy"", ""bolly""), (""dwayne"", ""duane""),
... (""dixon"", ""dickson""), (""billy"", ""susan"")]
>>> winkler_scores = [1.000, 0.967, 0.947, 0.944, 0.911, 0.893, 0.858, 0.853, 0.000]
>>> jaro_scores =    [1.000, 0.933, 0.933, 0.889, 0.889, 0.867, 0.822, 0.790, 0.000]
One way to match the values on the Winkler's paper is to provide a different
p scaling factor for different pairs of strings, e.g.
>>> p_factors = [0.1, 0.125, 0.20, 0.125, 0.20, 0.20, 0.20, 0.15, 0.1]
>>> for (s1, s2), jscore, wscore, p in zip(winkler_examples, jaro_scores, winkler_scores, p_factors):
...     assert round(jaro_similarity(s1, s2), 3) == jscore
...     assert round(jaro_winkler_similarity(s1, s2, p=p), 3) == wscore
Test using outputs from https://www.census.gov/srd/papers/pdf/rr94-5.pdf from
""Table 2.1. Comparison of String Comparators Using Last Names, First Names, and Street Names""
>>> winkler_examples = [('SHACKLEFORD', 'SHACKELFORD'), ('DUNNINGHAM', 'CUNNIGHAM'),
... ('NICHLESON', 'NICHULSON'), ('JONES', 'JOHNSON'), ('MASSEY', 'MASSIE'),
... ('ABROMS', 'ABRAMS'), ('HARDIN', 'MARTINEZ'), ('ITMAN', 'SMITH'),
... ('JERALDINE', 'GERALDINE'), ('MARHTA', 'MARTHA'), ('MICHELLE', 'MICHAEL'),
... ('JULIES', 'JULIUS'), ('TANYA', 'TONYA'), ('DWAYNE', 'DUANE'), ('SEAN', 'SUSAN'),
... ('JON', 'JOHN'), ('JON', 'JAN'), ('BROOKHAVEN', 'BRROKHAVEN'),
... ('BROOK HALLOW', 'BROOK HLLW'), ('DECATUR', 'DECATIR'), ('FITZRUREITER', 'FITZENREITER'),
... ('HIGBEE', 'HIGHEE'), ('HIGBEE', 'HIGVEE'), ('LACURA', 'LOCURA'), ('IOWA', 'IONA'), ('1ST', 'IST')]
>>> jaro_scores =   [0.970, 0.896, 0.926, 0.790, 0.889, 0.889, 0.722, 0.467, 0.926,
... 0.944, 0.869, 0.889, 0.867, 0.822, 0.783, 0.917, 0.000, 0.933, 0.944, 0.905,
... 0.856, 0.889, 0.889, 0.889, 0.833, 0.000]
>>> winkler_scores = [0.982, 0.896, 0.956, 0.832, 0.944, 0.922, 0.722, 0.467, 0.926,
... 0.961, 0.921, 0.933, 0.880, 0.858, 0.805, 0.933, 0.000, 0.947, 0.967, 0.943,
... 0.913, 0.922, 0.922, 0.900, 0.867, 0.000]
One way to match the values on the Winkler's paper is to provide a different
p scaling factor for different pairs of strings, e.g.
>>> p_factors = [0.1, 0.1, 0.1, 0.1, 0.125, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.20,
... 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
>>> for (s1, s2), jscore, wscore, p in zip(winkler_examples, jaro_scores, winkler_scores, p_factors):
...     if (s1, s2) in [('JON', 'JAN'), ('1ST', 'IST')]:
...         continue  # Skip bad examples from the paper.
...     assert round(jaro_similarity(s1, s2), 3) == jscore
...     assert round(jaro_winkler_similarity(s1, s2, p=p), 3) == wscore
This test-case proves that the output of Jaro-Winkler similarity depends on
the product  l * p and not on the product max_l * p. Here the product max_l * p > 1
however the product l * p <= 1
>>> round(jaro_winkler_similarity('TANYA', 'TONYA', p=0.1, max_l=100), 3)
0.88
""""""
if not 0 <= max_l * p <= 1:
warnings.warn(
str(
""The product  `max_l * p` might not fall between [0,1].""
""Jaro-Winkler similarity might not be between 0 and 1.""
)
)
jaro_sim = jaro_similarity(s1, s2)
l = 0
for s1_i, s2_i in zip(s1, s2):
if s1_i == s2_i:
l += 1
else:
break
if l == max_l:
break
return jaro_sim + (l * p * (1 - jaro_sim))
",[],0,[],/metrics/distance.py_jaro_winkler_similarity
1930,/home/amandapotts/git/nltk/nltk/metrics/distance.py_demo,"def demo():
string_distance_examples = [
(""rain"", ""shine""),
(""abcdef"", ""acbdef""),
(""language"", ""lnaguaeg""),
(""language"", ""lnaugage""),
(""language"", ""lngauage""),
]
for s1, s2 in string_distance_examples:
print(f""Edit distance btwn '{s1}' and '{s2}':"", edit_distance(s1, s2))
print(
f""Edit dist with transpositions btwn '{s1}' and '{s2}':"",
edit_distance(s1, s2, transpositions=True),
)
print(f""Jaro similarity btwn '{s1}' and '{s2}':"", jaro_similarity(s1, s2))
print(
f""Jaro-Winkler similarity btwn '{s1}' and '{s2}':"",
jaro_winkler_similarity(s1, s2),
)
print(
f""Jaro-Winkler distance btwn '{s1}' and '{s2}':"",
1 - jaro_winkler_similarity(s1, s2),
)
s1 = {1, 2, 3, 4}
s2 = {3, 4, 5}
print(""s1:"", s1)
print(""s2:"", s2)
print(""Binary distance:"", binary_distance(s1, s2))
print(""Jaccard distance:"", jaccard_distance(s1, s2))
print(""MASI distance:"", masi_distance(s1, s2))
",[],0,[],/metrics/distance.py_demo
1931,/home/amandapotts/git/nltk/nltk/metrics/aline.py_align,"def align(str1, str2, epsilon=0):
""""""
Compute the alignment of two phonetic strings.
:param str str1: First string to be aligned
:param str str2: Second string to be aligned
:type epsilon: float (0.0 to 1.0)
:param epsilon: Adjusts threshold similarity score for near-optimal alignments
:rtype: list(list(tuple(str, str)))
:return: Alignment(s) of str1 and str2
(Kondrak 2002: 51)
""""""
if np is None:
raise ImportError(""You need numpy in order to use the align function"")
assert 0.0 <= epsilon <= 1.0, ""Epsilon must be between 0.0 and 1.0.""
m = len(str1)
n = len(str2)
S = np.zeros((m + 1, n + 1), dtype=float)
for i in range(1, m + 1):
for j in range(1, n + 1):
edit1 = S[i - 1, j] + sigma_skip(str1[i - 1])
edit2 = S[i, j - 1] + sigma_skip(str2[j - 1])
edit3 = S[i - 1, j - 1] + sigma_sub(str1[i - 1], str2[j - 1])
if i > 1:
edit4 = S[i - 2, j - 1] + sigma_exp(str2[j - 1], str1[i - 2 : i])
else:
edit4 = -inf
if j > 1:
edit5 = S[i - 1, j - 2] + sigma_exp(str1[i - 1], str2[j - 2 : j])
else:
edit5 = -inf
S[i, j] = max(edit1, edit2, edit3, edit4, edit5, 0)
T = (1 - epsilon) * np.amax(S)  # Threshold score for near-optimal alignments
alignments = []
for i in range(1, m + 1):
for j in range(1, n + 1):
if S[i, j] >= T:
alignments.append(_retrieve(i, j, 0, S, T, str1, str2, []))
return alignments
","['zeros', 'amax']",2,"['zeros((m + 1, n + 1), dtype=float)', 'amax(S)']",/metrics/aline.py_align
1932,/home/amandapotts/git/nltk/nltk/metrics/aline.py__retrieve,"def _retrieve(i, j, s, S, T, str1, str2, out):
""""""
Retrieve the path through the similarity matrix S starting at (i, j).
:rtype: list(tuple(str, str))
:return: Alignment of str1 and str2
""""""
if S[i, j] == 0:
return out
else:
if j > 1 and S[i - 1, j - 2] + sigma_exp(str1[i - 1], str2[j - 2 : j]) + s >= T:
out.insert(0, (str1[i - 1], str2[j - 2 : j]))
_retrieve(
i - 1,
j - 2,
s + sigma_exp(str1[i - 1], str2[j - 2 : j]),
S,
T,
str1,
str2,
out,
)
elif (
i > 1 and S[i - 2, j - 1] + sigma_exp(str2[j - 1], str1[i - 2 : i]) + s >= T
):
out.insert(0, (str1[i - 2 : i], str2[j - 1]))
_retrieve(
i - 2,
j - 1,
s + sigma_exp(str2[j - 1], str1[i - 2 : i]),
S,
T,
str1,
str2,
out,
)
elif S[i, j - 1] + sigma_skip(str2[j - 1]) + s >= T:
out.insert(0, (""-"", str2[j - 1]))
_retrieve(i, j - 1, s + sigma_skip(str2[j - 1]), S, T, str1, str2, out)
elif S[i - 1, j] + sigma_skip(str1[i - 1]) + s >= T:
out.insert(0, (str1[i - 1], ""-""))
_retrieve(i - 1, j, s + sigma_skip(str1[i - 1]), S, T, str1, str2, out)
elif S[i - 1, j - 1] + sigma_sub(str1[i - 1], str2[j - 1]) + s >= T:
out.insert(0, (str1[i - 1], str2[j - 1]))
_retrieve(
i - 1,
j - 1,
s + sigma_sub(str1[i - 1], str2[j - 1]),
S,
T,
str1,
str2,
out,
)
return out
",[],0,[],/metrics/aline.py__retrieve
1933,/home/amandapotts/git/nltk/nltk/metrics/aline.py_sigma_skip,"def sigma_skip(p):
""""""
Returns score of an indel of P.
(Kondrak 2002: 54)
""""""
return C_skip
",[],0,[],/metrics/aline.py_sigma_skip
1934,/home/amandapotts/git/nltk/nltk/metrics/aline.py_sigma_sub,"def sigma_sub(p, q):
""""""
Returns score of a substitution of P with Q.
(Kondrak 2002: 54)
""""""
return C_sub - delta(p, q) - V(p) - V(q)
",[],0,[],/metrics/aline.py_sigma_sub
1935,/home/amandapotts/git/nltk/nltk/metrics/aline.py_sigma_exp,"def sigma_exp(p, q):
""""""
Returns score of an expansion/compression.
(Kondrak 2002: 54)
""""""
q1 = q[0]
q2 = q[1]
return C_exp - delta(p, q1) - delta(p, q2) - V(p) - max(V(q1), V(q2))
",[],0,[],/metrics/aline.py_sigma_exp
1936,/home/amandapotts/git/nltk/nltk/metrics/aline.py_delta,"def delta(p, q):
""""""
Return weighted sum of difference between P and Q.
(Kondrak 2002: 54)
""""""
features = R(p, q)
total = 0
if np is not None:
return np.dot(
[diff(p, q, f) for f in features], [salience[f] for f in features]
)
for f in features:
total += diff(p, q, f) * salience[f]
return total
",['dot'],1,[],/metrics/aline.py_delta
1937,/home/amandapotts/git/nltk/nltk/metrics/aline.py_diff,"def diff(p, q, f):
""""""
Returns difference between phonetic segments P and Q for feature F.
(Kondrak 2002: 52, 54)
""""""
p_features, q_features = feature_matrix[p], feature_matrix[q]
return abs(similarity_matrix[p_features[f]] - similarity_matrix[q_features[f]])
",[],0,[],/metrics/aline.py_diff
1938,/home/amandapotts/git/nltk/nltk/metrics/aline.py_R,"def R(p, q):
""""""
Return relevant features for segment comparison.
(Kondrak 2002: 54)
""""""
if p in consonants or q in consonants:
return R_c
return R_v
",[],0,[],/metrics/aline.py_R
1939,/home/amandapotts/git/nltk/nltk/metrics/aline.py_V,"def V(p):
""""""
Return vowel weight if P is vowel.
(Kondrak 2002: 54)
""""""
if p in consonants:
return 0
return C_vwl
",[],0,[],/metrics/aline.py_V
1940,/home/amandapotts/git/nltk/nltk/metrics/aline.py_demo,"def demo():
""""""
A demonstration of the result of aligning phonetic sequences
used in Kondrak's (2002) dissertation.
""""""
data = [pair.split("","") for pair in cognate_data.split(""\n"")]
for pair in data:
alignment = align(pair[0], pair[1])[0]
alignment = [f""({a[0]}, {a[1]})"" for a in alignment]
alignment = "" "".join(alignment)
print(f""{pair[0]} ~ {pair[1]} : {alignment}"")
",[],0,[],/metrics/aline.py_demo
1941,/home/amandapotts/git/nltk/nltk/metrics/spearman.py__rank_dists,"def _rank_dists(ranks1, ranks2):
""""""Finds the difference between the values in ranks1 and ranks2 for keys
present in both dicts. If the arguments are not dicts, they are converted
from (key, rank) sequences.
""""""
ranks1 = dict(ranks1)
ranks2 = dict(ranks2)
for k in ranks1:
try:
yield k, ranks1[k] - ranks2[k]
except KeyError:
pass
",[],0,[],/metrics/spearman.py__rank_dists
1942,/home/amandapotts/git/nltk/nltk/metrics/spearman.py_spearman_correlation,"def spearman_correlation(ranks1, ranks2):
""""""Returns the Spearman correlation coefficient for two rankings, which
should be dicts or sequences of (key, rank). The coefficient ranges from
-1.0 (ranks are opposite) to 1.0 (ranks are identical), and is only
calculated for keys in both rankings (for meaningful results, remove keys
present in only one list before ranking).""""""
n = 0
res = 0
for k, d in _rank_dists(ranks1, ranks2):
res += d * d
n += 1
try:
return 1 - (6 * res / (n * (n * n - 1)))
except ZeroDivisionError:
return 0.0
",[],0,[],/metrics/spearman.py_spearman_correlation
1943,/home/amandapotts/git/nltk/nltk/metrics/spearman.py_ranks_from_sequence,"def ranks_from_sequence(seq):
""""""Given a sequence, yields each element with an increasing rank, suitable
for use as an argument to ``spearman_correlation``.
""""""
return ((k, i) for i, k in enumerate(seq))
",[],0,[],/metrics/spearman.py_ranks_from_sequence
1944,/home/amandapotts/git/nltk/nltk/metrics/spearman.py_ranks_from_scores,"def ranks_from_scores(scores, rank_gap=1e-15):
""""""Given a sequence of (key, score) tuples, yields each key with an
increasing rank, tying with previous key's rank if the difference between
their scores is less than rank_gap. Suitable for use as an argument to
``spearman_correlation``.
""""""
prev_score = None
rank = 0
for i, (key, score) in enumerate(scores):
try:
if abs(score - prev_score) > rank_gap:
rank = i
except TypeError:
pass
yield key, rank
prev_score = score
",[],0,[],/metrics/spearman.py_ranks_from_scores
1945,/home/amandapotts/git/nltk/nltk/metrics/association.py_fisher_exact,"def fisher_exact(*_args, **_kwargs):
raise NotImplementedError
",[],0,[],/metrics/association.py_fisher_exact
1946,/home/amandapotts/git/nltk/nltk/metrics/association.py__contingency,"def _contingency(*marginals):
""""""Calculates values of a contingency table from marginal values.""""""
raise NotImplementedError(
""The contingency table is not available"" ""in the general ngram case""
)
",[],0,[],/metrics/association.py__contingency
1947,/home/amandapotts/git/nltk/nltk/metrics/association.py__marginals,"def _marginals(*contingency):
""""""Calculates values of contingency table marginals from its values.""""""
raise NotImplementedError(
""The contingency table is not available"" ""in the general ngram case""
)
",[],0,[],/metrics/association.py__marginals
1948,/home/amandapotts/git/nltk/nltk/metrics/association.py__expected_values,"def _expected_values(cls, cont):
""""""Calculates expected values for a contingency table.""""""
n_all = sum(cont)
bits = [1 << i for i in range(cls._n)]
for i in range(len(cont)):
yield (
_product(
sum(cont[x] for x in range(2**cls._n) if (x & j) == (i & j))
for j in bits
)
/ (n_all ** (cls._n - 1))
)
",[],0,[],/metrics/association.py__expected_values
1949,/home/amandapotts/git/nltk/nltk/metrics/association.py_raw_freq,"def raw_freq(*marginals):
""""""Scores ngrams by their frequency""""""
return marginals[NGRAM] / marginals[TOTAL]
",[],0,[],/metrics/association.py_raw_freq
1950,/home/amandapotts/git/nltk/nltk/metrics/association.py_student_t,"def student_t(cls, *marginals):
""""""Scores ngrams using Student's t test with independence hypothesis
for unigrams, as in Manning and Schutze 5.3.1.
""""""
return (
marginals[NGRAM]
- _product(marginals[UNIGRAMS]) / (marginals[TOTAL] ** (cls._n - 1))
) / (marginals[NGRAM] + _SMALL) ** 0.5
",[],0,[],/metrics/association.py_student_t
1951,/home/amandapotts/git/nltk/nltk/metrics/association.py_chi_sq,"def chi_sq(cls, *marginals):
""""""Scores ngrams using Pearson's chi-square as in Manning and Schutze
5.3.3.
""""""
cont = cls._contingency(*marginals)
exps = cls._expected_values(cont)
return sum((obs - exp) ** 2 / (exp + _SMALL) for obs, exp in zip(cont, exps))
",[],0,[],/metrics/association.py_chi_sq
1952,/home/amandapotts/git/nltk/nltk/metrics/association.py_mi_like,"def mi_like(*marginals, **kwargs):
""""""Scores ngrams using a variant of mutual information. The keyword
argument power sets an exponent (default 3) for the numerator. No
logarithm of the result is calculated.
""""""
return marginals[NGRAM] ** kwargs.get(""power"", 3) / _product(
marginals[UNIGRAMS]
)
",[],0,[],/metrics/association.py_mi_like
1953,/home/amandapotts/git/nltk/nltk/metrics/association.py_pmi,"def pmi(cls, *marginals):
""""""Scores ngrams by pointwise mutual information, as in Manning and
Schutze 5.4.
""""""
return _log2(marginals[NGRAM] * marginals[TOTAL] ** (cls._n - 1)) - _log2(
_product(marginals[UNIGRAMS])
)
",[],0,[],/metrics/association.py_pmi
1954,/home/amandapotts/git/nltk/nltk/metrics/association.py_likelihood_ratio,"def likelihood_ratio(cls, *marginals):
""""""Scores ngrams using likelihood ratios as in Manning and Schutze 5.3.4.""""""
cont = cls._contingency(*marginals)
return 2 * sum(
obs * _ln(obs / (exp + _SMALL) + _SMALL)
for obs, exp in zip(cont, cls._expected_values(cont))
)
",[],0,[],/metrics/association.py_likelihood_ratio
1955,/home/amandapotts/git/nltk/nltk/metrics/association.py_poisson_stirling,"def poisson_stirling(cls, *marginals):
""""""Scores ngrams using the Poisson-Stirling measure.""""""
exp = _product(marginals[UNIGRAMS]) / (marginals[TOTAL] ** (cls._n - 1))
return marginals[NGRAM] * (_log2(marginals[NGRAM] / exp) - 1)
",[],0,[],/metrics/association.py_poisson_stirling
1956,/home/amandapotts/git/nltk/nltk/metrics/association.py_jaccard,"def jaccard(cls, *marginals):
""""""Scores ngrams using the Jaccard index.""""""
cont = cls._contingency(*marginals)
return cont[0] / sum(cont[:-1])
",[],0,[],/metrics/association.py_jaccard
1957,/home/amandapotts/git/nltk/nltk/metrics/association.py__contingency,"def _contingency(n_ii, n_ix_xi_tuple, n_xx):
""""""Calculates values of a bigram contingency table from marginal values.""""""
(n_ix, n_xi) = n_ix_xi_tuple
n_oi = n_xi - n_ii
n_io = n_ix - n_ii
return (n_ii, n_oi, n_io, n_xx - n_ii - n_oi - n_io)
",[],0,[],/metrics/association.py__contingency
1958,/home/amandapotts/git/nltk/nltk/metrics/association.py__marginals,"def _marginals(n_ii, n_oi, n_io, n_oo):
""""""Calculates values of contingency table marginals from its values.""""""
return (n_ii, (n_oi + n_ii, n_io + n_ii), n_oo + n_oi + n_io + n_ii)
",[],0,[],/metrics/association.py__marginals
1959,/home/amandapotts/git/nltk/nltk/metrics/association.py__expected_values,"def _expected_values(cont):
""""""Calculates expected values for a contingency table.""""""
n_xx = sum(cont)
for i in range(4):
yield (cont[i] + cont[i ^ 1]) * (cont[i] + cont[i ^ 2]) / n_xx
",[],0,[],/metrics/association.py__expected_values
1960,/home/amandapotts/git/nltk/nltk/metrics/association.py_phi_sq,"def phi_sq(cls, *marginals):
""""""Scores bigrams using phi-square, the square of the Pearson correlation
coefficient.
""""""
n_ii, n_io, n_oi, n_oo = cls._contingency(*marginals)
return (n_ii * n_oo - n_io * n_oi) ** 2 / (
(n_ii + n_io) * (n_ii + n_oi) * (n_io + n_oo) * (n_oi + n_oo)
)
",[],0,[],/metrics/association.py_phi_sq
1961,/home/amandapotts/git/nltk/nltk/metrics/association.py_chi_sq,"def chi_sq(cls, n_ii, n_ix_xi_tuple, n_xx):
""""""Scores bigrams using chi-square, i.e. phi-sq multiplied by the number
of bigrams, as in Manning and Schutze 5.3.3.
""""""
(n_ix, n_xi) = n_ix_xi_tuple
return n_xx * cls.phi_sq(n_ii, (n_ix, n_xi), n_xx)
",[],0,[],/metrics/association.py_chi_sq
1962,/home/amandapotts/git/nltk/nltk/metrics/association.py_fisher,"def fisher(cls, *marginals):
""""""Scores bigrams using Fisher's Exact Test (Pedersen 1996).  Less
sensitive to small counts than PMI or Chi Sq, but also more expensive
to compute. Requires scipy.
""""""
n_ii, n_io, n_oi, n_oo = cls._contingency(*marginals)
(odds, pvalue) = fisher_exact([[n_ii, n_io], [n_oi, n_oo]], alternative=""less"")
return pvalue
",[],0,[],/metrics/association.py_fisher
1963,/home/amandapotts/git/nltk/nltk/metrics/association.py_dice,"def dice(n_ii, n_ix_xi_tuple, n_xx):
""""""Scores bigrams using Dice's coefficient.""""""
(n_ix, n_xi) = n_ix_xi_tuple
return 2 * n_ii / (n_ix + n_xi)
",[],0,[],/metrics/association.py_dice
1964,/home/amandapotts/git/nltk/nltk/metrics/association.py__contingency,"def _contingency(n_iii, n_iix_tuple, n_ixx_tuple, n_xxx):
""""""Calculates values of a trigram contingency table (or cube) from
marginal values.
>>> TrigramAssocMeasures._contingency(1, (1, 1, 1), (1, 73, 1), 2000)
(1, 0, 0, 0, 0, 72, 0, 1927)
""""""
(n_iix, n_ixi, n_xii) = n_iix_tuple
(n_ixx, n_xix, n_xxi) = n_ixx_tuple
n_oii = n_xii - n_iii
n_ioi = n_ixi - n_iii
n_iio = n_iix - n_iii
n_ooi = n_xxi - n_iii - n_oii - n_ioi
n_oio = n_xix - n_iii - n_oii - n_iio
n_ioo = n_ixx - n_iii - n_ioi - n_iio
n_ooo = n_xxx - n_iii - n_oii - n_ioi - n_iio - n_ooi - n_oio - n_ioo
return (n_iii, n_oii, n_ioi, n_ooi, n_iio, n_oio, n_ioo, n_ooo)
",[],0,[],/metrics/association.py__contingency
1965,/home/amandapotts/git/nltk/nltk/metrics/association.py__marginals,"def _marginals(*contingency):
""""""Calculates values of contingency table marginals from its values.
>>> TrigramAssocMeasures._marginals(1, 0, 0, 0, 0, 72, 0, 1927)
(1, (1, 1, 1), (1, 73, 1), 2000)
""""""
n_iii, n_oii, n_ioi, n_ooi, n_iio, n_oio, n_ioo, n_ooo = contingency
return (
n_iii,
(n_iii + n_iio, n_iii + n_ioi, n_iii + n_oii),
(
n_iii + n_ioi + n_iio + n_ioo,
n_iii + n_oii + n_iio + n_oio,
n_iii + n_oii + n_ioi + n_ooi,
),
sum(contingency),
)
",[],0,[],/metrics/association.py__marginals
1966,/home/amandapotts/git/nltk/nltk/metrics/association.py__contingency,"def _contingency(n_iiii, n_iiix_tuple, n_iixx_tuple, n_ixxx_tuple, n_xxxx):
""""""Calculates values of a quadgram contingency table from
marginal values.
""""""
(n_iiix, n_iixi, n_ixii, n_xiii) = n_iiix_tuple
(n_iixx, n_ixix, n_ixxi, n_xixi, n_xxii, n_xiix) = n_iixx_tuple
(n_ixxx, n_xixx, n_xxix, n_xxxi) = n_ixxx_tuple
n_oiii = n_xiii - n_iiii
n_ioii = n_ixii - n_iiii
n_iioi = n_iixi - n_iiii
n_ooii = n_xxii - n_iiii - n_oiii - n_ioii
n_oioi = n_xixi - n_iiii - n_oiii - n_iioi
n_iooi = n_ixxi - n_iiii - n_ioii - n_iioi
n_oooi = n_xxxi - n_iiii - n_oiii - n_ioii - n_iioi - n_ooii - n_iooi - n_oioi
n_iiio = n_iiix - n_iiii
n_oiio = n_xiix - n_iiii - n_oiii - n_iiio
n_ioio = n_ixix - n_iiii - n_ioii - n_iiio
n_ooio = n_xxix - n_iiii - n_oiii - n_ioii - n_iiio - n_ooii - n_ioio - n_oiio
n_iioo = n_iixx - n_iiii - n_iioi - n_iiio
n_oioo = n_xixx - n_iiii - n_oiii - n_iioi - n_iiio - n_oioi - n_oiio - n_iioo
n_iooo = n_ixxx - n_iiii - n_ioii - n_iioi - n_iiio - n_iooi - n_iioo - n_ioio
n_oooo = (
n_xxxx
- n_iiii
- n_oiii
- n_ioii
- n_iioi
- n_ooii
- n_oioi
- n_iooi
- n_oooi
- n_iiio
- n_oiio
- n_ioio
- n_ooio
- n_iioo
- n_oioo
- n_iooo
)
return (
n_iiii,
n_oiii,
n_ioii,
n_ooii,
n_iioi,
n_oioi,
n_iooi,
n_oooi,
n_iiio,
n_oiio,
n_ioio,
n_ooio,
n_iioo,
n_oioo,
n_iooo,
n_oooo,
)
",[],0,[],/metrics/association.py__contingency
1967,/home/amandapotts/git/nltk/nltk/metrics/association.py__marginals,"def _marginals(*contingency):
""""""Calculates values of contingency table marginals from its values.
QuadgramAssocMeasures._marginals(1, 0, 2, 46, 552, 825, 2577, 34967, 1, 0, 2, 48, 7250, 9031, 28585, 356653)
(1, (2, 553, 3, 1), (7804, 6, 3132, 1378, 49, 2), (38970, 17660, 100, 38970), 440540)
""""""
(
n_iiii,
n_oiii,
n_ioii,
n_ooii,
n_iioi,
n_oioi,
n_iooi,
n_oooi,
n_iiio,
n_oiio,
n_ioio,
n_ooio,
n_iioo,
n_oioo,
n_iooo,
n_oooo,
) = contingency
n_iiix = n_iiii + n_iiio
n_iixi = n_iiii + n_iioi
n_ixii = n_iiii + n_ioii
n_xiii = n_iiii + n_oiii
n_iixx = n_iiii + n_iioi + n_iiio + n_iioo
n_ixix = n_iiii + n_ioii + n_iiio + n_ioio
n_ixxi = n_iiii + n_ioii + n_iioi + n_iooi
n_xixi = n_iiii + n_oiii + n_iioi + n_oioi
n_xxii = n_iiii + n_oiii + n_ioii + n_ooii
n_xiix = n_iiii + n_oiii + n_iiio + n_oiio
n_ixxx = n_iiii + n_ioii + n_iioi + n_iiio + n_iooi + n_iioo + n_ioio + n_iooo
n_xixx = n_iiii + n_oiii + n_iioi + n_iiio + n_oioi + n_oiio + n_iioo + n_oioo
n_xxix = n_iiii + n_oiii + n_ioii + n_iiio + n_ooii + n_ioio + n_oiio + n_ooio
n_xxxi = n_iiii + n_oiii + n_ioii + n_iioi + n_ooii + n_iooi + n_oioi + n_oooi
n_all = sum(contingency)
return (
n_iiii,
(n_iiix, n_iixi, n_ixii, n_xiii),
(n_iixx, n_ixix, n_ixxi, n_xixi, n_xxii, n_xiix),
(n_ixxx, n_xixx, n_xxix, n_xxxi),
n_all,
)
",[],0,[],/metrics/association.py__marginals
1968,/home/amandapotts/git/nltk/nltk/metrics/association.py___init__,"def __init__(self, measures):
""""""Constructs a ContingencyMeasures given a NgramAssocMeasures class""""""
self.__class__.__name__ = ""Contingency"" + measures.__class__.__name__
for k in dir(measures):
if k.startswith(""__""):
continue
v = getattr(measures, k)
if not k.startswith(""_""):
v = self._make_contingency_fn(measures, v)
setattr(self, k, v)
",[],0,[],/metrics/association.py___init__
1969,/home/amandapotts/git/nltk/nltk/metrics/association.py__make_contingency_fn,"def _make_contingency_fn(measures, old_fn):
""""""From an association measure function, produces a new function which
accepts contingency table values as its arguments.
""""""
",[],0,[],/metrics/association.py__make_contingency_fn
1970,/home/amandapotts/git/nltk/nltk/metrics/association.py_res,"def res(*contingency):
return old_fn(*measures._marginals(*contingency))
",[],0,[],/metrics/association.py_res
1971,/home/amandapotts/git/nltk/nltk/metrics/paice.py_get_words_from_dictionary,"def get_words_from_dictionary(lemmas):
""""""
Get original set of words used for analysis.
:param lemmas: A dictionary where keys are lemmas and values are sets
or lists of words corresponding to that lemma.
:type lemmas: dict(str): list(str)
:return: Set of words that exist as values in the dictionary
:rtype: set(str)
""""""
words = set()
for lemma in lemmas:
words.update(set(lemmas[lemma]))
return words
",[],0,[],/metrics/paice.py_get_words_from_dictionary
1972,/home/amandapotts/git/nltk/nltk/metrics/paice.py__truncate,"def _truncate(words, cutlength):
""""""Group words by stems defined by truncating them at given length.
:param words: Set of words used for analysis
:param cutlength: Words are stemmed by cutting at this length.
:type words: set(str) or list(str)
:type cutlength: int
:return: Dictionary where keys are stems and values are sets of words
corresponding to that stem.
:rtype: dict(str): set(str)
""""""
stems = {}
for word in words:
stem = word[:cutlength]
try:
stems[stem].update([word])
except KeyError:
stems[stem] = {word}
return stems
",[],0,[],/metrics/paice.py__truncate
1973,/home/amandapotts/git/nltk/nltk/metrics/paice.py__count_intersection,"def _count_intersection(l1, l2):
""""""Count intersection between two line segments defined by coordinate pairs.
:param l1: Tuple of two coordinate pairs defining the first line segment
:param l2: Tuple of two coordinate pairs defining the second line segment
:type l1: tuple(float, float)
:type l2: tuple(float, float)
:return: Coordinates of the intersection
:rtype: tuple(float, float)
""""""
x1, y1 = l1[0]
x2, y2 = l1[1]
x3, y3 = l2[0]
x4, y4 = l2[1]
denominator = (x1 - x2) * (y3 - y4) - (y1 - y2) * (x3 - x4)
if denominator == 0.0:  # lines are parallel
if x1 == x2 == x3 == x4 == 0.0:
return (0.0, y4)
x = (
(x1 * y2 - y1 * x2) * (x3 - x4) - (x1 - x2) * (x3 * y4 - y3 * x4)
) / denominator
y = (
(x1 * y2 - y1 * x2) * (y3 - y4) - (y1 - y2) * (x3 * y4 - y3 * x4)
) / denominator
return (x, y)
",[],0,[],/metrics/paice.py__count_intersection
1974,/home/amandapotts/git/nltk/nltk/metrics/paice.py__get_derivative,"def _get_derivative(coordinates):
""""""Get derivative of the line from (0,0) to given coordinates.
:param coordinates: A coordinate pair
:type coordinates: tuple(float, float)
:return: Derivative
:rtype: float
""""""
try:
return coordinates[1] / coordinates[0]
except ZeroDivisionError:
return float(""inf"")
",[],0,[],/metrics/paice.py__get_derivative
1975,/home/amandapotts/git/nltk/nltk/metrics/paice.py__calculate_cut,"def _calculate_cut(lemmawords, stems):
""""""Count understemmed and overstemmed pairs for (lemma, stem) pair with common words.
:param lemmawords: Set or list of words corresponding to certain lemma.
:param stems: A dictionary where keys are stems and values are sets
or lists of words corresponding to that stem.
:type lemmawords: set(str) or list(str)
:type stems: dict(str): set(str)
:return: Amount of understemmed and overstemmed pairs contributed by words
existing in both lemmawords and stems.
:rtype: tuple(float, float)
""""""
umt, wmt = 0.0, 0.0
for stem in stems:
cut = set(lemmawords) & set(stems[stem])
if cut:
cutcount = len(cut)
stemcount = len(stems[stem])
umt += cutcount * (len(lemmawords) - cutcount)
wmt += cutcount * (stemcount - cutcount)
return (umt, wmt)
",[],0,[],/metrics/paice.py__calculate_cut
1976,/home/amandapotts/git/nltk/nltk/metrics/paice.py__calculate,"def _calculate(lemmas, stems):
""""""Calculate actual and maximum possible amounts of understemmed and overstemmed word pairs.
:param lemmas: A dictionary where keys are lemmas and values are sets
or lists of words corresponding to that lemma.
:param stems: A dictionary where keys are stems and values are sets
or lists of words corresponding to that stem.
:type lemmas: dict(str): list(str)
:type stems: dict(str): set(str)
:return: Global unachieved merge total (gumt),
global desired merge total (gdmt),
global wrongly merged total (gwmt) and
global desired non-merge total (gdnt).
:rtype: tuple(float, float, float, float)
""""""
n = sum(len(lemmas[word]) for word in lemmas)
gdmt, gdnt, gumt, gwmt = (0.0, 0.0, 0.0, 0.0)
for lemma in lemmas:
lemmacount = len(lemmas[lemma])
gdmt += lemmacount * (lemmacount - 1)
gdnt += lemmacount * (n - lemmacount)
umt, wmt = _calculate_cut(lemmas[lemma], stems)
gumt += umt
gwmt += wmt
return (gumt / 2, gdmt / 2, gwmt / 2, gdnt / 2)
",[],0,[],/metrics/paice.py__calculate
1977,/home/amandapotts/git/nltk/nltk/metrics/paice.py__indexes,"def _indexes(gumt, gdmt, gwmt, gdnt):
""""""Count Understemming Index (UI), Overstemming Index (OI) and Stemming Weight (SW).
:param gumt, gdmt, gwmt, gdnt: Global unachieved merge total (gumt),
global desired merge total (gdmt),
global wrongly merged total (gwmt) and
global desired non-merge total (gdnt).
:type gumt, gdmt, gwmt, gdnt: float
:return: Understemming Index (UI),
Overstemming Index (OI) and
Stemming Weight (SW).
:rtype: tuple(float, float, float)
""""""
try:
ui = gumt / gdmt
except ZeroDivisionError:
ui = 0.0
try:
oi = gwmt / gdnt
except ZeroDivisionError:
oi = 0.0
try:
sw = oi / ui
except ZeroDivisionError:
if oi == 0.0:
sw = float(""nan"")
else:
sw = float(""inf"")
return (ui, oi, sw)
",[],0,[],/metrics/paice.py__indexes
1978,/home/amandapotts/git/nltk/nltk/metrics/paice.py___init__,"def __init__(self, lemmas, stems):
""""""
:param lemmas: A dictionary where keys are lemmas and values are sets
or lists of words corresponding to that lemma.
:param stems: A dictionary where keys are stems and values are sets
or lists of words corresponding to that stem.
:type lemmas: dict(str): list(str)
:type stems: dict(str): set(str)
""""""
self.lemmas = lemmas
self.stems = stems
self.coords = []
self.gumt, self.gdmt, self.gwmt, self.gdnt = (None, None, None, None)
self.ui, self.oi, self.sw = (None, None, None)
self.errt = None
self.update()
",[],0,[],/metrics/paice.py___init__
1979,/home/amandapotts/git/nltk/nltk/metrics/paice.py___str__,"def __str__(self):
text = [""Global Unachieved Merge Total (GUMT): %s\n"" % self.gumt]
text.append(""Global Desired Merge Total (GDMT): %s\n"" % self.gdmt)
text.append(""Global Wrongly-Merged Total (GWMT): %s\n"" % self.gwmt)
text.append(""Global Desired Non-merge Total (GDNT): %s\n"" % self.gdnt)
text.append(""Understemming Index (GUMT / GDMT): %s\n"" % self.ui)
text.append(""Overstemming Index (GWMT / GDNT): %s\n"" % self.oi)
text.append(""Stemming Weight (OI / UI): %s\n"" % self.sw)
text.append(""Error-Rate Relative to Truncation (ERRT): %s\r\n"" % self.errt)
coordinates = "" "".join([""(%s, %s)"" % item for item in self.coords])
text.append(""Truncation line: %s"" % coordinates)
return """".join(text)
",[],0,[],/metrics/paice.py___str__
1980,/home/amandapotts/git/nltk/nltk/metrics/paice.py__get_truncation_indexes,"def _get_truncation_indexes(self, words, cutlength):
""""""Count (UI, OI) when stemming is done by truncating words at \'cutlength\'.
:param words: Words used for the analysis
:param cutlength: Words are stemmed by cutting them at this length
:type words: set(str) or list(str)
:type cutlength: int
:return: Understemming and overstemming indexes
:rtype: tuple(int, int)
""""""
truncated = _truncate(words, cutlength)
gumt, gdmt, gwmt, gdnt = _calculate(self.lemmas, truncated)
ui, oi = _indexes(gumt, gdmt, gwmt, gdnt)[:2]
return (ui, oi)
",[],0,[],/metrics/paice.py__get_truncation_indexes
1981,/home/amandapotts/git/nltk/nltk/metrics/paice.py__get_truncation_coordinates,"def _get_truncation_coordinates(self, cutlength=0):
""""""Count (UI, OI) pairs for truncation points until we find the segment where (ui, oi) crosses the truncation line.
:param cutlength: Optional parameter to start counting from (ui, oi)
coordinates gotten by stemming at this length. Useful for speeding up
the calculations when you know the approximate location of the
intersection.
:type cutlength: int
:return: List of coordinate pairs that define the truncation line
:rtype: list(tuple(float, float))
""""""
words = get_words_from_dictionary(self.lemmas)
maxlength = max(len(word) for word in words)
coords = []
while cutlength <= maxlength:
pair = self._get_truncation_indexes(words, cutlength)
if pair not in coords:
coords.append(pair)
if pair == (0.0, 0.0):
return coords
if len(coords) >= 2 and pair[0] > 0.0:
derivative1 = _get_derivative(coords[-2])
derivative2 = _get_derivative(coords[-1])
if derivative1 >= self.sw >= derivative2:
return coords
cutlength += 1
return coords
",[],0,[],/metrics/paice.py__get_truncation_coordinates
1982,/home/amandapotts/git/nltk/nltk/metrics/paice.py__errt,"def _errt(self):
""""""Count Error-Rate Relative to Truncation (ERRT).
:return: ERRT, length of the line from origo to (UI, OI) divided by
the length of the line from origo to the point defined by the same
line when extended until the truncation line.
:rtype: float
""""""
self.coords = self._get_truncation_coordinates()
if (0.0, 0.0) in self.coords:
if (self.ui, self.oi) != (0.0, 0.0):
return float(""inf"")
else:
return float(""nan"")
if (self.ui, self.oi) == (0.0, 0.0):
return 0.0
intersection = _count_intersection(
((0, 0), (self.ui, self.oi)), self.coords[-2:]
)
op = sqrt(self.ui**2 + self.oi**2)
ot = sqrt(intersection[0] ** 2 + intersection[1] ** 2)
return op / ot
",[],0,[],/metrics/paice.py__errt
1983,/home/amandapotts/git/nltk/nltk/metrics/paice.py_update,"def update(self):
""""""Update statistics after lemmas and stems have been set.""""""
self.gumt, self.gdmt, self.gwmt, self.gdnt = _calculate(self.lemmas, self.stems)
self.ui, self.oi, self.sw = _indexes(self.gumt, self.gdmt, self.gwmt, self.gdnt)
self.errt = self._errt()
",[],0,[],/metrics/paice.py_update
1984,/home/amandapotts/git/nltk/nltk/metrics/paice.py_demo,"def demo():
""""""Demonstration of the module.""""""
lemmas = {
""kneel"": [""kneel"", ""knelt""],
""range"": [""range"", ""ranged""],
""ring"": [""ring"", ""rang"", ""rung""],
}
stems = {
""kneel"": [""kneel""],
""knelt"": [""knelt""],
""rang"": [""rang"", ""range"", ""ranged""],
""ring"": [""ring""],
""rung"": [""rung""],
}
print(""Words grouped by their lemmas:"")
for lemma in sorted(lemmas):
print(""{} => {}"".format(lemma, "" "".join(lemmas[lemma])))
print()
print(""Same words grouped by a stemming algorithm:"")
for stem in sorted(stems):
print(""{} => {}"".format(stem, "" "".join(stems[stem])))
print()
p = Paice(lemmas, stems)
print(p)
print()
stems = {
""kneel"": [""kneel""],
""knelt"": [""knelt""],
""rang"": [""rang""],
""range"": [""range"", ""ranged""],
""ring"": [""ring""],
""rung"": [""rung""],
}
print(""Counting stats after changing stemming results:"")
for stem in sorted(stems):
print(""{} => {}"".format(stem, "" "".join(stems[stem])))
print()
p.stems = stems
p.update()
print(p)
",[],0,[],/metrics/paice.py_demo
1985,/home/amandapotts/git/nltk/nltk/metrics/segmentation.py_windowdiff,"def windowdiff(seg1, seg2, k, boundary=""1"", weighted=False):
""""""
Compute the windowdiff score for a pair of segmentations.  A
segmentation is any sequence over a vocabulary of two items
(e.g. ""0"", ""1""), where the specified boundary value is used to
mark the edge of a segmentation.
>>> s1 = ""000100000010""
>>> s2 = ""000010000100""
>>> s3 = ""100000010000""
>>> '%.2f' % windowdiff(s1, s1, 3)
'0.00'
>>> '%.2f' % windowdiff(s1, s2, 3)
'0.30'
>>> '%.2f' % windowdiff(s2, s3, 3)
'0.80'
:param seg1: a segmentation
:type seg1: str or list
:param seg2: a segmentation
:type seg2: str or list
:param k: window width
:type k: int
:param boundary: boundary value
:type boundary: str or int or bool
:param weighted: use the weighted variant of windowdiff
:type weighted: boolean
:rtype: float
""""""
if len(seg1) != len(seg2):
raise ValueError(""Segmentations have unequal length"")
if k > len(seg1):
raise ValueError(
""Window width k should be smaller or equal than segmentation lengths""
)
wd = 0
for i in range(len(seg1) - k + 1):
ndiff = abs(seg1[i : i + k].count(boundary) - seg2[i : i + k].count(boundary))
if weighted:
wd += ndiff
else:
wd += min(1, ndiff)
return wd / (len(seg1) - k + 1.0)
",[],0,[],/metrics/segmentation.py_windowdiff
1986,/home/amandapotts/git/nltk/nltk/metrics/segmentation.py__init_mat,"def _init_mat(nrows, ncols, ins_cost, del_cost):
mat = np.empty((nrows, ncols))
mat[0, :] = ins_cost * np.arange(ncols)
mat[:, 0] = del_cost * np.arange(nrows)
return mat
","['empty', 'arange', 'arange']",3,"['empty((nrows, ncols))', 'arange(ncols)', 'arange(nrows)']",/metrics/segmentation.py__init_mat
1987,/home/amandapotts/git/nltk/nltk/metrics/segmentation.py__ghd_aux,"def _ghd_aux(mat, rowv, colv, ins_cost, del_cost, shift_cost_coeff):
for i, rowi in enumerate(rowv):
for j, colj in enumerate(colv):
shift_cost = shift_cost_coeff * abs(rowi - colj) + mat[i, j]
if rowi == colj:
tcost = mat[i, j]
elif rowi > colj:
tcost = del_cost + mat[i, j + 1]
else:
tcost = ins_cost + mat[i + 1, j]
mat[i + 1, j + 1] = min(tcost, shift_cost)
",[],0,[],/metrics/segmentation.py__ghd_aux
1988,/home/amandapotts/git/nltk/nltk/metrics/segmentation.py_ghd,"def ghd(ref, hyp, ins_cost=2.0, del_cost=2.0, shift_cost_coeff=1.0, boundary=""1""):
""""""
Compute the Generalized Hamming Distance for a reference and a hypothetical
segmentation, corresponding to the cost related to the transformation
of the hypothetical segmentation into the reference segmentation
through boundary insertion, deletion and shift operations.
A segmentation is any sequence over a vocabulary of two items
(e.g. ""0"", ""1""), where the specified boundary value is used to
mark the edge of a segmentation.
Recommended parameter values are a shift_cost_coeff of 2.
Associated with a ins_cost, and del_cost equal to the mean segment
length in the reference segmentation.
>>> # Same examples as Kulyukin C++ implementation
>>> ghd('1100100000', '1100010000', 1.0, 1.0, 0.5)
0.5
>>> ghd('1100100000', '1100000001', 1.0, 1.0, 0.5)
2.0
>>> ghd('011', '110', 1.0, 1.0, 0.5)
1.0
>>> ghd('1', '0', 1.0, 1.0, 0.5)
1.0
>>> ghd('111', '000', 1.0, 1.0, 0.5)
3.0
>>> ghd('000', '111', 1.0, 2.0, 0.5)
6.0
:param ref: the reference segmentation
:type ref: str or list
:param hyp: the hypothetical segmentation
:type hyp: str or list
:param ins_cost: insertion cost
:type ins_cost: float
:param del_cost: deletion cost
:type del_cost: float
:param shift_cost_coeff: constant used to compute the cost of a shift.
``shift cost = shift_cost_coeff * |i - j|`` where ``i`` and ``j``
are the positions indicating the shift
:type shift_cost_coeff: float
:param boundary: boundary value
:type boundary: str or int or bool
:rtype: float
""""""
ref_idx = [i for (i, val) in enumerate(ref) if val == boundary]
hyp_idx = [i for (i, val) in enumerate(hyp) if val == boundary]
nref_bound = len(ref_idx)
nhyp_bound = len(hyp_idx)
if nref_bound == 0 and nhyp_bound == 0:
return 0.0
elif nref_bound > 0 and nhyp_bound == 0:
return nref_bound * ins_cost
elif nref_bound == 0 and nhyp_bound > 0:
return nhyp_bound * del_cost
mat = _init_mat(nhyp_bound + 1, nref_bound + 1, ins_cost, del_cost)
_ghd_aux(mat, hyp_idx, ref_idx, ins_cost, del_cost, shift_cost_coeff)
return mat[-1, -1]
",[],0,[],/metrics/segmentation.py_ghd
1989,/home/amandapotts/git/nltk/nltk/metrics/segmentation.py_pk,"def pk(ref, hyp, k=None, boundary=""1""):
""""""
Compute the Pk metric for a pair of segmentations A segmentation
is any sequence over a vocabulary of two items (e.g. ""0"", ""1""),
where the specified boundary value is used to mark the edge of a
segmentation.
>>> '%.2f' % pk('0100'*100, '1'*400, 2)
'0.50'
>>> '%.2f' % pk('0100'*100, '0'*400, 2)
'0.50'
>>> '%.2f' % pk('0100'*100, '0100'*100, 2)
'0.00'
:param ref: the reference segmentation
:type ref: str or list
:param hyp: the segmentation to evaluate
:type hyp: str or list
:param k: window size, if None, set to half of the average reference segment length
:type boundary: str or int or bool
:param boundary: boundary value
:type boundary: str or int or bool
:rtype: float
""""""
if k is None:
k = int(round(len(ref) / (ref.count(boundary) * 2.0)))
err = 0
for i in range(len(ref) - k + 1):
r = ref[i : i + k].count(boundary) > 0
h = hyp[i : i + k].count(boundary) > 0
if r != h:
err += 1
return err / (len(ref) - k + 1.0)
",[],0,[],/metrics/segmentation.py_pk
1990,/home/amandapotts/git/nltk/nltk/metrics/agreement.py___init__,"def __init__(self, data=None, distance=binary_distance):
""""""Initialize an annotation task.
The data argument can be None (to create an empty annotation task) or a sequence of 3-tuples,
each representing a coder's labeling of an item:
``(coder,item,label)``
The distance argument is a function taking two arguments (labels) and producing a numerical distance.
The distance from a label to itself should be zero:
``distance(l,l) = 0``
""""""
self.distance = distance
self.I = set()
self.K = set()
self.C = set()
self.data = []
if data is not None:
self.load_array(data)
",[],0,[],/metrics/agreement.py___init__
1991,/home/amandapotts/git/nltk/nltk/metrics/agreement.py_load_array,"def load_array(self, array):
""""""Load an sequence of annotation results, appending to any data already loaded.
The argument is a sequence of 3-tuples, each representing a coder's labeling of an item:
(coder,item,label)
""""""
for coder, item, labels in array:
self.C.add(coder)
self.K.add(labels)
self.I.add(item)
self.data.append({""coder"": coder, ""labels"": labels, ""item"": item})
",[],0,[],/metrics/agreement.py_load_array
1992,/home/amandapotts/git/nltk/nltk/metrics/agreement.py_agr,"def agr(self, cA, cB, i, data=None):
""""""Agreement between two coders on a given item""""""
data = data or self.data
k1 = next(x for x in data if x[""coder""] in (cA, cB) and x[""item""] == i)
if k1[""coder""] == cA:
k2 = next(x for x in data if x[""coder""] == cB and x[""item""] == i)
else:
k2 = next(x for x in data if x[""coder""] == cA and x[""item""] == i)
ret = 1.0 - float(self.distance(k1[""labels""], k2[""labels""]))
log.debug(""Observed agreement between %s and %s on %s: %f"", cA, cB, i, ret)
log.debug(
'Distance between ""%r"" and ""%r"": %f', k1[""labels""], k2[""labels""], 1.0 - ret
)
return ret
",[],0,[],/metrics/agreement.py_agr
1993,/home/amandapotts/git/nltk/nltk/metrics/agreement.py_Nk,"def Nk(self, k):
return float(sum(1 for x in self.data if x[""labels""] == k))
",[],0,[],/metrics/agreement.py_Nk
1994,/home/amandapotts/git/nltk/nltk/metrics/agreement.py_Nik,"def Nik(self, i, k):
return float(sum(1 for x in self.data if x[""item""] == i and x[""labels""] == k))
",[],0,[],/metrics/agreement.py_Nik
1995,/home/amandapotts/git/nltk/nltk/metrics/agreement.py_Nck,"def Nck(self, c, k):
return float(sum(1 for x in self.data if x[""coder""] == c and x[""labels""] == k))
",[],0,[],/metrics/agreement.py_Nck
1996,/home/amandapotts/git/nltk/nltk/metrics/agreement.py_N,"def N(self, k=None, i=None, c=None):
""""""Implements the ""n-notation"" used in Artstein and Poesio (2007)""""""
if k is not None and i is None and c is None:
ret = self.Nk(k)
elif k is not None and i is not None and c is None:
ret = self.Nik(i, k)
elif k is not None and c is not None and i is None:
ret = self.Nck(c, k)
else:
raise ValueError(
f""You must pass either i or c, not both! (k={k!r},i={i!r},c={c!r})""
)
log.debug(""Count on N[%s,%s,%s]: %d"", k, i, c, ret)
return ret
",[],0,[],/metrics/agreement.py_N
1997,/home/amandapotts/git/nltk/nltk/metrics/agreement.py__grouped_data,"def _grouped_data(self, field, data=None):
data = data or self.data
return groupby(sorted(data, key=itemgetter(field)), itemgetter(field))
",[],0,[],/metrics/agreement.py__grouped_data
1998,/home/amandapotts/git/nltk/nltk/metrics/agreement.py_Ao,"def Ao(self, cA, cB):
""""""Observed agreement between two coders on all items.""""""
data = self._grouped_data(
""item"", (x for x in self.data if x[""coder""] in (cA, cB))
)
ret = sum(self.agr(cA, cB, item, item_data) for item, item_data in data) / len(
self.I
)
log.debug(""Observed agreement between %s and %s: %f"", cA, cB, ret)
return ret
",[],0,[],/metrics/agreement.py_Ao
1999,/home/amandapotts/git/nltk/nltk/metrics/agreement.py__pairwise_average,"def _pairwise_average(self, function):
""""""
Calculates the average of function results for each coder pair
""""""
total = 0
n = 0
s = self.C.copy()
for cA in self.C:
s.remove(cA)
for cB in s:
total += function(cA, cB)
n += 1
ret = total / n
return ret
",[],0,[],/metrics/agreement.py__pairwise_average
2000,/home/amandapotts/git/nltk/nltk/metrics/agreement.py_avg_Ao,"def avg_Ao(self):
""""""Average observed agreement across all coders and items.""""""
ret = self._pairwise_average(self.Ao)
log.debug(""Average observed agreement: %f"", ret)
return ret
",[],0,[],/metrics/agreement.py_avg_Ao
2001,/home/amandapotts/git/nltk/nltk/metrics/agreement.py_Do_Kw_pairwise,"def Do_Kw_pairwise(self, cA, cB, max_distance=1.0):
""""""The observed disagreement for the weighted kappa coefficient.""""""
total = 0.0
data = (x for x in self.data if x[""coder""] in (cA, cB))
for i, itemdata in self._grouped_data(""item"", data):
total += self.distance(next(itemdata)[""labels""], next(itemdata)[""labels""])
ret = total / (len(self.I) * max_distance)
log.debug(""Observed disagreement between %s and %s: %f"", cA, cB, ret)
return ret
",[],0,[],/metrics/agreement.py_Do_Kw_pairwise
2002,/home/amandapotts/git/nltk/nltk/metrics/agreement.py_S,"def S(self):
""""""Bennett, Albert and Goldstein 1954""""""
Ae = 1.0 / len(self.K)
ret = (self.avg_Ao() - Ae) / (1.0 - Ae)
return ret
",[],0,[],/metrics/agreement.py_S
2003,/home/amandapotts/git/nltk/nltk/metrics/agreement.py_pi,"def pi(self):
""""""Scott 1955
Equivalent to K from Siegel and Castellan (1988).
""""""
total = 0.0
label_freqs = FreqDist(x[""labels""] for x in self.data)
for k, f in label_freqs.items():
total += f**2
Ae = total / ((len(self.I) * len(self.C)) ** 2)
return (self.avg_Ao() - Ae) / (1 - Ae)
",[],0,[],/metrics/agreement.py_pi
2004,/home/amandapotts/git/nltk/nltk/metrics/agreement.py_Ae_kappa,"def Ae_kappa(self, cA, cB):
Ae = 0.0
nitems = float(len(self.I))
label_freqs = ConditionalFreqDist((x[""labels""], x[""coder""]) for x in self.data)
for k in label_freqs.conditions():
Ae += (label_freqs[k][cA] / nitems) * (label_freqs[k][cB] / nitems)
return Ae
",[],0,[],/metrics/agreement.py_Ae_kappa
2005,/home/amandapotts/git/nltk/nltk/metrics/agreement.py_kappa_pairwise,"def kappa_pairwise(self, cA, cB):
"""""" """"""
Ae = self.Ae_kappa(cA, cB)
ret = (self.Ao(cA, cB) - Ae) / (1.0 - Ae)
log.debug(""Expected agreement between %s and %s: %f"", cA, cB, Ae)
return ret
",[],0,[],/metrics/agreement.py_kappa_pairwise
2006,/home/amandapotts/git/nltk/nltk/metrics/agreement.py_kappa,"def kappa(self):
""""""Cohen 1960
Averages naively over kappas for each coder pair.
""""""
return self._pairwise_average(self.kappa_pairwise)
",[],0,[],/metrics/agreement.py_kappa
2007,/home/amandapotts/git/nltk/nltk/metrics/agreement.py_multi_kappa,"def multi_kappa(self):
""""""Davies and Fleiss 1982
Averages over observed and expected agreements for each coder pair.
""""""
Ae = self._pairwise_average(self.Ae_kappa)
return (self.avg_Ao() - Ae) / (1.0 - Ae)
",[],0,[],/metrics/agreement.py_multi_kappa
2008,/home/amandapotts/git/nltk/nltk/metrics/agreement.py_Disagreement,"def Disagreement(self, label_freqs):
total_labels = sum(label_freqs.values())
pairs = 0.0
for j, nj in label_freqs.items():
for l, nl in label_freqs.items():
pairs += float(nj * nl) * self.distance(l, j)
return 1.0 * pairs / (total_labels * (total_labels - 1))
",[],0,[],/metrics/agreement.py_Disagreement
2009,/home/amandapotts/git/nltk/nltk/metrics/agreement.py_alpha,"def alpha(self):
""""""Krippendorff 1980""""""
if len(self.K) == 0:
raise ValueError(""Cannot calculate alpha, no data present!"")
if len(self.K) == 1:
log.debug(""Only one annotation value, alpha returning 1."")
return 1
if len(self.C) == 1 and len(self.I) == 1:
raise ValueError(""Cannot calculate alpha, only one coder and item present!"")
total_disagreement = 0.0
total_ratings = 0
all_valid_labels_freq = FreqDist([])
total_do = 0.0  # Total observed disagreement for all items.
for i, itemdata in self._grouped_data(""item""):
label_freqs = FreqDist(x[""labels""] for x in itemdata)
labels_count = sum(label_freqs.values())
if labels_count < 2:
continue
all_valid_labels_freq += label_freqs
total_do += self.Disagreement(label_freqs) * labels_count
do = total_do / sum(all_valid_labels_freq.values())
de = self.Disagreement(all_valid_labels_freq)  # Expected disagreement.
k_alpha = 1.0 - do / de
return k_alpha
",[],0,[],/metrics/agreement.py_alpha
2010,/home/amandapotts/git/nltk/nltk/metrics/agreement.py_weighted_kappa_pairwise,"def weighted_kappa_pairwise(self, cA, cB, max_distance=1.0):
""""""Cohen 1968""""""
total = 0.0
label_freqs = ConditionalFreqDist(
(x[""coder""], x[""labels""]) for x in self.data if x[""coder""] in (cA, cB)
)
for j in self.K:
for l in self.K:
total += label_freqs[cA][j] * label_freqs[cB][l] * self.distance(j, l)
De = total / (max_distance * pow(len(self.I), 2))
log.debug(""Expected disagreement between %s and %s: %f"", cA, cB, De)
Do = self.Do_Kw_pairwise(cA, cB)
ret = 1.0 - (Do / De)
return ret
",[],0,[],/metrics/agreement.py_weighted_kappa_pairwise
2011,/home/amandapotts/git/nltk/nltk/metrics/confusionmatrix.py___init__,"def __init__(self, reference, test, sort_by_count=False):
""""""
Construct a new confusion matrix from a list of reference
values and a corresponding list of test values.
:type reference: list
:param reference: An ordered list of reference values.
:type test: list
:param test: A list of values to compare against the
corresponding reference values.
:raise ValueError: If ``reference`` and ``length`` do not have
the same length.
""""""
if len(reference) != len(test):
raise ValueError(""Lists must have the same length."")
if sort_by_count:
ref_fdist = FreqDist(reference)
test_fdist = FreqDist(test)
",[],0,[],/metrics/confusionmatrix.py___init__
2012,/home/amandapotts/git/nltk/nltk/metrics/confusionmatrix.py_key,"def key(v):
return -(ref_fdist[v] + test_fdist[v])
",[],0,[],/metrics/confusionmatrix.py_key
2013,/home/amandapotts/git/nltk/nltk/metrics/confusionmatrix.py___getitem__,"def __getitem__(self, li_lj_tuple):
""""""
:return: The number of times that value ``li`` was expected and
value ``lj`` was given.
:rtype: int
""""""
(li, lj) = li_lj_tuple
i = self._indices[li]
j = self._indices[lj]
return self._confusion[i][j]
",[],0,[],/metrics/confusionmatrix.py___getitem__
2014,/home/amandapotts/git/nltk/nltk/metrics/confusionmatrix.py___repr__,"def __repr__(self):
return f""<ConfusionMatrix: {self._correct}/{self._total} correct>""
",[],0,[],/metrics/confusionmatrix.py___repr__
2015,/home/amandapotts/git/nltk/nltk/metrics/confusionmatrix.py___str__,"def __str__(self):
return self.pretty_format()
",[],0,[],/metrics/confusionmatrix.py___str__
2016,/home/amandapotts/git/nltk/nltk/metrics/confusionmatrix.py_key,"def key(self):
values = self._values
str = ""Value key:\n""
indexlen = len(repr(len(values) - 1))
key_format = ""  %"" + repr(indexlen) + ""d: %s\n""
str += """".join([key_format % (i, values[i]) for i in range(len(values))])
return str
",[],0,[],/metrics/confusionmatrix.py_key
2017,/home/amandapotts/git/nltk/nltk/metrics/confusionmatrix.py_recall,"def recall(self, value):
""""""Given a value in the confusion matrix, return the recall
that corresponds to this value. The recall is defined as:
- *r* = true positive / (true positive + false positive)
and can loosely be considered the ratio of how often ``value``
was predicted correctly relative to how often ``value`` was
the true result.
:param value: value used in the ConfusionMatrix
:return: the recall corresponding to ``value``.
:rtype: float
""""""
TP = self[value, value]
TP_FN = sum(self[value, pred_value] for pred_value in self._values)
if TP_FN == 0:
return 0.0
return TP / TP_FN
",[],0,[],/metrics/confusionmatrix.py_recall
2018,/home/amandapotts/git/nltk/nltk/metrics/confusionmatrix.py_precision,"def precision(self, value):
""""""Given a value in the confusion matrix, return the precision
that corresponds to this value. The precision is defined as:
- *p* = true positive / (true positive + false negative)
and can loosely be considered the ratio of how often ``value``
was predicted correctly relative to the number of predictions
for ``value``.
:param value: value used in the ConfusionMatrix
:return: the precision corresponding to ``value``.
:rtype: float
""""""
TP = self[value, value]
TP_FP = sum(self[real_value, value] for real_value in self._values)
if TP_FP == 0:
return 0.0
return TP / TP_FP
",[],0,[],/metrics/confusionmatrix.py_precision
2019,/home/amandapotts/git/nltk/nltk/metrics/confusionmatrix.py_f_measure,"def f_measure(self, value, alpha=0.5):
""""""
Given a value used in the confusion matrix, return the f-measure
that corresponds to this value. The f-measure is the harmonic mean
of the ``precision`` and ``recall``, weighted by ``alpha``.
In particular, given the precision *p* and recall *r* defined by:
- *p* = true positive / (true positive + false negative)
- *r* = true positive / (true positive + false positive)
The f-measure is:
- *1/(alpha/p + (1-alpha)/r)*
With ``alpha = 0.5``, this reduces to:
- *2pr / (p + r)*
:param value: value used in the ConfusionMatrix
:param alpha: Ratio of the cost of false negative compared to false
positives. Defaults to 0.5, where the costs are equal.
:type alpha: float
:return: the F-measure corresponding to ``value``.
:rtype: float
""""""
p = self.precision(value)
r = self.recall(value)
if p == 0.0 or r == 0.0:
return 0.0
return 1.0 / (alpha / p + (1 - alpha) / r)
",[],0,[],/metrics/confusionmatrix.py_f_measure
2020,/home/amandapotts/git/nltk/nltk/metrics/confusionmatrix.py_demo,"def demo():
reference = ""DET NN VB DET JJ NN NN IN DET NN"".split()
test = ""DET VB VB DET NN NN NN IN DET NN"".split()
print(""Reference ="", reference)
print(""Test    ="", test)
print(""Confusion matrix:"")
print(ConfusionMatrix(reference, test))
print(ConfusionMatrix(reference, test).pretty_format(sort_by_count=True))
print(ConfusionMatrix(reference, test).recall(""VB""))
",[],0,[],/metrics/confusionmatrix.py_demo
2021,/home/amandapotts/git/nltk/nltk/metrics/scores.py_accuracy,"def accuracy(reference, test):
""""""
Given a list of reference values and a corresponding list of test
values, return the fraction of corresponding values that are
equal.  In particular, return the fraction of indices
``0<i<=len(test)`` such that ``test[i] == reference[i]``.
:type reference: list
:param reference: An ordered list of reference values.
:type test: list
:param test: A list of values to compare against the corresponding
reference values.
:raise ValueError: If ``reference`` and ``length`` do not have the
same length.
""""""
if len(reference) != len(test):
raise ValueError(""Lists must have the same length."")
return sum(x == y for x, y in zip(reference, test)) / len(test)
",[],0,[],/metrics/scores.py_accuracy
2022,/home/amandapotts/git/nltk/nltk/metrics/scores.py_precision,"def precision(reference, test):
""""""
Given a set of reference values and a set of test values, return
the fraction of test values that appear in the reference set.
In particular, return card(``reference`` intersection ``test``)/card(``test``).
If ``test`` is empty, then return None.
:type reference: set
:param reference: A set of reference values.
:type test: set
:param test: A set of values to compare against the reference set.
:rtype: float or None
""""""
if not hasattr(reference, ""intersection"") or not hasattr(test, ""intersection""):
raise TypeError(""reference and test should be sets"")
if len(test) == 0:
return None
else:
return len(reference.intersection(test)) / len(test)
",[],0,[],/metrics/scores.py_precision
2023,/home/amandapotts/git/nltk/nltk/metrics/scores.py_recall,"def recall(reference, test):
""""""
Given a set of reference values and a set of test values, return
the fraction of reference values that appear in the test set.
In particular, return card(``reference`` intersection ``test``)/card(``reference``).
If ``reference`` is empty, then return None.
:type reference: set
:param reference: A set of reference values.
:type test: set
:param test: A set of values to compare against the reference set.
:rtype: float or None
""""""
if not hasattr(reference, ""intersection"") or not hasattr(test, ""intersection""):
raise TypeError(""reference and test should be sets"")
if len(reference) == 0:
return None
else:
return len(reference.intersection(test)) / len(reference)
",[],0,[],/metrics/scores.py_recall
2024,/home/amandapotts/git/nltk/nltk/metrics/scores.py_f_measure,"def f_measure(reference, test, alpha=0.5):
""""""
Given a set of reference values and a set of test values, return
the f-measure of the test values, when compared against the
reference values.  The f-measure is the harmonic mean of the
``precision`` and ``recall``, weighted by ``alpha``.  In particular,
given the precision *p* and recall *r* defined by:
- *p* = card(``reference`` intersection ``test``)/card(``test``)
- *r* = card(``reference`` intersection ``test``)/card(``reference``)
The f-measure is:
- *1/(alpha/p + (1-alpha)/r)*
If either ``reference`` or ``test`` is empty, then ``f_measure``
returns None.
:type reference: set
:param reference: A set of reference values.
:type test: set
:param test: A set of values to compare against the reference set.
:rtype: float or None
""""""
p = precision(reference, test)
r = recall(reference, test)
if p is None or r is None:
return None
if p == 0 or r == 0:
return 0
return 1.0 / (alpha / p + (1 - alpha) / r)
",[],0,[],/metrics/scores.py_f_measure
2025,/home/amandapotts/git/nltk/nltk/metrics/scores.py_log_likelihood,"def log_likelihood(reference, test):
""""""
Given a list of reference values and a corresponding list of test
probability distributions, return the average log likelihood of
the reference values, given the probability distributions.
:param reference: A list of reference values
:type reference: list
:param test: A list of probability distributions over values to
compare against the corresponding reference values.
:type test: list(ProbDistI)
""""""
if len(reference) != len(test):
raise ValueError(""Lists must have the same length."")
total_likelihood = sum(dist.logprob(val) for (val, dist) in zip(reference, test))
return total_likelihood / len(reference)
",[],0,[],/metrics/scores.py_log_likelihood
2026,/home/amandapotts/git/nltk/nltk/metrics/scores.py_demo,"def demo():
print(""-"" * 75)
reference = ""DET NN VB DET JJ NN NN IN DET NN"".split()
test = ""DET VB VB DET NN NN NN IN DET NN"".split()
print(""Reference ="", reference)
print(""Test    ="", test)
print(""Accuracy:"", accuracy(reference, test))
print(""-"" * 75)
reference_set = set(reference)
test_set = set(test)
print(""Reference ="", reference_set)
print(""Test =   "", test_set)
print(""Precision:"", precision(reference_set, test_set))
print(""   Recall:"", recall(reference_set, test_set))
print(""F-Measure:"", f_measure(reference_set, test_set))
print(""-"" * 75)
",[],0,[],/metrics/scores.py_demo
2027,/home/amandapotts/git/nltk/nltk/sem/boxer.py___init__,"def __init__(
self,
boxer_drs_interpreter=None,
elimeq=False,
bin_dir=None,
verbose=False,
resolve=True,
",[],0,[],/sem/boxer.py___init__
2028,/home/amandapotts/git/nltk/nltk/sem/boxer.py_set_bin_dir,"def set_bin_dir(self, bin_dir, verbose=False):
self._candc_bin = self._find_binary(""candc"", bin_dir, verbose)
self._candc_models_path = os.path.normpath(
os.path.join(self._candc_bin[:-5], ""../models"")
)
self._boxer_bin = self._find_binary(""boxer"", bin_dir, verbose)
",[],0,[],/sem/boxer.py_set_bin_dir
2029,/home/amandapotts/git/nltk/nltk/sem/boxer.py_interpret,"def interpret(self, input, discourse_id=None, question=False, verbose=False):
""""""
Use Boxer to give a first order representation.
:param input: str Input sentence to parse
:param occur_index: bool Should predicates be occurrence indexed?
:param discourse_id: str An identifier to be inserted to each occurrence-indexed predicate.
:return: ``drt.DrtExpression``
""""""
discourse_ids = [discourse_id] if discourse_id is not None else None
(d,) = self.interpret_multi_sents([[input]], discourse_ids, question, verbose)
if not d:
raise Exception(f'Unable to interpret: ""{input}""')
return d
",[],0,[],/sem/boxer.py_interpret
2030,/home/amandapotts/git/nltk/nltk/sem/boxer.py_interpret_multi,"def interpret_multi(self, input, discourse_id=None, question=False, verbose=False):
""""""
Use Boxer to give a first order representation.
:param input: list of str Input sentences to parse as a single discourse
:param occur_index: bool Should predicates be occurrence indexed?
:param discourse_id: str An identifier to be inserted to each occurrence-indexed predicate.
:return: ``drt.DrtExpression``
""""""
discourse_ids = [discourse_id] if discourse_id is not None else None
(d,) = self.interpret_multi_sents([input], discourse_ids, question, verbose)
if not d:
raise Exception(f'Unable to interpret: ""{input}""')
return d
",[],0,[],/sem/boxer.py_interpret_multi
2031,/home/amandapotts/git/nltk/nltk/sem/boxer.py_interpret_sents,"def interpret_sents(
self, inputs, discourse_ids=None, question=False, verbose=False
",[],0,[],/sem/boxer.py_interpret_sents
2032,/home/amandapotts/git/nltk/nltk/sem/boxer.py_interpret_multi_sents,"def interpret_multi_sents(
self, inputs, discourse_ids=None, question=False, verbose=False
",[],0,[],/sem/boxer.py_interpret_multi_sents
2033,/home/amandapotts/git/nltk/nltk/sem/boxer.py__call_candc,"def _call_candc(self, inputs, discourse_ids, question, verbose=False):
""""""
Call the ``candc`` binary with the given input.
:param inputs: list of list of str Input discourses to parse
:param discourse_ids: list of str Identifiers to be inserted to each occurrence-indexed predicate.
:param filename: str A filename for the output file
:return: stdout
""""""
args = [
""--models"",
os.path.join(self._candc_models_path, [""boxer"", ""questions""][question]),
""--candc-printer"",
""boxer"",
]
return self._call(
""\n"".join(
sum(
([f""<META>'{id}'""] + d for d, id in zip(inputs, discourse_ids)),
[],
)
),
self._candc_bin,
args,
verbose,
)
",[],0,[],/sem/boxer.py__call_candc
2034,/home/amandapotts/git/nltk/nltk/sem/boxer.py__call_boxer,"def _call_boxer(self, candc_out, verbose=False):
""""""
Call the ``boxer`` binary with the given input.
:param candc_out: str output from C&C parser
:return: stdout
""""""
f = None
try:
fd, temp_filename = tempfile.mkstemp(
prefix=""boxer-"", suffix="".in"", text=True
)
f = os.fdopen(fd, ""w"")
f.write(candc_out.decode(""utf-8""))
finally:
if f:
f.close()
args = [
""--box"",
""false"",
""--semantics"",
""drs"",
""--resolve"",
[""false"", ""true""][self._resolve],
""--elimeq"",
[""false"", ""true""][self._elimeq],
""--format"",
""prolog"",
""--instantiate"",
""true"",
""--input"",
temp_filename,
]
stdout = self._call(None, self._boxer_bin, args, verbose)
os.remove(temp_filename)
return stdout
",[],0,[],/sem/boxer.py__call_boxer
2035,/home/amandapotts/git/nltk/nltk/sem/boxer.py__find_binary,"def _find_binary(self, name, bin_dir, verbose=False):
return find_binary(
name,
path_to_bin=bin_dir,
env_vars=[""CANDC""],
url=""http://svn.ask.it.usyd.edu.au/trac/candc/"",
binary_names=[name, name + "".exe""],
verbose=verbose,
)
",[],0,[],/sem/boxer.py__find_binary
2036,/home/amandapotts/git/nltk/nltk/sem/boxer.py__call,"def _call(self, input_str, binary, args=[], verbose=False):
""""""
Call the binary with the given input.
:param input_str: A string whose contents are used as stdin.
:param binary: The location of the binary to call
:param args: A list of command-line arguments.
:return: stdout
""""""
if verbose:
print(""Calling:"", binary)
print(""Args:"", args)
print(""Input:"", input_str)
print(""Command:"", binary + "" "" + "" "".join(args))
if input_str is None:
cmd = [binary] + args
p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
else:
cmd = 'echo ""{}"" | {} {}'.format(input_str, binary, "" "".join(args))
p = subprocess.Popen(
cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True
)
stdout, stderr = p.communicate()
if verbose:
print(""Return code:"", p.returncode)
if stdout:
print(""stdout:\n"", stdout, ""\n"")
if stderr:
print(""stderr:\n"", stderr, ""\n"")
if p.returncode != 0:
raise Exception(
""ERROR CALLING: {} {}\nReturncode: {}\n{}"".format(
binary, "" "".join(args), p.returncode, stderr
)
)
return stdout
",[],0,[],/sem/boxer.py__call
2037,/home/amandapotts/git/nltk/nltk/sem/boxer.py__parse_to_drs_dict,"def _parse_to_drs_dict(self, boxer_out, use_disc_id):
lines = boxer_out.decode(""utf-8"").split(""\n"")
drs_dict = {}
i = 0
while i < len(lines):
line = lines[i]
if line.startswith(""id(""):
comma_idx = line.index("","")
discourse_id = line[3:comma_idx]
if discourse_id[0] == ""'"" and discourse_id[-1] == ""'"":
discourse_id = discourse_id[1:-1]
drs_id = line[comma_idx + 1 : line.index("")"")]
i += 1
line = lines[i]
assert line.startswith(f""sem({drs_id},"")
if line[-4:] == ""').'"":
line = line[:-4] + "").""
assert line.endswith("").""), f""can't parse line: {line}""
search_start = len(f""sem({drs_id},["")
brace_count = 1
drs_start = -1
for j, c in enumerate(line[search_start:]):
if c == ""["":
brace_count += 1
if c == ""]"":
brace_count -= 1
if brace_count == 0:
drs_start = search_start + j + 1
if line[drs_start : drs_start + 3] == ""','"":
drs_start = drs_start + 3
else:
drs_start = drs_start + 1
break
assert drs_start > -1
drs_input = line[drs_start:-2].strip()
parsed = self._parse_drs(drs_input, discourse_id, use_disc_id)
drs_dict[discourse_id] = self._boxer_drs_interpreter.interpret(parsed)
i += 1
return drs_dict
",[],0,[],/sem/boxer.py__parse_to_drs_dict
2038,/home/amandapotts/git/nltk/nltk/sem/boxer.py__parse_drs,"def _parse_drs(self, drs_string, discourse_id, use_disc_id):
return BoxerOutputDrsParser([None, discourse_id][use_disc_id]).parse(drs_string)
",[],0,[],/sem/boxer.py__parse_drs
2039,/home/amandapotts/git/nltk/nltk/sem/boxer.py___init__,"def __init__(self, discourse_id=None):
""""""
This class is used to parse the Prolog DRS output from Boxer into a
hierarchy of python objects.
""""""
DrtParser.__init__(self)
self.discourse_id = discourse_id
self.sentence_id_offset = None
self.quote_chars = [(""'"", ""'"", ""\\"", False)]
",[],0,[],/sem/boxer.py___init__
2040,/home/amandapotts/git/nltk/nltk/sem/boxer.py_parse,"def parse(self, data, signature=None):
return DrtParser.parse(self, data, signature)
",[],0,[],/sem/boxer.py_parse
2041,/home/amandapotts/git/nltk/nltk/sem/boxer.py_get_all_symbols,"def get_all_symbols(self):
return [""("", "")"", "","", ""["", ""]"", "":""]
",[],0,[],/sem/boxer.py_get_all_symbols
2042,/home/amandapotts/git/nltk/nltk/sem/boxer.py_handle,"def handle(self, tok, context):
return self.handle_drs(tok)
",[],0,[],/sem/boxer.py_handle
2043,/home/amandapotts/git/nltk/nltk/sem/boxer.py_attempt_adjuncts,"def attempt_adjuncts(self, expression, context):
return expression
",[],0,[],/sem/boxer.py_attempt_adjuncts
2044,/home/amandapotts/git/nltk/nltk/sem/boxer.py_parse_condition,"def parse_condition(self, indices):
""""""
Parse a DRS condition
:return: list of ``DrtExpression``
""""""
tok = self.token()
accum = self.handle_condition(tok, indices)
if accum is None:
raise UnexpectedTokenException(tok)
return accum
",[],0,[],/sem/boxer.py_parse_condition
2045,/home/amandapotts/git/nltk/nltk/sem/boxer.py_handle_drs,"def handle_drs(self, tok):
if tok == ""drs"":
return self.parse_drs()
elif tok in [""merge"", ""smerge""]:
return self._handle_binary_expression(self._make_merge_expression)(None, [])
elif tok in [""alfa""]:
return self._handle_alfa(self._make_merge_expression)(None, [])
",[],0,[],/sem/boxer.py_handle_drs
2046,/home/amandapotts/git/nltk/nltk/sem/boxer.py_handle_condition,"def handle_condition(self, tok, indices):
""""""
Handle a DRS condition
:param indices: list of int
:return: list of ``DrtExpression``
""""""
if tok == ""not"":
return [self._handle_not()]
if tok == ""or"":
conds = [self._handle_binary_expression(self._make_or_expression)]
elif tok == ""imp"":
conds = [self._handle_binary_expression(self._make_imp_expression)]
elif tok == ""eq"":
conds = [self._handle_eq()]
elif tok == ""prop"":
conds = [self._handle_prop()]
elif tok == ""pred"":
conds = [self._handle_pred()]
elif tok == ""named"":
conds = [self._handle_named()]
elif tok == ""rel"":
conds = [self._handle_rel()]
elif tok == ""timex"":
conds = self._handle_timex()
elif tok == ""card"":
conds = [self._handle_card()]
elif tok == ""whq"":
conds = [self._handle_whq()]
elif tok == ""duplex"":
conds = [self._handle_duplex()]
else:
conds = []
return sum(
(
[cond(sent_index, word_indices) for cond in conds]
for sent_index, word_indices in self._sent_and_word_indices(indices)
),
[],
)
",[],0,[],/sem/boxer.py_handle_condition
2047,/home/amandapotts/git/nltk/nltk/sem/boxer.py__handle_not,"def _handle_not(self):
self.assertToken(self.token(), ""("")
drs = self.process_next_expression(None)
self.assertToken(self.token(), "")"")
return BoxerNot(drs)
",[],0,[],/sem/boxer.py__handle_not
2048,/home/amandapotts/git/nltk/nltk/sem/boxer.py__handle_pred,"def _handle_pred(self):
self.assertToken(self.token(), ""("")
variable = self.parse_variable()
self.assertToken(self.token(), "","")
name = self.token()
self.assertToken(self.token(), "","")
pos = self.token()
self.assertToken(self.token(), "","")
sense = int(self.token())
self.assertToken(self.token(), "")"")
",[],0,[],/sem/boxer.py__handle_pred
2049,/home/amandapotts/git/nltk/nltk/sem/boxer.py__handle_pred_f,"def _handle_pred_f(sent_index, word_indices):
return BoxerPred(
self.discourse_id, sent_index, word_indices, variable, name, pos, sense
)
",[],0,[],/sem/boxer.py__handle_pred_f
2050,/home/amandapotts/git/nltk/nltk/sem/boxer.py__handle_timex,"def _handle_timex(self):
self.assertToken(self.token(), ""("")
arg = self.parse_variable()
self.assertToken(self.token(), "","")
new_conds = self._handle_time_expression(arg)
self.assertToken(self.token(), "")"")
return new_conds
",[],0,[],/sem/boxer.py__handle_timex
2051,/home/amandapotts/git/nltk/nltk/sem/boxer.py__handle_date,"def _handle_date(self, arg):
conds = []
((sent_index, word_indices),) = self._sent_and_word_indices(
self._parse_index_list()
)
self.assertToken(self.token(), ""("")
pol = self.token()
self.assertToken(self.token(), "")"")
conds.append(
BoxerPred(
self.discourse_id,
sent_index,
word_indices,
arg,
f""date_pol_{pol}"",
""a"",
0,
)
)
self.assertToken(self.token(), "","")
((sent_index, word_indices),) = self._sent_and_word_indices(
self._parse_index_list()
)
year = self.token()
if year != ""XXXX"":
year = year.replace("":"", ""_"")
conds.append(
BoxerPred(
self.discourse_id,
sent_index,
word_indices,
arg,
f""date_year_{year}"",
""a"",
0,
)
)
self.assertToken(self.token(), "","")
((sent_index, word_indices),) = self._sent_and_word_indices(
self._parse_index_list()
)
month = self.token()
if month != ""XX"":
conds.append(
BoxerPred(
self.discourse_id,
sent_index,
word_indices,
arg,
f""date_month_{month}"",
""a"",
0,
)
)
self.assertToken(self.token(), "","")
((sent_index, word_indices),) = self._sent_and_word_indices(
self._parse_index_list()
)
day = self.token()
if day != ""XX"":
conds.append(
BoxerPred(
self.discourse_id,
sent_index,
word_indices,
arg,
f""date_day_{day}"",
""a"",
0,
)
)
return conds
",[],0,[],/sem/boxer.py__handle_date
2052,/home/amandapotts/git/nltk/nltk/sem/boxer.py__handle_time,"def _handle_time(self, arg):
conds = []
self._parse_index_list()
hour = self.token()
if hour != ""XX"":
conds.append(self._make_atom(""r_hour_2"", arg, hour))
self.assertToken(self.token(), "","")
self._parse_index_list()
min = self.token()
if min != ""XX"":
conds.append(self._make_atom(""r_min_2"", arg, min))
self.assertToken(self.token(), "","")
self._parse_index_list()
sec = self.token()
if sec != ""XX"":
conds.append(self._make_atom(""r_sec_2"", arg, sec))
return conds
",[],0,[],/sem/boxer.py__handle_time
2053,/home/amandapotts/git/nltk/nltk/sem/boxer.py__parse_index_list,"def _parse_index_list(self):
indices = []
self.assertToken(self.token(), ""["")
while self.token(0) != ""]"":
indices.append(self.parse_index())
if self.token(0) == "","":
self.token()  # swallow ','
self.token()  # swallow ']'
self.assertToken(self.token(), "":"")
return indices
",[],0,[],/sem/boxer.py__parse_index_list
2054,/home/amandapotts/git/nltk/nltk/sem/boxer.py_parse_drs,"def parse_drs(self):
self.assertToken(self.token(), ""("")
self.assertToken(self.token(), ""["")
refs = set()
while self.token(0) != ""]"":
indices = self._parse_index_list()
refs.add(self.parse_variable())
if self.token(0) == "","":
self.token()  # swallow ','
self.token()  # swallow ']'
self.assertToken(self.token(), "","")
self.assertToken(self.token(), ""["")
conds = []
while self.token(0) != ""]"":
indices = self._parse_index_list()
conds.extend(self.parse_condition(indices))
if self.token(0) == "","":
self.token()  # swallow ','
self.token()  # swallow ']'
self.assertToken(self.token(), "")"")
return BoxerDrs(list(refs), conds)
",[],0,[],/sem/boxer.py_parse_drs
2055,/home/amandapotts/git/nltk/nltk/sem/boxer.py__make_merge_expression,"def _make_merge_expression(self, sent_index, word_indices, drs1, drs2):
return BoxerDrs(drs1.refs + drs2.refs, drs1.conds + drs2.conds)
",[],0,[],/sem/boxer.py__make_merge_expression
2056,/home/amandapotts/git/nltk/nltk/sem/boxer.py__make_or_expression,"def _make_or_expression(self, sent_index, word_indices, drs1, drs2):
return BoxerOr(self.discourse_id, sent_index, word_indices, drs1, drs2)
",[],0,[],/sem/boxer.py__make_or_expression
2057,/home/amandapotts/git/nltk/nltk/sem/boxer.py__make_imp_expression,"def _make_imp_expression(self, sent_index, word_indices, drs1, drs2):
return BoxerDrs(drs1.refs, drs1.conds, drs2)
",[],0,[],/sem/boxer.py__make_imp_expression
2058,/home/amandapotts/git/nltk/nltk/sem/boxer.py_parse_variable,"def parse_variable(self):
var = self.token()
assert re.match(r""^[exps]\d+$"", var), var
return var
",[],0,[],/sem/boxer.py_parse_variable
2059,/home/amandapotts/git/nltk/nltk/sem/boxer.py_parse_index,"def parse_index(self):
return int(self.token())
",[],0,[],/sem/boxer.py_parse_index
2060,/home/amandapotts/git/nltk/nltk/sem/boxer.py__sent_and_word_indices,"def _sent_and_word_indices(self, indices):
""""""
:return: list of (sent_index, word_indices) tuples
""""""
sent_indices = {(i / 1000) - 1 for i in indices if i >= 0}
if sent_indices:
pairs = []
for sent_index in sent_indices:
word_indices = [
(i % 1000) - 1 for i in indices if sent_index == (i / 1000) - 1
]
pairs.append((sent_index, word_indices))
return pairs
else:
word_indices = [(i % 1000) - 1 for i in indices]
return [(None, word_indices)]
",[],0,[],/sem/boxer.py__sent_and_word_indices
2061,/home/amandapotts/git/nltk/nltk/sem/boxer.py___init__,"def __init__(self, discourse_id=None):
DrtParser.__init__(self)
self.discourse_id = discourse_id
",[],0,[],/sem/boxer.py___init__
2062,/home/amandapotts/git/nltk/nltk/sem/boxer.py_get_all_symbols,"def get_all_symbols(self):
return [
DrtTokens.OPEN,
DrtTokens.CLOSE,
DrtTokens.COMMA,
DrtTokens.OPEN_BRACKET,
DrtTokens.CLOSE_BRACKET,
]
",[],0,[],/sem/boxer.py_get_all_symbols
2063,/home/amandapotts/git/nltk/nltk/sem/boxer.py_attempt_adjuncts,"def attempt_adjuncts(self, expression, context):
return expression
",[],0,[],/sem/boxer.py_attempt_adjuncts
2064,/home/amandapotts/git/nltk/nltk/sem/boxer.py_handle,"def handle(self, tok, context):
try:
if tok == ""pred"":
self.assertNextToken(DrtTokens.OPEN)
disc_id = (
self.discourse_id if self.discourse_id is not None else self.token()
)
self.assertNextToken(DrtTokens.COMMA)
sent_id = self.nullableIntToken()
self.assertNextToken(DrtTokens.COMMA)
word_ids = list(map(int, self.handle_refs()))
self.assertNextToken(DrtTokens.COMMA)
variable = int(self.token())
self.assertNextToken(DrtTokens.COMMA)
name = self.token()
self.assertNextToken(DrtTokens.COMMA)
pos = self.token()
self.assertNextToken(DrtTokens.COMMA)
sense = int(self.token())
self.assertNextToken(DrtTokens.CLOSE)
return BoxerPred(disc_id, sent_id, word_ids, variable, name, pos, sense)
elif tok == ""named"":
self.assertNextToken(DrtTokens.OPEN)
disc_id = (
self.discourse_id if self.discourse_id is not None else self.token()
)
self.assertNextToken(DrtTokens.COMMA)
sent_id = int(self.token())
self.assertNextToken(DrtTokens.COMMA)
word_ids = map(int, self.handle_refs())
self.assertNextToken(DrtTokens.COMMA)
variable = int(self.token())
self.assertNextToken(DrtTokens.COMMA)
name = self.token()
self.assertNextToken(DrtTokens.COMMA)
type = self.token()
self.assertNextToken(DrtTokens.COMMA)
sense = int(self.token())
self.assertNextToken(DrtTokens.CLOSE)
return BoxerNamed(
disc_id, sent_id, word_ids, variable, name, type, sense
)
elif tok == ""rel"":
self.assertNextToken(DrtTokens.OPEN)
disc_id = (
self.discourse_id if self.discourse_id is not None else self.token()
)
self.assertNextToken(DrtTokens.COMMA)
sent_id = self.nullableIntToken()
self.assertNextToken(DrtTokens.COMMA)
word_ids = list(map(int, self.handle_refs()))
self.assertNextToken(DrtTokens.COMMA)
var1 = int(self.token())
self.assertNextToken(DrtTokens.COMMA)
var2 = int(self.token())
self.assertNextToken(DrtTokens.COMMA)
rel = self.token()
self.assertNextToken(DrtTokens.COMMA)
sense = int(self.token())
self.assertNextToken(DrtTokens.CLOSE)
return BoxerRel(disc_id, sent_id, word_ids, var1, var2, rel, sense)
elif tok == ""prop"":
self.assertNextToken(DrtTokens.OPEN)
disc_id = (
self.discourse_id if self.discourse_id is not None else self.token()
)
self.assertNextToken(DrtTokens.COMMA)
sent_id = int(self.token())
self.assertNextToken(DrtTokens.COMMA)
word_ids = list(map(int, self.handle_refs()))
self.assertNextToken(DrtTokens.COMMA)
variable = int(self.token())
self.assertNextToken(DrtTokens.COMMA)
drs = self.process_next_expression(None)
self.assertNextToken(DrtTokens.CLOSE)
return BoxerProp(disc_id, sent_id, word_ids, variable, drs)
elif tok == ""not"":
self.assertNextToken(DrtTokens.OPEN)
drs = self.process_next_expression(None)
self.assertNextToken(DrtTokens.CLOSE)
return BoxerNot(drs)
elif tok == ""imp"":
self.assertNextToken(DrtTokens.OPEN)
drs1 = self.process_next_expression(None)
self.assertNextToken(DrtTokens.COMMA)
drs2 = self.process_next_expression(None)
self.assertNextToken(DrtTokens.CLOSE)
return BoxerDrs(drs1.refs, drs1.conds, drs2)
elif tok == ""or"":
self.assertNextToken(DrtTokens.OPEN)
disc_id = (
self.discourse_id if self.discourse_id is not None else self.token()
)
self.assertNextToken(DrtTokens.COMMA)
sent_id = self.nullableIntToken()
self.assertNextToken(DrtTokens.COMMA)
word_ids = map(int, self.handle_refs())
self.assertNextToken(DrtTokens.COMMA)
drs1 = self.process_next_expression(None)
self.assertNextToken(DrtTokens.COMMA)
drs2 = self.process_next_expression(None)
self.assertNextToken(DrtTokens.CLOSE)
return BoxerOr(disc_id, sent_id, word_ids, drs1, drs2)
elif tok == ""eq"":
self.assertNextToken(DrtTokens.OPEN)
disc_id = (
self.discourse_id if self.discourse_id is not None else self.token()
)
self.assertNextToken(DrtTokens.COMMA)
sent_id = self.nullableIntToken()
self.assertNextToken(DrtTokens.COMMA)
word_ids = list(map(int, self.handle_refs()))
self.assertNextToken(DrtTokens.COMMA)
var1 = int(self.token())
self.assertNextToken(DrtTokens.COMMA)
var2 = int(self.token())
self.assertNextToken(DrtTokens.CLOSE)
return BoxerEq(disc_id, sent_id, word_ids, var1, var2)
elif tok == ""card"":
self.assertNextToken(DrtTokens.OPEN)
disc_id = (
self.discourse_id if self.discourse_id is not None else self.token()
)
self.assertNextToken(DrtTokens.COMMA)
sent_id = self.nullableIntToken()
self.assertNextToken(DrtTokens.COMMA)
word_ids = map(int, self.handle_refs())
self.assertNextToken(DrtTokens.COMMA)
var = int(self.token())
self.assertNextToken(DrtTokens.COMMA)
value = self.token()
self.assertNextToken(DrtTokens.COMMA)
type = self.token()
self.assertNextToken(DrtTokens.CLOSE)
return BoxerCard(disc_id, sent_id, word_ids, var, value, type)
elif tok == ""whq"":
self.assertNextToken(DrtTokens.OPEN)
disc_id = (
self.discourse_id if self.discourse_id is not None else self.token()
)
self.assertNextToken(DrtTokens.COMMA)
sent_id = self.nullableIntToken()
self.assertNextToken(DrtTokens.COMMA)
word_ids = list(map(int, self.handle_refs()))
self.assertNextToken(DrtTokens.COMMA)
ans_types = self.handle_refs()
self.assertNextToken(DrtTokens.COMMA)
drs1 = self.process_next_expression(None)
self.assertNextToken(DrtTokens.COMMA)
var = int(self.token())
self.assertNextToken(DrtTokens.COMMA)
drs2 = self.process_next_expression(None)
self.assertNextToken(DrtTokens.CLOSE)
return BoxerWhq(disc_id, sent_id, word_ids, ans_types, drs1, var, drs2)
except Exception as e:
raise LogicalExpressionException(self._currentIndex, str(e)) from e
assert False, repr(tok)
",[],0,[],/sem/boxer.py_handle
2065,/home/amandapotts/git/nltk/nltk/sem/boxer.py_nullableIntToken,"def nullableIntToken(self):
t = self.token()
return int(t) if t != ""None"" else None
",[],0,[],/sem/boxer.py_nullableIntToken
2066,/home/amandapotts/git/nltk/nltk/sem/boxer.py_get_next_token_variable,"def get_next_token_variable(self, description):
try:
return self.token()
except ExpectedMoreTokensException as e:
raise ExpectedMoreTokensException(e.index, ""Variable expected."") from e
",[],0,[],/sem/boxer.py_get_next_token_variable
2067,/home/amandapotts/git/nltk/nltk/sem/boxer.py_variables,"def variables(self):
""""""
:return: (set<variables>, set<events>, set<propositions>)
""""""
variables, events, propositions = self._variables()
return (variables - (events | propositions), events, propositions - events)
",[],0,[],/sem/boxer.py_variables
2068,/home/amandapotts/git/nltk/nltk/sem/boxer.py_variable_types,"def variable_types(self):
vartypes = {}
for t, vars in zip((""z"", ""e"", ""p""), self.variables()):
for v in vars:
vartypes[v] = t
return vartypes
",[],0,[],/sem/boxer.py_variable_types
2069,/home/amandapotts/git/nltk/nltk/sem/boxer.py__variables,"def _variables(self):
""""""
:return: (set<variables>, set<events>, set<propositions>)
""""""
return (set(), set(), set())
",[],0,[],/sem/boxer.py__variables
2070,/home/amandapotts/git/nltk/nltk/sem/boxer.py_atoms,"def atoms(self):
return set()
",[],0,[],/sem/boxer.py_atoms
2071,/home/amandapotts/git/nltk/nltk/sem/boxer.py_clean,"def clean(self):
return self
",[],0,[],/sem/boxer.py_clean
2072,/home/amandapotts/git/nltk/nltk/sem/boxer.py__clean_name,"def _clean_name(self, name):
return name.replace(""-"", ""_"").replace(""'"", ""_"")
",[],0,[],/sem/boxer.py__clean_name
2073,/home/amandapotts/git/nltk/nltk/sem/boxer.py_renumber_sentences,"def renumber_sentences(self, f):
return self
",[],0,[],/sem/boxer.py_renumber_sentences
2074,/home/amandapotts/git/nltk/nltk/sem/boxer.py___hash__,"def __hash__(self):
return hash(f""{self}"")
",[],0,[],/sem/boxer.py___hash__
2075,/home/amandapotts/git/nltk/nltk/sem/boxer.py___init__,"def __init__(self, refs, conds, consequent=None):
AbstractBoxerDrs.__init__(self)
self.refs = refs
self.conds = conds
self.consequent = consequent
",[],0,[],/sem/boxer.py___init__
2076,/home/amandapotts/git/nltk/nltk/sem/boxer.py__variables,"def _variables(self):
variables = (set(), set(), set())
for cond in self.conds:
for s, v in zip(variables, cond._variables()):
s.update(v)
if self.consequent is not None:
for s, v in zip(variables, self.consequent._variables()):
s.update(v)
return variables
",[],0,[],/sem/boxer.py__variables
2077,/home/amandapotts/git/nltk/nltk/sem/boxer.py_atoms,"def atoms(self):
atoms = reduce(operator.or_, (cond.atoms() for cond in self.conds), set())
if self.consequent is not None:
atoms.update(self.consequent.atoms())
return atoms
",[],0,[],/sem/boxer.py_atoms
2078,/home/amandapotts/git/nltk/nltk/sem/boxer.py_clean,"def clean(self):
consequent = self.consequent.clean() if self.consequent else None
return BoxerDrs(self.refs, [c.clean() for c in self.conds], consequent)
",[],0,[],/sem/boxer.py_clean
2079,/home/amandapotts/git/nltk/nltk/sem/boxer.py_renumber_sentences,"def renumber_sentences(self, f):
consequent = self.consequent.renumber_sentences(f) if self.consequent else None
return BoxerDrs(
self.refs, [c.renumber_sentences(f) for c in self.conds], consequent
)
",[],0,[],/sem/boxer.py_renumber_sentences
2080,/home/amandapotts/git/nltk/nltk/sem/boxer.py___repr__,"def __repr__(self):
s = ""drs([{}], [{}])"".format(
"", "".join(""%s"" % r for r in self.refs),
"", "".join(""%s"" % c for c in self.conds),
)
if self.consequent is not None:
s = f""imp({s}, {self.consequent})""
return s
",[],0,[],/sem/boxer.py___repr__
2081,/home/amandapotts/git/nltk/nltk/sem/boxer.py___eq__,"def __eq__(self, other):
return (
self.__class__ == other.__class__
and self.refs == other.refs
and len(self.conds) == len(other.conds)
and reduce(
operator.and_, (c1 == c2 for c1, c2 in zip(self.conds, other.conds))
)
and self.consequent == other.consequent
)
",[],0,[],/sem/boxer.py___eq__
2082,/home/amandapotts/git/nltk/nltk/sem/boxer.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/sem/boxer.py___ne__
2083,/home/amandapotts/git/nltk/nltk/sem/boxer.py___init__,"def __init__(self, drs):
AbstractBoxerDrs.__init__(self)
self.drs = drs
",[],0,[],/sem/boxer.py___init__
2084,/home/amandapotts/git/nltk/nltk/sem/boxer.py__variables,"def _variables(self):
return self.drs._variables()
",[],0,[],/sem/boxer.py__variables
2085,/home/amandapotts/git/nltk/nltk/sem/boxer.py_atoms,"def atoms(self):
return self.drs.atoms()
",[],0,[],/sem/boxer.py_atoms
2086,/home/amandapotts/git/nltk/nltk/sem/boxer.py_clean,"def clean(self):
return BoxerNot(self.drs.clean())
",[],0,[],/sem/boxer.py_clean
2087,/home/amandapotts/git/nltk/nltk/sem/boxer.py_renumber_sentences,"def renumber_sentences(self, f):
return BoxerNot(self.drs.renumber_sentences(f))
",[],0,[],/sem/boxer.py_renumber_sentences
2088,/home/amandapotts/git/nltk/nltk/sem/boxer.py___repr__,"def __repr__(self):
return ""not(%s)"" % (self.drs)
",[],0,[],/sem/boxer.py___repr__
2089,/home/amandapotts/git/nltk/nltk/sem/boxer.py___eq__,"def __eq__(self, other):
return self.__class__ == other.__class__ and self.drs == other.drs
",[],0,[],/sem/boxer.py___eq__
2090,/home/amandapotts/git/nltk/nltk/sem/boxer.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/sem/boxer.py___ne__
2091,/home/amandapotts/git/nltk/nltk/sem/boxer.py___init__,"def __init__(self, discourse_id, sent_index, word_indices):
AbstractBoxerDrs.__init__(self)
self.discourse_id = discourse_id
self.sent_index = sent_index
self.word_indices = word_indices
",[],0,[],/sem/boxer.py___init__
2092,/home/amandapotts/git/nltk/nltk/sem/boxer.py_atoms,"def atoms(self):
return {self}
",[],0,[],/sem/boxer.py_atoms
2093,/home/amandapotts/git/nltk/nltk/sem/boxer.py___eq__,"def __eq__(self, other):
return (
self.__class__ == other.__class__
and self.discourse_id == other.discourse_id
and self.sent_index == other.sent_index
and self.word_indices == other.word_indices
and reduce(operator.and_, (s == o for s, o in zip(self, other)))
)
",[],0,[],/sem/boxer.py___eq__
2094,/home/amandapotts/git/nltk/nltk/sem/boxer.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/sem/boxer.py___ne__
2095,/home/amandapotts/git/nltk/nltk/sem/boxer.py___repr__,"def __repr__(self):
s = ""{}({}, {}, [{}]"".format(
self._pred(),
self.discourse_id,
self.sent_index,
"", "".join(""%s"" % wi for wi in self.word_indices),
)
for v in self:
s += "", %s"" % v
return s + "")""
",[],0,[],/sem/boxer.py___repr__
2096,/home/amandapotts/git/nltk/nltk/sem/boxer.py___init__,"def __init__(self, discourse_id, sent_index, word_indices, var, name, pos, sense):
BoxerIndexed.__init__(self, discourse_id, sent_index, word_indices)
self.var = var
self.name = name
self.pos = pos
self.sense = sense
",[],0,[],/sem/boxer.py___init__
2097,/home/amandapotts/git/nltk/nltk/sem/boxer.py__variables,"def _variables(self):
return ({self.var}, set(), set())
",[],0,[],/sem/boxer.py__variables
2098,/home/amandapotts/git/nltk/nltk/sem/boxer.py_change_var,"def change_var(self, var):
return BoxerPred(
self.discourse_id,
self.sent_index,
self.word_indices,
var,
self.name,
self.pos,
self.sense,
)
",[],0,[],/sem/boxer.py_change_var
2099,/home/amandapotts/git/nltk/nltk/sem/boxer.py_clean,"def clean(self):
return BoxerPred(
self.discourse_id,
self.sent_index,
self.word_indices,
self.var,
self._clean_name(self.name),
self.pos,
self.sense,
)
",[],0,[],/sem/boxer.py_clean
2100,/home/amandapotts/git/nltk/nltk/sem/boxer.py_renumber_sentences,"def renumber_sentences(self, f):
new_sent_index = f(self.sent_index)
return BoxerPred(
self.discourse_id,
new_sent_index,
self.word_indices,
self.var,
self.name,
self.pos,
self.sense,
)
",[],0,[],/sem/boxer.py_renumber_sentences
2101,/home/amandapotts/git/nltk/nltk/sem/boxer.py___iter__,"def __iter__(self):
return iter((self.var, self.name, self.pos, self.sense))
",[],0,[],/sem/boxer.py___iter__
2102,/home/amandapotts/git/nltk/nltk/sem/boxer.py__pred,"def _pred(self):
return ""pred""
",[],0,[],/sem/boxer.py__pred
2103,/home/amandapotts/git/nltk/nltk/sem/boxer.py___init__,"def __init__(self, discourse_id, sent_index, word_indices, var, name, type, sense):
BoxerIndexed.__init__(self, discourse_id, sent_index, word_indices)
self.var = var
self.name = name
self.type = type
self.sense = sense
",[],0,[],/sem/boxer.py___init__
2104,/home/amandapotts/git/nltk/nltk/sem/boxer.py__variables,"def _variables(self):
return ({self.var}, set(), set())
",[],0,[],/sem/boxer.py__variables
2105,/home/amandapotts/git/nltk/nltk/sem/boxer.py_change_var,"def change_var(self, var):
return BoxerNamed(
self.discourse_id,
self.sent_index,
self.word_indices,
var,
self.name,
self.type,
self.sense,
)
",[],0,[],/sem/boxer.py_change_var
2106,/home/amandapotts/git/nltk/nltk/sem/boxer.py_clean,"def clean(self):
return BoxerNamed(
self.discourse_id,
self.sent_index,
self.word_indices,
self.var,
self._clean_name(self.name),
self.type,
self.sense,
)
",[],0,[],/sem/boxer.py_clean
2107,/home/amandapotts/git/nltk/nltk/sem/boxer.py_renumber_sentences,"def renumber_sentences(self, f):
return BoxerNamed(
self.discourse_id,
f(self.sent_index),
self.word_indices,
self.var,
self.name,
self.type,
self.sense,
)
",[],0,[],/sem/boxer.py_renumber_sentences
2108,/home/amandapotts/git/nltk/nltk/sem/boxer.py___iter__,"def __iter__(self):
return iter((self.var, self.name, self.type, self.sense))
",[],0,[],/sem/boxer.py___iter__
2109,/home/amandapotts/git/nltk/nltk/sem/boxer.py__pred,"def _pred(self):
return ""named""
",[],0,[],/sem/boxer.py__pred
2110,/home/amandapotts/git/nltk/nltk/sem/boxer.py___init__,"def __init__(self, discourse_id, sent_index, word_indices, var1, var2, rel, sense):
BoxerIndexed.__init__(self, discourse_id, sent_index, word_indices)
self.var1 = var1
self.var2 = var2
self.rel = rel
self.sense = sense
",[],0,[],/sem/boxer.py___init__
2111,/home/amandapotts/git/nltk/nltk/sem/boxer.py__variables,"def _variables(self):
return ({self.var1, self.var2}, set(), set())
",[],0,[],/sem/boxer.py__variables
2112,/home/amandapotts/git/nltk/nltk/sem/boxer.py_clean,"def clean(self):
return BoxerRel(
self.discourse_id,
self.sent_index,
self.word_indices,
self.var1,
self.var2,
self._clean_name(self.rel),
self.sense,
)
",[],0,[],/sem/boxer.py_clean
2113,/home/amandapotts/git/nltk/nltk/sem/boxer.py_renumber_sentences,"def renumber_sentences(self, f):
return BoxerRel(
self.discourse_id,
f(self.sent_index),
self.word_indices,
self.var1,
self.var2,
self.rel,
self.sense,
)
",[],0,[],/sem/boxer.py_renumber_sentences
2114,/home/amandapotts/git/nltk/nltk/sem/boxer.py___iter__,"def __iter__(self):
return iter((self.var1, self.var2, self.rel, self.sense))
",[],0,[],/sem/boxer.py___iter__
2115,/home/amandapotts/git/nltk/nltk/sem/boxer.py__pred,"def _pred(self):
return ""rel""
",[],0,[],/sem/boxer.py__pred
2116,/home/amandapotts/git/nltk/nltk/sem/boxer.py___init__,"def __init__(self, discourse_id, sent_index, word_indices, var, drs):
BoxerIndexed.__init__(self, discourse_id, sent_index, word_indices)
self.var = var
self.drs = drs
",[],0,[],/sem/boxer.py___init__
2117,/home/amandapotts/git/nltk/nltk/sem/boxer.py__variables,"def _variables(self):
return tuple(
map(operator.or_, (set(), set(), {self.var}), self.drs._variables())
)
",[],0,[],/sem/boxer.py__variables
2118,/home/amandapotts/git/nltk/nltk/sem/boxer.py_referenced_labels,"def referenced_labels(self):
return {self.drs}
",[],0,[],/sem/boxer.py_referenced_labels
2119,/home/amandapotts/git/nltk/nltk/sem/boxer.py_atoms,"def atoms(self):
return self.drs.atoms()
",[],0,[],/sem/boxer.py_atoms
2120,/home/amandapotts/git/nltk/nltk/sem/boxer.py_clean,"def clean(self):
return BoxerProp(
self.discourse_id,
self.sent_index,
self.word_indices,
self.var,
self.drs.clean(),
)
",[],0,[],/sem/boxer.py_clean
2121,/home/amandapotts/git/nltk/nltk/sem/boxer.py_renumber_sentences,"def renumber_sentences(self, f):
return BoxerProp(
self.discourse_id,
f(self.sent_index),
self.word_indices,
self.var,
self.drs.renumber_sentences(f),
)
",[],0,[],/sem/boxer.py_renumber_sentences
2122,/home/amandapotts/git/nltk/nltk/sem/boxer.py___iter__,"def __iter__(self):
return iter((self.var, self.drs))
",[],0,[],/sem/boxer.py___iter__
2123,/home/amandapotts/git/nltk/nltk/sem/boxer.py__pred,"def _pred(self):
return ""prop""
",[],0,[],/sem/boxer.py__pred
2124,/home/amandapotts/git/nltk/nltk/sem/boxer.py___init__,"def __init__(self, discourse_id, sent_index, word_indices, var1, var2):
BoxerIndexed.__init__(self, discourse_id, sent_index, word_indices)
self.var1 = var1
self.var2 = var2
",[],0,[],/sem/boxer.py___init__
2125,/home/amandapotts/git/nltk/nltk/sem/boxer.py__variables,"def _variables(self):
return ({self.var1, self.var2}, set(), set())
",[],0,[],/sem/boxer.py__variables
2126,/home/amandapotts/git/nltk/nltk/sem/boxer.py_atoms,"def atoms(self):
return set()
",[],0,[],/sem/boxer.py_atoms
2127,/home/amandapotts/git/nltk/nltk/sem/boxer.py_renumber_sentences,"def renumber_sentences(self, f):
return BoxerEq(
self.discourse_id,
f(self.sent_index),
self.word_indices,
self.var1,
self.var2,
)
",[],0,[],/sem/boxer.py_renumber_sentences
2128,/home/amandapotts/git/nltk/nltk/sem/boxer.py___iter__,"def __iter__(self):
return iter((self.var1, self.var2))
",[],0,[],/sem/boxer.py___iter__
2129,/home/amandapotts/git/nltk/nltk/sem/boxer.py__pred,"def _pred(self):
return ""eq""
",[],0,[],/sem/boxer.py__pred
2130,/home/amandapotts/git/nltk/nltk/sem/boxer.py___init__,"def __init__(self, discourse_id, sent_index, word_indices, var, value, type):
BoxerIndexed.__init__(self, discourse_id, sent_index, word_indices)
self.var = var
self.value = value
self.type = type
",[],0,[],/sem/boxer.py___init__
2131,/home/amandapotts/git/nltk/nltk/sem/boxer.py__variables,"def _variables(self):
return ({self.var}, set(), set())
",[],0,[],/sem/boxer.py__variables
2132,/home/amandapotts/git/nltk/nltk/sem/boxer.py_renumber_sentences,"def renumber_sentences(self, f):
return BoxerCard(
self.discourse_id,
f(self.sent_index),
self.word_indices,
self.var,
self.value,
self.type,
)
",[],0,[],/sem/boxer.py_renumber_sentences
2133,/home/amandapotts/git/nltk/nltk/sem/boxer.py___iter__,"def __iter__(self):
return iter((self.var, self.value, self.type))
",[],0,[],/sem/boxer.py___iter__
2134,/home/amandapotts/git/nltk/nltk/sem/boxer.py__pred,"def _pred(self):
return ""card""
",[],0,[],/sem/boxer.py__pred
2135,/home/amandapotts/git/nltk/nltk/sem/boxer.py___init__,"def __init__(self, discourse_id, sent_index, word_indices, drs1, drs2):
BoxerIndexed.__init__(self, discourse_id, sent_index, word_indices)
self.drs1 = drs1
self.drs2 = drs2
",[],0,[],/sem/boxer.py___init__
2136,/home/amandapotts/git/nltk/nltk/sem/boxer.py__variables,"def _variables(self):
return tuple(map(operator.or_, self.drs1._variables(), self.drs2._variables()))
",[],0,[],/sem/boxer.py__variables
2137,/home/amandapotts/git/nltk/nltk/sem/boxer.py_atoms,"def atoms(self):
return self.drs1.atoms() | self.drs2.atoms()
",[],0,[],/sem/boxer.py_atoms
2138,/home/amandapotts/git/nltk/nltk/sem/boxer.py_clean,"def clean(self):
return BoxerOr(
self.discourse_id,
self.sent_index,
self.word_indices,
self.drs1.clean(),
self.drs2.clean(),
)
",[],0,[],/sem/boxer.py_clean
2139,/home/amandapotts/git/nltk/nltk/sem/boxer.py_renumber_sentences,"def renumber_sentences(self, f):
return BoxerOr(
self.discourse_id,
f(self.sent_index),
self.word_indices,
self.drs1,
self.drs2,
)
",[],0,[],/sem/boxer.py_renumber_sentences
2140,/home/amandapotts/git/nltk/nltk/sem/boxer.py___iter__,"def __iter__(self):
return iter((self.drs1, self.drs2))
",[],0,[],/sem/boxer.py___iter__
2141,/home/amandapotts/git/nltk/nltk/sem/boxer.py__pred,"def _pred(self):
return ""or""
",[],0,[],/sem/boxer.py__pred
2142,/home/amandapotts/git/nltk/nltk/sem/boxer.py___init__,"def __init__(
self, discourse_id, sent_index, word_indices, ans_types, drs1, variable, drs2
",[],0,[],/sem/boxer.py___init__
2143,/home/amandapotts/git/nltk/nltk/sem/boxer.py__variables,"def _variables(self):
return tuple(
map(
operator.or_,
({self.variable}, set(), set()),
self.drs1._variables(),
self.drs2._variables(),
)
)
",[],0,[],/sem/boxer.py__variables
2144,/home/amandapotts/git/nltk/nltk/sem/boxer.py_atoms,"def atoms(self):
return self.drs1.atoms() | self.drs2.atoms()
",[],0,[],/sem/boxer.py_atoms
2145,/home/amandapotts/git/nltk/nltk/sem/boxer.py_clean,"def clean(self):
return BoxerWhq(
self.discourse_id,
self.sent_index,
self.word_indices,
self.ans_types,
self.drs1.clean(),
self.variable,
self.drs2.clean(),
)
",[],0,[],/sem/boxer.py_clean
2146,/home/amandapotts/git/nltk/nltk/sem/boxer.py_renumber_sentences,"def renumber_sentences(self, f):
return BoxerWhq(
self.discourse_id,
f(self.sent_index),
self.word_indices,
self.ans_types,
self.drs1,
self.variable,
self.drs2,
)
",[],0,[],/sem/boxer.py_renumber_sentences
2147,/home/amandapotts/git/nltk/nltk/sem/boxer.py___iter__,"def __iter__(self):
return iter(
(""["" + "","".join(self.ans_types) + ""]"", self.drs1, self.variable, self.drs2)
)
",[],0,[],/sem/boxer.py___iter__
2148,/home/amandapotts/git/nltk/nltk/sem/boxer.py__pred,"def _pred(self):
return ""whq""
",[],0,[],/sem/boxer.py__pred
2149,/home/amandapotts/git/nltk/nltk/sem/boxer.py_interpret,"def interpret(self, ex):
return ex
",[],0,[],/sem/boxer.py_interpret
2150,/home/amandapotts/git/nltk/nltk/sem/boxer.py___init__,"def __init__(self, occur_index=False):
self._occur_index = occur_index
",[],0,[],/sem/boxer.py___init__
2151,/home/amandapotts/git/nltk/nltk/sem/boxer.py_interpret,"def interpret(self, ex):
""""""
:param ex: ``AbstractBoxerDrs``
:return: ``DrtExpression``
""""""
if isinstance(ex, BoxerDrs):
drs = DRS(
[Variable(r) for r in ex.refs], list(map(self.interpret, ex.conds))
)
if ex.consequent is not None:
drs.consequent = self.interpret(ex.consequent)
return drs
elif isinstance(ex, BoxerNot):
return DrtNegatedExpression(self.interpret(ex.drs))
elif isinstance(ex, BoxerPred):
pred = self._add_occur_indexing(f""{ex.pos}_{ex.name}"", ex)
return self._make_atom(pred, ex.var)
elif isinstance(ex, BoxerNamed):
pred = self._add_occur_indexing(f""ne_{ex.type}_{ex.name}"", ex)
return self._make_atom(pred, ex.var)
elif isinstance(ex, BoxerRel):
pred = self._add_occur_indexing(""%s"" % (ex.rel), ex)
return self._make_atom(pred, ex.var1, ex.var2)
elif isinstance(ex, BoxerProp):
return DrtProposition(Variable(ex.var), self.interpret(ex.drs))
elif isinstance(ex, BoxerEq):
return DrtEqualityExpression(
DrtVariableExpression(Variable(ex.var1)),
DrtVariableExpression(Variable(ex.var2)),
)
elif isinstance(ex, BoxerCard):
pred = self._add_occur_indexing(f""card_{ex.type}_{ex.value}"", ex)
return self._make_atom(pred, ex.var)
elif isinstance(ex, BoxerOr):
return DrtOrExpression(self.interpret(ex.drs1), self.interpret(ex.drs2))
elif isinstance(ex, BoxerWhq):
drs1 = self.interpret(ex.drs1)
drs2 = self.interpret(ex.drs2)
return DRS(drs1.refs + drs2.refs, drs1.conds + drs2.conds)
assert False, f""{ex.__class__.__name__}: {ex}""
",[],0,[],/sem/boxer.py_interpret
2152,/home/amandapotts/git/nltk/nltk/sem/boxer.py__make_atom,"def _make_atom(self, pred, *args):
accum = DrtVariableExpression(Variable(pred))
for arg in args:
accum = DrtApplicationExpression(
accum, DrtVariableExpression(Variable(arg))
)
return accum
",[],0,[],/sem/boxer.py__make_atom
2153,/home/amandapotts/git/nltk/nltk/sem/boxer.py__add_occur_indexing,"def _add_occur_indexing(self, base, ex):
if self._occur_index and ex.sent_index is not None:
if ex.discourse_id:
base += ""_%s"" % ex.discourse_id
base += ""_s%s"" % ex.sent_index
base += ""_w%s"" % sorted(ex.word_indices)[0]
return base
",[],0,[],/sem/boxer.py__add_occur_indexing
2154,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py___init__,"def __init__(self):
LogicParser.__init__(self)
self.operator_precedence = {APP: 1, Tokens.IMP: 2, None: 3}
self.right_associated_operations += [Tokens.IMP]
",[],0,[],/sem/linearlogic.py___init__
2155,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py_get_all_symbols,"def get_all_symbols(self):
return Tokens.TOKENS
",[],0,[],/sem/linearlogic.py_get_all_symbols
2156,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py_handle,"def handle(self, tok, context):
if tok not in Tokens.TOKENS:
return self.handle_variable(tok, context)
elif tok == Tokens.OPEN:
return self.handle_open(tok, context)
",[],0,[],/sem/linearlogic.py_handle
2157,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py_get_BooleanExpression_factory,"def get_BooleanExpression_factory(self, tok):
if tok == Tokens.IMP:
return ImpExpression
else:
return None
",[],0,[],/sem/linearlogic.py_get_BooleanExpression_factory
2158,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py_make_BooleanExpression,"def make_BooleanExpression(self, factory, first, second):
return factory(first, second)
",[],0,[],/sem/linearlogic.py_make_BooleanExpression
2159,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py_attempt_ApplicationExpression,"def attempt_ApplicationExpression(self, expression, context):
""""""Attempt to make an application expression.  If the next tokens
are an argument in parens, then the argument expression is a
function being applied to the arguments.  Otherwise, return the
argument expression.""""""
if self.has_priority(APP, context):
if self.inRange(0) and self.token(0) == Tokens.OPEN:
self.token()  # swallow then open paren
argument = self.process_next_expression(APP)
self.assertNextToken(Tokens.CLOSE)
expression = ApplicationExpression(expression, argument, None)
return expression
",[],0,[],/sem/linearlogic.py_attempt_ApplicationExpression
2160,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py_make_VariableExpression,"def make_VariableExpression(self, name):
if name[0].isupper():
return VariableExpression(name)
else:
return ConstantExpression(name)
",[],0,[],/sem/linearlogic.py_make_VariableExpression
2161,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py_fromstring,"def fromstring(cls, s):
return cls._linear_logic_parser.parse(s)
",[],0,[],/sem/linearlogic.py_fromstring
2162,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py_applyto,"def applyto(self, other, other_indices=None):
return ApplicationExpression(self, other, other_indices)
",[],0,[],/sem/linearlogic.py_applyto
2163,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py___call__,"def __call__(self, other):
return self.applyto(other)
",[],0,[],/sem/linearlogic.py___call__
2164,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py___repr__,"def __repr__(self):
return f""<{self.__class__.__name__} {self}>""
",[],0,[],/sem/linearlogic.py___repr__
2165,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py___init__,"def __init__(self, name, dependencies=None):
""""""
:param name: str for the constant name
:param dependencies: list of int for the indices on which this atom is dependent
""""""
assert isinstance(name, str)
self.name = name
if not dependencies:
dependencies = []
self.dependencies = dependencies
",[],0,[],/sem/linearlogic.py___init__
2166,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py_simplify,"def simplify(self, bindings=None):
""""""
If 'self' is bound by 'bindings', return the atomic to which it is bound.
Otherwise, return self.
:param bindings: ``BindingDict`` A dictionary of bindings used to simplify
:return: ``AtomicExpression``
""""""
if bindings and self in bindings:
return bindings[self]
else:
return self
",[],0,[],/sem/linearlogic.py_simplify
2167,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py_compile_pos,"def compile_pos(self, index_counter, glueFormulaFactory):
""""""
From Iddo Lev's PhD Dissertation p108-109
:param index_counter: ``Counter`` for unique indices
:param glueFormulaFactory: ``GlueFormula`` for creating new glue formulas
:return: (``Expression``,set) for the compiled linear logic and any newly created glue formulas
""""""
self.dependencies = []
return (self, [])
",[],0,[],/sem/linearlogic.py_compile_pos
2168,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py_compile_neg,"def compile_neg(self, index_counter, glueFormulaFactory):
""""""
From Iddo Lev's PhD Dissertation p108-109
:param index_counter: ``Counter`` for unique indices
:param glueFormulaFactory: ``GlueFormula`` for creating new glue formulas
:return: (``Expression``,set) for the compiled linear logic and any newly created glue formulas
""""""
self.dependencies = []
return (self, [])
",[],0,[],/sem/linearlogic.py_compile_neg
2169,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py_initialize_labels,"def initialize_labels(self, fstruct):
self.name = fstruct.initialize_label(self.name.lower())
",[],0,[],/sem/linearlogic.py_initialize_labels
2170,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py___eq__,"def __eq__(self, other):
return self.__class__ == other.__class__ and self.name == other.name
",[],0,[],/sem/linearlogic.py___eq__
2171,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/sem/linearlogic.py___ne__
2172,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py___str__,"def __str__(self):
accum = self.name
if self.dependencies:
accum += ""%s"" % self.dependencies
return accum
",[],0,[],/sem/linearlogic.py___str__
2173,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py___hash__,"def __hash__(self):
return hash(self.name)
",[],0,[],/sem/linearlogic.py___hash__
2174,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py_unify,"def unify(self, other, bindings):
""""""
If 'other' is a constant, then it must be equal to 'self'.  If 'other' is a variable,
then it must not be bound to anything other than 'self'.
:param other: ``Expression``
:param bindings: ``BindingDict`` A dictionary of all current bindings
:return: ``BindingDict`` A new combined dictionary of of 'bindings' and any new binding
:raise UnificationException: If 'self' and 'other' cannot be unified in the context of 'bindings'
""""""
assert isinstance(other, Expression)
if isinstance(other, VariableExpression):
try:
return bindings + BindingDict([(other, self)])
except VariableBindingException:
pass
elif self == other:
return bindings
raise UnificationException(self, other, bindings)
",[],0,[],/sem/linearlogic.py_unify
2175,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py_unify,"def unify(self, other, bindings):
""""""
'self' must not be bound to anything other than 'other'.
:param other: ``Expression``
:param bindings: ``BindingDict`` A dictionary of all current bindings
:return: ``BindingDict`` A new combined dictionary of of 'bindings' and the new binding
:raise UnificationException: If 'self' and 'other' cannot be unified in the context of 'bindings'
""""""
assert isinstance(other, Expression)
try:
if self == other:
return bindings
else:
return bindings + BindingDict([(self, other)])
except VariableBindingException as e:
raise UnificationException(self, other, bindings) from e
",[],0,[],/sem/linearlogic.py_unify
2176,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py___init__,"def __init__(self, antecedent, consequent):
""""""
:param antecedent: ``Expression`` for the antecedent
:param consequent: ``Expression`` for the consequent
""""""
assert isinstance(antecedent, Expression)
assert isinstance(consequent, Expression)
self.antecedent = antecedent
self.consequent = consequent
",[],0,[],/sem/linearlogic.py___init__
2177,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py_simplify,"def simplify(self, bindings=None):
return self.__class__(
self.antecedent.simplify(bindings), self.consequent.simplify(bindings)
)
",[],0,[],/sem/linearlogic.py_simplify
2178,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py_unify,"def unify(self, other, bindings):
""""""
Both the antecedent and consequent of 'self' and 'other' must unify.
:param other: ``ImpExpression``
:param bindings: ``BindingDict`` A dictionary of all current bindings
:return: ``BindingDict`` A new combined dictionary of of 'bindings' and any new bindings
:raise UnificationException: If 'self' and 'other' cannot be unified in the context of 'bindings'
""""""
assert isinstance(other, ImpExpression)
try:
return (
bindings
+ self.antecedent.unify(other.antecedent, bindings)
+ self.consequent.unify(other.consequent, bindings)
)
except VariableBindingException as e:
raise UnificationException(self, other, bindings) from e
",[],0,[],/sem/linearlogic.py_unify
2179,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py_compile_pos,"def compile_pos(self, index_counter, glueFormulaFactory):
""""""
From Iddo Lev's PhD Dissertation p108-109
:param index_counter: ``Counter`` for unique indices
:param glueFormulaFactory: ``GlueFormula`` for creating new glue formulas
:return: (``Expression``,set) for the compiled linear logic and any newly created glue formulas
""""""
(a, a_new) = self.antecedent.compile_neg(index_counter, glueFormulaFactory)
(c, c_new) = self.consequent.compile_pos(index_counter, glueFormulaFactory)
return (ImpExpression(a, c), a_new + c_new)
",[],0,[],/sem/linearlogic.py_compile_pos
2180,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py_compile_neg,"def compile_neg(self, index_counter, glueFormulaFactory):
""""""
From Iddo Lev's PhD Dissertation p108-109
:param index_counter: ``Counter`` for unique indices
:param glueFormulaFactory: ``GlueFormula`` for creating new glue formulas
:return: (``Expression``,list of ``GlueFormula``) for the compiled linear logic and any newly created glue formulas
""""""
(a, a_new) = self.antecedent.compile_pos(index_counter, glueFormulaFactory)
(c, c_new) = self.consequent.compile_neg(index_counter, glueFormulaFactory)
fresh_index = index_counter.get()
c.dependencies.append(fresh_index)
new_v = glueFormulaFactory(""v%s"" % fresh_index, a, {fresh_index})
return (c, a_new + c_new + [new_v])
",[],0,[],/sem/linearlogic.py_compile_neg
2181,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py_initialize_labels,"def initialize_labels(self, fstruct):
self.antecedent.initialize_labels(fstruct)
self.consequent.initialize_labels(fstruct)
",[],0,[],/sem/linearlogic.py_initialize_labels
2182,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py___eq__,"def __eq__(self, other):
return (
self.__class__ == other.__class__
and self.antecedent == other.antecedent
and self.consequent == other.consequent
)
",[],0,[],/sem/linearlogic.py___eq__
2183,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/sem/linearlogic.py___ne__
2184,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py___str__,"def __str__(self):
return ""{}{} {} {}{}"".format(
Tokens.OPEN,
self.antecedent,
Tokens.IMP,
self.consequent,
Tokens.CLOSE,
)
",[],0,[],/sem/linearlogic.py___str__
2185,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py___hash__,"def __hash__(self):
return hash(f""{hash(self.antecedent)}{Tokens.IMP}{hash(self.consequent)}"")
",[],0,[],/sem/linearlogic.py___hash__
2186,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py___init__,"def __init__(self, function, argument, argument_indices=None):
""""""
:param function: ``Expression`` for the function
:param argument: ``Expression`` for the argument
:param argument_indices: set for the indices of the glue formula from which the argument came
:raise LinearLogicApplicationException: If 'function' cannot be applied to 'argument' given 'argument_indices'.
""""""
function_simp = function.simplify()
argument_simp = argument.simplify()
assert isinstance(function_simp, ImpExpression)
assert isinstance(argument_simp, Expression)
bindings = BindingDict()
try:
if isinstance(function, ApplicationExpression):
bindings += function.bindings
if isinstance(argument, ApplicationExpression):
bindings += argument.bindings
bindings += function_simp.antecedent.unify(argument_simp, bindings)
except UnificationException as e:
raise LinearLogicApplicationException(
f""Cannot apply {function_simp} to {argument_simp}. {e}""
) from e
if argument_indices:
if not set(function_simp.antecedent.dependencies) < argument_indices:
raise LinearLogicApplicationException(
""Dependencies unfulfilled when attempting to apply Linear Logic formula %s to %s""
% (function_simp, argument_simp)
)
if set(function_simp.antecedent.dependencies) == argument_indices:
raise LinearLogicApplicationException(
""Dependencies not a proper subset of indices when attempting to apply Linear Logic formula %s to %s""
% (function_simp, argument_simp)
)
self.function = function
self.argument = argument
self.bindings = bindings
",[],0,[],/sem/linearlogic.py___init__
2187,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py_simplify,"def simplify(self, bindings=None):
""""""
Since function is an implication, return its consequent.  There should be
no need to check that the application is valid since the checking is done
by the constructor.
:param bindings: ``BindingDict`` A dictionary of bindings used to simplify
:return: ``Expression``
""""""
if not bindings:
bindings = self.bindings
return self.function.simplify(bindings).consequent
",[],0,[],/sem/linearlogic.py_simplify
2188,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py___eq__,"def __eq__(self, other):
return (
self.__class__ == other.__class__
and self.function == other.function
and self.argument == other.argument
)
",[],0,[],/sem/linearlogic.py___eq__
2189,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/sem/linearlogic.py___ne__
2190,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py___str__,"def __str__(self):
return ""%s"" % self.function + Tokens.OPEN + ""%s"" % self.argument + Tokens.CLOSE
",[],0,[],/sem/linearlogic.py___str__
2191,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py___hash__,"def __hash__(self):
return hash(f""{hash(self.antecedent)}{Tokens.OPEN}{hash(self.consequent)}"")
",[],0,[],/sem/linearlogic.py___hash__
2192,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py___init__,"def __init__(self, bindings=None):
""""""
:param bindings:
list [(``VariableExpression``, ``AtomicExpression``)] to initialize the dictionary
dict {``VariableExpression``: ``AtomicExpression``} to initialize the dictionary
""""""
self.d = {}
if isinstance(bindings, dict):
bindings = bindings.items()
if bindings:
for v, b in bindings:
self[v] = b
",[],0,[],/sem/linearlogic.py___init__
2193,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py___setitem__,"def __setitem__(self, variable, binding):
""""""
A binding is consistent with the dict if its variable is not already bound, OR if its
variable is already bound to its argument.
:param variable: ``VariableExpression`` The variable bind
:param binding: ``Expression`` The expression to which 'variable' should be bound
:raise VariableBindingException: If the variable cannot be bound in this dictionary
""""""
assert isinstance(variable, VariableExpression)
assert isinstance(binding, Expression)
assert variable != binding
existing = self.d.get(variable, None)
if not existing or binding == existing:
self.d[variable] = binding
else:
raise VariableBindingException(
""Variable %s already bound to another value"" % (variable)
)
",[],0,[],/sem/linearlogic.py___setitem__
2194,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py___getitem__,"def __getitem__(self, variable):
""""""
Return the expression to which 'variable' is bound
""""""
assert isinstance(variable, VariableExpression)
intermediate = self.d[variable]
while intermediate:
try:
intermediate = self.d[intermediate]
except KeyError:
return intermediate
",[],0,[],/sem/linearlogic.py___getitem__
2195,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py___contains__,"def __contains__(self, item):
return item in self.d
",[],0,[],/sem/linearlogic.py___contains__
2196,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py___add__,"def __add__(self, other):
""""""
:param other: ``BindingDict`` The dict with which to combine self
:return: ``BindingDict`` A new dict containing all the elements of both parameters
:raise VariableBindingException: If the parameter dictionaries are not consistent with each other
""""""
try:
combined = BindingDict()
for v in self.d:
combined[v] = self.d[v]
for v in other.d:
combined[v] = other.d[v]
return combined
except VariableBindingException as e:
raise VariableBindingException(
""Attempting to add two contradicting""
"" VariableBindingsLists: %s, %s"" % (self, other)
) from e
",[],0,[],/sem/linearlogic.py___add__
2197,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/sem/linearlogic.py___ne__
2198,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py___eq__,"def __eq__(self, other):
if not isinstance(other, BindingDict):
raise TypeError
return self.d == other.d
",[],0,[],/sem/linearlogic.py___eq__
2199,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py___str__,"def __str__(self):
return ""{"" + "", "".join(f""{v}: {self.d[v]}"" for v in sorted(self.d.keys())) + ""}""
",[],0,[],/sem/linearlogic.py___str__
2200,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py___repr__,"def __repr__(self):
return ""BindingDict: %s"" % self
",[],0,[],/sem/linearlogic.py___repr__
2201,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py___init__,"def __init__(self, a, b, bindings):
Exception.__init__(self, f""Cannot unify {a} with {b} given {bindings}"")
",[],0,[],/sem/linearlogic.py___init__
2202,/home/amandapotts/git/nltk/nltk/sem/linearlogic.py_demo,"def demo():
lexpr = Expression.fromstring
print(lexpr(r""f""))
print(lexpr(r""(g -o f)""))
print(lexpr(r""((g -o G) -o G)""))
print(lexpr(r""g -o h -o f""))
print(lexpr(r""(g -o f)(g)"").simplify())
print(lexpr(r""(H -o f)(g)"").simplify())
print(lexpr(r""((g -o G) -o G)((g -o f))"").simplify())
print(lexpr(r""(H -o H)((g -o f))"").simplify())
",[],0,[],/sem/linearlogic.py_demo
2203,/home/amandapotts/git/nltk/nltk/sem/glue.py___init__,"def __init__(self, meaning, glue, indices=None):
if not indices:
indices = set()
if isinstance(meaning, str):
self.meaning = Expression.fromstring(meaning)
elif isinstance(meaning, Expression):
self.meaning = meaning
else:
raise RuntimeError(
""Meaning term neither string or expression: %s, %s""
% (meaning, meaning.__class__)
)
if isinstance(glue, str):
self.glue = linearlogic.LinearLogicParser().parse(glue)
elif isinstance(glue, linearlogic.Expression):
self.glue = glue
else:
raise RuntimeError(
""Glue term neither string or expression: %s, %s""
% (glue, glue.__class__)
)
self.indices = indices
",[],0,[],/sem/glue.py___init__
2204,/home/amandapotts/git/nltk/nltk/sem/glue.py_applyto,"def applyto(self, arg):
""""""self = (\\x.(walk x), (subj -o f))
arg  = (john        ,  subj)
returns ((walk john),          f)
""""""
if self.indices & arg.indices:  # if the sets are NOT disjoint
raise linearlogic.LinearLogicApplicationException(
f""'{self}' applied to '{arg}'.  Indices are not disjoint.""
)
else:  # if the sets ARE disjoint
return_indices = self.indices | arg.indices
try:
return_glue = linearlogic.ApplicationExpression(
self.glue, arg.glue, arg.indices
)
except linearlogic.LinearLogicApplicationException as e:
raise linearlogic.LinearLogicApplicationException(
f""'{self.simplify()}' applied to '{arg.simplify()}'""
) from e
arg_meaning_abstracted = arg.meaning
if return_indices:
for dep in self.glue.simplify().antecedent.dependencies[
::-1
]:  # if self.glue is (A -o B), dep is in A.dependencies
arg_meaning_abstracted = self.make_LambdaExpression(
Variable(""v%s"" % dep), arg_meaning_abstracted
)
return_meaning = self.meaning.applyto(arg_meaning_abstracted)
return self.__class__(return_meaning, return_glue, return_indices)
",[],0,[],/sem/glue.py_applyto
2205,/home/amandapotts/git/nltk/nltk/sem/glue.py_make_VariableExpression,"def make_VariableExpression(self, name):
return VariableExpression(name)
",[],0,[],/sem/glue.py_make_VariableExpression
2206,/home/amandapotts/git/nltk/nltk/sem/glue.py_make_LambdaExpression,"def make_LambdaExpression(self, variable, term):
return LambdaExpression(variable, term)
",[],0,[],/sem/glue.py_make_LambdaExpression
2207,/home/amandapotts/git/nltk/nltk/sem/glue.py_compile,"def compile(self, counter=None):
""""""From Iddo Lev's PhD Dissertation p108-109""""""
if not counter:
counter = Counter()
(compiled_glue, new_forms) = self.glue.simplify().compile_pos(
counter, self.__class__
)
return new_forms + [
self.__class__(self.meaning, compiled_glue, {counter.get()})
]
",[],0,[],/sem/glue.py_compile
2208,/home/amandapotts/git/nltk/nltk/sem/glue.py_simplify,"def simplify(self):
return self.__class__(
self.meaning.simplify(), self.glue.simplify(), self.indices
)
",[],0,[],/sem/glue.py_simplify
2209,/home/amandapotts/git/nltk/nltk/sem/glue.py___eq__,"def __eq__(self, other):
return (
self.__class__ == other.__class__
and self.meaning == other.meaning
and self.glue == other.glue
)
",[],0,[],/sem/glue.py___eq__
2210,/home/amandapotts/git/nltk/nltk/sem/glue.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/sem/glue.py___ne__
2211,/home/amandapotts/git/nltk/nltk/sem/glue.py___lt__,"def __lt__(self, other):
return str(self) < str(other)
",[],0,[],/sem/glue.py___lt__
2212,/home/amandapotts/git/nltk/nltk/sem/glue.py___str__,"def __str__(self):
assert isinstance(self.indices, set)
accum = f""{self.meaning} : {self.glue}""
if self.indices:
accum += (
"" : {"" + "", "".join(str(index) for index in sorted(self.indices)) + ""}""
)
return accum
",[],0,[],/sem/glue.py___str__
2213,/home/amandapotts/git/nltk/nltk/sem/glue.py___repr__,"def __repr__(self):
return ""%s"" % self
",[],0,[],/sem/glue.py___repr__
2214,/home/amandapotts/git/nltk/nltk/sem/glue.py___init__,"def __init__(self, filename, encoding=None):
self.filename = filename
self.file_encoding = encoding
self.read_file()
",[],0,[],/sem/glue.py___init__
2215,/home/amandapotts/git/nltk/nltk/sem/glue.py___str__,"def __str__(self):
accum = """"
for pos in self:
str_pos = ""%s"" % pos
for relset in self[pos]:
i = 1
for gf in self[pos][relset]:
if i == 1:
accum += str_pos + "": ""
else:
accum += "" "" * (len(str_pos) + 2)
accum += ""%s"" % gf
if relset and i == len(self[pos][relset]):
accum += "" : %s"" % relset
accum += ""\n""
i += 1
return accum
",[],0,[],/sem/glue.py___str__
2216,/home/amandapotts/git/nltk/nltk/sem/glue.py_to_glueformula_list,"def to_glueformula_list(self, depgraph, node=None, counter=None, verbose=False):
if node is None:
top = depgraph.nodes[0]
depList = list(chain.from_iterable(top[""deps""].values()))
root = depgraph.nodes[depList[0]]
return self.to_glueformula_list(depgraph, root, Counter(), verbose)
glueformulas = self.lookup(node, depgraph, counter)
for dep_idx in chain.from_iterable(node[""deps""].values()):
dep = depgraph.nodes[dep_idx]
glueformulas.extend(
self.to_glueformula_list(depgraph, dep, counter, verbose)
)
return glueformulas
",[],0,[],/sem/glue.py_to_glueformula_list
2217,/home/amandapotts/git/nltk/nltk/sem/glue.py_lookup,"def lookup(self, node, depgraph, counter):
semtype_names = self.get_semtypes(node)
semtype = None
for name in semtype_names:
if name in self:
semtype = self[name]
break
if semtype is None:
return []
self.add_missing_dependencies(node, depgraph)
lookup = self._lookup_semtype_option(semtype, node, depgraph)
if not len(lookup):
raise KeyError(
""There is no GlueDict entry for sem type of '%s' ""
""with tag '%s', and rel '%s'"" % (node[""word""], node[""tag""], node[""rel""])
)
return self.get_glueformulas_from_semtype_entry(
lookup, node[""word""], node, depgraph, counter
)
",[],0,[],/sem/glue.py_lookup
2218,/home/amandapotts/git/nltk/nltk/sem/glue.py_add_missing_dependencies,"def add_missing_dependencies(self, node, depgraph):
rel = node[""rel""].lower()
if rel == ""main"":
headnode = depgraph.nodes[node[""head""]]
subj = self.lookup_unique(""subj"", headnode, depgraph)
relation = subj[""rel""]
node[""deps""].setdefault(relation, [])
node[""deps""][relation].append(subj[""address""])
",[],0,[],/sem/glue.py_add_missing_dependencies
2219,/home/amandapotts/git/nltk/nltk/sem/glue.py__lookup_semtype_option,"def _lookup_semtype_option(self, semtype, node, depgraph):
relationships = frozenset(
depgraph.nodes[dep][""rel""].lower()
for dep in chain.from_iterable(node[""deps""].values())
if depgraph.nodes[dep][""rel""].lower() not in OPTIONAL_RELATIONSHIPS
)
try:
lookup = semtype[relationships]
except KeyError:
best_match = frozenset()
for relset_option in set(semtype) - {None}:
if (
len(relset_option) > len(best_match)
and relset_option < relationships
):
best_match = relset_option
if not best_match:
if None in semtype:
best_match = None
else:
return None
lookup = semtype[best_match]
return lookup
",[],0,[],/sem/glue.py__lookup_semtype_option
2220,/home/amandapotts/git/nltk/nltk/sem/glue.py_get_semtypes,"def get_semtypes(self, node):
""""""
Based on the node, return a list of plausible semtypes in order of
plausibility.
""""""
rel = node[""rel""].lower()
word = node[""word""].lower()
if rel == ""spec"":
if word in SPEC_SEMTYPES:
return [SPEC_SEMTYPES[word]]
else:
return [SPEC_SEMTYPES[""default""]]
elif rel in [""nmod"", ""vmod""]:
return [node[""tag""], rel]
else:
return [node[""tag""]]
",[],0,[],/sem/glue.py_get_semtypes
2221,/home/amandapotts/git/nltk/nltk/sem/glue.py_get_glueformulas_from_semtype_entry,"def get_glueformulas_from_semtype_entry(
self, lookup, word, node, depgraph, counter
",[],0,[],/sem/glue.py_get_glueformulas_from_semtype_entry
2222,/home/amandapotts/git/nltk/nltk/sem/glue.py_get_meaning_formula,"def get_meaning_formula(self, generic, word):
""""""
:param generic: A meaning formula string containing the
parameter ""<word>""
:param word: The actual word to be replace ""<word>""
""""""
word = word.replace(""."", """")
return generic.replace(""<word>"", word)
",[],0,[],/sem/glue.py_get_meaning_formula
2223,/home/amandapotts/git/nltk/nltk/sem/glue.py_initialize_labels,"def initialize_labels(self, expr, node, depgraph, unique_index):
if isinstance(expr, linearlogic.AtomicExpression):
name = self.find_label_name(expr.name.lower(), node, depgraph, unique_index)
if name[0].isupper():
return linearlogic.VariableExpression(name)
else:
return linearlogic.ConstantExpression(name)
else:
return linearlogic.ImpExpression(
self.initialize_labels(expr.antecedent, node, depgraph, unique_index),
self.initialize_labels(expr.consequent, node, depgraph, unique_index),
)
",[],0,[],/sem/glue.py_initialize_labels
2224,/home/amandapotts/git/nltk/nltk/sem/glue.py_find_label_name,"def find_label_name(self, name, node, depgraph, unique_index):
try:
dot = name.index(""."")
before_dot = name[:dot]
after_dot = name[dot + 1 :]
if before_dot == ""super"":
return self.find_label_name(
after_dot, depgraph.nodes[node[""head""]], depgraph, unique_index
)
else:
return self.find_label_name(
after_dot,
self.lookup_unique(before_dot, node, depgraph),
depgraph,
unique_index,
)
except ValueError:
lbl = self.get_label(node)
if name == ""f"":
return lbl
elif name == ""v"":
return ""%sv"" % lbl
elif name == ""r"":
return ""%sr"" % lbl
elif name == ""super"":
return self.get_label(depgraph.nodes[node[""head""]])
elif name == ""var"":
return f""{lbl.upper()}{unique_index}""
elif name == ""a"":
return self.get_label(self.lookup_unique(""conja"", node, depgraph))
elif name == ""b"":
return self.get_label(self.lookup_unique(""conjb"", node, depgraph))
else:
return self.get_label(self.lookup_unique(name, node, depgraph))
",[],0,[],/sem/glue.py_find_label_name
2225,/home/amandapotts/git/nltk/nltk/sem/glue.py_get_label,"def get_label(self, node):
""""""
Pick an alphabetic character as identifier for an entity in the model.
:param value: where to index into the list of characters
:type value: int
""""""
value = node[""address""]
letter = [
""f"",
""g"",
""h"",
""i"",
""j"",
""k"",
""l"",
""m"",
""n"",
""o"",
""p"",
""q"",
""r"",
""s"",
""t"",
""u"",
""v"",
""w"",
""x"",
""y"",
""z"",
""a"",
""b"",
""c"",
""d"",
""e"",
][value - 1]
num = int(value) // 26
if num > 0:
return letter + str(num)
else:
return letter
",[],0,[],/sem/glue.py_get_label
2226,/home/amandapotts/git/nltk/nltk/sem/glue.py_lookup_unique,"def lookup_unique(self, rel, node, depgraph):
""""""
Lookup 'key'. There should be exactly one item in the associated relation.
""""""
deps = [
depgraph.nodes[dep]
for dep in chain.from_iterable(node[""deps""].values())
if depgraph.nodes[dep][""rel""].lower() == rel.lower()
]
if len(deps) == 0:
raise KeyError(
""'{}' doesn't contain a feature '{}'"".format(node[""word""], rel)
)
elif len(deps) > 1:
raise KeyError(
""'{}' should only have one feature '{}'"".format(node[""word""], rel)
)
else:
return deps[0]
",[],0,[],/sem/glue.py_lookup_unique
2227,/home/amandapotts/git/nltk/nltk/sem/glue.py_get_GlueFormula_factory,"def get_GlueFormula_factory(self):
return GlueFormula
",[],0,[],/sem/glue.py_get_GlueFormula_factory
2228,/home/amandapotts/git/nltk/nltk/sem/glue.py___init__,"def __init__(
self, semtype_file=None, remove_duplicates=False, depparser=None, verbose=False
",[],0,[],/sem/glue.py___init__
2229,/home/amandapotts/git/nltk/nltk/sem/glue.py_train_depparser,"def train_depparser(self, depgraphs=None):
if depgraphs:
self.depparser.train(depgraphs)
else:
self.depparser.train_from_file(
nltk.data.find(
os.path.join(""grammars"", ""sample_grammars"", ""glue_train.conll"")
)
)
",[],0,[],/sem/glue.py_train_depparser
2230,/home/amandapotts/git/nltk/nltk/sem/glue.py_parse_to_meaning,"def parse_to_meaning(self, sentence):
readings = []
for agenda in self.parse_to_compiled(sentence):
readings.extend(self.get_readings(agenda))
return readings
",[],0,[],/sem/glue.py_parse_to_meaning
2231,/home/amandapotts/git/nltk/nltk/sem/glue.py_get_readings,"def get_readings(self, agenda):
readings = []
agenda_length = len(agenda)
atomics = dict()
nonatomics = dict()
while agenda:  # is not empty
cur = agenda.pop()
glue_simp = cur.glue.simplify()
if isinstance(
glue_simp, linearlogic.ImpExpression
):  # if cur.glue is non-atomic
for key in atomics:
try:
if isinstance(cur.glue, linearlogic.ApplicationExpression):
bindings = cur.glue.bindings
else:
bindings = linearlogic.BindingDict()
glue_simp.antecedent.unify(key, bindings)
for atomic in atomics[key]:
if not (
cur.indices & atomic.indices
):  # if the sets of indices are disjoint
try:
agenda.append(cur.applyto(atomic))
except linearlogic.LinearLogicApplicationException:
pass
except linearlogic.UnificationException:
pass
try:
nonatomics[glue_simp.antecedent].append(cur)
except KeyError:
nonatomics[glue_simp.antecedent] = [cur]
else:  # else cur.glue is atomic
for key in nonatomics:
for nonatomic in nonatomics[key]:
try:
if isinstance(
nonatomic.glue, linearlogic.ApplicationExpression
):
bindings = nonatomic.glue.bindings
else:
bindings = linearlogic.BindingDict()
glue_simp.unify(key, bindings)
if not (
cur.indices & nonatomic.indices
):  # if the sets of indices are disjoint
try:
agenda.append(nonatomic.applyto(cur))
except linearlogic.LinearLogicApplicationException:
pass
except linearlogic.UnificationException:
pass
try:
atomics[glue_simp].append(cur)
except KeyError:
atomics[glue_simp] = [cur]
for entry in atomics:
for gf in atomics[entry]:
if len(gf.indices) == agenda_length:
self._add_to_reading_list(gf, readings)
for entry in nonatomics:
for gf in nonatomics[entry]:
if len(gf.indices) == agenda_length:
self._add_to_reading_list(gf, readings)
return readings
",[],0,[],/sem/glue.py_get_readings
2232,/home/amandapotts/git/nltk/nltk/sem/glue.py__add_to_reading_list,"def _add_to_reading_list(self, glueformula, reading_list):
add_reading = True
if self.remove_duplicates:
for reading in reading_list:
try:
if reading.equiv(glueformula.meaning, self.prover):
add_reading = False
break
except Exception as e:
print(""Error when checking logical equality of statements"", e)
if add_reading:
reading_list.append(glueformula.meaning)
",[],0,[],/sem/glue.py__add_to_reading_list
2233,/home/amandapotts/git/nltk/nltk/sem/glue.py_parse_to_compiled,"def parse_to_compiled(self, sentence):
gfls = [self.depgraph_to_glue(dg) for dg in self.dep_parse(sentence)]
return [self.gfl_to_compiled(gfl) for gfl in gfls]
",[],0,[],/sem/glue.py_parse_to_compiled
2234,/home/amandapotts/git/nltk/nltk/sem/glue.py_dep_parse,"def dep_parse(self, sentence):
""""""
Return a dependency graph for the sentence.
:param sentence: the sentence to be parsed
:type sentence: list(str)
:rtype: DependencyGraph
""""""
if self.depparser is None:
from nltk.parse import MaltParser
self.depparser = MaltParser(tagger=self.get_pos_tagger())
if not self.depparser._trained:
self.train_depparser()
return self.depparser.parse(sentence, verbose=self.verbose)
",[],0,[],/sem/glue.py_dep_parse
2235,/home/amandapotts/git/nltk/nltk/sem/glue.py_depgraph_to_glue,"def depgraph_to_glue(self, depgraph):
return self.get_glue_dict().to_glueformula_list(depgraph)
",[],0,[],/sem/glue.py_depgraph_to_glue
2236,/home/amandapotts/git/nltk/nltk/sem/glue.py_get_glue_dict,"def get_glue_dict(self):
return GlueDict(self.semtype_file)
",[],0,[],/sem/glue.py_get_glue_dict
2237,/home/amandapotts/git/nltk/nltk/sem/glue.py_gfl_to_compiled,"def gfl_to_compiled(self, gfl):
index_counter = Counter()
return_list = []
for gf in gfl:
return_list.extend(gf.compile(index_counter))
if self.verbose:
print(""Compiled Glue Premises:"")
for cgf in return_list:
print(cgf)
return return_list
",[],0,[],/sem/glue.py_gfl_to_compiled
2238,/home/amandapotts/git/nltk/nltk/sem/glue.py_get_pos_tagger,"def get_pos_tagger(self):
from nltk.corpus import brown
regexp_tagger = RegexpTagger(
[
(r""^-?[0-9]+(\.[0-9]+)?$"", ""CD""),  # cardinal numbers
(r""(The|the|A|a|An|an)$"", ""AT""),  # articles
(r"".*able$"", ""JJ""),  # adjectives
(r"".*ness$"", ""NN""),  # nouns formed from adjectives
(r"".*ly$"", ""RB""),  # adverbs
(r"".*s$"", ""NNS""),  # plural nouns
(r"".*ing$"", ""VBG""),  # gerunds
(r"".*ed$"", ""VBD""),  # past tense verbs
(r"".*"", ""NN""),  # nouns (default)
]
)
brown_train = brown.tagged_sents(categories=""news"")
unigram_tagger = UnigramTagger(brown_train, backoff=regexp_tagger)
bigram_tagger = BigramTagger(brown_train, backoff=unigram_tagger)
trigram_tagger = TrigramTagger(brown_train, backoff=bigram_tagger)
main_tagger = RegexpTagger(
[(r""(A|a|An|an)$"", ""ex_quant""), (r""(Every|every|All|all)$"", ""univ_quant"")],
backoff=trigram_tagger,
)
return main_tagger
",[],0,[],/sem/glue.py_get_pos_tagger
2239,/home/amandapotts/git/nltk/nltk/sem/glue.py___init__,"def __init__(self, meaning, glue, indices=None):
if not indices:
indices = set()
if isinstance(meaning, str):
self.meaning = drt.DrtExpression.fromstring(meaning)
elif isinstance(meaning, drt.DrtExpression):
self.meaning = meaning
else:
raise RuntimeError(
""Meaning term neither string or expression: %s, %s""
% (meaning, meaning.__class__)
)
if isinstance(glue, str):
self.glue = linearlogic.LinearLogicParser().parse(glue)
elif isinstance(glue, linearlogic.Expression):
self.glue = glue
else:
raise RuntimeError(
""Glue term neither string or expression: %s, %s""
% (glue, glue.__class__)
)
self.indices = indices
",[],0,[],/sem/glue.py___init__
2240,/home/amandapotts/git/nltk/nltk/sem/glue.py_make_VariableExpression,"def make_VariableExpression(self, name):
return drt.DrtVariableExpression(name)
",[],0,[],/sem/glue.py_make_VariableExpression
2241,/home/amandapotts/git/nltk/nltk/sem/glue.py_make_LambdaExpression,"def make_LambdaExpression(self, variable, term):
return drt.DrtLambdaExpression(variable, term)
",[],0,[],/sem/glue.py_make_LambdaExpression
2242,/home/amandapotts/git/nltk/nltk/sem/glue.py_get_GlueFormula_factory,"def get_GlueFormula_factory(self):
return DrtGlueFormula
",[],0,[],/sem/glue.py_get_GlueFormula_factory
2243,/home/amandapotts/git/nltk/nltk/sem/glue.py___init__,"def __init__(
self, semtype_file=None, remove_duplicates=False, depparser=None, verbose=False
",[],0,[],/sem/glue.py___init__
2244,/home/amandapotts/git/nltk/nltk/sem/glue.py_get_glue_dict,"def get_glue_dict(self):
return DrtGlueDict(self.semtype_file)
",[],0,[],/sem/glue.py_get_glue_dict
2245,/home/amandapotts/git/nltk/nltk/sem/glue.py_demo,"def demo(show_example=-1):
from nltk.parse import MaltParser
examples = [
""David sees Mary"",
""David eats a sandwich"",
""every man chases a dog"",
""every man believes a dog sleeps"",
""John gives David a sandwich"",
""John chases himself"",
]
print(""============== DEMO =============="")
tagger = RegexpTagger(
[
(""^(David|Mary|John)$"", ""NNP""),
(
""^(sees|eats|chases|believes|gives|sleeps|chases|persuades|tries|seems|leaves)$"",
""VB"",
),
(""^(go|order|vanish|find|approach)$"", ""VB""),
(""^(a)$"", ""ex_quant""),
(""^(every)$"", ""univ_quant""),
(""^(sandwich|man|dog|pizza|unicorn|cat|senator)$"", ""NN""),
(""^(big|gray|former)$"", ""JJ""),
(""^(him|himself)$"", ""PRP""),
]
)
depparser = MaltParser(tagger=tagger)
glue = Glue(depparser=depparser, verbose=False)
for i, sentence in enumerate(examples):
if i == show_example or show_example == -1:
print(f""[[[Example {i}]]]  {sentence}"")
for reading in glue.parse_to_meaning(sentence.split()):
print(reading.simplify())
print("""")
",[],0,[],/sem/glue.py_demo
2246,/home/amandapotts/git/nltk/nltk/sem/relextract.py__expand,"def _expand(type):
""""""
Expand an NE class name.
:type type: str
:rtype: str
""""""
try:
return short2long[type]
except KeyError:
return type
",[],0,[],/sem/relextract.py__expand
2247,/home/amandapotts/git/nltk/nltk/sem/relextract.py_class_abbrev,"def class_abbrev(type):
""""""
Abbreviate an NE class name.
:type type: str
:rtype: str
""""""
try:
return long2short[type]
except KeyError:
return type
",[],0,[],/sem/relextract.py_class_abbrev
2248,/home/amandapotts/git/nltk/nltk/sem/relextract.py__join,"def _join(lst, sep="" "", untag=False):
""""""
Join a list into a string, turning tags tuples into tag strings or just words.
:param untag: if ``True``, omit the tag from tagged input strings.
:type lst: list
:rtype: str
""""""
try:
return sep.join(lst)
except TypeError:
if untag:
return sep.join(tup[0] for tup in lst)
from nltk.tag import tuple2str
return sep.join(tuple2str(tup) for tup in lst)
",[],0,[],/sem/relextract.py__join
2249,/home/amandapotts/git/nltk/nltk/sem/relextract.py_descape_entity,"def descape_entity(m, defs=html.entities.entitydefs):
""""""
Translate one entity to its ISO Latin value.
Inspired by example from effbot.org
""""""
try:
return defs[m.group(1)]
except KeyError:
return m.group(0)  # use as is
",[],0,[],/sem/relextract.py_descape_entity
2250,/home/amandapotts/git/nltk/nltk/sem/relextract.py_list2sym,"def list2sym(lst):
""""""
Convert a list of strings into a canonical symbol.
:type lst: list
:return: a Unicode string without whitespace
:rtype: unicode
""""""
sym = _join(lst, ""_"", untag=True)
sym = sym.lower()
ENT = re.compile(r""&(\w+?)
sym = ENT.sub(descape_entity, sym)
sym = sym.replace(""."", """")
return sym
",[],0,[],/sem/relextract.py_list2sym
2251,/home/amandapotts/git/nltk/nltk/sem/relextract.py_tree2semi_rel,"def tree2semi_rel(tree):
""""""
Group a chunk structure into a list of 'semi-relations' of the form (list(str), ``Tree``).
In order to facilitate the construction of (``Tree``, string, ``Tree``) triples, this
identifies pairs whose first member is a list (possibly empty) of terminal
strings, and whose second member is a ``Tree`` of the form (NE_label, terminals).
:param tree: a chunk tree
:return: a list of pairs (list(str), ``Tree``)
:rtype: list of tuple
""""""
from nltk.tree import Tree
semi_rels = []
semi_rel = [[], None]
for dtr in tree:
if not isinstance(dtr, Tree):
semi_rel[0].append(dtr)
else:
semi_rel[1] = dtr
semi_rels.append(semi_rel)
semi_rel = [[], None]
return semi_rels
",[],0,[],/sem/relextract.py_tree2semi_rel
2252,/home/amandapotts/git/nltk/nltk/sem/relextract.py_semi_rel2reldict,"def semi_rel2reldict(pairs, window=5, trace=False):
""""""
Converts the pairs generated by ``tree2semi_rel`` into a 'reldict': a dictionary which
stores information about the subject and object NEs plus the filler between them.
Additionally, a left and right context of length =< window are captured (within
a given input sentence).
:param pairs: a pair of list(str) and ``Tree``, as generated by
:param window: a threshold for the number of items to include in the left and right context
:type window: int
:return: 'relation' dictionaries whose keys are 'lcon', 'subjclass', 'subjtext', 'subjsym', 'filler', objclass', objtext', 'objsym' and 'rcon'
:rtype: list(defaultdict)
""""""
result = []
while len(pairs) > 2:
reldict = defaultdict(str)
reldict[""lcon""] = _join(pairs[0][0][-window:])
reldict[""subjclass""] = pairs[0][1].label()
reldict[""subjtext""] = _join(pairs[0][1].leaves())
reldict[""subjsym""] = list2sym(pairs[0][1].leaves())
reldict[""filler""] = _join(pairs[1][0])
reldict[""untagged_filler""] = _join(pairs[1][0], untag=True)
reldict[""objclass""] = pairs[1][1].label()
reldict[""objtext""] = _join(pairs[1][1].leaves())
reldict[""objsym""] = list2sym(pairs[1][1].leaves())
reldict[""rcon""] = _join(pairs[2][0][:window])
if trace:
print(
""(%s(%s, %s)""
% (
reldict[""untagged_filler""],
reldict[""subjclass""],
reldict[""objclass""],
)
)
result.append(reldict)
pairs = pairs[1:]
return result
",[],0,[],/sem/relextract.py_semi_rel2reldict
2253,/home/amandapotts/git/nltk/nltk/sem/relextract.py_extract_rels,"def extract_rels(subjclass, objclass, doc, corpus=""ace"", pattern=None, window=10):
""""""
Filter the output of ``semi_rel2reldict`` according to specified NE classes and a filler pattern.
The parameters ``subjclass`` and ``objclass`` can be used to restrict the
Named Entities to particular types (any of 'LOCATION', 'ORGANIZATION',
'PERSON', 'DURATION', 'DATE', 'CARDINAL', 'PERCENT', 'MONEY', 'MEASURE').
:param subjclass: the class of the subject Named Entity.
:type subjclass: str
:param objclass: the class of the object Named Entity.
:type objclass: str
:param doc: input document
:type doc: ieer document or a list of chunk trees
:param corpus: name of the corpus to take as input
'ieer' and 'conll2002'
:type corpus: str
:param pattern: a regular expression for filtering the fillers of
retrieved triples.
:type pattern: SRE_Pattern
:param window: filters out fillers which exceed this threshold
:type window: int
:return: see ``mk_reldicts``
:rtype: list(defaultdict)
""""""
if subjclass and subjclass not in NE_CLASSES[corpus]:
if _expand(subjclass) in NE_CLASSES[corpus]:
subjclass = _expand(subjclass)
else:
raise ValueError(
""your value for the subject type has not been recognized: %s""
% subjclass
)
if objclass and objclass not in NE_CLASSES[corpus]:
if _expand(objclass) in NE_CLASSES[corpus]:
objclass = _expand(objclass)
else:
raise ValueError(
""your value for the object type has not been recognized: %s"" % objclass
)
if corpus == ""ace"" or corpus == ""conll2002"":
pairs = tree2semi_rel(doc)
elif corpus == ""ieer"":
pairs = tree2semi_rel(doc.text) + tree2semi_rel(doc.headline)
else:
raise ValueError(""corpus type not recognized"")
reldicts = semi_rel2reldict(pairs)
",[],0,[],/sem/relextract.py_extract_rels
2254,/home/amandapotts/git/nltk/nltk/sem/relextract.py_rtuple,"def rtuple(reldict, lcon=False, rcon=False):
""""""
Pretty print the reldict as an rtuple.
:param reldict: a relation dictionary
:type reldict: defaultdict
""""""
items = [
class_abbrev(reldict[""subjclass""]),
reldict[""subjtext""],
reldict[""filler""],
class_abbrev(reldict[""objclass""]),
reldict[""objtext""],
]
format = ""[%s: %r] %r [%s: %r]""
if lcon:
items = [reldict[""lcon""]] + items
format = ""...%r)"" + format
if rcon:
items.append(reldict[""rcon""])
format = format + ""(%r...""
printargs = tuple(items)
return format % printargs
",[],0,[],/sem/relextract.py_rtuple
2255,/home/amandapotts/git/nltk/nltk/sem/relextract.py_clause,"def clause(reldict, relsym):
""""""
Print the relation in clausal form.
:param reldict: a relation dictionary
:type reldict: defaultdict
:param relsym: a label for the relation
:type relsym: str
""""""
items = (relsym, reldict[""subjsym""], reldict[""objsym""])
return ""%s(%r, %r)"" % items
",[],0,[],/sem/relextract.py_clause
2256,/home/amandapotts/git/nltk/nltk/sem/relextract.py_in_demo,"def in_demo(trace=0, sql=True):
""""""
Select pairs of organizations and locations whose mentions occur with an
intervening occurrence of the preposition ""in"".
If the sql parameter is set to True, then the entity pairs are loaded into
an in-memory database, and subsequently pulled out using an SQL ""SELECT""
query.
""""""
from nltk.corpus import ieer
if sql:
try:
import sqlite3
connection = sqlite3.connect("":memory:"")
cur = connection.cursor()
cur.execute(
""""""create table Locations
(OrgName text, LocationName text, DocID text)""""""
)
except ImportError:
import warnings
warnings.warn(""Cannot import sqlite
IN = re.compile(r"".*\bin\b(?!\b.+ing)"")
print()
print(""IEER: in(ORG, LOC) -- just the clauses:"")
print(""="" * 45)
for file in ieer.fileids():
for doc in ieer.parsed_docs(file):
if trace:
print(doc.docno)
print(""="" * 15)
for rel in extract_rels(""ORG"", ""LOC"", doc, corpus=""ieer"", pattern=IN):
print(clause(rel, relsym=""IN""))
if sql:
try:
rtuple = (rel[""subjtext""], rel[""objtext""], doc.docno)
cur.execute(
""""""insert into Locations
values (?, ?, ?)"""""",
rtuple,
)
connection.commit()
except NameError:
pass
if sql:
try:
cur.execute(
""""""select OrgName from Locations
where LocationName = 'Atlanta'""""""
)
print()
print(""Extract data from SQL table: ORGs in Atlanta"")
print(""-"" * 15)
for row in cur:
print(row)
except NameError:
pass
",[],0,[],/sem/relextract.py_in_demo
2257,/home/amandapotts/git/nltk/nltk/sem/relextract.py_roles_demo,"def roles_demo(trace=0):
from nltk.corpus import ieer
roles = r""""""
(.*(                   # assorted roles
analyst|
chair(wo)?man|
commissioner|
counsel|
director|
economist|
editor|
executive|
foreman|
governor|
head|
lawyer|
leader|
librarian).*)|
manager|
partner|
president|
producer|
professor|
researcher|
spokes(wo)?man|
writer|
,\sof\sthe?\s*  # ""X, of (the) Y""
""""""
ROLES = re.compile(roles, re.VERBOSE)
print()
print(""IEER: has_role(PER, ORG) -- raw rtuples:"")
print(""="" * 45)
for file in ieer.fileids():
for doc in ieer.parsed_docs(file):
lcon = rcon = False
if trace:
print(doc.docno)
print(""="" * 15)
lcon = rcon = True
for rel in extract_rels(""PER"", ""ORG"", doc, corpus=""ieer"", pattern=ROLES):
print(rtuple(rel, lcon=lcon, rcon=rcon))
",[],0,[],/sem/relextract.py_roles_demo
2258,/home/amandapotts/git/nltk/nltk/sem/relextract.py_ieer_headlines,"def ieer_headlines():
from nltk.corpus import ieer
from nltk.tree import Tree
print(""IEER: First 20 Headlines"")
print(""="" * 45)
trees = [
(doc.docno, doc.headline)
for file in ieer.fileids()
for doc in ieer.parsed_docs(file)
]
for tree in trees[:20]:
print()
print(""%s:\n%s"" % tree)
",[],0,[],/sem/relextract.py_ieer_headlines
2259,/home/amandapotts/git/nltk/nltk/sem/relextract.py_conllned,"def conllned(trace=1):
""""""
Find the copula+'van' relation ('of') in the Dutch tagged training corpus
from CoNLL 2002.
""""""
from nltk.corpus import conll2002
vnv = """"""
(
is/V|    # 3rd sing present and
was/V|   # past forms of the verb zijn ('be')
werd/V|  # and also present
wordt/V  # past of worden ('become)
)
.*       # followed by anything
van/Prep # followed by van ('of')
""""""
VAN = re.compile(vnv, re.VERBOSE)
print()
print(""Dutch CoNLL2002: van(PER, ORG) -- raw rtuples with context:"")
print(""="" * 45)
for doc in conll2002.chunked_sents(""ned.train""):
lcon = rcon = False
if trace:
lcon = rcon = True
for rel in extract_rels(
""PER"", ""ORG"", doc, corpus=""conll2002"", pattern=VAN, window=10
):
print(rtuple(rel, lcon=lcon, rcon=rcon))
",[],0,[],/sem/relextract.py_conllned
2260,/home/amandapotts/git/nltk/nltk/sem/relextract.py_conllesp,"def conllesp():
from nltk.corpus import conll2002
de = """"""
.*
(
de/SP|
del/SP
)
""""""
DE = re.compile(de, re.VERBOSE)
print()
print(""Spanish CoNLL2002: de(ORG, LOC) -- just the first 10 clauses:"")
print(""="" * 45)
rels = [
rel
for doc in conll2002.chunked_sents(""esp.train"")
for rel in extract_rels(""ORG"", ""LOC"", doc, corpus=""conll2002"", pattern=DE)
]
for r in rels[:10]:
print(clause(r, relsym=""DE""))
print()
",[],0,[],/sem/relextract.py_conllesp
2261,/home/amandapotts/git/nltk/nltk/sem/relextract.py_ne_chunked,"def ne_chunked():
print()
print(""1500 Sentences from Penn Treebank, as processed by NLTK NE Chunker"")
print(""="" * 45)
ROLE = re.compile(
r"".*(chairman|president|trader|scientist|economist|analyst|partner).*""
)
rels = []
for i, sent in enumerate(nltk.corpus.treebank.tagged_sents()[:1500]):
sent = nltk.ne_chunk(sent)
rels = extract_rels(""PER"", ""ORG"", sent, corpus=""ace"", pattern=ROLE, window=7)
for rel in rels:
print(f""{i:<5}{rtuple(rel)}"")
",[],0,[],/sem/relextract.py_ne_chunked
2262,/home/amandapotts/git/nltk/nltk/sem/lfg.py_safeappend,"def safeappend(self, key, item):
""""""
Append 'item' to the list at 'key'.  If no list exists for 'key', then
construct one.
""""""
if key not in self:
self[key] = []
self[key].append(item)
",[],0,[],/sem/lfg.py_safeappend
2263,/home/amandapotts/git/nltk/nltk/sem/lfg.py___setitem__,"def __setitem__(self, key, value):
dict.__setitem__(self, key.lower(), value)
",[],0,[],/sem/lfg.py___setitem__
2264,/home/amandapotts/git/nltk/nltk/sem/lfg.py___getitem__,"def __getitem__(self, key):
return dict.__getitem__(self, key.lower())
",[],0,[],/sem/lfg.py___getitem__
2265,/home/amandapotts/git/nltk/nltk/sem/lfg.py___contains__,"def __contains__(self, key):
return dict.__contains__(self, key.lower())
",[],0,[],/sem/lfg.py___contains__
2266,/home/amandapotts/git/nltk/nltk/sem/lfg.py_to_glueformula_list,"def to_glueformula_list(self, glue_dict):
depgraph = self.to_depgraph()
return glue_dict.to_glueformula_list(depgraph)
",[],0,[],/sem/lfg.py_to_glueformula_list
2267,/home/amandapotts/git/nltk/nltk/sem/lfg.py_to_depgraph,"def to_depgraph(self, rel=None):
from nltk.parse.dependencygraph import DependencyGraph
depgraph = DependencyGraph()
nodes = depgraph.nodes
self._to_depgraph(nodes, 0, ""ROOT"")
for address, node in nodes.items():
for n2 in (n for n in nodes.values() if n[""rel""] != ""TOP""):
if n2[""head""] == address:
relation = n2[""rel""]
node[""deps""].setdefault(relation, [])
node[""deps""][relation].append(n2[""address""])
depgraph.root = nodes[1]
return depgraph
",[],0,[],/sem/lfg.py_to_depgraph
2268,/home/amandapotts/git/nltk/nltk/sem/lfg.py__to_depgraph,"def _to_depgraph(self, nodes, head, rel):
index = len(nodes)
nodes[index].update(
{
""address"": index,
""word"": self.pred[0],
""tag"": self.pred[1],
""head"": head,
""rel"": rel,
}
)
for feature in sorted(self):
for item in sorted(self[feature]):
if isinstance(item, FStructure):
item._to_depgraph(nodes, index, feature)
elif isinstance(item, tuple):
new_index = len(nodes)
nodes[new_index].update(
{
""address"": new_index,
""word"": item[0],
""tag"": item[1],
""head"": index,
""rel"": feature,
}
)
elif isinstance(item, list):
for n in item:
n._to_depgraph(nodes, index, feature)
else:
raise Exception(
""feature %s is not an FStruct, a list, or a tuple"" % feature
)
",[],0,[],/sem/lfg.py__to_depgraph
2269,/home/amandapotts/git/nltk/nltk/sem/lfg.py_read_depgraph,"def read_depgraph(depgraph):
return FStructure._read_depgraph(depgraph.root, depgraph)
",[],0,[],/sem/lfg.py_read_depgraph
2270,/home/amandapotts/git/nltk/nltk/sem/lfg.py__read_depgraph,"def _read_depgraph(node, depgraph, label_counter=None, parent=None):
if not label_counter:
label_counter = Counter()
if node[""rel""].lower() in [""spec"", ""punct""]:
return (node[""word""], node[""tag""])
else:
fstruct = FStructure()
fstruct.pred = None
fstruct.label = FStructure._make_label(label_counter.get())
fstruct.parent = parent
word, tag = node[""word""], node[""tag""]
if tag[:2] == ""VB"":
if tag[2:3] == ""D"":
fstruct.safeappend(""tense"", (""PAST"", ""tense""))
fstruct.pred = (word, tag[:2])
if not fstruct.pred:
fstruct.pred = (word, tag)
children = [
depgraph.nodes[idx]
for idx in chain.from_iterable(node[""deps""].values())
]
for child in children:
fstruct.safeappend(
child[""rel""],
FStructure._read_depgraph(child, depgraph, label_counter, fstruct),
)
return fstruct
",[],0,[],/sem/lfg.py__read_depgraph
2271,/home/amandapotts/git/nltk/nltk/sem/lfg.py__make_label,"def _make_label(value):
""""""
Pick an alphabetic character as identifier for an entity in the model.
:param value: where to index into the list of characters
:type value: int
""""""
letter = [
""f"",
""g"",
""h"",
""i"",
""j"",
""k"",
""l"",
""m"",
""n"",
""o"",
""p"",
""q"",
""r"",
""s"",
""t"",
""u"",
""v"",
""w"",
""x"",
""y"",
""z"",
""a"",
""b"",
""c"",
""d"",
""e"",
][value - 1]
num = int(value) // 26
if num > 0:
return letter + str(num)
else:
return letter
",[],0,[],/sem/lfg.py__make_label
2272,/home/amandapotts/git/nltk/nltk/sem/lfg.py___repr__,"def __repr__(self):
return self.__str__().replace(""\n"", """")
",[],0,[],/sem/lfg.py___repr__
2273,/home/amandapotts/git/nltk/nltk/sem/lfg.py___str__,"def __str__(self):
return self.pretty_format()
",[],0,[],/sem/lfg.py___str__
2274,/home/amandapotts/git/nltk/nltk/sem/lfg.py_pretty_format,"def pretty_format(self, indent=3):
try:
accum = ""%s:["" % self.label
except NameError:
accum = ""[""
try:
accum += ""pred '%s'"" % (self.pred[0])
except NameError:
pass
for feature in sorted(self):
for item in self[feature]:
if isinstance(item, FStructure):
next_indent = indent + len(feature) + 3 + len(self.label)
accum += ""\n{}{} {}"".format(
"" "" * (indent),
feature,
item.pretty_format(next_indent),
)
elif isinstance(item, tuple):
accum += ""\n{}{} '{}'"".format("" "" * (indent), feature, item[0])
elif isinstance(item, list):
accum += ""\n{}{} {{{}}}"".format(
"" "" * (indent),
feature,
(""\n%s"" % ("" "" * (indent + len(feature) + 2))).join(item),
)
else:  # ERROR
raise Exception(
""feature %s is not an FStruct, a list, or a tuple"" % feature
)
return accum + ""]""
",[],0,[],/sem/lfg.py_pretty_format
2275,/home/amandapotts/git/nltk/nltk/sem/lfg.py_demo_read_depgraph,"def demo_read_depgraph():
from nltk.parse.dependencygraph import DependencyGraph
dg1 = DependencyGraph(
""""""\
",[],0,[],/sem/lfg.py_demo_read_depgraph
2276,/home/amandapotts/git/nltk/nltk/sem/evaluate.py_trace,"def trace(f, *args, **kw):
argspec = inspect.getfullargspec(f)
d = dict(zip(argspec[0], args))
if d.pop(""trace"", None):
print()
for item in d.items():
print(""%s => %s"" % item)
return f(*args, **kw)
",[],0,[],/sem/evaluate.py_trace
2277,/home/amandapotts/git/nltk/nltk/sem/evaluate.py_is_rel,"def is_rel(s):
""""""
Check whether a set represents a relation (of any arity).
:param s: a set containing tuples of str elements
:type s: set
:rtype: bool
""""""
if len(s) == 0:
return True
elif all(isinstance(el, tuple) for el in s) and len(max(s)) == len(min(s)):
return True
else:
raise ValueError(""Set %r contains sequences of different lengths"" % s)
",[],0,[],/sem/evaluate.py_is_rel
2278,/home/amandapotts/git/nltk/nltk/sem/evaluate.py_set2rel,"def set2rel(s):
""""""
Convert a set containing individuals (strings or numbers) into a set of
unary tuples. Any tuples of strings already in the set are passed through
unchanged.
For example:
- set(['a', 'b']) => set([('a',), ('b',)])
- set([3, 27]) => set([('3',), ('27',)])
:type s: set
:rtype: set of tuple of str
""""""
new = set()
for elem in s:
if isinstance(elem, str):
new.add((elem,))
elif isinstance(elem, int):
new.add(str(elem))
else:
new.add(elem)
return new
",[],0,[],/sem/evaluate.py_set2rel
2279,/home/amandapotts/git/nltk/nltk/sem/evaluate.py_arity,"def arity(rel):
""""""
Check the arity of a relation.
:type rel: set of tuples
:rtype: int of tuple of str
""""""
if len(rel) == 0:
return 0
return len(list(rel)[0])
",[],0,[],/sem/evaluate.py_arity
2280,/home/amandapotts/git/nltk/nltk/sem/evaluate.py___init__,"def __init__(self, xs):
""""""
:param xs: a list of (symbol, value) pairs.
""""""
super().__init__()
for sym, val in xs:
if isinstance(val, str) or isinstance(val, bool):
self[sym] = val
elif isinstance(val, set):
self[sym] = set2rel(val)
else:
msg = textwrap.fill(
""Error in initializing Valuation. ""
""Unrecognized value for symbol '%s':\n%s"" % (sym, val),
width=66,
)
raise ValueError(msg)
",[],0,[],/sem/evaluate.py___init__
2281,/home/amandapotts/git/nltk/nltk/sem/evaluate.py___getitem__,"def __getitem__(self, key):
if key in self:
return dict.__getitem__(self, key)
else:
raise Undefined(""Unknown expression: '%s'"" % key)
",[],0,[],/sem/evaluate.py___getitem__
2282,/home/amandapotts/git/nltk/nltk/sem/evaluate.py___str__,"def __str__(self):
return pformat(self)
",[],0,[],/sem/evaluate.py___str__
2283,/home/amandapotts/git/nltk/nltk/sem/evaluate.py_domain,"def domain(self):
""""""Set-theoretic domain of the value-space of a Valuation.""""""
dom = []
for val in self.values():
if isinstance(val, str):
dom.append(val)
elif not isinstance(val, bool):
dom.extend(
[elem for tuple_ in val for elem in tuple_ if elem is not None]
)
return set(dom)
",[],0,[],/sem/evaluate.py_domain
2284,/home/amandapotts/git/nltk/nltk/sem/evaluate.py_symbols,"def symbols(self):
""""""The non-logical constants which the Valuation recognizes.""""""
return sorted(self.keys())
",[],0,[],/sem/evaluate.py_symbols
2285,/home/amandapotts/git/nltk/nltk/sem/evaluate.py_fromstring,"def fromstring(cls, s):
return read_valuation(s)
",[],0,[],/sem/evaluate.py_fromstring
2286,/home/amandapotts/git/nltk/nltk/sem/evaluate.py__read_valuation_line,"def _read_valuation_line(s):
""""""
Read a line in a valuation file.
Lines are expected to be of the form::
noosa => n
girl => {g1, g2}
chase => {(b1, g1), (b2, g1), (g1, d1), (g2, d2)}
:param s: input line
:type s: str
:return: a pair (symbol, value)
:rtype: tuple
""""""
pieces = _VAL_SPLIT_RE.split(s)
symbol = pieces[0]
value = pieces[1]
if value.startswith(""{""):
value = value[1:-1]
tuple_strings = _TUPLES_RE.findall(value)
if tuple_strings:
set_elements = []
for ts in tuple_strings:
ts = ts[1:-1]
element = tuple(_ELEMENT_SPLIT_RE.split(ts))
set_elements.append(element)
else:
set_elements = _ELEMENT_SPLIT_RE.split(value)
value = set(set_elements)
return symbol, value
",[],0,[],/sem/evaluate.py__read_valuation_line
2287,/home/amandapotts/git/nltk/nltk/sem/evaluate.py_read_valuation,"def read_valuation(s, encoding=None):
""""""
Convert a valuation string into a valuation.
:param s: a valuation string
:type s: str
:param encoding: the encoding of the input string, if it is binary
:type encoding: str
:return: a ``nltk.sem`` valuation
:rtype: Valuation
""""""
if encoding is not None:
s = s.decode(encoding)
statements = []
for linenum, line in enumerate(s.splitlines()):
line = line.strip()
if line.startswith(""#"") or line == """":
continue
try:
statements.append(_read_valuation_line(line))
except ValueError as e:
raise ValueError(f""Unable to parse line {linenum}: {line}"") from e
return Valuation(statements)
",[],0,[],/sem/evaluate.py_read_valuation
2288,/home/amandapotts/git/nltk/nltk/sem/evaluate.py___init__,"def __init__(self, domain, assign=None):
super().__init__()
self.domain = domain
if assign:
for var, val in assign:
assert val in self.domain, ""'{}' is not in the domain: {}"".format(
val,
self.domain,
)
assert is_indvar(var), (
""Wrong format for an Individual Variable: '%s'"" % var
)
self[var] = val
self.variant = None
self._addvariant()
",[],0,[],/sem/evaluate.py___init__
2289,/home/amandapotts/git/nltk/nltk/sem/evaluate.py___getitem__,"def __getitem__(self, key):
if key in self:
return dict.__getitem__(self, key)
else:
raise Undefined(""Not recognized as a variable: '%s'"" % key)
",[],0,[],/sem/evaluate.py___getitem__
2290,/home/amandapotts/git/nltk/nltk/sem/evaluate.py_copy,"def copy(self):
new = Assignment(self.domain)
new.update(self)
return new
",[],0,[],/sem/evaluate.py_copy
2291,/home/amandapotts/git/nltk/nltk/sem/evaluate.py_purge,"def purge(self, var=None):
""""""
Remove one or all keys (i.e. logic variables) from an
assignment, and update ``self.variant``.
:param var: a Variable acting as a key for the assignment.
""""""
if var:
del self[var]
else:
self.clear()
self._addvariant()
return None
",[],0,[],/sem/evaluate.py_purge
2292,/home/amandapotts/git/nltk/nltk/sem/evaluate.py___str__,"def __str__(self):
""""""
Pretty printing for assignments. {'x', 'u'} appears as 'g[u/x]'
""""""
gstring = ""g""
variant = sorted(self.variant)
for val, var in variant:
gstring += f""[{val}/{var}]""
return gstring
",[],0,[],/sem/evaluate.py___str__
2293,/home/amandapotts/git/nltk/nltk/sem/evaluate.py__addvariant,"def _addvariant(self):
""""""
Create a more pretty-printable version of the assignment.
""""""
list_ = []
for item in self.items():
pair = (item[1], item[0])
list_.append(pair)
self.variant = list_
return None
",[],0,[],/sem/evaluate.py__addvariant
2294,/home/amandapotts/git/nltk/nltk/sem/evaluate.py_add,"def add(self, var, val):
""""""
Add a new variable-value pair to the assignment, and update
``self.variant``.
""""""
assert val in self.domain, f""{val} is not in the domain {self.domain}""
assert is_indvar(var), ""Wrong format for an Individual Variable: '%s'"" % var
self[var] = val
self._addvariant()
return self
",[],0,[],/sem/evaluate.py_add
2295,/home/amandapotts/git/nltk/nltk/sem/evaluate.py___init__,"def __init__(self, domain, valuation):
assert isinstance(domain, set)
self.domain = domain
self.valuation = valuation
if not domain.issuperset(valuation.domain):
raise Error(
""The valuation domain, %s, must be a subset of the model's domain, %s""
% (valuation.domain, domain)
)
",[],0,[],/sem/evaluate.py___init__
2296,/home/amandapotts/git/nltk/nltk/sem/evaluate.py___repr__,"def __repr__(self):
return f""({self.domain!r}, {self.valuation!r})""
",[],0,[],/sem/evaluate.py___repr__
2297,/home/amandapotts/git/nltk/nltk/sem/evaluate.py___str__,"def __str__(self):
return f""Domain = {self.domain},\nValuation = \n{self.valuation}""
",[],0,[],/sem/evaluate.py___str__
2298,/home/amandapotts/git/nltk/nltk/sem/evaluate.py_evaluate,"def evaluate(self, expr, g, trace=None):
""""""
Read input expressions, and provide a handler for ``satisfy``
that blocks further propagation of the ``Undefined`` error.
:param expr: An ``Expression`` of ``logic``.
:type g: Assignment
:param g: an assignment to individual variables.
:rtype: bool or 'Undefined'
""""""
try:
parsed = Expression.fromstring(expr)
value = self.satisfy(parsed, g, trace=trace)
if trace:
print()
print(f""'{expr}' evaluates to {value} under M, {g}"")
return value
except Undefined:
if trace:
print()
print(f""'{expr}' is undefined under M, {g}"")
return ""Undefined""
",[],0,[],/sem/evaluate.py_evaluate
2299,/home/amandapotts/git/nltk/nltk/sem/evaluate.py_i,"def i(self, parsed, g, trace=False):
""""""
An interpretation function.
Assuming that ``parsed`` is atomic:
- if ``parsed`` is a non-logical constant, calls the valuation *V*
- else if ``parsed`` is an individual variable, calls assignment *g*
- else returns ``Undefined``.
:param parsed: an ``Expression`` of ``logic``.
:type g: Assignment
:param g: an assignment to individual variables.
:return: a semantic value
""""""
if parsed.variable.name in self.valuation.symbols:
return self.valuation[parsed.variable.name]
elif isinstance(parsed, IndividualVariableExpression):
return g[parsed.variable.name]
else:
raise Undefined(""Can't find a value for %s"" % parsed)
",[],0,[],/sem/evaluate.py_i
2300,/home/amandapotts/git/nltk/nltk/sem/evaluate.py_satisfiers,"def satisfiers(self, parsed, varex, g, trace=None, nesting=0):
""""""
Generate the entities from the model's domain that satisfy an open formula.
:param parsed: an open formula
:type parsed: Expression
:param varex: the relevant free individual variable in ``parsed``.
:type varex: VariableExpression or str
:param g: a variable assignment
:type g:  Assignment
:return: a set of the entities that satisfy ``parsed``.
""""""
spacer = ""   ""
indent = spacer + (spacer * nesting)
candidates = []
if isinstance(varex, str):
var = Variable(varex)
else:
var = varex
if var in parsed.free():
if trace:
print()
print(
(spacer * nesting)
+ f""Open formula is '{parsed}' with assignment {g}""
)
for u in self.domain:
new_g = g.copy()
new_g.add(var.name, u)
if trace and trace > 1:
lowtrace = trace - 1
else:
lowtrace = 0
value = self.satisfy(parsed, new_g, lowtrace)
if trace:
print(indent + ""(trying assignment %s)"" % new_g)
if value == False:
if trace:
print(indent + f""value of '{parsed}' under {new_g} is False"")
else:
candidates.append(u)
if trace:
print(indent + f""value of '{parsed}' under {new_g} is {value}"")
result = {c for c in candidates}
else:
raise Undefined(f""{var.name} is not free in {parsed}"")
return result
",[],0,[],/sem/evaluate.py_satisfiers
2301,/home/amandapotts/git/nltk/nltk/sem/evaluate.py_propdemo,"def propdemo(trace=None):
""""""Example of a propositional model.""""""
global val1, dom1, m1, g1
val1 = Valuation([(""P"", True), (""Q"", True), (""R"", False)])
dom1 = set()
m1 = Model(dom1, val1)
g1 = Assignment(dom1)
print()
print(""*"" * mult)
print(""Propositional Formulas Demo"")
print(""*"" * mult)
print(""(Propositional constants treated as nullary predicates)"")
print()
print(""Model m1:\n"", m1)
print(""*"" * mult)
sentences = [
""(P & Q)"",
""(P & R)"",
""- P"",
""- R"",
""- - P"",
""- (P & R)"",
""(P | R)"",
""(R | P)"",
""(R | R)"",
""(- P | R)"",
""(P | - P)"",
""(P -> Q)"",
""(P -> R)"",
""(R -> P)"",
""(P <-> P)"",
""(R <-> R)"",
""(P <-> R)"",
]
for sent in sentences:
if trace:
print()
m1.evaluate(sent, g1, trace)
else:
print(f""The value of '{sent}' is: {m1.evaluate(sent, g1)}"")
",[],0,[],/sem/evaluate.py_propdemo
2302,/home/amandapotts/git/nltk/nltk/sem/evaluate.py_folmodel,"def folmodel(quiet=False, trace=None):
""""""Example of a first-order model.""""""
global val2, v2, dom2, m2, g2
v2 = [
(""adam"", ""b1""),
(""betty"", ""g1""),
(""fido"", ""d1""),
(""girl"", {""g1"", ""g2""}),
(""boy"", {""b1"", ""b2""}),
(""dog"", {""d1""}),
(""love"", {(""b1"", ""g1""), (""b2"", ""g2""), (""g1"", ""b1""), (""g2"", ""b1"")}),
]
val2 = Valuation(v2)
dom2 = val2.domain
m2 = Model(dom2, val2)
g2 = Assignment(dom2, [(""x"", ""b1""), (""y"", ""g2"")])
if not quiet:
print()
print(""*"" * mult)
print(""Models Demo"")
print(""*"" * mult)
print(""Model m2:\n"", ""-"" * 14, ""\n"", m2)
print(""Variable assignment = "", g2)
exprs = [""adam"", ""boy"", ""love"", ""walks"", ""x"", ""y"", ""z""]
parsed_exprs = [Expression.fromstring(e) for e in exprs]
print()
for parsed in parsed_exprs:
try:
print(
""The interpretation of '%s' in m2 is %s""
% (parsed, m2.i(parsed, g2))
)
except Undefined:
print(""The interpretation of '%s' in m2 is Undefined"" % parsed)
applications = [
(""boy"", (""adam"")),
(""walks"", (""adam"",)),
(""love"", (""adam"", ""y"")),
(""love"", (""y"", ""adam"")),
]
for fun, args in applications:
try:
funval = m2.i(Expression.fromstring(fun), g2)
argsval = tuple(m2.i(Expression.fromstring(arg), g2) for arg in args)
print(f""{fun}({args}) evaluates to {argsval in funval}"")
except Undefined:
print(f""{fun}({args}) evaluates to Undefined"")
",[],0,[],/sem/evaluate.py_folmodel
2303,/home/amandapotts/git/nltk/nltk/sem/evaluate.py_foldemo,"def foldemo(trace=None):
""""""
Interpretation of closed expressions in a first-order model.
""""""
folmodel(quiet=True)
print()
print(""*"" * mult)
print(""FOL Formulas Demo"")
print(""*"" * mult)
formulas = [
""love (adam, betty)"",
""(adam = mia)"",
""\\x. (boy(x) | girl(x))"",
""\\x. boy(x)(adam)"",
""\\x y. love(x, y)"",
""\\x y. love(x, y)(adam)(betty)"",
""\\x y. love(x, y)(adam, betty)"",
""\\x y. (boy(x) & love(x, y))"",
""\\x. exists y. (boy(x) & love(x, y))"",
""exists z1. boy(z1)"",
""exists x. (boy(x) &  -(x = adam))"",
""exists x. (boy(x) & all y. love(y, x))"",
""all x. (boy(x) | girl(x))"",
""all x. (girl(x) -> exists y. boy(y) & love(x, y))"",  # Every girl loves exists boy.
""exists x. (boy(x) & all y. (girl(y) -> love(y, x)))"",  # There is exists boy that every girl loves.
""exists x. (boy(x) & all y. (girl(y) -> love(x, y)))"",  # exists boy loves every girl.
""all x. (dog(x) -> - girl(x))"",
""exists x. exists y. (love(x, y) & love(x, y))"",
]
for fmla in formulas:
g2.purge()
if trace:
m2.evaluate(fmla, g2, trace)
else:
print(f""The value of '{fmla}' is: {m2.evaluate(fmla, g2)}"")
",[],0,[],/sem/evaluate.py_foldemo
2304,/home/amandapotts/git/nltk/nltk/sem/evaluate.py_satdemo,"def satdemo(trace=None):
""""""Satisfiers of an open formula in a first order model.""""""
print()
print(""*"" * mult)
print(""Satisfiers Demo"")
print(""*"" * mult)
folmodel(quiet=True)
formulas = [
""boy(x)"",
""(x = x)"",
""(boy(x) | girl(x))"",
""(boy(x) & girl(x))"",
""love(adam, x)"",
""love(x, adam)"",
""-(x = adam)"",
""exists z22. love(x, z22)"",
""exists y. love(y, x)"",
""all y. (girl(y) -> love(x, y))"",
""all y. (girl(y) -> love(y, x))"",
""all y. (girl(y) -> (boy(x) & love(y, x)))"",
""(boy(x) & all y. (girl(y) -> love(x, y)))"",
""(boy(x) & all y. (girl(y) -> love(y, x)))"",
""(boy(x) & exists y. (girl(y) & love(y, x)))"",
""(girl(x) -> dog(x))"",
""all y. (dog(y) -> (x = y))"",
""exists y. love(y, x)"",
""exists y. (love(adam, y) & love(y, x))"",
]
if trace:
print(m2)
for fmla in formulas:
print(fmla)
Expression.fromstring(fmla)
parsed = [Expression.fromstring(fmla) for fmla in formulas]
for p in parsed:
g2.purge()
print(
""The satisfiers of '{}' are: {}"".format(p, m2.satisfiers(p, ""x"", g2, trace))
)
",[],0,[],/sem/evaluate.py_satdemo
2305,/home/amandapotts/git/nltk/nltk/sem/evaluate.py_demo,"def demo(num=0, trace=None):
""""""
Run exists demos.
- num = 1: propositional logic demo
- num = 2: first order model demo (only if trace is set)
- num = 3: first order sentences demo
- num = 4: satisfaction of open formulas demo
- any other value: run all the demos
:param trace: trace = 1, or trace = 2 for more verbose tracing
""""""
demos = {1: propdemo, 2: folmodel, 3: foldemo, 4: satdemo}
try:
demos[num](trace=trace)
except KeyError:
for num in demos:
demos[num](trace=trace)
",[],0,[],/sem/evaluate.py_demo
2306,/home/amandapotts/git/nltk/nltk/sem/util.py_parse_sents,"def parse_sents(inputs, grammar, trace=0):
""""""
Convert input sentences into syntactic trees.
:param inputs: sentences to be parsed
:type inputs: list(str)
:param grammar: ``FeatureGrammar`` or name of feature-based grammar
:type grammar: nltk.grammar.FeatureGrammar
:rtype: list(nltk.tree.Tree) or dict(list(str)): list(Tree)
:return: a mapping from input sentences to a list of ``Tree`` instances.
""""""
from nltk.grammar import FeatureGrammar
from nltk.parse import FeatureChartParser, load_parser
if isinstance(grammar, FeatureGrammar):
cp = FeatureChartParser(grammar)
else:
cp = load_parser(grammar, trace=trace)
parses = []
for sent in inputs:
tokens = sent.split()  # use a tokenizer?
syntrees = list(cp.parse(tokens))
parses.append(syntrees)
return parses
",[],0,[],/sem/util.py_parse_sents
2307,/home/amandapotts/git/nltk/nltk/sem/util.py_root_semrep,"def root_semrep(syntree, semkey=""SEM""):
""""""
Find the semantic representation at the root of a tree.
:param syntree: a parse ``Tree``
:param semkey: the feature label to use for the root semantics in the tree
:return: the semantic representation at the root of a ``Tree``
:rtype: sem.Expression
""""""
from nltk.grammar import FeatStructNonterminal
node = syntree.label()
assert isinstance(node, FeatStructNonterminal)
try:
return node[semkey]
except KeyError:
print(node, end="" "")
print(""has no specification for the feature %s"" % semkey)
raise
",[],0,[],/sem/util.py_root_semrep
2308,/home/amandapotts/git/nltk/nltk/sem/util.py_interpret_sents,"def interpret_sents(inputs, grammar, semkey=""SEM"", trace=0):
""""""
Add the semantic representation to each syntactic parse tree
of each input sentence.
:param inputs: a list of sentences
:type inputs: list(str)
:param grammar: ``FeatureGrammar`` or name of feature-based grammar
:type grammar: nltk.grammar.FeatureGrammar
:return: a mapping from sentences to lists of pairs (parse-tree, semantic-representations)
:rtype: list(list(tuple(nltk.tree.Tree, nltk.sem.logic.ConstantExpression)))
""""""
return [
[(syn, root_semrep(syn, semkey)) for syn in syntrees]
for syntrees in parse_sents(inputs, grammar, trace=trace)
]
",[],0,[],/sem/util.py_interpret_sents
2309,/home/amandapotts/git/nltk/nltk/sem/util.py_evaluate_sents,"def evaluate_sents(inputs, grammar, model, assignment, trace=0):
""""""
Add the truth-in-a-model value to each semantic representation
for each syntactic parse of each input sentences.
:param inputs: a list of sentences
:type inputs: list(str)
:param grammar: ``FeatureGrammar`` or name of feature-based grammar
:type grammar: nltk.grammar.FeatureGrammar
:return: a mapping from sentences to lists of triples (parse-tree, semantic-representations, evaluation-in-model)
:rtype: list(list(tuple(nltk.tree.Tree, nltk.sem.logic.ConstantExpression, bool or dict(str): bool)))
""""""
return [
[
(syn, sem, model.evaluate(""%s"" % sem, assignment, trace=trace))
for (syn, sem) in interpretations
]
for interpretations in interpret_sents(inputs, grammar)
]
",[],0,[],/sem/util.py_evaluate_sents
2310,/home/amandapotts/git/nltk/nltk/sem/util.py_demo_model0,"def demo_model0():
global m0, g0
v = [
(""john"", ""b1""),
(""mary"", ""g1""),
(""suzie"", ""g2""),
(""fido"", ""d1""),
(""tess"", ""d2""),
(""noosa"", ""n""),
(""girl"", {""g1"", ""g2""}),
(""boy"", {""b1"", ""b2""}),
(""dog"", {""d1"", ""d2""}),
(""bark"", {""d1"", ""d2""}),
(""walk"", {""b1"", ""g2"", ""d1""}),
(""chase"", {(""b1"", ""g1""), (""b2"", ""g1""), (""g1"", ""d1""), (""g2"", ""d2"")}),
(
""see"",
{(""b1"", ""g1""), (""b2"", ""d2""), (""g1"", ""b1""), (""d2"", ""b1""), (""g2"", ""n"")},
),
(""in"", {(""b1"", ""n""), (""b2"", ""n""), (""d2"", ""n"")}),
(""with"", {(""b1"", ""g1""), (""g1"", ""b1""), (""d1"", ""b1""), (""b1"", ""d1"")}),
]
val = evaluate.Valuation(v)
dom = val.domain
m0 = evaluate.Model(dom, val)
g0 = evaluate.Assignment(dom)
",[],0,[],/sem/util.py_demo_model0
2311,/home/amandapotts/git/nltk/nltk/sem/util.py_read_sents,"def read_sents(filename, encoding=""utf8""):
with codecs.open(filename, ""r"", encoding) as fp:
sents = [l.rstrip() for l in fp]
sents = [l for l in sents if len(l) > 0]
sents = [l for l in sents if not l[0] == ""#""]
return sents
",[],0,[],/sem/util.py_read_sents
2312,/home/amandapotts/git/nltk/nltk/sem/util.py_demo_legacy_grammar,"def demo_legacy_grammar():
""""""
Check that interpret_sents() is compatible with legacy grammars that use
a lowercase 'sem' feature.
Define 'test.fcfg' to be the following
""""""
from nltk.grammar import FeatureGrammar
g = FeatureGrammar.fromstring(
""""""
% start S
S[sem=<hello>] -> 'hello'
""""""
)
print(""Reading grammar: %s"" % g)
print(""*"" * 20)
for reading in interpret_sents([""hello""], g, semkey=""sem""):
syn, sem = reading[0]
print()
print(""output: "", sem)
",[],0,[],/sem/util.py_demo_legacy_grammar
2313,/home/amandapotts/git/nltk/nltk/sem/util.py_demo,"def demo():
import sys
from optparse import OptionParser
description = """"""
Parse and evaluate some sentences.
""""""
opts = OptionParser(description=description)
opts.set_defaults(
evaluate=True,
beta=True,
syntrace=0,
semtrace=0,
demo=""default"",
grammar="""",
sentences="""",
)
opts.add_option(
""-d"",
""--demo"",
dest=""demo"",
help=""choose demo D
metavar=""D"",
)
opts.add_option(
""-g"", ""--gram"", dest=""grammar"", help=""read in grammar G"", metavar=""G""
)
opts.add_option(
""-m"",
""--model"",
dest=""model"",
help=""import model M (omit '.py' suffix)"",
metavar=""M"",
)
opts.add_option(
""-s"",
""--sentences"",
dest=""sentences"",
help=""read in a file of test sentences S"",
metavar=""S"",
)
opts.add_option(
""-e"",
""--no-eval"",
action=""store_false"",
dest=""evaluate"",
help=""just do a syntactic analysis"",
)
opts.add_option(
""-b"",
""--no-beta-reduction"",
action=""store_false"",
dest=""beta"",
help=""don't carry out beta-reduction"",
)
opts.add_option(
""-t"",
""--syntrace"",
action=""count"",
dest=""syntrace"",
help=""set syntactic tracing on
)
opts.add_option(
""-T"",
""--semtrace"",
action=""count"",
dest=""semtrace"",
help=""set semantic tracing on"",
)
(options, args) = opts.parse_args()
SPACER = ""-"" * 30
demo_model0()
sents = [
""Fido sees a boy with Mary"",
""John sees Mary"",
""every girl chases a dog"",
""every boy chases a girl"",
""John walks with a girl in Noosa"",
""who walks"",
]
gramfile = ""grammars/sample_grammars/sem2.fcfg""
if options.sentences:
sentsfile = options.sentences
if options.grammar:
gramfile = options.grammar
if options.model:
exec(""import %s as model"" % options.model)
if sents is None:
sents = read_sents(sentsfile)
model = m0
g = g0
if options.evaluate:
evaluations = evaluate_sents(sents, gramfile, model, g, trace=options.semtrace)
else:
semreps = interpret_sents(sents, gramfile, trace=options.syntrace)
for i, sent in enumerate(sents):
n = 1
print(""\nSentence: %s"" % sent)
print(SPACER)
if options.evaluate:
for syntree, semrep, value in evaluations[i]:
if isinstance(value, dict):
value = set(value.keys())
print(""%d:  %s"" % (n, semrep))
print(value)
n += 1
else:
for syntree, semrep in semreps[i]:
print(""%d:  %s"" % (n, semrep))
n += 1
",[],0,[],/sem/util.py_demo
2314,/home/amandapotts/git/nltk/nltk/sem/drt_glue_demo.py___init__,"def __init__(self, examples):
self._top = Tk()
self._top.title(""DRT Glue Demo"")
self._init_bindings()
self._init_fonts(self._top)
self._examples = examples
self._readingCache = [None for example in examples]
self._show_grammar = IntVar(self._top)
self._show_grammar.set(1)
self._curExample = -1
self._readings = []
self._drs = None
self._drsWidget = None
self._error = None
self._init_glue()
self._init_menubar(self._top)
self._init_buttons(self._top)
self._init_exampleListbox(self._top)
self._init_readingListbox(self._top)
self._init_canvas(self._top)
self._canvas.bind(""<Configure>"", self._configure)
",[],0,[],/sem/drt_glue_demo.py___init__
2315,/home/amandapotts/git/nltk/nltk/sem/drt_glue_demo.py__init_glue,"def _init_glue(self):
tagger = RegexpTagger(
[
(""^(David|Mary|John)$"", ""NNP""),
(
""^(walks|sees|eats|chases|believes|gives|sleeps|chases|persuades|tries|seems|leaves)$"",
""VB"",
),
(""^(go|order|vanish|find|approach)$"", ""VB""),
(""^(a)$"", ""ex_quant""),
(""^(every)$"", ""univ_quant""),
(""^(sandwich|man|dog|pizza|unicorn|cat|senator)$"", ""NN""),
(""^(big|gray|former)$"", ""JJ""),
(""^(him|himself)$"", ""PRP""),
]
)
depparser = MaltParser(tagger=tagger)
self._glue = DrtGlue(depparser=depparser, remove_duplicates=False)
",[],0,[],/sem/drt_glue_demo.py__init_glue
2316,/home/amandapotts/git/nltk/nltk/sem/drt_glue_demo.py__init_fonts,"def _init_fonts(self, root):
self._sysfont = Font(font=Button()[""font""])
root.option_add(""*Font"", self._sysfont)
self._size = IntVar(root)
self._size.set(self._sysfont.cget(""size""))
self._boldfont = Font(family=""helvetica"", weight=""bold"", size=self._size.get())
self._font = Font(family=""helvetica"", size=self._size.get())
if self._size.get() < 0:
big = self._size.get() - 2
else:
big = self._size.get() + 2
self._bigfont = Font(family=""helvetica"", weight=""bold"", size=big)
",[],0,[],/sem/drt_glue_demo.py__init_fonts
2317,/home/amandapotts/git/nltk/nltk/sem/drt_glue_demo.py__init_exampleListbox,"def _init_exampleListbox(self, parent):
self._exampleFrame = listframe = Frame(parent)
self._exampleFrame.pack(fill=""both"", side=""left"", padx=2)
self._exampleList_label = Label(
self._exampleFrame, font=self._boldfont, text=""Examples""
)
self._exampleList_label.pack()
self._exampleList = Listbox(
self._exampleFrame,
selectmode=""single"",
relief=""groove"",
background=""white"",
foreground=""#909090"",
font=self._font,
selectforeground=""#004040"",
selectbackground=""#c0f0c0"",
)
self._exampleList.pack(side=""right"", fill=""both"", expand=1)
for example in self._examples:
self._exampleList.insert(""end"", (""  %s"" % example))
self._exampleList.config(height=min(len(self._examples), 25), width=40)
if len(self._examples) > 25:
listscroll = Scrollbar(self._exampleFrame, orient=""vertical"")
self._exampleList.config(yscrollcommand=listscroll.set)
listscroll.config(command=self._exampleList.yview)
listscroll.pack(side=""left"", fill=""y"")
self._exampleList.bind(""<<ListboxSelect>>"", self._exampleList_select)
",[],0,[],/sem/drt_glue_demo.py__init_exampleListbox
2318,/home/amandapotts/git/nltk/nltk/sem/drt_glue_demo.py__init_readingListbox,"def _init_readingListbox(self, parent):
self._readingFrame = listframe = Frame(parent)
self._readingFrame.pack(fill=""both"", side=""left"", padx=2)
self._readingList_label = Label(
self._readingFrame, font=self._boldfont, text=""Readings""
)
self._readingList_label.pack()
self._readingList = Listbox(
self._readingFrame,
selectmode=""single"",
relief=""groove"",
background=""white"",
foreground=""#909090"",
font=self._font,
selectforeground=""#004040"",
selectbackground=""#c0f0c0"",
)
self._readingList.pack(side=""right"", fill=""both"", expand=1)
listscroll = Scrollbar(self._readingFrame, orient=""vertical"")
self._readingList.config(yscrollcommand=listscroll.set)
listscroll.config(command=self._readingList.yview)
listscroll.pack(side=""right"", fill=""y"")
self._populate_readingListbox()
",[],0,[],/sem/drt_glue_demo.py__init_readingListbox
2319,/home/amandapotts/git/nltk/nltk/sem/drt_glue_demo.py__populate_readingListbox,"def _populate_readingListbox(self):
self._readingList.delete(0, ""end"")
for i in range(len(self._readings)):
self._readingList.insert(""end"", (""  %s"" % (i + 1)))
self._readingList.config(height=min(len(self._readings), 25), width=5)
self._readingList.bind(""<<ListboxSelect>>"", self._readingList_select)
",[],0,[],/sem/drt_glue_demo.py__populate_readingListbox
2320,/home/amandapotts/git/nltk/nltk/sem/drt_glue_demo.py__init_bindings,"def _init_bindings(self):
self._top.bind(""<Control-q>"", self.destroy)
self._top.bind(""<Control-x>"", self.destroy)
self._top.bind(""<Escape>"", self.destroy)
self._top.bind(""n"", self.next)
self._top.bind(""<space>"", self.next)
self._top.bind(""p"", self.prev)
self._top.bind(""<BackSpace>"", self.prev)
",[],0,[],/sem/drt_glue_demo.py__init_bindings
2321,/home/amandapotts/git/nltk/nltk/sem/drt_glue_demo.py__init_buttons,"def _init_buttons(self, parent):
self._buttonframe = buttonframe = Frame(parent)
buttonframe.pack(fill=""none"", side=""bottom"", padx=3, pady=2)
Button(
buttonframe,
text=""Prev"",
background=""#90c0d0"",
foreground=""black"",
command=self.prev,
).pack(side=""left"")
Button(
buttonframe,
text=""Next"",
background=""#90c0d0"",
foreground=""black"",
command=self.next,
).pack(side=""left"")
",[],0,[],/sem/drt_glue_demo.py__init_buttons
2322,/home/amandapotts/git/nltk/nltk/sem/drt_glue_demo.py__configure,"def _configure(self, event):
self._autostep = 0
(x1, y1, x2, y2) = self._cframe.scrollregion()
y2 = event.height - 6
self._canvas[""scrollregion""] = ""%d %d %d %d"" % (x1, y1, x2, y2)
self._redraw()
",[],0,[],/sem/drt_glue_demo.py__configure
2323,/home/amandapotts/git/nltk/nltk/sem/drt_glue_demo.py__init_canvas,"def _init_canvas(self, parent):
self._cframe = CanvasFrame(
parent,
background=""white"",
closeenough=10,
border=2,
relief=""sunken"",
)
self._cframe.pack(expand=1, fill=""both"", side=""top"", pady=2)
canvas = self._canvas = self._cframe.canvas()
self._tree = None
self._textwidgets = []
self._textline = None
",[],0,[],/sem/drt_glue_demo.py__init_canvas
2324,/home/amandapotts/git/nltk/nltk/sem/drt_glue_demo.py__init_menubar,"def _init_menubar(self, parent):
menubar = Menu(parent)
filemenu = Menu(menubar, tearoff=0)
filemenu.add_command(
label=""Exit"", underline=1, command=self.destroy, accelerator=""q""
)
menubar.add_cascade(label=""File"", underline=0, menu=filemenu)
actionmenu = Menu(menubar, tearoff=0)
actionmenu.add_command(
label=""Next"", underline=0, command=self.next, accelerator=""n, Space""
)
actionmenu.add_command(
label=""Previous"", underline=0, command=self.prev, accelerator=""p, Backspace""
)
menubar.add_cascade(label=""Action"", underline=0, menu=actionmenu)
optionmenu = Menu(menubar, tearoff=0)
optionmenu.add_checkbutton(
label=""Remove Duplicates"",
underline=0,
variable=self._glue.remove_duplicates,
command=self._toggle_remove_duplicates,
accelerator=""r"",
)
menubar.add_cascade(label=""Options"", underline=0, menu=optionmenu)
viewmenu = Menu(menubar, tearoff=0)
viewmenu.add_radiobutton(
label=""Tiny"",
variable=self._size,
underline=0,
value=10,
command=self.resize,
)
viewmenu.add_radiobutton(
label=""Small"",
variable=self._size,
underline=0,
value=12,
command=self.resize,
)
viewmenu.add_radiobutton(
label=""Medium"",
variable=self._size,
underline=0,
value=14,
command=self.resize,
)
viewmenu.add_radiobutton(
label=""Large"",
variable=self._size,
underline=0,
value=18,
command=self.resize,
)
viewmenu.add_radiobutton(
label=""Huge"",
variable=self._size,
underline=0,
value=24,
command=self.resize,
)
menubar.add_cascade(label=""View"", underline=0, menu=viewmenu)
helpmenu = Menu(menubar, tearoff=0)
helpmenu.add_command(label=""About"", underline=0, command=self.about)
menubar.add_cascade(label=""Help"", underline=0, menu=helpmenu)
parent.config(menu=menubar)
",[],0,[],/sem/drt_glue_demo.py__init_menubar
2325,/home/amandapotts/git/nltk/nltk/sem/drt_glue_demo.py__redraw,"def _redraw(self):
canvas = self._canvas
if self._drsWidget is not None:
self._drsWidget.clear()
if self._drs:
self._drsWidget = DrsWidget(self._canvas, self._drs)
self._drsWidget.draw()
if self._error:
self._drsWidget = DrsWidget(self._canvas, self._error)
self._drsWidget.draw()
",[],0,[],/sem/drt_glue_demo.py__redraw
2326,/home/amandapotts/git/nltk/nltk/sem/drt_glue_demo.py_destroy,"def destroy(self, *e):
self._autostep = 0
if self._top is None:
return
self._top.destroy()
self._top = None
",[],0,[],/sem/drt_glue_demo.py_destroy
2327,/home/amandapotts/git/nltk/nltk/sem/drt_glue_demo.py_prev,"def prev(self, *e):
selection = self._readingList.curselection()
readingListSize = self._readingList.size()
if readingListSize > 0:
if len(selection) == 1:
index = int(selection[0])
if index <= 0:
self._select_previous_example()
else:
self._readingList_store_selection(index - 1)
else:
self._readingList_store_selection(readingListSize - 1)
else:
self._select_previous_example()
",[],0,[],/sem/drt_glue_demo.py_prev
2328,/home/amandapotts/git/nltk/nltk/sem/drt_glue_demo.py__select_previous_example,"def _select_previous_example(self):
if self._curExample > 0:
self._exampleList_store_selection(self._curExample - 1)
else:
self._exampleList_store_selection(len(self._examples) - 1)
",[],0,[],/sem/drt_glue_demo.py__select_previous_example
2329,/home/amandapotts/git/nltk/nltk/sem/drt_glue_demo.py_next,"def next(self, *e):
selection = self._readingList.curselection()
readingListSize = self._readingList.size()
if readingListSize > 0:
if len(selection) == 1:
index = int(selection[0])
if index >= (readingListSize - 1):
self._select_next_example()
else:
self._readingList_store_selection(index + 1)
else:
self._readingList_store_selection(0)
else:
self._select_next_example()
",[],0,[],/sem/drt_glue_demo.py_next
2330,/home/amandapotts/git/nltk/nltk/sem/drt_glue_demo.py__select_next_example,"def _select_next_example(self):
if self._curExample < len(self._examples) - 1:
self._exampleList_store_selection(self._curExample + 1)
else:
self._exampleList_store_selection(0)
",[],0,[],/sem/drt_glue_demo.py__select_next_example
2331,/home/amandapotts/git/nltk/nltk/sem/drt_glue_demo.py_about,"def about(self, *e):
ABOUT = (
""NLTK Discourse Representation Theory (DRT) Glue Semantics Demo\n""
+ ""Written by Daniel H. Garrette""
)
TITLE = ""About: NLTK DRT Glue Demo""
try:
from tkinter.messagebox import Message
Message(message=ABOUT, title=TITLE).show()
except:
ShowText(self._top, TITLE, ABOUT)
",[],0,[],/sem/drt_glue_demo.py_about
2332,/home/amandapotts/git/nltk/nltk/sem/drt_glue_demo.py_postscript,"def postscript(self, *e):
self._autostep = 0
self._cframe.print_to_file()
",[],0,[],/sem/drt_glue_demo.py_postscript
2333,/home/amandapotts/git/nltk/nltk/sem/drt_glue_demo.py_mainloop,"def mainloop(self, *args, **kwargs):
""""""
Enter the Tkinter mainloop.  This function must be called if
this demo is created from a non-interactive program (e.g.
from a secript)
the script completes.
""""""
if in_idle():
return
self._top.mainloop(*args, **kwargs)
",[],0,[],/sem/drt_glue_demo.py_mainloop
2334,/home/amandapotts/git/nltk/nltk/sem/drt_glue_demo.py_resize,"def resize(self, size=None):
if size is not None:
self._size.set(size)
size = self._size.get()
self._font.configure(size=-(abs(size)))
self._boldfont.configure(size=-(abs(size)))
self._sysfont.configure(size=-(abs(size)))
self._bigfont.configure(size=-(abs(size + 2)))
self._redraw()
",[],0,[],/sem/drt_glue_demo.py_resize
2335,/home/amandapotts/git/nltk/nltk/sem/drt_glue_demo.py__toggle_remove_duplicates,"def _toggle_remove_duplicates(self):
self._glue.remove_duplicates = not self._glue.remove_duplicates
self._exampleList.selection_clear(0, ""end"")
self._readings = []
self._populate_readingListbox()
self._readingCache = [None for ex in self._examples]
self._curExample = -1
self._error = None
self._drs = None
self._redraw()
",[],0,[],/sem/drt_glue_demo.py__toggle_remove_duplicates
2336,/home/amandapotts/git/nltk/nltk/sem/drt_glue_demo.py__exampleList_select,"def _exampleList_select(self, event):
selection = self._exampleList.curselection()
if len(selection) != 1:
return
self._exampleList_store_selection(int(selection[0]))
",[],0,[],/sem/drt_glue_demo.py__exampleList_select
2337,/home/amandapotts/git/nltk/nltk/sem/drt_glue_demo.py__exampleList_store_selection,"def _exampleList_store_selection(self, index):
self._curExample = index
example = self._examples[index]
self._exampleList.selection_clear(0, ""end"")
if example:
cache = self._readingCache[index]
if cache:
if isinstance(cache, list):
self._readings = cache
self._error = None
else:
self._readings = []
self._error = cache
else:
try:
self._readings = self._glue.parse_to_meaning(example)
self._error = None
self._readingCache[index] = self._readings
except Exception as e:
self._readings = []
self._error = DrtVariableExpression(Variable(""Error: "" + str(e)))
self._readingCache[index] = self._error
self._exampleList.delete(index)
self._exampleList.insert(index, (""  %s *"" % example))
self._exampleList.config(
height=min(len(self._examples), 25), width=40
)
self._populate_readingListbox()
self._exampleList.selection_set(index)
self._drs = None
self._redraw()
",[],0,[],/sem/drt_glue_demo.py__exampleList_store_selection
2338,/home/amandapotts/git/nltk/nltk/sem/drt_glue_demo.py__readingList_select,"def _readingList_select(self, event):
selection = self._readingList.curselection()
if len(selection) != 1:
return
self._readingList_store_selection(int(selection[0]))
",[],0,[],/sem/drt_glue_demo.py__readingList_select
2339,/home/amandapotts/git/nltk/nltk/sem/drt_glue_demo.py__readingList_store_selection,"def _readingList_store_selection(self, index):
reading = self._readings[index]
self._readingList.selection_clear(0, ""end"")
if reading:
self._readingList.selection_set(index)
self._drs = reading.simplify().normalize().resolve_anaphora()
self._redraw()
",[],0,[],/sem/drt_glue_demo.py__readingList_store_selection
2340,/home/amandapotts/git/nltk/nltk/sem/drt_glue_demo.py___init__,"def __init__(self, canvas, drs, **attribs):
self._drs = drs
self._canvas = canvas
canvas.font = Font(
font=canvas.itemcget(canvas.create_text(0, 0, text=""""), ""font"")
)
canvas._BUFFER = 3
self.bbox = (0, 0, 0, 0)
",[],0,[],/sem/drt_glue_demo.py___init__
2341,/home/amandapotts/git/nltk/nltk/sem/drt_glue_demo.py_draw,"def draw(self):
(right, bottom) = DrsDrawer(self._drs, canvas=self._canvas).draw()
self.bbox = (0, 0, right + 1, bottom + 1)
",[],0,[],/sem/drt_glue_demo.py_draw
2342,/home/amandapotts/git/nltk/nltk/sem/drt_glue_demo.py_clear,"def clear(self):
self._canvas.create_rectangle(self.bbox, fill=""white"", width=""0"")
",[],0,[],/sem/drt_glue_demo.py_clear
2343,/home/amandapotts/git/nltk/nltk/sem/drt_glue_demo.py_demo,"def demo():
examples = [
""John walks"",
""David sees Mary"",
""David eats a sandwich"",
""every man chases a dog"",
""John chases himself"",
]
DrtGlueDemo(examples).mainloop()
",[],0,[],/sem/drt_glue_demo.py_demo
2344,/home/amandapotts/git/nltk/nltk/sem/chat80.py___init__,"def __init__(self, prefLabel, arity, altLabels=[], closures=[], extension=set()):
""""""
:param prefLabel: the preferred label for the concept
:type prefLabel: str
:param arity: the arity of the concept
:type arity: int
:param altLabels: other (related) labels
:type altLabels: list
:param closures: closure properties of the extension
(list items can be ``symmetric``, ``reflexive``, ``transitive``)
:type closures: list
:param extension: the extensional value of the concept
:type extension: set
""""""
self.prefLabel = prefLabel
self.arity = arity
self.altLabels = altLabels
self.closures = closures
self._extension = extension
self.extension = sorted(list(extension))
",[],0,[],/sem/chat80.py___init__
2345,/home/amandapotts/git/nltk/nltk/sem/chat80.py___str__,"def __str__(self):
return ""Label = '{}'\nArity = {}\nExtension = {}"".format(
self.prefLabel,
self.arity,
self.extension,
)
",[],0,[],/sem/chat80.py___str__
2346,/home/amandapotts/git/nltk/nltk/sem/chat80.py___repr__,"def __repr__(self):
return ""Concept('%s')"" % self.prefLabel
",[],0,[],/sem/chat80.py___repr__
2347,/home/amandapotts/git/nltk/nltk/sem/chat80.py_augment,"def augment(self, data):
""""""
Add more data to the ``Concept``'s extension set.
:param data: a new semantic value
:type data: string or pair of strings
:rtype: set
""""""
self._extension.add(data)
self.extension = sorted(list(self._extension))
return self._extension
",[],0,[],/sem/chat80.py_augment
2348,/home/amandapotts/git/nltk/nltk/sem/chat80.py__make_graph,"def _make_graph(self, s):
""""""
Convert a set of pairs into an adjacency linked list encoding of a graph.
""""""
g = {}
for x, y in s:
if x in g:
g[x].append(y)
else:
g[x] = [y]
return g
",[],0,[],/sem/chat80.py__make_graph
2349,/home/amandapotts/git/nltk/nltk/sem/chat80.py__transclose,"def _transclose(self, g):
""""""
Compute the transitive closure of a graph represented as a linked list.
""""""
for x in g:
for adjacent in g[x]:
if adjacent in g:
for y in g[adjacent]:
if y not in g[x]:
g[x].append(y)
return g
",[],0,[],/sem/chat80.py__transclose
2350,/home/amandapotts/git/nltk/nltk/sem/chat80.py__make_pairs,"def _make_pairs(self, g):
""""""
Convert an adjacency linked list back into a set of pairs.
""""""
pairs = []
for node in g:
for adjacent in g[node]:
pairs.append((node, adjacent))
return set(pairs)
",[],0,[],/sem/chat80.py__make_pairs
2351,/home/amandapotts/git/nltk/nltk/sem/chat80.py_close,"def close(self):
""""""
Close a binary relation in the ``Concept``'s extension set.
:return: a new extension for the ``Concept`` in which the
relation is closed under a given property
""""""
from nltk.sem import is_rel
assert is_rel(self._extension)
if ""symmetric"" in self.closures:
pairs = []
for x, y in self._extension:
pairs.append((y, x))
sym = set(pairs)
self._extension = self._extension.union(sym)
if ""transitive"" in self.closures:
all = self._make_graph(self._extension)
closed = self._transclose(all)
trans = self._make_pairs(closed)
self._extension = self._extension.union(trans)
self.extension = sorted(list(self._extension))
",[],0,[],/sem/chat80.py_close
2352,/home/amandapotts/git/nltk/nltk/sem/chat80.py_clause2concepts,"def clause2concepts(filename, rel_name, schema, closures=[]):
""""""
Convert a file of Prolog clauses into a list of ``Concept`` objects.
:param filename: filename containing the relations
:type filename: str
:param rel_name: name of the relation
:type rel_name: str
:param schema: the schema used in a set of relational tuples
:type schema: list
:param closures: closure properties for the extension of the concept
:type closures: list
:return: a list of ``Concept`` objects
:rtype: list
""""""
concepts = []
subj = 0
pkey = schema[0]
fields = schema[1:]
records = _str2records(filename, rel_name)
if not filename in not_unary:
concepts.append(unary_concept(pkey, subj, records))
for field in fields:
obj = schema.index(field)
concepts.append(binary_concept(field, closures, subj, obj, records))
return concepts
",[],0,[],/sem/chat80.py_clause2concepts
2353,/home/amandapotts/git/nltk/nltk/sem/chat80.py_cities2table,"def cities2table(filename, rel_name, dbname, verbose=False, setup=False):
""""""
Convert a file of Prolog clauses into a database table.
This is not generic, since it doesn't allow arbitrary
schemas to be set as a parameter.
Intended usage::
cities2table('cities.pl', 'city', 'city.db', verbose=True, setup=True)
:param filename: filename containing the relations
:type filename: str
:param rel_name: name of the relation
:type rel_name: str
:param dbname: filename of persistent store
:type schema: str
""""""
import sqlite3
records = _str2records(filename, rel_name)
connection = sqlite3.connect(dbname)
cur = connection.cursor()
if setup:
cur.execute(
""""""CREATE TABLE city_table
(City text, Country text, Population int)""""""
)
table_name = ""city_table""
for t in records:
cur.execute(""insert into %s values (?,?,?)"" % table_name, t)
if verbose:
print(""inserting values into %s: "" % table_name, t)
connection.commit()
if verbose:
print(""Committing update to %s"" % dbname)
cur.close()
",[],0,[],/sem/chat80.py_cities2table
2354,/home/amandapotts/git/nltk/nltk/sem/chat80.py_sql_query,"def sql_query(dbname, query):
""""""
Execute an SQL query over a database.
:param dbname: filename of persistent store
:type schema: str
:param query: SQL query
:type rel_name: str
""""""
import sqlite3
try:
path = nltk.data.find(dbname)
connection = sqlite3.connect(str(path))
cur = connection.cursor()
return cur.execute(query)
except (ValueError, sqlite3.OperationalError):
import warnings
warnings.warn(
""Make sure the database file %s is installed and uncompressed."" % dbname
)
raise
",[],0,[],/sem/chat80.py_sql_query
2355,/home/amandapotts/git/nltk/nltk/sem/chat80.py__str2records,"def _str2records(filename, rel):
""""""
Read a file into memory and convert each relation clause into a list.
""""""
recs = []
contents = nltk.data.load(""corpora/chat80/%s"" % filename, format=""text"")
for line in contents.splitlines():
if line.startswith(rel):
line = re.sub(rel + r""\("", """", line)
line = re.sub(r""\)\.$"", """", line)
record = line.split("","")
recs.append(record)
return recs
",[],0,[],/sem/chat80.py__str2records
2356,/home/amandapotts/git/nltk/nltk/sem/chat80.py_unary_concept,"def unary_concept(label, subj, records):
""""""
Make a unary concept out of the primary key in a record.
A record is a list of entities in some relation, such as
``['france', 'paris']``, where ``'france'`` is acting as the primary
key.
:param label: the preferred label for the concept
:type label: string
:param subj: position in the record of the subject of the predicate
:type subj: int
:param records: a list of records
:type records: list of lists
:return: ``Concept`` of arity 1
:rtype: Concept
""""""
c = Concept(label, arity=1, extension=set())
for record in records:
c.augment(record[subj])
return c
",[],0,[],/sem/chat80.py_unary_concept
2357,/home/amandapotts/git/nltk/nltk/sem/chat80.py_binary_concept,"def binary_concept(label, closures, subj, obj, records):
""""""
Make a binary concept out of the primary key and another field in a record.
A record is a list of entities in some relation, such as
``['france', 'paris']``, where ``'france'`` is acting as the primary
key, and ``'paris'`` stands in the ``'capital_of'`` relation to
``'france'``.
More generally, given a record such as ``['a', 'b', 'c']``, where
label is bound to ``'B'``, and ``obj`` bound to 1, the derived
binary concept will have label ``'B_of'``, and its extension will
be a set of pairs such as ``('a', 'b')``.
:param label: the base part of the preferred label for the concept
:type label: str
:param closures: closure properties for the extension of the concept
:type closures: list
:param subj: position in the record of the subject of the predicate
:type subj: int
:param obj: position in the record of the object of the predicate
:type obj: int
:param records: a list of records
:type records: list of lists
:return: ``Concept`` of arity 2
:rtype: Concept
""""""
if not label == ""border"" and not label == ""contain"":
label = label + ""_of""
c = Concept(label, arity=2, closures=closures, extension=set())
for record in records:
c.augment((record[subj], record[obj]))
c.close()
return c
",[],0,[],/sem/chat80.py_binary_concept
2358,/home/amandapotts/git/nltk/nltk/sem/chat80.py_process_bundle,"def process_bundle(rels):
""""""
Given a list of relation metadata bundles, make a corresponding
dictionary of concepts, indexed by the relation name.
:param rels: bundle of metadata needed for constructing a concept
:type rels: list(dict)
:return: a dictionary of concepts, indexed by the relation name.
:rtype: dict(str): Concept
""""""
concepts = {}
for rel in rels:
rel_name = rel[""rel_name""]
closures = rel[""closures""]
schema = rel[""schema""]
filename = rel[""filename""]
concept_list = clause2concepts(filename, rel_name, schema, closures)
for c in concept_list:
label = c.prefLabel
if label in concepts:
for data in c.extension:
concepts[label].augment(data)
concepts[label].close()
else:
concepts[label] = c
return concepts
",[],0,[],/sem/chat80.py_process_bundle
2359,/home/amandapotts/git/nltk/nltk/sem/chat80.py_make_valuation,"def make_valuation(concepts, read=False, lexicon=False):
""""""
Convert a list of ``Concept`` objects into a list of (label, extension) pairs
optionally create a ``Valuation`` object.
:param concepts: concepts
:type concepts: list(Concept)
:param read: if ``True``, ``(symbol, set)`` pairs are read into a ``Valuation``
:type read: bool
:rtype: list or Valuation
""""""
vals = []
for c in concepts:
vals.append((c.prefLabel, c.extension))
if lexicon:
read = True
if read:
from nltk.sem import Valuation
val = Valuation({})
val.update(vals)
val = label_indivs(val, lexicon=lexicon)
return val
else:
return vals
",[],0,[],/sem/chat80.py_make_valuation
2360,/home/amandapotts/git/nltk/nltk/sem/chat80.py_val_dump,"def val_dump(rels, db):
""""""
Make a ``Valuation`` from a list of relation metadata bundles and dump to
persistent database.
:param rels: bundle of metadata needed for constructing a concept
:type rels: list of dict
:param db: name of file to which data is written.
The suffix '.db' will be automatically appended.
:type db: str
""""""
concepts = process_bundle(rels).values()
valuation = make_valuation(concepts, read=True)
db_out = shelve.open(db, ""n"")
db_out.update(valuation)
db_out.close()
",[],0,[],/sem/chat80.py_val_dump
2361,/home/amandapotts/git/nltk/nltk/sem/chat80.py_val_load,"def val_load(db):
""""""
Load a ``Valuation`` from a persistent database.
:param db: name of file from which data is read.
The suffix '.db' should be omitted from the name.
:type db: str
""""""
dbname = db + "".db""
if not os.access(dbname, os.R_OK):
sys.exit(""Cannot read file: %s"" % dbname)
else:
db_in = shelve.open(db)
from nltk.sem import Valuation
val = Valuation(db_in)
return val
",[],0,[],/sem/chat80.py_val_load
2362,/home/amandapotts/git/nltk/nltk/sem/chat80.py_label_indivs,"def label_indivs(valuation, lexicon=False):
""""""
Assign individual constants to the individuals in the domain of a ``Valuation``.
Given a valuation with an entry of the form ``{'rel': {'a': True}}``,
add a new entry ``{'a': 'a'}``.
:type valuation: Valuation
:rtype: Valuation
""""""
domain = valuation.domain
pairs = [(e, e) for e in domain]
if lexicon:
lex = make_lex(domain)
with open(""chat_pnames.cfg"", ""w"") as outfile:
outfile.writelines(lex)
valuation.update(pairs)
return valuation
",[],0,[],/sem/chat80.py_label_indivs
2363,/home/amandapotts/git/nltk/nltk/sem/chat80.py_make_lex,"def make_lex(symbols):
""""""
Create lexical CFG rules for each individual symbol.
Given a valuation with an entry of the form ``{'zloty': 'zloty'}``,
create a lexical rule for the proper name 'Zloty'.
:param symbols: a list of individual constants in the semantic representation
:type symbols: sequence -- set(str)
:rtype: list(str)
""""""
lex = []
header = """"""
",[],0,[],/sem/chat80.py_make_lex
2364,/home/amandapotts/git/nltk/nltk/sem/chat80.py_concepts,"def concepts(items=items):
""""""
Build a list of concepts corresponding to the relation names in ``items``.
:param items: names of the Chat-80 relations to extract
:type items: list(str)
:return: the ``Concept`` objects which are extracted from the relations
:rtype: list(Concept)
""""""
if isinstance(items, str):
items = (items,)
rels = [item_metadata[r] for r in items]
concept_map = process_bundle(rels)
return concept_map.values()
",[],0,[],/sem/chat80.py_concepts
2365,/home/amandapotts/git/nltk/nltk/sem/chat80.py_main,"def main():
import sys
from optparse import OptionParser
description = """"""
",[],0,[],/sem/chat80.py_main
2366,/home/amandapotts/git/nltk/nltk/sem/chat80.py_sql_demo,"def sql_demo():
""""""
Print out every row from the 'city.db' database.
""""""
print()
print(""Using SQL to extract rows from 'city.db' RDB."")
for row in sql_query(""corpora/city_database/city.db"", ""SELECT * FROM city_table""):
print(row)
",[],0,[],/sem/chat80.py_sql_demo
2367,/home/amandapotts/git/nltk/nltk/sem/logic.py_boolean_ops,"def boolean_ops():
""""""
Boolean operators
""""""
names = [""negation"", ""conjunction"", ""disjunction"", ""implication"", ""equivalence""]
for pair in zip(names, [Tokens.NOT, Tokens.AND, Tokens.OR, Tokens.IMP, Tokens.IFF]):
print(""%-15s\t%s"" % pair)
",[],0,[],/sem/logic.py_boolean_ops
2368,/home/amandapotts/git/nltk/nltk/sem/logic.py_equality_preds,"def equality_preds():
""""""
Equality predicates
""""""
names = [""equality"", ""inequality""]
for pair in zip(names, [Tokens.EQ, Tokens.NEQ]):
print(""%-15s\t%s"" % pair)
",[],0,[],/sem/logic.py_equality_preds
2369,/home/amandapotts/git/nltk/nltk/sem/logic.py___init__,"def __init__(self, type_check=False):
""""""
:param type_check: should type checking be performed
to their types?
:type type_check: bool
""""""
assert isinstance(type_check, bool)
self._currentIndex = 0
self._buffer = []
self.type_check = type_check
""""""A list of tuples of quote characters.  The 4-tuple is comprised
of the start character, the end character, the escape character, and
a boolean indicating whether the quotes should be included in the
result. Quotes are used to signify that a token should be treated as
atomic, ignoring any special characters within the token.  The escape
character allows the quote end character to be used within the quote.
If True, the boolean indicates that the final token should contain the
quote and escape characters.
This method exists to be overridden""""""
self.quote_chars = []
self.operator_precedence = dict(
[(x, 1) for x in Tokens.LAMBDA_LIST]
+ [(x, 2) for x in Tokens.NOT_LIST]
+ [(APP, 3)]
+ [(x, 4) for x in Tokens.EQ_LIST + Tokens.NEQ_LIST]
+ [(x, 5) for x in Tokens.QUANTS]
+ [(x, 6) for x in Tokens.AND_LIST]
+ [(x, 7) for x in Tokens.OR_LIST]
+ [(x, 8) for x in Tokens.IMP_LIST]
+ [(x, 9) for x in Tokens.IFF_LIST]
+ [(None, 10)]
)
self.right_associated_operations = [APP]
",[],0,[],/sem/logic.py___init__
2370,/home/amandapotts/git/nltk/nltk/sem/logic.py_parse,"def parse(self, data, signature=None):
""""""
Parse the expression.
:param data: str for the input to be parsed
:param signature: ``dict<str, str>`` that maps variable names to type
strings
:returns: a parsed Expression
""""""
data = data.rstrip()
self._currentIndex = 0
self._buffer, mapping = self.process(data)
try:
result = self.process_next_expression(None)
if self.inRange(0):
raise UnexpectedTokenException(self._currentIndex + 1, self.token(0))
except LogicalExpressionException as e:
msg = ""{}\n{}\n{}^"".format(e, data, "" "" * mapping[e.index - 1])
raise LogicalExpressionException(None, msg) from e
if self.type_check:
result.typecheck(signature)
return result
",[],0,[],/sem/logic.py_parse
2371,/home/amandapotts/git/nltk/nltk/sem/logic.py_process,"def process(self, data):
""""""Split the data into tokens""""""
out = []
mapping = {}
tokenTrie = Trie(self.get_all_symbols())
token = """"
data_idx = 0
token_start_idx = data_idx
while data_idx < len(data):
cur_data_idx = data_idx
quoted_token, data_idx = self.process_quoted_token(data_idx, data)
if quoted_token:
if not token:
token_start_idx = cur_data_idx
token += quoted_token
continue
st = tokenTrie
c = data[data_idx]
symbol = """"
while c in st:
symbol += c
st = st[c]
if len(data) - data_idx > len(symbol):
c = data[data_idx + len(symbol)]
else:
break
if Trie.LEAF in st:
if token:
mapping[len(out)] = token_start_idx
out.append(token)
token = """"
mapping[len(out)] = data_idx
out.append(symbol)
data_idx += len(symbol)
else:
if data[data_idx] in "" \t\n"":  # any whitespace
if token:
mapping[len(out)] = token_start_idx
out.append(token)
token = """"
else:
if not token:
token_start_idx = data_idx
token += data[data_idx]
data_idx += 1
if token:
mapping[len(out)] = token_start_idx
out.append(token)
mapping[len(out)] = len(data)
mapping[len(out) + 1] = len(data) + 1
return out, mapping
",[],0,[],/sem/logic.py_process
2372,/home/amandapotts/git/nltk/nltk/sem/logic.py_process_quoted_token,"def process_quoted_token(self, data_idx, data):
token = """"
c = data[data_idx]
i = data_idx
for start, end, escape, incl_quotes in self.quote_chars:
if c == start:
if incl_quotes:
token += c
i += 1
while data[i] != end:
if data[i] == escape:
if incl_quotes:
token += data[i]
i += 1
if len(data) == i:  # if there are no more chars
raise LogicalExpressionException(
None,
""End of input reached.  ""
""Escape character [%s] found at end."" % escape,
)
token += data[i]
else:
token += data[i]
i += 1
if len(data) == i:
raise LogicalExpressionException(
None, ""End of input reached.  "" ""Expected: [%s]"" % end
)
if incl_quotes:
token += data[i]
i += 1
if not token:
raise LogicalExpressionException(None, ""Empty quoted token found"")
break
return token, i
",[],0,[],/sem/logic.py_process_quoted_token
2373,/home/amandapotts/git/nltk/nltk/sem/logic.py_get_all_symbols,"def get_all_symbols(self):
""""""This method exists to be overridden""""""
return Tokens.SYMBOLS
",[],0,[],/sem/logic.py_get_all_symbols
2374,/home/amandapotts/git/nltk/nltk/sem/logic.py_inRange,"def inRange(self, location):
""""""Return TRUE if the given location is within the buffer""""""
return self._currentIndex + location < len(self._buffer)
",[],0,[],/sem/logic.py_inRange
2375,/home/amandapotts/git/nltk/nltk/sem/logic.py_token,"def token(self, location=None):
""""""Get the next waiting token.  If a location is given, then
return the token at currentIndex+location without advancing
currentIndex
try:
if location is None:
tok = self._buffer[self._currentIndex]
self._currentIndex += 1
else:
tok = self._buffer[self._currentIndex + location]
return tok
except IndexError as e:
raise ExpectedMoreTokensException(self._currentIndex + 1) from e
",[],0,[],/sem/logic.py_token
2376,/home/amandapotts/git/nltk/nltk/sem/logic.py_isvariable,"def isvariable(self, tok):
return tok not in Tokens.TOKENS
",[],0,[],/sem/logic.py_isvariable
2377,/home/amandapotts/git/nltk/nltk/sem/logic.py_process_next_expression,"def process_next_expression(self, context):
""""""Parse the next complete expression from the stream and return it.""""""
try:
tok = self.token()
except ExpectedMoreTokensException as e:
raise ExpectedMoreTokensException(
self._currentIndex + 1, message=""Expression expected.""
) from e
accum = self.handle(tok, context)
if not accum:
raise UnexpectedTokenException(
self._currentIndex, tok, message=""Expression expected.""
)
return self.attempt_adjuncts(accum, context)
",[],0,[],/sem/logic.py_process_next_expression
2378,/home/amandapotts/git/nltk/nltk/sem/logic.py_attempt_adjuncts,"def attempt_adjuncts(self, expression, context):
cur_idx = None
while cur_idx != self._currentIndex:  # while adjuncts are added
cur_idx = self._currentIndex
expression = self.attempt_EqualityExpression(expression, context)
expression = self.attempt_ApplicationExpression(expression, context)
expression = self.attempt_BooleanExpression(expression, context)
return expression
",[],0,[],/sem/logic.py_attempt_adjuncts
2379,/home/amandapotts/git/nltk/nltk/sem/logic.py_handle_negation,"def handle_negation(self, tok, context):
return self.make_NegatedExpression(self.process_next_expression(Tokens.NOT))
",[],0,[],/sem/logic.py_handle_negation
2380,/home/amandapotts/git/nltk/nltk/sem/logic.py_make_NegatedExpression,"def make_NegatedExpression(self, expression):
return NegatedExpression(expression)
",[],0,[],/sem/logic.py_make_NegatedExpression
2381,/home/amandapotts/git/nltk/nltk/sem/logic.py_handle_variable,"def handle_variable(self, tok, context):
accum = self.make_VariableExpression(tok)
if self.inRange(0) and self.token(0) == Tokens.OPEN:
if not isinstance(accum, FunctionVariableExpression) and not isinstance(
accum, ConstantExpression
):
raise LogicalExpressionException(
self._currentIndex,
""'%s' is an illegal predicate name.  ""
""Individual variables may not be used as ""
""predicates."" % tok,
)
self.token()  # swallow the Open Paren
accum = self.make_ApplicationExpression(
accum, self.process_next_expression(APP)
)
while self.inRange(0) and self.token(0) == Tokens.COMMA:
self.token()  # swallow the comma
accum = self.make_ApplicationExpression(
accum, self.process_next_expression(APP)
)
self.assertNextToken(Tokens.CLOSE)
return accum
",[],0,[],/sem/logic.py_handle_variable
2382,/home/amandapotts/git/nltk/nltk/sem/logic.py_get_next_token_variable,"def get_next_token_variable(self, description):
try:
tok = self.token()
except ExpectedMoreTokensException as e:
raise ExpectedMoreTokensException(e.index, ""Variable expected."") from e
if isinstance(self.make_VariableExpression(tok), ConstantExpression):
raise LogicalExpressionException(
self._currentIndex,
""'%s' is an illegal variable name.  ""
""Constants may not be %s."" % (tok, description),
)
return Variable(tok)
",[],0,[],/sem/logic.py_get_next_token_variable
2383,/home/amandapotts/git/nltk/nltk/sem/logic.py_handle_quant,"def handle_quant(self, tok, context):
factory = self.get_QuantifiedExpression_factory(tok)
if not self.inRange(0):
raise ExpectedMoreTokensException(
self._currentIndex + 2,
message=""Variable and Expression expected following quantifier '%s'.""
% tok,
)
vars = [self.get_next_token_variable(""quantified"")]
while True:
if not self.inRange(0) or (
self.token(0) == Tokens.DOT and not self.inRange(1)
):
raise ExpectedMoreTokensException(
self._currentIndex + 2, message=""Expression expected.""
)
if not self.isvariable(self.token(0)):
break
vars.append(self.get_next_token_variable(""quantified""))
if self.inRange(0) and self.token(0) == Tokens.DOT:
self.token()  # swallow the dot
accum = self.process_next_expression(tok)
while vars:
accum = self.make_QuanifiedExpression(factory, vars.pop(), accum)
return accum
",[],0,[],/sem/logic.py_handle_quant
2384,/home/amandapotts/git/nltk/nltk/sem/logic.py_get_QuantifiedExpression_factory,"def get_QuantifiedExpression_factory(self, tok):
""""""This method serves as a hook for other logic parsers that
have different quantifiers""""""
if tok in Tokens.EXISTS_LIST:
return ExistsExpression
elif tok in Tokens.ALL_LIST:
return AllExpression
elif tok in Tokens.IOTA_LIST:
return IotaExpression
else:
self.assertToken(tok, Tokens.QUANTS)
",[],0,[],/sem/logic.py_get_QuantifiedExpression_factory
2385,/home/amandapotts/git/nltk/nltk/sem/logic.py_make_QuanifiedExpression,"def make_QuanifiedExpression(self, factory, variable, term):
return factory(variable, term)
",[],0,[],/sem/logic.py_make_QuanifiedExpression
2386,/home/amandapotts/git/nltk/nltk/sem/logic.py_handle_open,"def handle_open(self, tok, context):
accum = self.process_next_expression(None)
self.assertNextToken(Tokens.CLOSE)
return accum
",[],0,[],/sem/logic.py_handle_open
2387,/home/amandapotts/git/nltk/nltk/sem/logic.py_attempt_EqualityExpression,"def attempt_EqualityExpression(self, expression, context):
""""""Attempt to make an equality expression.  If the next token is an
equality operator, then an EqualityExpression will be returned.
Otherwise, the parameter will be returned.""""""
if self.inRange(0):
tok = self.token(0)
if tok in Tokens.EQ_LIST + Tokens.NEQ_LIST and self.has_priority(
tok, context
):
self.token()  # swallow the ""="" or ""!=""
expression = self.make_EqualityExpression(
expression, self.process_next_expression(tok)
)
if tok in Tokens.NEQ_LIST:
expression = self.make_NegatedExpression(expression)
return expression
",[],0,[],/sem/logic.py_attempt_EqualityExpression
2388,/home/amandapotts/git/nltk/nltk/sem/logic.py_make_EqualityExpression,"def make_EqualityExpression(self, first, second):
""""""This method serves as a hook for other logic parsers that
have different equality expression classes""""""
return EqualityExpression(first, second)
",[],0,[],/sem/logic.py_make_EqualityExpression
2389,/home/amandapotts/git/nltk/nltk/sem/logic.py_attempt_BooleanExpression,"def attempt_BooleanExpression(self, expression, context):
""""""Attempt to make a boolean expression.  If the next token is a boolean
operator, then a BooleanExpression will be returned.  Otherwise, the
parameter will be returned.""""""
while self.inRange(0):
tok = self.token(0)
factory = self.get_BooleanExpression_factory(tok)
if factory and self.has_priority(tok, context):
self.token()  # swallow the operator
expression = self.make_BooleanExpression(
factory, expression, self.process_next_expression(tok)
)
else:
break
return expression
",[],0,[],/sem/logic.py_attempt_BooleanExpression
2390,/home/amandapotts/git/nltk/nltk/sem/logic.py_get_BooleanExpression_factory,"def get_BooleanExpression_factory(self, tok):
""""""This method serves as a hook for other logic parsers that
have different boolean operators""""""
if tok in Tokens.AND_LIST:
return AndExpression
elif tok in Tokens.OR_LIST:
return OrExpression
elif tok in Tokens.IMP_LIST:
return ImpExpression
elif tok in Tokens.IFF_LIST:
return IffExpression
else:
return None
",[],0,[],/sem/logic.py_get_BooleanExpression_factory
2391,/home/amandapotts/git/nltk/nltk/sem/logic.py_make_BooleanExpression,"def make_BooleanExpression(self, factory, first, second):
return factory(first, second)
",[],0,[],/sem/logic.py_make_BooleanExpression
2392,/home/amandapotts/git/nltk/nltk/sem/logic.py_attempt_ApplicationExpression,"def attempt_ApplicationExpression(self, expression, context):
""""""Attempt to make an application expression.  The next tokens are
a list of arguments in parens, then the argument expression is a
function being applied to the arguments.  Otherwise, return the
argument expression.""""""
if self.has_priority(APP, context):
if self.inRange(0) and self.token(0) == Tokens.OPEN:
if (
not isinstance(expression, LambdaExpression)
and not isinstance(expression, ApplicationExpression)
and not isinstance(expression, FunctionVariableExpression)
and not isinstance(expression, ConstantExpression)
):
raise LogicalExpressionException(
self._currentIndex,
(""The function '%s"" % expression)
+ ""' is not a Lambda Expression, an ""
""Application Expression, or a ""
""functional predicate, so it may ""
""not take arguments."",
)
self.token()  # swallow then open paren
accum = self.make_ApplicationExpression(
expression, self.process_next_expression(APP)
)
while self.inRange(0) and self.token(0) == Tokens.COMMA:
self.token()  # swallow the comma
accum = self.make_ApplicationExpression(
accum, self.process_next_expression(APP)
)
self.assertNextToken(Tokens.CLOSE)
return accum
return expression
",[],0,[],/sem/logic.py_attempt_ApplicationExpression
2393,/home/amandapotts/git/nltk/nltk/sem/logic.py_make_ApplicationExpression,"def make_ApplicationExpression(self, function, argument):
return ApplicationExpression(function, argument)
",[],0,[],/sem/logic.py_make_ApplicationExpression
2394,/home/amandapotts/git/nltk/nltk/sem/logic.py_make_VariableExpression,"def make_VariableExpression(self, name):
return VariableExpression(Variable(name))
",[],0,[],/sem/logic.py_make_VariableExpression
2395,/home/amandapotts/git/nltk/nltk/sem/logic.py_make_LambdaExpression,"def make_LambdaExpression(self, variable, term):
return LambdaExpression(variable, term)
",[],0,[],/sem/logic.py_make_LambdaExpression
2396,/home/amandapotts/git/nltk/nltk/sem/logic.py_has_priority,"def has_priority(self, operation, context):
return self.operator_precedence[operation] < self.operator_precedence[
context
] or (
operation in self.right_associated_operations
and self.operator_precedence[operation] == self.operator_precedence[context]
)
",[],0,[],/sem/logic.py_has_priority
2397,/home/amandapotts/git/nltk/nltk/sem/logic.py_assertNextToken,"def assertNextToken(self, expected):
try:
tok = self.token()
except ExpectedMoreTokensException as e:
raise ExpectedMoreTokensException(
e.index, message=""Expected token '%s'."" % expected
) from e
if isinstance(expected, list):
if tok not in expected:
raise UnexpectedTokenException(self._currentIndex, tok, expected)
else:
if tok != expected:
raise UnexpectedTokenException(self._currentIndex, tok, expected)
",[],0,[],/sem/logic.py_assertNextToken
2398,/home/amandapotts/git/nltk/nltk/sem/logic.py_assertToken,"def assertToken(self, tok, expected):
if isinstance(expected, list):
if tok not in expected:
raise UnexpectedTokenException(self._currentIndex, tok, expected)
else:
if tok != expected:
raise UnexpectedTokenException(self._currentIndex, tok, expected)
",[],0,[],/sem/logic.py_assertToken
2399,/home/amandapotts/git/nltk/nltk/sem/logic.py___repr__,"def __repr__(self):
if self.inRange(0):
msg = ""Next token: "" + self.token(0)
else:
msg = ""No more tokens""
return ""<"" + self.__class__.__name__ + "": "" + msg + "">""
",[],0,[],/sem/logic.py___repr__
2400,/home/amandapotts/git/nltk/nltk/sem/logic.py_read_logic,"def read_logic(s, logic_parser=None, encoding=None):
""""""
Convert a file of First Order Formulas into a list of {Expression}s.
:param s: the contents of the file
:type s: str
:param logic_parser: The parser to be used to parse the logical expression
:type logic_parser: LogicParser
:param encoding: the encoding of the input string, if it is binary
:type encoding: str
:return: a list of parsed formulas.
:rtype: list(Expression)
""""""
if encoding is not None:
s = s.decode(encoding)
if logic_parser is None:
logic_parser = LogicParser()
statements = []
for linenum, line in enumerate(s.splitlines()):
line = line.strip()
if line.startswith(""#"") or line == """":
continue
try:
statements.append(logic_parser.parse(line))
except LogicalExpressionException as e:
raise ValueError(f""Unable to parse line {linenum}: {line}"") from e
return statements
",[],0,[],/sem/logic.py_read_logic
2401,/home/amandapotts/git/nltk/nltk/sem/logic.py___init__,"def __init__(self, name):
""""""
:param name: the name of the variable
""""""
assert isinstance(name, str), ""%s is not a string"" % name
self.name = name
",[],0,[],/sem/logic.py___init__
2402,/home/amandapotts/git/nltk/nltk/sem/logic.py___eq__,"def __eq__(self, other):
return isinstance(other, Variable) and self.name == other.name
",[],0,[],/sem/logic.py___eq__
2403,/home/amandapotts/git/nltk/nltk/sem/logic.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/sem/logic.py___ne__
2404,/home/amandapotts/git/nltk/nltk/sem/logic.py___lt__,"def __lt__(self, other):
if not isinstance(other, Variable):
raise TypeError
return self.name < other.name
",[],0,[],/sem/logic.py___lt__
2405,/home/amandapotts/git/nltk/nltk/sem/logic.py_substitute_bindings,"def substitute_bindings(self, bindings):
return bindings.get(self, self)
",[],0,[],/sem/logic.py_substitute_bindings
2406,/home/amandapotts/git/nltk/nltk/sem/logic.py___hash__,"def __hash__(self):
return hash(self.name)
",[],0,[],/sem/logic.py___hash__
2407,/home/amandapotts/git/nltk/nltk/sem/logic.py___str__,"def __str__(self):
return self.name
",[],0,[],/sem/logic.py___str__
2408,/home/amandapotts/git/nltk/nltk/sem/logic.py___repr__,"def __repr__(self):
return ""Variable('%s')"" % self.name
",[],0,[],/sem/logic.py___repr__
2409,/home/amandapotts/git/nltk/nltk/sem/logic.py_unique_variable,"def unique_variable(pattern=None, ignore=None):
""""""
Return a new, unique variable.
:param pattern: ``Variable`` that is being replaced.  The new variable must
be the same type.
:param term: a set of ``Variable`` objects that should not be returned from
this function.
:rtype: Variable
""""""
if pattern is not None:
if is_indvar(pattern.name):
prefix = ""z""
elif is_funcvar(pattern.name):
prefix = ""F""
elif is_eventvar(pattern.name):
prefix = ""e0""
else:
assert False, ""Cannot generate a unique constant""
else:
prefix = ""z""
v = Variable(f""{prefix}{_counter.get()}"")
while ignore is not None and v in ignore:
v = Variable(f""{prefix}{_counter.get()}"")
return v
",[],0,[],/sem/logic.py_unique_variable
2410,/home/amandapotts/git/nltk/nltk/sem/logic.py_skolem_function,"def skolem_function(univ_scope=None):
""""""
Return a skolem function over the variables in univ_scope
param univ_scope
""""""
skolem = VariableExpression(Variable(""F%s"" % _counter.get()))
if univ_scope:
for v in list(univ_scope):
skolem = skolem(VariableExpression(v))
return skolem
",[],0,[],/sem/logic.py_skolem_function
2411,/home/amandapotts/git/nltk/nltk/sem/logic.py___repr__,"def __repr__(self):
return ""%s"" % self
",[],0,[],/sem/logic.py___repr__
2412,/home/amandapotts/git/nltk/nltk/sem/logic.py___hash__,"def __hash__(self):
return hash(""%s"" % self)
",[],0,[],/sem/logic.py___hash__
2413,/home/amandapotts/git/nltk/nltk/sem/logic.py_fromstring,"def fromstring(cls, s):
return read_type(s)
",[],0,[],/sem/logic.py_fromstring
2414,/home/amandapotts/git/nltk/nltk/sem/logic.py___init__,"def __init__(self, first, second):
assert isinstance(first, Type), ""%s is not a Type"" % first
assert isinstance(second, Type), ""%s is not a Type"" % second
self.first = first
self.second = second
",[],0,[],/sem/logic.py___init__
2415,/home/amandapotts/git/nltk/nltk/sem/logic.py___eq__,"def __eq__(self, other):
return (
isinstance(other, ComplexType)
and self.first == other.first
and self.second == other.second
)
",[],0,[],/sem/logic.py___eq__
2416,/home/amandapotts/git/nltk/nltk/sem/logic.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/sem/logic.py___ne__
2417,/home/amandapotts/git/nltk/nltk/sem/logic.py_matches,"def matches(self, other):
if isinstance(other, ComplexType):
return self.first.matches(other.first) and self.second.matches(other.second)
else:
return self == ANY_TYPE
",[],0,[],/sem/logic.py_matches
2418,/home/amandapotts/git/nltk/nltk/sem/logic.py_resolve,"def resolve(self, other):
if other == ANY_TYPE:
return self
elif isinstance(other, ComplexType):
f = self.first.resolve(other.first)
s = self.second.resolve(other.second)
if f and s:
return ComplexType(f, s)
else:
return None
elif self == ANY_TYPE:
return other
else:
return None
",[],0,[],/sem/logic.py_resolve
2419,/home/amandapotts/git/nltk/nltk/sem/logic.py___str__,"def __str__(self):
if self == ANY_TYPE:
return ""%s"" % ANY_TYPE
else:
return f""<{self.first},{self.second}>""
",[],0,[],/sem/logic.py___str__
2420,/home/amandapotts/git/nltk/nltk/sem/logic.py_str,"def str(self):
if self == ANY_TYPE:
return ANY_TYPE.str()
else:
return f""({self.first.str()} -> {self.second.str()})""
",[],0,[],/sem/logic.py_str
2421,/home/amandapotts/git/nltk/nltk/sem/logic.py___eq__,"def __eq__(self, other):
return isinstance(other, BasicType) and (""%s"" % self) == (""%s"" % other)
",[],0,[],/sem/logic.py___eq__
2422,/home/amandapotts/git/nltk/nltk/sem/logic.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/sem/logic.py___ne__
2423,/home/amandapotts/git/nltk/nltk/sem/logic.py_matches,"def matches(self, other):
return other == ANY_TYPE or self == other
",[],0,[],/sem/logic.py_matches
2424,/home/amandapotts/git/nltk/nltk/sem/logic.py_resolve,"def resolve(self, other):
if self.matches(other):
return self
else:
return None
",[],0,[],/sem/logic.py_resolve
2425,/home/amandapotts/git/nltk/nltk/sem/logic.py___str__,"def __str__(self):
return ""e""
",[],0,[],/sem/logic.py___str__
2426,/home/amandapotts/git/nltk/nltk/sem/logic.py_str,"def str(self):
return ""IND""
",[],0,[],/sem/logic.py_str
2427,/home/amandapotts/git/nltk/nltk/sem/logic.py___str__,"def __str__(self):
return ""t""
",[],0,[],/sem/logic.py___str__
2428,/home/amandapotts/git/nltk/nltk/sem/logic.py_str,"def str(self):
return ""BOOL""
",[],0,[],/sem/logic.py_str
2429,/home/amandapotts/git/nltk/nltk/sem/logic.py___str__,"def __str__(self):
return ""v""
",[],0,[],/sem/logic.py___str__
2430,/home/amandapotts/git/nltk/nltk/sem/logic.py_str,"def str(self):
return ""EVENT""
",[],0,[],/sem/logic.py_str
2431,/home/amandapotts/git/nltk/nltk/sem/logic.py___init__,"def __init__(self):
pass
",[],0,[],/sem/logic.py___init__
2432,/home/amandapotts/git/nltk/nltk/sem/logic.py_first,"def first(self):
return self
",[],0,[],/sem/logic.py_first
2433,/home/amandapotts/git/nltk/nltk/sem/logic.py_second,"def second(self):
return self
",[],0,[],/sem/logic.py_second
2434,/home/amandapotts/git/nltk/nltk/sem/logic.py___eq__,"def __eq__(self, other):
return isinstance(other, AnyType) or other.__eq__(self)
",[],0,[],/sem/logic.py___eq__
2435,/home/amandapotts/git/nltk/nltk/sem/logic.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/sem/logic.py___ne__
2436,/home/amandapotts/git/nltk/nltk/sem/logic.py_matches,"def matches(self, other):
return True
",[],0,[],/sem/logic.py_matches
2437,/home/amandapotts/git/nltk/nltk/sem/logic.py_resolve,"def resolve(self, other):
return other
",[],0,[],/sem/logic.py_resolve
2438,/home/amandapotts/git/nltk/nltk/sem/logic.py___str__,"def __str__(self):
return ""?""
",[],0,[],/sem/logic.py___str__
2439,/home/amandapotts/git/nltk/nltk/sem/logic.py_str,"def str(self):
return ""ANY""
",[],0,[],/sem/logic.py_str
2440,/home/amandapotts/git/nltk/nltk/sem/logic.py_read_type,"def read_type(type_string):
assert isinstance(type_string, str)
type_string = type_string.replace("" "", """")  # remove spaces
if type_string[0] == ""<"":
assert type_string[-1] == "">""
paren_count = 0
for i, char in enumerate(type_string):
if char == ""<"":
paren_count += 1
elif char == "">"":
paren_count -= 1
assert paren_count > 0
elif char == "","":
if paren_count == 1:
break
return ComplexType(
read_type(type_string[1:i]), read_type(type_string[i + 1 : -1])
)
elif type_string[0] == ""%s"" % ENTITY_TYPE:
return ENTITY_TYPE
elif type_string[0] == ""%s"" % TRUTH_TYPE:
return TRUTH_TYPE
elif type_string[0] == ""%s"" % ANY_TYPE:
return ANY_TYPE
else:
raise LogicalExpressionException(
None, ""Unexpected character: '%s'."" % type_string[0]
)
",[],0,[],/sem/logic.py_read_type
2441,/home/amandapotts/git/nltk/nltk/sem/logic.py___init__,"def __init__(self, msg):
super().__init__(msg)
",[],0,[],/sem/logic.py___init__
2442,/home/amandapotts/git/nltk/nltk/sem/logic.py___init__,"def __init__(self, variable, expression=None):
if expression:
msg = (
""The variable '%s' was found in multiple places with different""
"" types in '%s'."" % (variable, expression)
)
else:
msg = (
""The variable '%s' was found in multiple places with different""
"" types."" % (variable)
)
super().__init__(msg)
",[],0,[],/sem/logic.py___init__
2443,/home/amandapotts/git/nltk/nltk/sem/logic.py___init__,"def __init__(self, expression, other_type):
super().__init__(
""The type of '%s', '%s', cannot be resolved with type '%s'""
% (expression, expression.type, other_type)
)
",[],0,[],/sem/logic.py___init__
2444,/home/amandapotts/git/nltk/nltk/sem/logic.py___init__,"def __init__(self, expression, other_type, allowed_type):
super().__init__(
""Cannot set type of %s '%s' to '%s'
% (expression.__class__.__name__, expression, other_type, allowed_type)
)
",[],0,[],/sem/logic.py___init__
2445,/home/amandapotts/git/nltk/nltk/sem/logic.py_typecheck,"def typecheck(expressions, signature=None):
""""""
Ensure correct typing across a collection of ``Expression`` objects.
:param expressions: a collection of expressions
:param signature: dict that maps variable names to types (or string
representations of types)
""""""
for expression in expressions:
signature = expression.typecheck(signature)
for expression in expressions[:-1]:
expression.typecheck(signature)
return signature
",[],0,[],/sem/logic.py_typecheck
2446,/home/amandapotts/git/nltk/nltk/sem/logic.py_substitute_bindings,"def substitute_bindings(self, bindings):
""""""
:return: The object that is obtained by replacing
each variable bound by ``bindings`` with its values.
Aliases are already resolved. (maybe?)
:rtype: (any)
""""""
raise NotImplementedError()
",[],0,[],/sem/logic.py_substitute_bindings
2447,/home/amandapotts/git/nltk/nltk/sem/logic.py_variables,"def variables(self):
""""""
:return: A list of all variables in this object.
""""""
raise NotImplementedError()
",[],0,[],/sem/logic.py_variables
2448,/home/amandapotts/git/nltk/nltk/sem/logic.py_fromstring,"def fromstring(cls, s, type_check=False, signature=None):
if type_check:
return cls._type_checking_logic_parser.parse(s, signature)
else:
return cls._logic_parser.parse(s, signature)
",[],0,[],/sem/logic.py_fromstring
2449,/home/amandapotts/git/nltk/nltk/sem/logic.py___call__,"def __call__(self, other, *additional):
accum = self.applyto(other)
for a in additional:
accum = accum(a)
return accum
",[],0,[],/sem/logic.py___call__
2450,/home/amandapotts/git/nltk/nltk/sem/logic.py_applyto,"def applyto(self, other):
assert isinstance(other, Expression), ""%s is not an Expression"" % other
return ApplicationExpression(self, other)
",[],0,[],/sem/logic.py_applyto
2451,/home/amandapotts/git/nltk/nltk/sem/logic.py___neg__,"def __neg__(self):
return NegatedExpression(self)
",[],0,[],/sem/logic.py___neg__
2452,/home/amandapotts/git/nltk/nltk/sem/logic.py_negate,"def negate(self):
""""""If this is a negated expression, remove the negation.
Otherwise add a negation.""""""
return -self
",[],0,[],/sem/logic.py_negate
2453,/home/amandapotts/git/nltk/nltk/sem/logic.py___and__,"def __and__(self, other):
if not isinstance(other, Expression):
raise TypeError(""%s is not an Expression"" % other)
return AndExpression(self, other)
",[],0,[],/sem/logic.py___and__
2454,/home/amandapotts/git/nltk/nltk/sem/logic.py___or__,"def __or__(self, other):
if not isinstance(other, Expression):
raise TypeError(""%s is not an Expression"" % other)
return OrExpression(self, other)
",[],0,[],/sem/logic.py___or__
2455,/home/amandapotts/git/nltk/nltk/sem/logic.py___gt__,"def __gt__(self, other):
if not isinstance(other, Expression):
raise TypeError(""%s is not an Expression"" % other)
return ImpExpression(self, other)
",[],0,[],/sem/logic.py___gt__
2456,/home/amandapotts/git/nltk/nltk/sem/logic.py___lt__,"def __lt__(self, other):
if not isinstance(other, Expression):
raise TypeError(""%s is not an Expression"" % other)
return IffExpression(self, other)
",[],0,[],/sem/logic.py___lt__
2457,/home/amandapotts/git/nltk/nltk/sem/logic.py___eq__,"def __eq__(self, other):
return NotImplemented
",[],0,[],/sem/logic.py___eq__
2458,/home/amandapotts/git/nltk/nltk/sem/logic.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/sem/logic.py___ne__
2459,/home/amandapotts/git/nltk/nltk/sem/logic.py_equiv,"def equiv(self, other, prover=None):
""""""
Check for logical equivalence.
Pass the expression (self <-> other) to the theorem prover.
If the prover says it is valid, then the self and other are equal.
:param other: an ``Expression`` to check equality against
:param prover: a ``nltk.inference.api.Prover``
""""""
assert isinstance(other, Expression), ""%s is not an Expression"" % other
if prover is None:
from nltk.inference import Prover9
prover = Prover9()
bicond = IffExpression(self.simplify(), other.simplify())
return prover.prove(bicond)
",[],0,[],/sem/logic.py_equiv
2460,/home/amandapotts/git/nltk/nltk/sem/logic.py___hash__,"def __hash__(self):
return hash(repr(self))
",[],0,[],/sem/logic.py___hash__
2461,/home/amandapotts/git/nltk/nltk/sem/logic.py_substitute_bindings,"def substitute_bindings(self, bindings):
expr = self
for var in expr.variables():
if var in bindings:
val = bindings[var]
if isinstance(val, Variable):
val = self.make_VariableExpression(val)
elif not isinstance(val, Expression):
raise ValueError(
""Can not substitute a non-expression ""
""value into an expression: %r"" % (val,)
)
val = val.substitute_bindings(bindings)
expr = expr.replace(var, val)
return expr.simplify()
",[],0,[],/sem/logic.py_substitute_bindings
2462,/home/amandapotts/git/nltk/nltk/sem/logic.py_typecheck,"def typecheck(self, signature=None):
""""""
Infer and check types.  Raise exceptions if necessary.
:param signature: dict that maps variable names to types (or string
representations of types)
:return: the signature, plus any additional type mappings
""""""
sig = defaultdict(list)
if signature:
for key in signature:
val = signature[key]
varEx = VariableExpression(Variable(key))
if isinstance(val, Type):
varEx.type = val
else:
varEx.type = read_type(val)
sig[key].append(varEx)
self._set_type(signature=sig)
return {key: sig[key][0].type for key in sig}
",[],0,[],/sem/logic.py_typecheck
2463,/home/amandapotts/git/nltk/nltk/sem/logic.py_findtype,"def findtype(self, variable):
""""""
Find the type of the given variable as it is used in this expression.
For example, finding the type of ""P"" in ""P(x) & Q(x,y)"" yields ""<e,t>""
:param variable: Variable
""""""
raise NotImplementedError()
",[],0,[],/sem/logic.py_findtype
2464,/home/amandapotts/git/nltk/nltk/sem/logic.py__set_type,"def _set_type(self, other_type=ANY_TYPE, signature=None):
""""""
Set the type of this expression to be the given type.  Raise type
exceptions where applicable.
:param other_type: Type
:param signature: dict(str -> list(AbstractVariableExpression))
""""""
raise NotImplementedError()
",[],0,[],/sem/logic.py__set_type
2465,/home/amandapotts/git/nltk/nltk/sem/logic.py_normalize,"def normalize(self, newvars=None):
""""""Rename auto-generated unique variables""""""
",[],0,[],/sem/logic.py_normalize
2466,/home/amandapotts/git/nltk/nltk/sem/logic.py_visit,"def visit(self, function, combinator):
""""""
Recursively visit subexpressions.  Apply 'function' to each
subexpression and pass the result of each function application
to the 'combinator' for aggregation:
return combinator(map(function, self.subexpressions))
Bound variables are neither applied upon by the function nor given to
the combinator.
:param function: ``Function<Expression,T>`` to call on each subexpression
:param combinator: ``Function<list<T>,R>`` to combine the results of the
function calls
:return: result of combination ``R``
""""""
raise NotImplementedError()
",[],0,[],/sem/logic.py_visit
2467,/home/amandapotts/git/nltk/nltk/sem/logic.py___repr__,"def __repr__(self):
return f""<{self.__class__.__name__} {self}>""
",[],0,[],/sem/logic.py___repr__
2468,/home/amandapotts/git/nltk/nltk/sem/logic.py___str__,"def __str__(self):
return self.str()
",[],0,[],/sem/logic.py___str__
2469,/home/amandapotts/git/nltk/nltk/sem/logic.py_variables,"def variables(self):
""""""
Return a set of all the variables for binding substitution.
The variables returned include all free (non-bound) individual
variables and any variable starting with '?' or '@'.
:return: set of ``Variable`` objects
""""""
return self.free() | {
p for p in self.predicates() | self.constants() if re.match(""^[?@]"", p.name)
}
",[],0,[],/sem/logic.py_variables
2470,/home/amandapotts/git/nltk/nltk/sem/logic.py_make_VariableExpression,"def make_VariableExpression(self, variable):
return VariableExpression(variable)
",[],0,[],/sem/logic.py_make_VariableExpression
2471,/home/amandapotts/git/nltk/nltk/sem/logic.py___init__,"def __init__(self, function, argument):
""""""
:param function: ``Expression``, for the function expression
:param argument: ``Expression``, for the argument
""""""
assert isinstance(function, Expression), ""%s is not an Expression"" % function
assert isinstance(argument, Expression), ""%s is not an Expression"" % argument
self.function = function
self.argument = argument
",[],0,[],/sem/logic.py___init__
2472,/home/amandapotts/git/nltk/nltk/sem/logic.py_simplify,"def simplify(self):
function = self.function.simplify()
argument = self.argument.simplify()
if isinstance(function, LambdaExpression):
return function.term.replace(function.variable, argument).simplify()
else:
return self.__class__(function, argument)
",[],0,[],/sem/logic.py_simplify
2473,/home/amandapotts/git/nltk/nltk/sem/logic.py_type,"def type(self):
if isinstance(self.function.type, ComplexType):
return self.function.type.second
else:
return ANY_TYPE
",[],0,[],/sem/logic.py_type
2474,/home/amandapotts/git/nltk/nltk/sem/logic.py__set_type,"def _set_type(self, other_type=ANY_TYPE, signature=None):
"""""":see Expression._set_type()""""""
assert isinstance(other_type, Type)
if signature is None:
signature = defaultdict(list)
self.argument._set_type(ANY_TYPE, signature)
try:
self.function._set_type(
ComplexType(self.argument.type, other_type), signature
)
except TypeResolutionException as e:
raise TypeException(
""The function '%s' is of type '%s' and cannot be applied ""
""to '%s' of type '%s'.  Its argument must match type '%s'.""
% (
self.function,
self.function.type,
self.argument,
self.argument.type,
self.function.type.first,
)
) from e
",[],0,[],/sem/logic.py__set_type
2475,/home/amandapotts/git/nltk/nltk/sem/logic.py_findtype,"def findtype(self, variable):
"""""":see Expression.findtype()""""""
assert isinstance(variable, Variable), ""%s is not a Variable"" % variable
if self.is_atom():
function, args = self.uncurry()
else:
function = self.function
args = [self.argument]
found = [arg.findtype(variable) for arg in [function] + args]
unique = []
for f in found:
if f != ANY_TYPE:
if unique:
for u in unique:
if f.matches(u):
break
else:
unique.append(f)
if len(unique) == 1:
return list(unique)[0]
else:
return ANY_TYPE
",[],0,[],/sem/logic.py_findtype
2476,/home/amandapotts/git/nltk/nltk/sem/logic.py_constants,"def constants(self):
"""""":see: Expression.constants()""""""
if isinstance(self.function, AbstractVariableExpression):
function_constants = set()
else:
function_constants = self.function.constants()
return function_constants | self.argument.constants()
",[],0,[],/sem/logic.py_constants
2477,/home/amandapotts/git/nltk/nltk/sem/logic.py_predicates,"def predicates(self):
"""""":see: Expression.predicates()""""""
if isinstance(self.function, ConstantExpression):
function_preds = {self.function.variable}
else:
function_preds = self.function.predicates()
return function_preds | self.argument.predicates()
",[],0,[],/sem/logic.py_predicates
2478,/home/amandapotts/git/nltk/nltk/sem/logic.py_visit,"def visit(self, function, combinator):
"""""":see: Expression.visit()""""""
return combinator([function(self.function), function(self.argument)])
",[],0,[],/sem/logic.py_visit
2479,/home/amandapotts/git/nltk/nltk/sem/logic.py___eq__,"def __eq__(self, other):
return (
isinstance(other, ApplicationExpression)
and self.function == other.function
and self.argument == other.argument
)
",[],0,[],/sem/logic.py___eq__
2480,/home/amandapotts/git/nltk/nltk/sem/logic.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/sem/logic.py___ne__
2481,/home/amandapotts/git/nltk/nltk/sem/logic.py___str__,"def __str__(self):
if self.is_atom():
function, args = self.uncurry()
arg_str = "","".join(""%s"" % arg for arg in args)
else:
function = self.function
arg_str = ""%s"" % self.argument
function_str = ""%s"" % function
parenthesize_function = False
if isinstance(function, LambdaExpression):
if isinstance(function.term, ApplicationExpression):
if not isinstance(function.term.function, AbstractVariableExpression):
parenthesize_function = True
elif not isinstance(function.term, BooleanExpression):
parenthesize_function = True
elif isinstance(function, ApplicationExpression):
parenthesize_function = True
if parenthesize_function:
function_str = Tokens.OPEN + function_str + Tokens.CLOSE
return function_str + Tokens.OPEN + arg_str + Tokens.CLOSE
",[],0,[],/sem/logic.py___str__
2482,/home/amandapotts/git/nltk/nltk/sem/logic.py_uncurry,"def uncurry(self):
""""""
Uncurry this application expression
return: A tuple (base-function, arg-list)
""""""
function = self.function
args = [self.argument]
while isinstance(function, ApplicationExpression):
args.insert(0, function.argument)
function = function.function
return (function, args)
",[],0,[],/sem/logic.py_uncurry
2483,/home/amandapotts/git/nltk/nltk/sem/logic.py_args,"def args(self):
""""""
Return uncurried arg-list
""""""
return self.uncurry()[1]
",[],0,[],/sem/logic.py_args
2484,/home/amandapotts/git/nltk/nltk/sem/logic.py___init__,"def __init__(self, variable):
""""""
:param variable: ``Variable``, for the variable
""""""
assert isinstance(variable, Variable), ""%s is not a Variable"" % variable
self.variable = variable
",[],0,[],/sem/logic.py___init__
2485,/home/amandapotts/git/nltk/nltk/sem/logic.py_simplify,"def simplify(self):
return self
",[],0,[],/sem/logic.py_simplify
2486,/home/amandapotts/git/nltk/nltk/sem/logic.py_replace,"def replace(self, variable, expression, replace_bound=False, alpha_convert=True):
"""""":see: Expression.replace()""""""
assert isinstance(variable, Variable), ""%s is not an Variable"" % variable
assert isinstance(expression, Expression), (
""%s is not an Expression"" % expression
)
if self.variable == variable:
return expression
else:
return self
",[],0,[],/sem/logic.py_replace
2487,/home/amandapotts/git/nltk/nltk/sem/logic.py__set_type,"def _set_type(self, other_type=ANY_TYPE, signature=None):
"""""":see Expression._set_type()""""""
assert isinstance(other_type, Type)
if signature is None:
signature = defaultdict(list)
resolution = other_type
for varEx in signature[self.variable.name]:
resolution = varEx.type.resolve(resolution)
if not resolution:
raise InconsistentTypeHierarchyException(self)
signature[self.variable.name].append(self)
for varEx in signature[self.variable.name]:
varEx.type = resolution
",[],0,[],/sem/logic.py__set_type
2488,/home/amandapotts/git/nltk/nltk/sem/logic.py_findtype,"def findtype(self, variable):
"""""":see Expression.findtype()""""""
assert isinstance(variable, Variable), ""%s is not a Variable"" % variable
if self.variable == variable:
return self.type
else:
return ANY_TYPE
",[],0,[],/sem/logic.py_findtype
2489,/home/amandapotts/git/nltk/nltk/sem/logic.py_predicates,"def predicates(self):
"""""":see: Expression.predicates()""""""
return set()
",[],0,[],/sem/logic.py_predicates
2490,/home/amandapotts/git/nltk/nltk/sem/logic.py___eq__,"def __eq__(self, other):
""""""Allow equality between instances of ``AbstractVariableExpression``
subtypes.""""""
return (
isinstance(other, AbstractVariableExpression)
and self.variable == other.variable
)
",[],0,[],/sem/logic.py___eq__
2491,/home/amandapotts/git/nltk/nltk/sem/logic.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/sem/logic.py___ne__
2492,/home/amandapotts/git/nltk/nltk/sem/logic.py___lt__,"def __lt__(self, other):
if not isinstance(other, AbstractVariableExpression):
raise TypeError
return self.variable < other.variable
",[],0,[],/sem/logic.py___lt__
2493,/home/amandapotts/git/nltk/nltk/sem/logic.py___str__,"def __str__(self):
return ""%s"" % self.variable
",[],0,[],/sem/logic.py___str__
2494,/home/amandapotts/git/nltk/nltk/sem/logic.py__set_type,"def _set_type(self, other_type=ANY_TYPE, signature=None):
"""""":see Expression._set_type()""""""
assert isinstance(other_type, Type)
if signature is None:
signature = defaultdict(list)
if not other_type.matches(ENTITY_TYPE):
raise IllegalTypeException(self, other_type, ENTITY_TYPE)
signature[self.variable.name].append(self)
",[],0,[],/sem/logic.py__set_type
2495,/home/amandapotts/git/nltk/nltk/sem/logic.py__get_type,"def _get_type(self):
return ENTITY_TYPE
",[],0,[],/sem/logic.py__get_type
2496,/home/amandapotts/git/nltk/nltk/sem/logic.py_free,"def free(self):
"""""":see: Expression.free()""""""
return {self.variable}
",[],0,[],/sem/logic.py_free
2497,/home/amandapotts/git/nltk/nltk/sem/logic.py_constants,"def constants(self):
"""""":see: Expression.constants()""""""
return set()
",[],0,[],/sem/logic.py_constants
2498,/home/amandapotts/git/nltk/nltk/sem/logic.py_free,"def free(self):
"""""":see: Expression.free()""""""
return {self.variable}
",[],0,[],/sem/logic.py_free
2499,/home/amandapotts/git/nltk/nltk/sem/logic.py_constants,"def constants(self):
"""""":see: Expression.constants()""""""
return set()
",[],0,[],/sem/logic.py_constants
2500,/home/amandapotts/git/nltk/nltk/sem/logic.py__set_type,"def _set_type(self, other_type=ANY_TYPE, signature=None):
"""""":see Expression._set_type()""""""
assert isinstance(other_type, Type)
if signature is None:
signature = defaultdict(list)
if other_type == ANY_TYPE:
resolution = ENTITY_TYPE
else:
resolution = other_type
if self.type != ENTITY_TYPE:
resolution = resolution.resolve(self.type)
for varEx in signature[self.variable.name]:
resolution = varEx.type.resolve(resolution)
if not resolution:
raise InconsistentTypeHierarchyException(self)
signature[self.variable.name].append(self)
for varEx in signature[self.variable.name]:
varEx.type = resolution
",[],0,[],/sem/logic.py__set_type
2501,/home/amandapotts/git/nltk/nltk/sem/logic.py_free,"def free(self):
"""""":see: Expression.free()""""""
return set()
",[],0,[],/sem/logic.py_free
2502,/home/amandapotts/git/nltk/nltk/sem/logic.py_constants,"def constants(self):
"""""":see: Expression.constants()""""""
return {self.variable}
",[],0,[],/sem/logic.py_constants
2503,/home/amandapotts/git/nltk/nltk/sem/logic.py_VariableExpression,"def VariableExpression(variable):
""""""
This is a factory method that instantiates and returns a subtype of
``AbstractVariableExpression`` appropriate for the given variable.
""""""
assert isinstance(variable, Variable), ""%s is not a Variable"" % variable
if is_indvar(variable.name):
return IndividualVariableExpression(variable)
elif is_funcvar(variable.name):
return FunctionVariableExpression(variable)
elif is_eventvar(variable.name):
return EventVariableExpression(variable)
else:
return ConstantExpression(variable)
",[],0,[],/sem/logic.py_VariableExpression
2504,/home/amandapotts/git/nltk/nltk/sem/logic.py___init__,"def __init__(self, variable, term):
""""""
:param variable: ``Variable``, for the variable
:param term: ``Expression``, for the term
""""""
assert isinstance(variable, Variable), ""%s is not a Variable"" % variable
assert isinstance(term, Expression), ""%s is not an Expression"" % term
self.variable = variable
self.term = term
",[],0,[],/sem/logic.py___init__
2505,/home/amandapotts/git/nltk/nltk/sem/logic.py_replace,"def replace(self, variable, expression, replace_bound=False, alpha_convert=True):
"""""":see: Expression.replace()""""""
assert isinstance(variable, Variable), ""%s is not a Variable"" % variable
assert isinstance(expression, Expression), (
""%s is not an Expression"" % expression
)
if self.variable == variable:
if replace_bound:
assert isinstance(expression, AbstractVariableExpression), (
""%s is not a AbstractVariableExpression"" % expression
)
return self.__class__(
expression.variable,
self.term.replace(variable, expression, True, alpha_convert),
)
else:
return self
else:
if alpha_convert and self.variable in expression.free():
self = self.alpha_convert(unique_variable(pattern=self.variable))
return self.__class__(
self.variable,
self.term.replace(variable, expression, replace_bound, alpha_convert),
)
",[],0,[],/sem/logic.py_replace
2506,/home/amandapotts/git/nltk/nltk/sem/logic.py_alpha_convert,"def alpha_convert(self, newvar):
""""""Rename all occurrences of the variable introduced by this variable
binder in the expression to ``newvar``.
:param newvar: ``Variable``, for the new variable
""""""
assert isinstance(newvar, Variable), ""%s is not a Variable"" % newvar
return self.__class__(
newvar, self.term.replace(self.variable, VariableExpression(newvar), True)
)
",[],0,[],/sem/logic.py_alpha_convert
2507,/home/amandapotts/git/nltk/nltk/sem/logic.py_free,"def free(self):
"""""":see: Expression.free()""""""
return self.term.free() - {self.variable}
",[],0,[],/sem/logic.py_free
2508,/home/amandapotts/git/nltk/nltk/sem/logic.py_findtype,"def findtype(self, variable):
"""""":see Expression.findtype()""""""
assert isinstance(variable, Variable), ""%s is not a Variable"" % variable
if variable == self.variable:
return ANY_TYPE
else:
return self.term.findtype(variable)
",[],0,[],/sem/logic.py_findtype
2509,/home/amandapotts/git/nltk/nltk/sem/logic.py_visit,"def visit(self, function, combinator):
"""""":see: Expression.visit()""""""
return combinator([function(self.term)])
",[],0,[],/sem/logic.py_visit
2510,/home/amandapotts/git/nltk/nltk/sem/logic.py_visit_structured,"def visit_structured(self, function, combinator):
"""""":see: Expression.visit_structured()""""""
return combinator(self.variable, function(self.term))
",[],0,[],/sem/logic.py_visit_structured
2511,/home/amandapotts/git/nltk/nltk/sem/logic.py___eq__,"def __eq__(self, other):
r""""""Defines equality modulo alphabetic variance.  If we are comparing
\x.M  and \y.N, then check equality of M and N[x/y].""""""
if isinstance(self, other.__class__) or isinstance(other, self.__class__):
if self.variable == other.variable:
return self.term == other.term
else:
varex = VariableExpression(self.variable)
return self.term == other.term.replace(other.variable, varex)
else:
return False
",[],0,[],/sem/logic.py___eq__
2512,/home/amandapotts/git/nltk/nltk/sem/logic.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/sem/logic.py___ne__
2513,/home/amandapotts/git/nltk/nltk/sem/logic.py_type,"def type(self):
return ComplexType(self.term.findtype(self.variable), self.term.type)
",[],0,[],/sem/logic.py_type
2514,/home/amandapotts/git/nltk/nltk/sem/logic.py__set_type,"def _set_type(self, other_type=ANY_TYPE, signature=None):
"""""":see Expression._set_type()""""""
assert isinstance(other_type, Type)
if signature is None:
signature = defaultdict(list)
self.term._set_type(other_type.second, signature)
if not self.type.resolve(other_type):
raise TypeResolutionException(self, other_type)
",[],0,[],/sem/logic.py__set_type
2515,/home/amandapotts/git/nltk/nltk/sem/logic.py___str__,"def __str__(self):
variables = [self.variable]
term = self.term
while term.__class__ == self.__class__:
variables.append(term.variable)
term = term.term
return (
Tokens.LAMBDA
+ "" "".join(""%s"" % v for v in variables)
+ Tokens.DOT
+ ""%s"" % term
)
",[],0,[],/sem/logic.py___str__
2516,/home/amandapotts/git/nltk/nltk/sem/logic.py_type,"def type(self):
return TRUTH_TYPE
",[],0,[],/sem/logic.py_type
2517,/home/amandapotts/git/nltk/nltk/sem/logic.py__set_type,"def _set_type(self, other_type=ANY_TYPE, signature=None):
"""""":see Expression._set_type()""""""
assert isinstance(other_type, Type)
if signature is None:
signature = defaultdict(list)
if not other_type.matches(TRUTH_TYPE):
raise IllegalTypeException(self, other_type, TRUTH_TYPE)
self.term._set_type(TRUTH_TYPE, signature)
",[],0,[],/sem/logic.py__set_type
2518,/home/amandapotts/git/nltk/nltk/sem/logic.py___str__,"def __str__(self):
variables = [self.variable]
term = self.term
while term.__class__ == self.__class__:
variables.append(term.variable)
term = term.term
return (
self.getQuantifier()
+ "" ""
+ "" "".join(""%s"" % v for v in variables)
+ Tokens.DOT
+ ""%s"" % term
)
",[],0,[],/sem/logic.py___str__
2519,/home/amandapotts/git/nltk/nltk/sem/logic.py_getQuantifier,"def getQuantifier(self):
return Tokens.EXISTS
",[],0,[],/sem/logic.py_getQuantifier
2520,/home/amandapotts/git/nltk/nltk/sem/logic.py_getQuantifier,"def getQuantifier(self):
return Tokens.ALL
",[],0,[],/sem/logic.py_getQuantifier
2521,/home/amandapotts/git/nltk/nltk/sem/logic.py_getQuantifier,"def getQuantifier(self):
return Tokens.IOTA
",[],0,[],/sem/logic.py_getQuantifier
2522,/home/amandapotts/git/nltk/nltk/sem/logic.py___init__,"def __init__(self, term):
assert isinstance(term, Expression), ""%s is not an Expression"" % term
self.term = term
",[],0,[],/sem/logic.py___init__
2523,/home/amandapotts/git/nltk/nltk/sem/logic.py_type,"def type(self):
return TRUTH_TYPE
",[],0,[],/sem/logic.py_type
2524,/home/amandapotts/git/nltk/nltk/sem/logic.py__set_type,"def _set_type(self, other_type=ANY_TYPE, signature=None):
"""""":see Expression._set_type()""""""
assert isinstance(other_type, Type)
if signature is None:
signature = defaultdict(list)
if not other_type.matches(TRUTH_TYPE):
raise IllegalTypeException(self, other_type, TRUTH_TYPE)
self.term._set_type(TRUTH_TYPE, signature)
",[],0,[],/sem/logic.py__set_type
2525,/home/amandapotts/git/nltk/nltk/sem/logic.py_findtype,"def findtype(self, variable):
assert isinstance(variable, Variable), ""%s is not a Variable"" % variable
return self.term.findtype(variable)
",[],0,[],/sem/logic.py_findtype
2526,/home/amandapotts/git/nltk/nltk/sem/logic.py_visit,"def visit(self, function, combinator):
"""""":see: Expression.visit()""""""
return combinator([function(self.term)])
",[],0,[],/sem/logic.py_visit
2527,/home/amandapotts/git/nltk/nltk/sem/logic.py_negate,"def negate(self):
"""""":see: Expression.negate()""""""
return self.term
",[],0,[],/sem/logic.py_negate
2528,/home/amandapotts/git/nltk/nltk/sem/logic.py___eq__,"def __eq__(self, other):
return isinstance(other, NegatedExpression) and self.term == other.term
",[],0,[],/sem/logic.py___eq__
2529,/home/amandapotts/git/nltk/nltk/sem/logic.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/sem/logic.py___ne__
2530,/home/amandapotts/git/nltk/nltk/sem/logic.py___str__,"def __str__(self):
return Tokens.NOT + ""%s"" % self.term
",[],0,[],/sem/logic.py___str__
2531,/home/amandapotts/git/nltk/nltk/sem/logic.py___init__,"def __init__(self, first, second):
assert isinstance(first, Expression), ""%s is not an Expression"" % first
assert isinstance(second, Expression), ""%s is not an Expression"" % second
self.first = first
self.second = second
",[],0,[],/sem/logic.py___init__
2532,/home/amandapotts/git/nltk/nltk/sem/logic.py_type,"def type(self):
return TRUTH_TYPE
",[],0,[],/sem/logic.py_type
2533,/home/amandapotts/git/nltk/nltk/sem/logic.py_findtype,"def findtype(self, variable):
"""""":see Expression.findtype()""""""
assert isinstance(variable, Variable), ""%s is not a Variable"" % variable
f = self.first.findtype(variable)
s = self.second.findtype(variable)
if f == s or s == ANY_TYPE:
return f
elif f == ANY_TYPE:
return s
else:
return ANY_TYPE
",[],0,[],/sem/logic.py_findtype
2534,/home/amandapotts/git/nltk/nltk/sem/logic.py_visit,"def visit(self, function, combinator):
"""""":see: Expression.visit()""""""
return combinator([function(self.first), function(self.second)])
",[],0,[],/sem/logic.py_visit
2535,/home/amandapotts/git/nltk/nltk/sem/logic.py___eq__,"def __eq__(self, other):
return (
(isinstance(self, other.__class__) or isinstance(other, self.__class__))
and self.first == other.first
and self.second == other.second
)
",[],0,[],/sem/logic.py___eq__
2536,/home/amandapotts/git/nltk/nltk/sem/logic.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/sem/logic.py___ne__
2537,/home/amandapotts/git/nltk/nltk/sem/logic.py___str__,"def __str__(self):
first = self._str_subex(self.first)
second = self._str_subex(self.second)
return Tokens.OPEN + first + "" "" + self.getOp() + "" "" + second + Tokens.CLOSE
",[],0,[],/sem/logic.py___str__
2538,/home/amandapotts/git/nltk/nltk/sem/logic.py__str_subex,"def _str_subex(self, subex):
return ""%s"" % subex
",[],0,[],/sem/logic.py__str_subex
2539,/home/amandapotts/git/nltk/nltk/sem/logic.py__set_type,"def _set_type(self, other_type=ANY_TYPE, signature=None):
"""""":see Expression._set_type()""""""
assert isinstance(other_type, Type)
if signature is None:
signature = defaultdict(list)
if not other_type.matches(TRUTH_TYPE):
raise IllegalTypeException(self, other_type, TRUTH_TYPE)
self.first._set_type(TRUTH_TYPE, signature)
self.second._set_type(TRUTH_TYPE, signature)
",[],0,[],/sem/logic.py__set_type
2540,/home/amandapotts/git/nltk/nltk/sem/logic.py_getOp,"def getOp(self):
return Tokens.AND
",[],0,[],/sem/logic.py_getOp
2541,/home/amandapotts/git/nltk/nltk/sem/logic.py__str_subex,"def _str_subex(self, subex):
s = ""%s"" % subex
if isinstance(subex, AndExpression):
return s[1:-1]
return s
",[],0,[],/sem/logic.py__str_subex
2542,/home/amandapotts/git/nltk/nltk/sem/logic.py_getOp,"def getOp(self):
return Tokens.OR
",[],0,[],/sem/logic.py_getOp
2543,/home/amandapotts/git/nltk/nltk/sem/logic.py__str_subex,"def _str_subex(self, subex):
s = ""%s"" % subex
if isinstance(subex, OrExpression):
return s[1:-1]
return s
",[],0,[],/sem/logic.py__str_subex
2544,/home/amandapotts/git/nltk/nltk/sem/logic.py_getOp,"def getOp(self):
return Tokens.IMP
",[],0,[],/sem/logic.py_getOp
2545,/home/amandapotts/git/nltk/nltk/sem/logic.py_getOp,"def getOp(self):
return Tokens.IFF
",[],0,[],/sem/logic.py_getOp
2546,/home/amandapotts/git/nltk/nltk/sem/logic.py__set_type,"def _set_type(self, other_type=ANY_TYPE, signature=None):
"""""":see Expression._set_type()""""""
assert isinstance(other_type, Type)
if signature is None:
signature = defaultdict(list)
if not other_type.matches(TRUTH_TYPE):
raise IllegalTypeException(self, other_type, TRUTH_TYPE)
self.first._set_type(ENTITY_TYPE, signature)
self.second._set_type(ENTITY_TYPE, signature)
",[],0,[],/sem/logic.py__set_type
2547,/home/amandapotts/git/nltk/nltk/sem/logic.py_getOp,"def getOp(self):
return Tokens.EQ
",[],0,[],/sem/logic.py_getOp
2548,/home/amandapotts/git/nltk/nltk/sem/logic.py___init__,"def __init__(self, index, message):
self.index = index
Exception.__init__(self, message)
",[],0,[],/sem/logic.py___init__
2549,/home/amandapotts/git/nltk/nltk/sem/logic.py___init__,"def __init__(self, index, unexpected=None, expected=None, message=None):
if unexpected and expected:
msg = ""Unexpected token: '%s'.  "" ""Expected token '%s'."" % (
unexpected,
expected,
)
elif unexpected:
msg = ""Unexpected token: '%s'."" % unexpected
if message:
msg += ""  "" + message
else:
msg = ""Expected token '%s'."" % expected
LogicalExpressionException.__init__(self, index, msg)
",[],0,[],/sem/logic.py___init__
2550,/home/amandapotts/git/nltk/nltk/sem/logic.py___init__,"def __init__(self, index, message=None):
if not message:
message = ""More tokens expected.""
LogicalExpressionException.__init__(
self, index, ""End of input found.  "" + message
)
",[],0,[],/sem/logic.py___init__
2551,/home/amandapotts/git/nltk/nltk/sem/logic.py_is_indvar,"def is_indvar(expr):
""""""
An individual variable must be a single lowercase character other than 'e',
followed by zero or more digits.
:param expr: str
:return: bool True if expr is of the correct form
""""""
assert isinstance(expr, str), ""%s is not a string"" % expr
return re.match(r""^[a-df-z]\d*$"", expr) is not None
",[],0,[],/sem/logic.py_is_indvar
2552,/home/amandapotts/git/nltk/nltk/sem/logic.py_is_funcvar,"def is_funcvar(expr):
""""""
A function variable must be a single uppercase character followed by
zero or more digits.
:param expr: str
:return: bool True if expr is of the correct form
""""""
assert isinstance(expr, str), ""%s is not a string"" % expr
return re.match(r""^[A-Z]\d*$"", expr) is not None
",[],0,[],/sem/logic.py_is_funcvar
2553,/home/amandapotts/git/nltk/nltk/sem/logic.py_is_eventvar,"def is_eventvar(expr):
""""""
An event variable must be a single lowercase 'e' character followed by
zero or more digits.
:param expr: str
:return: bool True if expr is of the correct form
""""""
assert isinstance(expr, str), ""%s is not a string"" % expr
return re.match(r""^e\d*$"", expr) is not None
",[],0,[],/sem/logic.py_is_eventvar
2554,/home/amandapotts/git/nltk/nltk/sem/logic.py_demo,"def demo():
lexpr = Expression.fromstring
print(""="" * 20 + ""Test reader"" + ""="" * 20)
print(lexpr(r""john""))
print(lexpr(r""man(x)""))
print(lexpr(r""-man(x)""))
print(lexpr(r""(man(x) & tall(x) & walks(x))""))
print(lexpr(r""exists x.(man(x) & tall(x) & walks(x))""))
print(lexpr(r""\x.man(x)""))
print(lexpr(r""\x.man(x)(john)""))
print(lexpr(r""\x y.sees(x,y)""))
print(lexpr(r""\x y.sees(x,y)(a,b)""))
print(lexpr(r""(\x.exists y.walks(x,y))(x)""))
print(lexpr(r""exists x.x = y""))
print(lexpr(r""exists x.(x = y)""))
print(lexpr(""P(x) & x=y & P(y)""))
print(lexpr(r""\P Q.exists x.(P(x) & Q(x))""))
print(lexpr(r""man(x) <-> tall(x)""))
print(""="" * 20 + ""Test simplify"" + ""="" * 20)
print(lexpr(r""\x.\y.sees(x,y)(john)(mary)"").simplify())
print(lexpr(r""\x.\y.sees(x,y)(john, mary)"").simplify())
print(lexpr(r""all x.(man(x) & (\x.exists y.walks(x,y))(x))"").simplify())
print(lexpr(r""(\P.\Q.exists x.(P(x) & Q(x)))(\x.dog(x))(\x.bark(x))"").simplify())
print(""="" * 20 + ""Test alpha conversion and binder expression equality"" + ""="" * 20)
e1 = lexpr(""exists x.P(x)"")
print(e1)
e2 = e1.alpha_convert(Variable(""z""))
print(e2)
print(e1 == e2)
",[],0,[],/sem/logic.py_demo
2555,/home/amandapotts/git/nltk/nltk/sem/logic.py_demo_errors,"def demo_errors():
print(""="" * 20 + ""Test reader errors"" + ""="" * 20)
demoException(""(P(x) & Q(x)"")
demoException(""((P(x) &) & Q(x))"")
demoException(""P(x) -> "")
demoException(""P(x"")
demoException(""P(x,"")
demoException(""P(x,)"")
demoException(""exists"")
demoException(""exists x."")
demoException(""\\"")
demoException(""\\ x y."")
demoException(""P(x)Q(x)"")
demoException(""(P(x)Q(x)"")
demoException(""exists x -> y"")
",[],0,[],/sem/logic.py_demo_errors
2556,/home/amandapotts/git/nltk/nltk/sem/logic.py_demoException,"def demoException(s):
try:
Expression.fromstring(s)
except LogicalExpressionException as e:
print(f""{e.__class__.__name__}: {e}"")
",[],0,[],/sem/logic.py_demoException
2557,/home/amandapotts/git/nltk/nltk/sem/logic.py_printtype,"def printtype(ex):
print(f""{ex.str()} : {ex.type}"")
",[],0,[],/sem/logic.py_printtype
2558,/home/amandapotts/git/nltk/nltk/sem/cooper_storage.py___init__,"def __init__(self, featstruct):
""""""
:param featstruct: The value of the ``sem`` node in a tree from
``parse_with_bindops()``
:type featstruct: FeatStruct (with features ``core`` and ``store``)
""""""
self.featstruct = featstruct
self.readings = []
try:
self.core = featstruct[""CORE""]
self.store = featstruct[""STORE""]
except KeyError:
print(""%s is not a Cooper storage structure"" % featstruct)
",[],0,[],/sem/cooper_storage.py___init__
2559,/home/amandapotts/git/nltk/nltk/sem/cooper_storage.py__permute,"def _permute(self, lst):
""""""
:return: An iterator over the permutations of the input list
:type lst: list
:rtype: iter
""""""
",[],0,[],/sem/cooper_storage.py__permute
2560,/home/amandapotts/git/nltk/nltk/sem/cooper_storage.py_s_retrieve,"def s_retrieve(self, trace=False):
r""""""
Carry out S-Retrieval of binding operators in store. If hack=True,
serialize the bindop and core as strings and reparse. Ugh.
Each permutation of the store (i.e. list of binding operators) is
taken to be a possible scoping of quantifiers. We iterate through the
binding operators in each permutation, and successively apply them to
the current term, starting with the core semantic representation,
working from the inside out.
Binding operators are of the form::
bo(\P.all x.(man(x) -> P(x)),z1)
""""""
for perm, store_perm in enumerate(self._permute(self.store)):
if trace:
print(""Permutation %s"" % (perm + 1))
term = self.core
for bindop in store_perm:
quant, varex = tuple(bindop.args)
term = ApplicationExpression(
quant, LambdaExpression(varex.variable, term)
)
if trace:
print(""  "", term)
term = term.simplify()
self.readings.append(term)
",[],0,[],/sem/cooper_storage.py_s_retrieve
2561,/home/amandapotts/git/nltk/nltk/sem/cooper_storage.py_parse_with_bindops,"def parse_with_bindops(sentence, grammar=None, trace=0):
""""""
Use a grammar with Binding Operators to parse a sentence.
""""""
if not grammar:
grammar = ""grammars/book_grammars/storage.fcfg""
parser = load_parser(grammar, trace=trace, chart_class=InstantiateVarsChart)
tokens = sentence.split()
return list(parser.parse(tokens))
",[],0,[],/sem/cooper_storage.py_parse_with_bindops
2562,/home/amandapotts/git/nltk/nltk/sem/cooper_storage.py_demo,"def demo():
from nltk.sem import cooper_storage as cs
sentence = ""every girl chases a dog""
print()
print(""Analysis of sentence '%s'"" % sentence)
print(""="" * 50)
trees = cs.parse_with_bindops(sentence, trace=0)
for tree in trees:
semrep = cs.CooperStore(tree.label()[""SEM""])
print()
print(""Binding operators:"")
print(""-"" * 15)
for s in semrep.store:
print(s)
print()
print(""Core:"")
print(""-"" * 15)
print(semrep.core)
print()
print(""S-Retrieval:"")
print(""-"" * 15)
semrep.s_retrieve(trace=True)
print(""Readings:"")
print(""-"" * 15)
for i, reading in enumerate(semrep.readings):
print(f""{i + 1}: {reading}"")
",[],0,[],/sem/cooper_storage.py_demo
2563,/home/amandapotts/git/nltk/nltk/sem/hole.py___init__,"def __init__(self, usr):
""""""
Constructor.  `usr' is a ``sem.Expression`` representing an
Underspecified Representation Structure (USR).  A USR has the following
special predicates:
ALL(l,v,n),
EXISTS(l,v,n),
AND(l,n,n),
OR(l,n,n),
IMP(l,n,n),
IFF(l,n,n),
PRED(l,v,n,v[,v]*) where the brackets and star indicate zero or more repetitions,
LEQ(n,n),
HOLE(n),
LABEL(n)
where l is the label of the node described by the predicate, n is either
a label or a hole, and v is a variable.
""""""
self.holes = set()
self.labels = set()
self.fragments = {}  # mapping of label -> formula fragment
self.constraints = set()  # set of Constraints
self._break_down(usr)
self.top_most_labels = self._find_top_most_labels()
self.top_hole = self._find_top_hole()
",[],0,[],/sem/hole.py___init__
2564,/home/amandapotts/git/nltk/nltk/sem/hole.py_is_node,"def is_node(self, x):
""""""
Return true if x is a node (label or hole) in this semantic
representation.
""""""
return x in (self.labels | self.holes)
",[],0,[],/sem/hole.py_is_node
2565,/home/amandapotts/git/nltk/nltk/sem/hole.py__break_down,"def _break_down(self, usr):
""""""
Extract holes, labels, formula fragments and constraints from the hole
semantics underspecified representation (USR).
""""""
if isinstance(usr, AndExpression):
self._break_down(usr.first)
self._break_down(usr.second)
elif isinstance(usr, ApplicationExpression):
func, args = usr.uncurry()
if func.variable.name == Constants.LEQ:
self.constraints.add(Constraint(args[0], args[1]))
elif func.variable.name == Constants.HOLE:
self.holes.add(args[0])
elif func.variable.name == Constants.LABEL:
self.labels.add(args[0])
else:
label = args[0]
assert label not in self.fragments
self.fragments[label] = (func, args[1:])
else:
raise ValueError(usr.label())
",[],0,[],/sem/hole.py__break_down
2566,/home/amandapotts/git/nltk/nltk/sem/hole.py__find_top_nodes,"def _find_top_nodes(self, node_list):
top_nodes = node_list.copy()
for f in self.fragments.values():
args = f[1]
for arg in args:
if arg in node_list:
top_nodes.discard(arg)
return top_nodes
",[],0,[],/sem/hole.py__find_top_nodes
2567,/home/amandapotts/git/nltk/nltk/sem/hole.py__find_top_most_labels,"def _find_top_most_labels(self):
""""""
Return the set of labels which are not referenced directly as part of
another formula fragment.  These will be the top-most labels for the
subtree that they are part of.
""""""
return self._find_top_nodes(self.labels)
",[],0,[],/sem/hole.py__find_top_most_labels
2568,/home/amandapotts/git/nltk/nltk/sem/hole.py__find_top_hole,"def _find_top_hole(self):
""""""
Return the hole that will be the top of the formula tree.
""""""
top_holes = self._find_top_nodes(self.holes)
assert len(top_holes) == 1  # it must be unique
return top_holes.pop()
",[],0,[],/sem/hole.py__find_top_hole
2569,/home/amandapotts/git/nltk/nltk/sem/hole.py_pluggings,"def pluggings(self):
""""""
Calculate and return all the legal pluggings (mappings of labels to
holes) of this semantics given the constraints.
""""""
record = []
self._plug_nodes([(self.top_hole, [])], self.top_most_labels, {}, record)
return record
",[],0,[],/sem/hole.py_pluggings
2570,/home/amandapotts/git/nltk/nltk/sem/hole.py__plug_nodes,"def _plug_nodes(self, queue, potential_labels, plug_acc, record):
""""""
Plug the nodes in `queue' with the labels in `potential_labels'.
Each element of `queue' is a tuple of the node to plug and the list of
ancestor holes from the root of the graph to that node.
`potential_labels' is a set of the labels which are still available for
plugging.
`plug_acc' is the incomplete mapping of holes to labels made on the
current branch of the search tree so far.
`record' is a list of all the complete pluggings that we have found in
total so far.  It is the only parameter that is destructively updated.
""""""
if queue != []:
(node, ancestors) = queue[0]
if node in self.holes:
self._plug_hole(
node, ancestors, queue[1:], potential_labels, plug_acc, record
)
else:
assert node in self.labels
args = self.fragments[node][1]
head = [(a, ancestors) for a in args if self.is_node(a)]
self._plug_nodes(head + queue[1:], potential_labels, plug_acc, record)
else:
raise Exception(""queue empty"")
",[],0,[],/sem/hole.py__plug_nodes
2571,/home/amandapotts/git/nltk/nltk/sem/hole.py__plug_hole,"def _plug_hole(self, hole, ancestors0, queue, potential_labels0, plug_acc0, record):
""""""
Try all possible ways of plugging a single hole.
See _plug_nodes for the meanings of the parameters.
""""""
assert hole not in ancestors0
ancestors = [hole] + ancestors0
for l in potential_labels0:
if self._violates_constraints(l, ancestors):
continue
plug_acc = plug_acc0.copy()
plug_acc[hole] = l
potential_labels = potential_labels0.copy()
potential_labels.remove(l)
if len(potential_labels) == 0:
self._sanity_check_plugging(plug_acc, self.top_hole, [])
record.append(plug_acc)
else:
self._plug_nodes(
queue + [(l, ancestors)], potential_labels, plug_acc, record
)
",[],0,[],/sem/hole.py__plug_hole
2572,/home/amandapotts/git/nltk/nltk/sem/hole.py__violates_constraints,"def _violates_constraints(self, label, ancestors):
""""""
Return True if the `label' cannot be placed underneath the holes given
by the set `ancestors' because it would violate the constraints imposed
on it.
""""""
for c in self.constraints:
if c.lhs == label:
if c.rhs not in ancestors:
return True
return False
",[],0,[],/sem/hole.py__violates_constraints
2573,/home/amandapotts/git/nltk/nltk/sem/hole.py__sanity_check_plugging,"def _sanity_check_plugging(self, plugging, node, ancestors):
""""""
Make sure that a given plugging is legal.  We recursively go through
each node and make sure that no constraints are violated.
We also check that all holes have been filled.
""""""
if node in self.holes:
ancestors = [node] + ancestors
label = plugging[node]
else:
label = node
assert label in self.labels
for c in self.constraints:
if c.lhs == label:
assert c.rhs in ancestors
args = self.fragments[label][1]
for arg in args:
if self.is_node(arg):
self._sanity_check_plugging(plugging, arg, [label] + ancestors)
",[],0,[],/sem/hole.py__sanity_check_plugging
2574,/home/amandapotts/git/nltk/nltk/sem/hole.py_formula_tree,"def formula_tree(self, plugging):
""""""
Return the first-order logic formula tree for this underspecified
representation using the plugging given.
""""""
return self._formula_tree(plugging, self.top_hole)
",[],0,[],/sem/hole.py_formula_tree
2575,/home/amandapotts/git/nltk/nltk/sem/hole.py__formula_tree,"def _formula_tree(self, plugging, node):
if node in plugging:
return self._formula_tree(plugging, plugging[node])
elif node in self.fragments:
pred, args = self.fragments[node]
children = [self._formula_tree(plugging, arg) for arg in args]
return reduce(Constants.MAP[pred.variable.name], children)
else:
return node
",[],0,[],/sem/hole.py__formula_tree
2576,/home/amandapotts/git/nltk/nltk/sem/hole.py___init__,"def __init__(self, lhs, rhs):
self.lhs = lhs
self.rhs = rhs
",[],0,[],/sem/hole.py___init__
2577,/home/amandapotts/git/nltk/nltk/sem/hole.py___eq__,"def __eq__(self, other):
if self.__class__ == other.__class__:
return self.lhs == other.lhs and self.rhs == other.rhs
else:
return False
",[],0,[],/sem/hole.py___eq__
2578,/home/amandapotts/git/nltk/nltk/sem/hole.py___ne__,"def __ne__(self, other):
return not (self == other)
",[],0,[],/sem/hole.py___ne__
2579,/home/amandapotts/git/nltk/nltk/sem/hole.py___hash__,"def __hash__(self):
return hash(repr(self))
",[],0,[],/sem/hole.py___hash__
2580,/home/amandapotts/git/nltk/nltk/sem/hole.py___repr__,"def __repr__(self):
return f""({self.lhs} < {self.rhs})""
",[],0,[],/sem/hole.py___repr__
2581,/home/amandapotts/git/nltk/nltk/sem/hole.py_hole_readings,"def hole_readings(sentence, grammar_filename=None, verbose=False):
if not grammar_filename:
grammar_filename = ""grammars/sample_grammars/hole.fcfg""
if verbose:
print(""Reading grammar file"", grammar_filename)
parser = load_parser(grammar_filename)
tokens = sentence.split()
trees = list(parser.parse(tokens))
if verbose:
print(""Got %d different parses"" % len(trees))
all_readings = []
for tree in trees:
sem = tree.label()[""SEM""].simplify()
if verbose:
print(""Raw:       "", sem)
while isinstance(sem, LambdaExpression):
sem = sem.term
skolemized = skolemize(sem)
if verbose:
print(""Skolemized:"", skolemized)
hole_sem = HoleSemantics(skolemized)
if verbose:
print(""Holes:       "", hole_sem.holes)
print(""Labels:      "", hole_sem.labels)
print(""Constraints: "", hole_sem.constraints)
print(""Top hole:    "", hole_sem.top_hole)
print(""Top labels:  "", hole_sem.top_most_labels)
print(""Fragments:"")
for l, f in hole_sem.fragments.items():
print(f""\t{l}: {f}"")
pluggings = hole_sem.pluggings()
readings = list(map(hole_sem.formula_tree, pluggings))
if verbose:
for i, r in enumerate(readings):
print()
print(""%d. %s"" % (i, r))
print()
all_readings.extend(readings)
return all_readings
",[],0,[],/sem/hole.py_hole_readings
2582,/home/amandapotts/git/nltk/nltk/sem/skolemize.py_skolemize,"def skolemize(expression, univ_scope=None, used_variables=None):
""""""
Skolemize the expression and convert to conjunctive normal form (CNF)
""""""
if univ_scope is None:
univ_scope = set()
if used_variables is None:
used_variables = set()
if isinstance(expression, AllExpression):
term = skolemize(
expression.term,
univ_scope | {expression.variable},
used_variables | {expression.variable},
)
return term.replace(
expression.variable,
VariableExpression(unique_variable(ignore=used_variables)),
)
elif isinstance(expression, AndExpression):
return skolemize(expression.first, univ_scope, used_variables) & skolemize(
expression.second, univ_scope, used_variables
)
elif isinstance(expression, OrExpression):
return to_cnf(
skolemize(expression.first, univ_scope, used_variables),
skolemize(expression.second, univ_scope, used_variables),
)
elif isinstance(expression, ImpExpression):
return to_cnf(
skolemize(-expression.first, univ_scope, used_variables),
skolemize(expression.second, univ_scope, used_variables),
)
elif isinstance(expression, IffExpression):
return to_cnf(
skolemize(-expression.first, univ_scope, used_variables),
skolemize(expression.second, univ_scope, used_variables),
) & to_cnf(
skolemize(expression.first, univ_scope, used_variables),
skolemize(-expression.second, univ_scope, used_variables),
)
elif isinstance(expression, EqualityExpression):
return expression
elif isinstance(expression, NegatedExpression):
negated = expression.term
if isinstance(negated, AllExpression):
term = skolemize(
-negated.term, univ_scope, used_variables | {negated.variable}
)
if univ_scope:
return term.replace(negated.variable, skolem_function(univ_scope))
else:
skolem_constant = VariableExpression(
unique_variable(ignore=used_variables)
)
return term.replace(negated.variable, skolem_constant)
elif isinstance(negated, AndExpression):
return to_cnf(
skolemize(-negated.first, univ_scope, used_variables),
skolemize(-negated.second, univ_scope, used_variables),
)
elif isinstance(negated, OrExpression):
return skolemize(-negated.first, univ_scope, used_variables) & skolemize(
-negated.second, univ_scope, used_variables
)
elif isinstance(negated, ImpExpression):
return skolemize(negated.first, univ_scope, used_variables) & skolemize(
-negated.second, univ_scope, used_variables
)
elif isinstance(negated, IffExpression):
return to_cnf(
skolemize(-negated.first, univ_scope, used_variables),
skolemize(-negated.second, univ_scope, used_variables),
) & to_cnf(
skolemize(negated.first, univ_scope, used_variables),
skolemize(negated.second, univ_scope, used_variables),
)
elif isinstance(negated, EqualityExpression):
return expression
elif isinstance(negated, NegatedExpression):
return skolemize(negated.term, univ_scope, used_variables)
elif isinstance(negated, ExistsExpression):
term = skolemize(
-negated.term,
univ_scope | {negated.variable},
used_variables | {negated.variable},
)
return term.replace(
negated.variable,
VariableExpression(unique_variable(ignore=used_variables)),
)
elif isinstance(negated, ApplicationExpression):
return expression
else:
raise Exception(""'%s' cannot be skolemized"" % expression)
elif isinstance(expression, ExistsExpression):
term = skolemize(
expression.term, univ_scope, used_variables | {expression.variable}
)
if univ_scope:
return term.replace(expression.variable, skolem_function(univ_scope))
else:
skolem_constant = VariableExpression(unique_variable(ignore=used_variables))
return term.replace(expression.variable, skolem_constant)
elif isinstance(expression, ApplicationExpression):
return expression
else:
raise Exception(""'%s' cannot be skolemized"" % expression)
",[],0,[],/sem/skolemize.py_skolemize
2583,/home/amandapotts/git/nltk/nltk/sem/skolemize.py_to_cnf,"def to_cnf(first, second):
""""""
Convert this split disjunction to conjunctive normal form (CNF)
""""""
if isinstance(first, AndExpression):
r_first = to_cnf(first.first, second)
r_second = to_cnf(first.second, second)
return r_first & r_second
elif isinstance(second, AndExpression):
r_first = to_cnf(first, second.first)
r_second = to_cnf(first, second.second)
return r_first & r_second
else:
return first | second
",[],0,[],/sem/skolemize.py_to_cnf
2584,/home/amandapotts/git/nltk/nltk/sem/drt.py___init__,"def __init__(self):
LogicParser.__init__(self)
self.operator_precedence = dict(
[(x, 1) for x in DrtTokens.LAMBDA_LIST]
+ [(x, 2) for x in DrtTokens.NOT_LIST]
+ [(APP, 3)]
+ [(x, 4) for x in DrtTokens.EQ_LIST + Tokens.NEQ_LIST]
+ [(DrtTokens.COLON, 5)]
+ [(DrtTokens.DRS_CONC, 6)]
+ [(x, 7) for x in DrtTokens.OR_LIST]
+ [(x, 8) for x in DrtTokens.IMP_LIST]
+ [(None, 9)]
)
",[],0,[],/sem/drt.py___init__
2585,/home/amandapotts/git/nltk/nltk/sem/drt.py_get_all_symbols,"def get_all_symbols(self):
""""""This method exists to be overridden""""""
return DrtTokens.SYMBOLS
",[],0,[],/sem/drt.py_get_all_symbols
2586,/home/amandapotts/git/nltk/nltk/sem/drt.py_isvariable,"def isvariable(self, tok):
return tok not in DrtTokens.TOKENS
",[],0,[],/sem/drt.py_isvariable
2587,/home/amandapotts/git/nltk/nltk/sem/drt.py_make_NegatedExpression,"def make_NegatedExpression(self, expression):
return DrtNegatedExpression(expression)
",[],0,[],/sem/drt.py_make_NegatedExpression
2588,/home/amandapotts/git/nltk/nltk/sem/drt.py_handle_DRS,"def handle_DRS(self, tok, context):
refs = self.handle_refs()
if (
self.inRange(0) and self.token(0) == DrtTokens.COMMA
):  # if there is a comma (it's optional)
self.token()  # swallow the comma
conds = self.handle_conds(context)
self.assertNextToken(DrtTokens.CLOSE)
return DRS(refs, conds, None)
",[],0,[],/sem/drt.py_handle_DRS
2589,/home/amandapotts/git/nltk/nltk/sem/drt.py_handle_refs,"def handle_refs(self):
self.assertNextToken(DrtTokens.OPEN_BRACKET)
refs = []
while self.inRange(0) and self.token(0) != DrtTokens.CLOSE_BRACKET:
if refs and self.token(0) == DrtTokens.COMMA:
self.token()  # swallow the comma
refs.append(self.get_next_token_variable(""quantified""))
self.assertNextToken(DrtTokens.CLOSE_BRACKET)
return refs
",[],0,[],/sem/drt.py_handle_refs
2590,/home/amandapotts/git/nltk/nltk/sem/drt.py_handle_conds,"def handle_conds(self, context):
self.assertNextToken(DrtTokens.OPEN_BRACKET)
conds = []
while self.inRange(0) and self.token(0) != DrtTokens.CLOSE_BRACKET:
if conds and self.token(0) == DrtTokens.COMMA:
self.token()  # swallow the comma
conds.append(self.process_next_expression(context))
self.assertNextToken(DrtTokens.CLOSE_BRACKET)
return conds
",[],0,[],/sem/drt.py_handle_conds
2591,/home/amandapotts/git/nltk/nltk/sem/drt.py_handle_prop,"def handle_prop(self, tok, context):
variable = self.make_VariableExpression(tok)
self.assertNextToken("":"")
drs = self.process_next_expression(DrtTokens.COLON)
return DrtProposition(variable, drs)
",[],0,[],/sem/drt.py_handle_prop
2592,/home/amandapotts/git/nltk/nltk/sem/drt.py_make_EqualityExpression,"def make_EqualityExpression(self, first, second):
""""""This method serves as a hook for other logic parsers that
have different equality expression classes""""""
return DrtEqualityExpression(first, second)
",[],0,[],/sem/drt.py_make_EqualityExpression
2593,/home/amandapotts/git/nltk/nltk/sem/drt.py_make_imp_expression,"def make_imp_expression(first, second):
if isinstance(first, DRS):
return DRS(first.refs, first.conds, second)
if isinstance(first, DrtConcatenation):
return DrtConcatenation(first.first, first.second, second)
raise Exception(""Antecedent of implication must be a DRS"")
",[],0,[],/sem/drt.py_make_imp_expression
2594,/home/amandapotts/git/nltk/nltk/sem/drt.py_make_BooleanExpression,"def make_BooleanExpression(self, factory, first, second):
return factory(first, second)
",[],0,[],/sem/drt.py_make_BooleanExpression
2595,/home/amandapotts/git/nltk/nltk/sem/drt.py_make_ApplicationExpression,"def make_ApplicationExpression(self, function, argument):
return DrtApplicationExpression(function, argument)
",[],0,[],/sem/drt.py_make_ApplicationExpression
2596,/home/amandapotts/git/nltk/nltk/sem/drt.py_make_VariableExpression,"def make_VariableExpression(self, name):
return DrtVariableExpression(Variable(name))
",[],0,[],/sem/drt.py_make_VariableExpression
2597,/home/amandapotts/git/nltk/nltk/sem/drt.py_make_LambdaExpression,"def make_LambdaExpression(self, variables, term):
return DrtLambdaExpression(variables, term)
",[],0,[],/sem/drt.py_make_LambdaExpression
2598,/home/amandapotts/git/nltk/nltk/sem/drt.py_fromstring,"def fromstring(cls, s):
return cls._drt_parser.parse(s)
",[],0,[],/sem/drt.py_fromstring
2599,/home/amandapotts/git/nltk/nltk/sem/drt.py_applyto,"def applyto(self, other):
return DrtApplicationExpression(self, other)
",[],0,[],/sem/drt.py_applyto
2600,/home/amandapotts/git/nltk/nltk/sem/drt.py___neg__,"def __neg__(self):
return DrtNegatedExpression(self)
",[],0,[],/sem/drt.py___neg__
2601,/home/amandapotts/git/nltk/nltk/sem/drt.py___and__,"def __and__(self, other):
return NotImplemented
",[],0,[],/sem/drt.py___and__
2602,/home/amandapotts/git/nltk/nltk/sem/drt.py___or__,"def __or__(self, other):
assert isinstance(other, DrtExpression)
return DrtOrExpression(self, other)
",[],0,[],/sem/drt.py___or__
2603,/home/amandapotts/git/nltk/nltk/sem/drt.py___gt__,"def __gt__(self, other):
assert isinstance(other, DrtExpression)
if isinstance(self, DRS):
return DRS(self.refs, self.conds, other)
if isinstance(self, DrtConcatenation):
return DrtConcatenation(self.first, self.second, other)
raise Exception(""Antecedent of implication must be a DRS"")
",[],0,[],/sem/drt.py___gt__
2604,/home/amandapotts/git/nltk/nltk/sem/drt.py_equiv,"def equiv(self, other, prover=None):
""""""
Check for logical equivalence.
Pass the expression (self <-> other) to the theorem prover.
If the prover says it is valid, then the self and other are equal.
:param other: an ``DrtExpression`` to check equality against
:param prover: a ``nltk.inference.api.Prover``
""""""
assert isinstance(other, DrtExpression)
f1 = self.simplify().fol()
f2 = other.simplify().fol()
return f1.equiv(f2, prover)
",[],0,[],/sem/drt.py_equiv
2605,/home/amandapotts/git/nltk/nltk/sem/drt.py_type,"def type(self):
raise AttributeError(
""'%s' object has no attribute 'type'"" % self.__class__.__name__
)
",[],0,[],/sem/drt.py_type
2606,/home/amandapotts/git/nltk/nltk/sem/drt.py_typecheck,"def typecheck(self, signature=None):
raise NotImplementedError()
",[],0,[],/sem/drt.py_typecheck
2607,/home/amandapotts/git/nltk/nltk/sem/drt.py___add__,"def __add__(self, other):
return DrtConcatenation(self, other, None)
",[],0,[],/sem/drt.py___add__
2608,/home/amandapotts/git/nltk/nltk/sem/drt.py_get_refs,"def get_refs(self, recursive=False):
""""""
Return the set of discourse referents in this DRS.
:param recursive: bool Also find discourse referents in subterms?
:return: list of ``Variable`` objects
""""""
raise NotImplementedError()
",[],0,[],/sem/drt.py_get_refs
2609,/home/amandapotts/git/nltk/nltk/sem/drt.py_is_pronoun_function,"def is_pronoun_function(self):
""""""Is self of the form ""PRO(x)""?""""""
return (
isinstance(self, DrtApplicationExpression)
and isinstance(self.function, DrtAbstractVariableExpression)
and self.function.variable.name == DrtTokens.PRONOUN
and isinstance(self.argument, DrtIndividualVariableExpression)
)
",[],0,[],/sem/drt.py_is_pronoun_function
2610,/home/amandapotts/git/nltk/nltk/sem/drt.py_make_EqualityExpression,"def make_EqualityExpression(self, first, second):
return DrtEqualityExpression(first, second)
",[],0,[],/sem/drt.py_make_EqualityExpression
2611,/home/amandapotts/git/nltk/nltk/sem/drt.py_make_VariableExpression,"def make_VariableExpression(self, variable):
return DrtVariableExpression(variable)
",[],0,[],/sem/drt.py_make_VariableExpression
2612,/home/amandapotts/git/nltk/nltk/sem/drt.py_resolve_anaphora,"def resolve_anaphora(self):
return resolve_anaphora(self)
",[],0,[],/sem/drt.py_resolve_anaphora
2613,/home/amandapotts/git/nltk/nltk/sem/drt.py_pretty_format,"def pretty_format(self):
""""""
Draw the DRS
:return: the pretty print string
""""""
return ""\n"".join(self._pretty())
",[],0,[],/sem/drt.py_pretty_format
2614,/home/amandapotts/git/nltk/nltk/sem/drt.py_pretty_print,"def pretty_print(self):
print(self.pretty_format())
",[],0,[],/sem/drt.py_pretty_print
2615,/home/amandapotts/git/nltk/nltk/sem/drt.py_draw,"def draw(self):
DrsDrawer(self).draw()
",[],0,[],/sem/drt.py_draw
2616,/home/amandapotts/git/nltk/nltk/sem/drt.py___init__,"def __init__(self, refs, conds, consequent=None):
""""""
:param refs: list of ``DrtIndividualVariableExpression`` for the
discourse referents
:param conds: list of ``Expression`` for the conditions
""""""
self.refs = refs
self.conds = conds
self.consequent = consequent
",[],0,[],/sem/drt.py___init__
2617,/home/amandapotts/git/nltk/nltk/sem/drt.py_replace,"def replace(self, variable, expression, replace_bound=False, alpha_convert=True):
""""""Replace all instances of variable v with expression E in self,
where v is free in self.""""""
if variable in self.refs:
if not replace_bound:
return self
else:
i = self.refs.index(variable)
if self.consequent:
consequent = self.consequent.replace(
variable, expression, True, alpha_convert
)
else:
consequent = None
return DRS(
self.refs[:i] + [expression.variable] + self.refs[i + 1 :],
[
cond.replace(variable, expression, True, alpha_convert)
for cond in self.conds
],
consequent,
)
else:
if alpha_convert:
for ref in set(self.refs) & expression.free():
newvar = unique_variable(ref)
newvarex = DrtVariableExpression(newvar)
i = self.refs.index(ref)
if self.consequent:
consequent = self.consequent.replace(
ref, newvarex, True, alpha_convert
)
else:
consequent = None
self = DRS(
self.refs[:i] + [newvar] + self.refs[i + 1 :],
[
cond.replace(ref, newvarex, True, alpha_convert)
for cond in self.conds
],
consequent,
)
if self.consequent:
consequent = self.consequent.replace(
variable, expression, replace_bound, alpha_convert
)
else:
consequent = None
return DRS(
self.refs,
[
cond.replace(variable, expression, replace_bound, alpha_convert)
for cond in self.conds
],
consequent,
)
",[],0,[],/sem/drt.py_replace
2618,/home/amandapotts/git/nltk/nltk/sem/drt.py_free,"def free(self):
"""""":see: Expression.free()""""""
conds_free = reduce(operator.or_, [c.free() for c in self.conds], set())
if self.consequent:
conds_free.update(self.consequent.free())
return conds_free - set(self.refs)
",[],0,[],/sem/drt.py_free
2619,/home/amandapotts/git/nltk/nltk/sem/drt.py_get_refs,"def get_refs(self, recursive=False):
"""""":see: AbstractExpression.get_refs()""""""
if recursive:
conds_refs = self.refs + list(
chain.from_iterable(c.get_refs(True) for c in self.conds)
)
if self.consequent:
conds_refs.extend(self.consequent.get_refs(True))
return conds_refs
else:
return self.refs
",[],0,[],/sem/drt.py_get_refs
2620,/home/amandapotts/git/nltk/nltk/sem/drt.py_visit,"def visit(self, function, combinator):
"""""":see: Expression.visit()""""""
parts = list(map(function, self.conds))
if self.consequent:
parts.append(function(self.consequent))
return combinator(parts)
",[],0,[],/sem/drt.py_visit
2621,/home/amandapotts/git/nltk/nltk/sem/drt.py_visit_structured,"def visit_structured(self, function, combinator):
"""""":see: Expression.visit_structured()""""""
consequent = function(self.consequent) if self.consequent else None
return combinator(self.refs, list(map(function, self.conds)), consequent)
",[],0,[],/sem/drt.py_visit_structured
2622,/home/amandapotts/git/nltk/nltk/sem/drt.py_eliminate_equality,"def eliminate_equality(self):
drs = self
i = 0
while i < len(drs.conds):
cond = drs.conds[i]
if (
isinstance(cond, EqualityExpression)
and isinstance(cond.first, AbstractVariableExpression)
and isinstance(cond.second, AbstractVariableExpression)
):
drs = DRS(
list(set(drs.refs) - {cond.second.variable}),
drs.conds[:i] + drs.conds[i + 1 :],
drs.consequent,
)
if cond.second.variable != cond.first.variable:
drs = drs.replace(cond.second.variable, cond.first, False, False)
i = 0
i -= 1
i += 1
conds = []
for cond in drs.conds:
new_cond = cond.eliminate_equality()
new_cond_simp = new_cond.simplify()
if (
not isinstance(new_cond_simp, DRS)
or new_cond_simp.refs
or new_cond_simp.conds
or new_cond_simp.consequent
):
conds.append(new_cond)
consequent = drs.consequent.eliminate_equality() if drs.consequent else None
return DRS(drs.refs, conds, consequent)
",[],0,[],/sem/drt.py_eliminate_equality
2623,/home/amandapotts/git/nltk/nltk/sem/drt.py_fol,"def fol(self):
if self.consequent:
accum = None
if self.conds:
accum = reduce(AndExpression, [c.fol() for c in self.conds])
if accum:
accum = ImpExpression(accum, self.consequent.fol())
else:
accum = self.consequent.fol()
for ref in self.refs[::-1]:
accum = AllExpression(ref, accum)
return accum
else:
if not self.conds:
raise Exception(""Cannot convert DRS with no conditions to FOL."")
accum = reduce(AndExpression, [c.fol() for c in self.conds])
for ref in map(Variable, self._order_ref_strings(self.refs)[::-1]):
accum = ExistsExpression(ref, accum)
return accum
",[],0,[],/sem/drt.py_fol
2624,/home/amandapotts/git/nltk/nltk/sem/drt.py___eq__,"def __eq__(self, other):
r""""""Defines equality modulo alphabetic variance.
If we are comparing \x.M  and \y.N, then check equality of M and N[x/y].""""""
if isinstance(other, DRS):
if len(self.refs) == len(other.refs):
converted_other = other
for r1, r2 in zip(self.refs, converted_other.refs):
varex = self.make_VariableExpression(r1)
converted_other = converted_other.replace(r2, varex, True)
if self.consequent == converted_other.consequent and len(
self.conds
) == len(converted_other.conds):
for c1, c2 in zip(self.conds, converted_other.conds):
if not (c1 == c2):
return False
return True
return False
",[],0,[],/sem/drt.py___eq__
2625,/home/amandapotts/git/nltk/nltk/sem/drt.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/sem/drt.py___ne__
2626,/home/amandapotts/git/nltk/nltk/sem/drt.py___str__,"def __str__(self):
drs = ""([{}],[{}])"".format(
"","".join(self._order_ref_strings(self.refs)),
"", "".join(""%s"" % cond for cond in self.conds),
)  # map(str, self.conds)))
if self.consequent:
return (
DrtTokens.OPEN
+ drs
+ "" ""
+ DrtTokens.IMP
+ "" ""
+ ""%s"" % self.consequent
+ DrtTokens.CLOSE
)
return drs
",[],0,[],/sem/drt.py___str__
2627,/home/amandapotts/git/nltk/nltk/sem/drt.py_DrtVariableExpression,"def DrtVariableExpression(variable):
""""""
This is a factory method that instantiates and returns a subtype of
``DrtAbstractVariableExpression`` appropriate for the given variable.
""""""
if is_indvar(variable.name):
return DrtIndividualVariableExpression(variable)
elif is_funcvar(variable.name):
return DrtFunctionVariableExpression(variable)
elif is_eventvar(variable.name):
return DrtEventVariableExpression(variable)
else:
return DrtConstantExpression(variable)
",[],0,[],/sem/drt.py_DrtVariableExpression
2628,/home/amandapotts/git/nltk/nltk/sem/drt.py_fol,"def fol(self):
return self
",[],0,[],/sem/drt.py_fol
2629,/home/amandapotts/git/nltk/nltk/sem/drt.py_get_refs,"def get_refs(self, recursive=False):
"""""":see: AbstractExpression.get_refs()""""""
return []
",[],0,[],/sem/drt.py_get_refs
2630,/home/amandapotts/git/nltk/nltk/sem/drt.py__pretty,"def _pretty(self):
s = ""%s"" % self
blank = "" "" * len(s)
return [blank, blank, s, blank]
",[],0,[],/sem/drt.py__pretty
2631,/home/amandapotts/git/nltk/nltk/sem/drt.py_eliminate_equality,"def eliminate_equality(self):
return self
",[],0,[],/sem/drt.py_eliminate_equality
2632,/home/amandapotts/git/nltk/nltk/sem/drt.py___init__,"def __init__(self, variable, drs):
self.variable = variable
self.drs = drs
",[],0,[],/sem/drt.py___init__
2633,/home/amandapotts/git/nltk/nltk/sem/drt.py_replace,"def replace(self, variable, expression, replace_bound=False, alpha_convert=True):
if self.variable == variable:
assert isinstance(
expression, DrtAbstractVariableExpression
), ""Can only replace a proposition label with a variable""
return DrtProposition(
expression.variable,
self.drs.replace(variable, expression, replace_bound, alpha_convert),
)
else:
return DrtProposition(
self.variable,
self.drs.replace(variable, expression, replace_bound, alpha_convert),
)
",[],0,[],/sem/drt.py_replace
2634,/home/amandapotts/git/nltk/nltk/sem/drt.py_eliminate_equality,"def eliminate_equality(self):
return DrtProposition(self.variable, self.drs.eliminate_equality())
",[],0,[],/sem/drt.py_eliminate_equality
2635,/home/amandapotts/git/nltk/nltk/sem/drt.py_get_refs,"def get_refs(self, recursive=False):
return self.drs.get_refs(True) if recursive else []
",[],0,[],/sem/drt.py_get_refs
2636,/home/amandapotts/git/nltk/nltk/sem/drt.py___eq__,"def __eq__(self, other):
return (
self.__class__ == other.__class__
and self.variable == other.variable
and self.drs == other.drs
)
",[],0,[],/sem/drt.py___eq__
2637,/home/amandapotts/git/nltk/nltk/sem/drt.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/sem/drt.py___ne__
2638,/home/amandapotts/git/nltk/nltk/sem/drt.py_fol,"def fol(self):
return self.drs.fol()
",[],0,[],/sem/drt.py_fol
2639,/home/amandapotts/git/nltk/nltk/sem/drt.py__pretty,"def _pretty(self):
drs_s = self.drs._pretty()
blank = "" "" * len(""%s"" % self.variable)
return (
[blank + "" "" + line for line in drs_s[:1]]
+ [""%s"" % self.variable + "":"" + line for line in drs_s[1:2]]
+ [blank + "" "" + line for line in drs_s[2:]]
)
",[],0,[],/sem/drt.py__pretty
2640,/home/amandapotts/git/nltk/nltk/sem/drt.py_visit,"def visit(self, function, combinator):
"""""":see: Expression.visit()""""""
return combinator([function(self.drs)])
",[],0,[],/sem/drt.py_visit
2641,/home/amandapotts/git/nltk/nltk/sem/drt.py_visit_structured,"def visit_structured(self, function, combinator):
"""""":see: Expression.visit_structured()""""""
return combinator(self.variable, function(self.drs))
",[],0,[],/sem/drt.py_visit_structured
2642,/home/amandapotts/git/nltk/nltk/sem/drt.py___str__,"def __str__(self):
return f""prop({self.variable}, {self.drs})""
",[],0,[],/sem/drt.py___str__
2643,/home/amandapotts/git/nltk/nltk/sem/drt.py_fol,"def fol(self):
return NegatedExpression(self.term.fol())
",[],0,[],/sem/drt.py_fol
2644,/home/amandapotts/git/nltk/nltk/sem/drt.py_get_refs,"def get_refs(self, recursive=False):
"""""":see: AbstractExpression.get_refs()""""""
return self.term.get_refs(recursive)
",[],0,[],/sem/drt.py_get_refs
2645,/home/amandapotts/git/nltk/nltk/sem/drt.py__pretty,"def _pretty(self):
term_lines = self.term._pretty()
return (
[""    "" + line for line in term_lines[:2]]
+ [""__  "" + line for line in term_lines[2:3]]
+ [""  | "" + line for line in term_lines[3:4]]
+ [""    "" + line for line in term_lines[4:]]
)
",[],0,[],/sem/drt.py__pretty
2646,/home/amandapotts/git/nltk/nltk/sem/drt.py_alpha_convert,"def alpha_convert(self, newvar):
""""""Rename all occurrences of the variable introduced by this variable
binder in the expression to ``newvar``.
:param newvar: ``Variable``, for the new variable
""""""
return self.__class__(
newvar,
self.term.replace(self.variable, DrtVariableExpression(newvar), True),
)
",[],0,[],/sem/drt.py_alpha_convert
2647,/home/amandapotts/git/nltk/nltk/sem/drt.py_fol,"def fol(self):
return LambdaExpression(self.variable, self.term.fol())
",[],0,[],/sem/drt.py_fol
2648,/home/amandapotts/git/nltk/nltk/sem/drt.py__pretty,"def _pretty(self):
variables = [self.variable]
term = self.term
while term.__class__ == self.__class__:
variables.append(term.variable)
term = term.term
var_string = "" "".join(""%s"" % v for v in variables) + DrtTokens.DOT
term_lines = term._pretty()
blank = "" "" * len(var_string)
return (
[""    "" + blank + line for line in term_lines[:1]]
+ [r"" \  "" + blank + line for line in term_lines[1:2]]
+ [r"" /\ "" + var_string + line for line in term_lines[2:3]]
+ [""    "" + blank + line for line in term_lines[3:]]
)
",[],0,[],/sem/drt.py__pretty
2649,/home/amandapotts/git/nltk/nltk/sem/drt.py_get_refs,"def get_refs(self, recursive=False):
"""""":see: AbstractExpression.get_refs()""""""
return (
[self.variable] + self.term.get_refs(True) if recursive else [self.variable]
)
",[],0,[],/sem/drt.py_get_refs
2650,/home/amandapotts/git/nltk/nltk/sem/drt.py_get_refs,"def get_refs(self, recursive=False):
"""""":see: AbstractExpression.get_refs()""""""
return (
self.first.get_refs(True) + self.second.get_refs(True) if recursive else []
)
",[],0,[],/sem/drt.py_get_refs
2651,/home/amandapotts/git/nltk/nltk/sem/drt.py__pretty,"def _pretty(self):
return DrtBinaryExpression._assemble_pretty(
self._pretty_subex(self.first),
self.getOp(),
self._pretty_subex(self.second),
)
",[],0,[],/sem/drt.py__pretty
2652,/home/amandapotts/git/nltk/nltk/sem/drt.py__assemble_pretty,"def _assemble_pretty(first_lines, op, second_lines):
max_lines = max(len(first_lines), len(second_lines))
first_lines = _pad_vertically(first_lines, max_lines)
second_lines = _pad_vertically(second_lines, max_lines)
blank = "" "" * len(op)
first_second_lines = list(zip(first_lines, second_lines))
return (
[
"" "" + first_line + "" "" + blank + "" "" + second_line + "" ""
for first_line, second_line in first_second_lines[:2]
]
+ [
""("" + first_line + "" "" + op + "" "" + second_line + "")""
for first_line, second_line in first_second_lines[2:3]
]
+ [
"" "" + first_line + "" "" + blank + "" "" + second_line + "" ""
for first_line, second_line in first_second_lines[3:]
]
)
",[],0,[],/sem/drt.py__assemble_pretty
2653,/home/amandapotts/git/nltk/nltk/sem/drt.py__pretty_subex,"def _pretty_subex(self, subex):
return subex._pretty()
",[],0,[],/sem/drt.py__pretty_subex
2654,/home/amandapotts/git/nltk/nltk/sem/drt.py_fol,"def fol(self):
return OrExpression(self.first.fol(), self.second.fol())
",[],0,[],/sem/drt.py_fol
2655,/home/amandapotts/git/nltk/nltk/sem/drt.py__pretty_subex,"def _pretty_subex(self, subex):
if isinstance(subex, DrtOrExpression):
return [line[1:-1] for line in subex._pretty()]
return DrtBooleanExpression._pretty_subex(self, subex)
",[],0,[],/sem/drt.py__pretty_subex
2656,/home/amandapotts/git/nltk/nltk/sem/drt.py_fol,"def fol(self):
return EqualityExpression(self.first.fol(), self.second.fol())
",[],0,[],/sem/drt.py_fol
2657,/home/amandapotts/git/nltk/nltk/sem/drt.py___init__,"def __init__(self, first, second, consequent=None):
DrtBooleanExpression.__init__(self, first, second)
self.consequent = consequent
",[],0,[],/sem/drt.py___init__
2658,/home/amandapotts/git/nltk/nltk/sem/drt.py_replace,"def replace(self, variable, expression, replace_bound=False, alpha_convert=True):
""""""Replace all instances of variable v with expression E in self,
where v is free in self.""""""
first = self.first
second = self.second
consequent = self.consequent
if variable in self.get_refs():
if replace_bound:
first = first.replace(
variable, expression, replace_bound, alpha_convert
)
second = second.replace(
variable, expression, replace_bound, alpha_convert
)
if consequent:
consequent = consequent.replace(
variable, expression, replace_bound, alpha_convert
)
else:
if alpha_convert:
for ref in set(self.get_refs(True)) & expression.free():
v = DrtVariableExpression(unique_variable(ref))
first = first.replace(ref, v, True, alpha_convert)
second = second.replace(ref, v, True, alpha_convert)
if consequent:
consequent = consequent.replace(ref, v, True, alpha_convert)
first = first.replace(variable, expression, replace_bound, alpha_convert)
second = second.replace(variable, expression, replace_bound, alpha_convert)
if consequent:
consequent = consequent.replace(
variable, expression, replace_bound, alpha_convert
)
return self.__class__(first, second, consequent)
",[],0,[],/sem/drt.py_replace
2659,/home/amandapotts/git/nltk/nltk/sem/drt.py_eliminate_equality,"def eliminate_equality(self):
drs = self.simplify()
assert not isinstance(drs, DrtConcatenation)
return drs.eliminate_equality()
",[],0,[],/sem/drt.py_eliminate_equality
2660,/home/amandapotts/git/nltk/nltk/sem/drt.py_simplify,"def simplify(self):
first = self.first.simplify()
second = self.second.simplify()
consequent = self.consequent.simplify() if self.consequent else None
if isinstance(first, DRS) and isinstance(second, DRS):
for ref in set(first.get_refs(True)) & set(second.get_refs(True)):
newvar = DrtVariableExpression(unique_variable(ref))
second = second.replace(ref, newvar, True)
return DRS(first.refs + second.refs, first.conds + second.conds, consequent)
else:
return self.__class__(first, second, consequent)
",[],0,[],/sem/drt.py_simplify
2661,/home/amandapotts/git/nltk/nltk/sem/drt.py_get_refs,"def get_refs(self, recursive=False):
"""""":see: AbstractExpression.get_refs()""""""
refs = self.first.get_refs(recursive) + self.second.get_refs(recursive)
if self.consequent and recursive:
refs.extend(self.consequent.get_refs(True))
return refs
",[],0,[],/sem/drt.py_get_refs
2662,/home/amandapotts/git/nltk/nltk/sem/drt.py_getOp,"def getOp(self):
return DrtTokens.DRS_CONC
",[],0,[],/sem/drt.py_getOp
2663,/home/amandapotts/git/nltk/nltk/sem/drt.py___eq__,"def __eq__(self, other):
r""""""Defines equality modulo alphabetic variance.
If we are comparing \x.M  and \y.N, then check equality of M and N[x/y].""""""
if isinstance(other, DrtConcatenation):
self_refs = self.get_refs()
other_refs = other.get_refs()
if len(self_refs) == len(other_refs):
converted_other = other
for r1, r2 in zip(self_refs, other_refs):
varex = self.make_VariableExpression(r1)
converted_other = converted_other.replace(r2, varex, True)
return (
self.first == converted_other.first
and self.second == converted_other.second
and self.consequent == converted_other.consequent
)
return False
",[],0,[],/sem/drt.py___eq__
2664,/home/amandapotts/git/nltk/nltk/sem/drt.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/sem/drt.py___ne__
2665,/home/amandapotts/git/nltk/nltk/sem/drt.py_fol,"def fol(self):
e = AndExpression(self.first.fol(), self.second.fol())
if self.consequent:
e = ImpExpression(e, self.consequent.fol())
return e
",[],0,[],/sem/drt.py_fol
2666,/home/amandapotts/git/nltk/nltk/sem/drt.py__pretty,"def _pretty(self):
drs = DrtBinaryExpression._assemble_pretty(
self._pretty_subex(self.first),
self.getOp(),
self._pretty_subex(self.second),
)
if self.consequent:
drs = DrtBinaryExpression._assemble_pretty(
drs, DrtTokens.IMP, self.consequent._pretty()
)
return drs
",[],0,[],/sem/drt.py__pretty
2667,/home/amandapotts/git/nltk/nltk/sem/drt.py__pretty_subex,"def _pretty_subex(self, subex):
if isinstance(subex, DrtConcatenation):
return [line[1:-1] for line in subex._pretty()]
return DrtBooleanExpression._pretty_subex(self, subex)
",[],0,[],/sem/drt.py__pretty_subex
2668,/home/amandapotts/git/nltk/nltk/sem/drt.py_visit,"def visit(self, function, combinator):
"""""":see: Expression.visit()""""""
if self.consequent:
return combinator(
[function(self.first), function(self.second), function(self.consequent)]
)
else:
return combinator([function(self.first), function(self.second)])
",[],0,[],/sem/drt.py_visit
2669,/home/amandapotts/git/nltk/nltk/sem/drt.py___str__,"def __str__(self):
first = self._str_subex(self.first)
second = self._str_subex(self.second)
drs = Tokens.OPEN + first + "" "" + self.getOp() + "" "" + second + Tokens.CLOSE
if self.consequent:
return (
DrtTokens.OPEN
+ drs
+ "" ""
+ DrtTokens.IMP
+ "" ""
+ ""%s"" % self.consequent
+ DrtTokens.CLOSE
)
return drs
",[],0,[],/sem/drt.py___str__
2670,/home/amandapotts/git/nltk/nltk/sem/drt.py__str_subex,"def _str_subex(self, subex):
s = ""%s"" % subex
if isinstance(subex, DrtConcatenation) and subex.consequent is None:
return s[1:-1]
return s
",[],0,[],/sem/drt.py__str_subex
2671,/home/amandapotts/git/nltk/nltk/sem/drt.py_fol,"def fol(self):
return ApplicationExpression(self.function.fol(), self.argument.fol())
",[],0,[],/sem/drt.py_fol
2672,/home/amandapotts/git/nltk/nltk/sem/drt.py_get_refs,"def get_refs(self, recursive=False):
"""""":see: AbstractExpression.get_refs()""""""
return (
self.function.get_refs(True) + self.argument.get_refs(True)
if recursive
else []
)
",[],0,[],/sem/drt.py_get_refs
2673,/home/amandapotts/git/nltk/nltk/sem/drt.py__pretty,"def _pretty(self):
function, args = self.uncurry()
function_lines = function._pretty()
args_lines = [arg._pretty() for arg in args]
max_lines = max(map(len, [function_lines] + args_lines))
function_lines = _pad_vertically(function_lines, max_lines)
args_lines = [_pad_vertically(arg_lines, max_lines) for arg_lines in args_lines]
func_args_lines = list(zip(function_lines, list(zip(*args_lines))))
return (
[
func_line + "" "" + "" "".join(args_line) + "" ""
for func_line, args_line in func_args_lines[:2]
]
+ [
func_line + ""("" + "","".join(args_line) + "")""
for func_line, args_line in func_args_lines[2:3]
]
+ [
func_line + "" "" + "" "".join(args_line) + "" ""
for func_line, args_line in func_args_lines[3:]
]
)
",[],0,[],/sem/drt.py__pretty
2674,/home/amandapotts/git/nltk/nltk/sem/drt.py__pad_vertically,"def _pad_vertically(lines, max_lines):
pad_line = ["" "" * len(lines[0])]
return lines + pad_line * (max_lines - len(lines))
",[],0,[],/sem/drt.py__pad_vertically
2675,/home/amandapotts/git/nltk/nltk/sem/drt.py_free,"def free(self):
""""""Set of free variables.""""""
return set(self)
",[],0,[],/sem/drt.py_free
2676,/home/amandapotts/git/nltk/nltk/sem/drt.py_replace,"def replace(self, variable, expression, replace_bound=False, alpha_convert=True):
""""""Replace all instances of variable v with expression E in self,
where v is free in self.""""""
result = PossibleAntecedents()
for item in self:
if item == variable:
self.append(expression)
else:
self.append(item)
return result
",[],0,[],/sem/drt.py_replace
2677,/home/amandapotts/git/nltk/nltk/sem/drt.py__pretty,"def _pretty(self):
s = ""%s"" % self
blank = "" "" * len(s)
return [blank, blank, s]
",[],0,[],/sem/drt.py__pretty
2678,/home/amandapotts/git/nltk/nltk/sem/drt.py___str__,"def __str__(self):
return ""["" + "","".join(""%s"" % it for it in self) + ""]""
",[],0,[],/sem/drt.py___str__
2679,/home/amandapotts/git/nltk/nltk/sem/drt.py_resolve_anaphora,"def resolve_anaphora(expression, trail=[]):
if isinstance(expression, ApplicationExpression):
if expression.is_pronoun_function():
possible_antecedents = PossibleAntecedents()
for ancestor in trail:
for ref in ancestor.get_refs():
refex = expression.make_VariableExpression(ref)
if refex.__class__ == expression.argument.__class__ and not (
refex == expression.argument
):
possible_antecedents.append(refex)
if len(possible_antecedents) == 1:
resolution = possible_antecedents[0]
else:
resolution = possible_antecedents
return expression.make_EqualityExpression(expression.argument, resolution)
else:
r_function = resolve_anaphora(expression.function, trail + [expression])
r_argument = resolve_anaphora(expression.argument, trail + [expression])
return expression.__class__(r_function, r_argument)
elif isinstance(expression, DRS):
r_conds = []
for cond in expression.conds:
r_cond = resolve_anaphora(cond, trail + [expression])
if isinstance(r_cond, EqualityExpression):
if isinstance(r_cond.first, PossibleAntecedents):
temp = r_cond.first
r_cond.first = r_cond.second
r_cond.second = temp
if isinstance(r_cond.second, PossibleAntecedents):
if not r_cond.second:
raise AnaphoraResolutionException(
""Variable '%s' does not ""
""resolve to anything."" % r_cond.first
)
r_conds.append(r_cond)
if expression.consequent:
consequent = resolve_anaphora(expression.consequent, trail + [expression])
else:
consequent = None
return expression.__class__(expression.refs, r_conds, consequent)
elif isinstance(expression, AbstractVariableExpression):
return expression
elif isinstance(expression, NegatedExpression):
return expression.__class__(
resolve_anaphora(expression.term, trail + [expression])
)
elif isinstance(expression, DrtConcatenation):
if expression.consequent:
consequent = resolve_anaphora(expression.consequent, trail + [expression])
else:
consequent = None
return expression.__class__(
resolve_anaphora(expression.first, trail + [expression]),
resolve_anaphora(expression.second, trail + [expression]),
consequent,
)
elif isinstance(expression, BinaryExpression):
return expression.__class__(
resolve_anaphora(expression.first, trail + [expression]),
resolve_anaphora(expression.second, trail + [expression]),
)
elif isinstance(expression, LambdaExpression):
return expression.__class__(
expression.variable, resolve_anaphora(expression.term, trail + [expression])
)
",[],0,[],/sem/drt.py_resolve_anaphora
2680,/home/amandapotts/git/nltk/nltk/sem/drt.py___init__,"def __init__(self, drs, size_canvas=True, canvas=None):
""""""
:param drs: ``DrtExpression``, The DRS to be drawn
:param size_canvas: bool, True if the canvas size should be the exact size of the DRS
:param canvas: ``Canvas`` The canvas on which to draw the DRS.  If none is given, create a new canvas.
""""""
master = None
if not canvas:
master = Tk()
master.title(""DRT"")
font = Font(family=""helvetica"", size=12)
if size_canvas:
canvas = Canvas(master, width=0, height=0)
canvas.font = font
self.canvas = canvas
(right, bottom) = self._visit(drs, self.OUTERSPACE, self.TOPSPACE)
width = max(right + self.OUTERSPACE, 100)
height = bottom + self.OUTERSPACE
canvas = Canvas(master, width=width, height=height)  # , bg='white')
else:
canvas = Canvas(master, width=300, height=300)
canvas.pack()
canvas.font = font
self.canvas = canvas
self.drs = drs
self.master = master
",[],0,[],/sem/drt.py___init__
2681,/home/amandapotts/git/nltk/nltk/sem/drt.py__get_text_height,"def _get_text_height(self):
""""""Get the height of a line of text""""""
return self.canvas.font.metrics(""linespace"")
",[],0,[],/sem/drt.py__get_text_height
2682,/home/amandapotts/git/nltk/nltk/sem/drt.py_draw,"def draw(self, x=OUTERSPACE, y=TOPSPACE):
""""""Draw the DRS""""""
self._handle(self.drs, self._draw_command, x, y)
if self.master and not in_idle():
self.master.mainloop()
else:
return self._visit(self.drs, x, y)
",[],0,[],/sem/drt.py_draw
2683,/home/amandapotts/git/nltk/nltk/sem/drt.py__visit,"def _visit(self, expression, x, y):
""""""
Return the bottom-rightmost point without actually drawing the item
:param expression: the item to visit
:param x: the top of the current drawing area
:param y: the left side of the current drawing area
:return: the bottom-rightmost point
""""""
return self._handle(expression, self._visit_command, x, y)
",[],0,[],/sem/drt.py__visit
2684,/home/amandapotts/git/nltk/nltk/sem/drt.py__draw_command,"def _draw_command(self, item, x, y):
""""""
Draw the given item at the given location
:param item: the item to draw
:param x: the top of the current drawing area
:param y: the left side of the current drawing area
:return: the bottom-rightmost point
""""""
if isinstance(item, str):
self.canvas.create_text(x, y, anchor=""nw"", font=self.canvas.font, text=item)
elif isinstance(item, tuple):
(right, bottom) = item
self.canvas.create_rectangle(x, y, right, bottom)
horiz_line_y = (
y + self._get_text_height() + (self.BUFFER * 2)
)  # the line separating refs from conds
self.canvas.create_line(x, horiz_line_y, right, horiz_line_y)
return self._visit_command(item, x, y)
",[],0,[],/sem/drt.py__draw_command
2685,/home/amandapotts/git/nltk/nltk/sem/drt.py__visit_command,"def _visit_command(self, item, x, y):
""""""
Return the bottom-rightmost point without actually drawing the item
:param item: the item to visit
:param x: the top of the current drawing area
:param y: the left side of the current drawing area
:return: the bottom-rightmost point
""""""
if isinstance(item, str):
return (x + self.canvas.font.measure(item), y + self._get_text_height())
elif isinstance(item, tuple):
return item
",[],0,[],/sem/drt.py__visit_command
2686,/home/amandapotts/git/nltk/nltk/sem/drt.py__handle,"def _handle(self, expression, command, x=0, y=0):
""""""
:param expression: the expression to handle
:param command: the function to apply, either _draw_command or _visit_command
:param x: the top of the current drawing area
:param y: the left side of the current drawing area
:return: the bottom-rightmost point
""""""
if command == self._visit_command:
try:
right = expression._drawing_width + x
bottom = expression._drawing_height + y
return (right, bottom)
except AttributeError:
pass
if isinstance(expression, DrtAbstractVariableExpression):
factory = self._handle_VariableExpression
elif isinstance(expression, DRS):
factory = self._handle_DRS
elif isinstance(expression, DrtNegatedExpression):
factory = self._handle_NegatedExpression
elif isinstance(expression, DrtLambdaExpression):
factory = self._handle_LambdaExpression
elif isinstance(expression, BinaryExpression):
factory = self._handle_BinaryExpression
elif isinstance(expression, DrtApplicationExpression):
factory = self._handle_ApplicationExpression
elif isinstance(expression, PossibleAntecedents):
factory = self._handle_VariableExpression
elif isinstance(expression, DrtProposition):
factory = self._handle_DrtProposition
else:
raise Exception(expression.__class__.__name__)
(right, bottom) = factory(expression, command, x, y)
expression._drawing_width = right - x
expression._drawing_height = bottom - y
return (right, bottom)
",[],0,[],/sem/drt.py__handle
2687,/home/amandapotts/git/nltk/nltk/sem/drt.py__handle_VariableExpression,"def _handle_VariableExpression(self, expression, command, x, y):
return command(""%s"" % expression, x, y)
",[],0,[],/sem/drt.py__handle_VariableExpression
2688,/home/amandapotts/git/nltk/nltk/sem/drt.py__handle_NegatedExpression,"def _handle_NegatedExpression(self, expression, command, x, y):
right = self._visit_command(DrtTokens.NOT, x, y)[0]
(right, bottom) = self._handle(expression.term, command, right, y)
command(
DrtTokens.NOT,
x,
self._get_centered_top(y, bottom - y, self._get_text_height()),
)
return (right, bottom)
",[],0,[],/sem/drt.py__handle_NegatedExpression
2689,/home/amandapotts/git/nltk/nltk/sem/drt.py__handle_DRS,"def _handle_DRS(self, expression, command, x, y):
left = x + self.BUFFER  # indent the left side
bottom = y + self.BUFFER  # indent the top
if expression.refs:
refs = "" "".join(""%s"" % r for r in expression.refs)
else:
refs = ""     ""
(max_right, bottom) = command(refs, left, bottom)
bottom += self.BUFFER * 2
if expression.conds:
for cond in expression.conds:
(right, bottom) = self._handle(cond, command, left, bottom)
max_right = max(max_right, right)
bottom += self.BUFFER
else:
bottom += self._get_text_height() + self.BUFFER
max_right += self.BUFFER
return command((max_right, bottom), x, y)
",[],0,[],/sem/drt.py__handle_DRS
2690,/home/amandapotts/git/nltk/nltk/sem/drt.py__handle_ApplicationExpression,"def _handle_ApplicationExpression(self, expression, command, x, y):
function, args = expression.uncurry()
if not isinstance(function, DrtAbstractVariableExpression):
function = expression.function
args = [expression.argument]
function_bottom = self._visit(function, x, y)[1]
max_bottom = max(
[function_bottom] + [self._visit(arg, x, y)[1] for arg in args]
)
line_height = max_bottom - y
function_drawing_top = self._get_centered_top(
y, line_height, function._drawing_height
)
right = self._handle(function, command, x, function_drawing_top)[0]
centred_string_top = self._get_centered_top(
y, line_height, self._get_text_height()
)
right = command(DrtTokens.OPEN, right, centred_string_top)[0]
for i, arg in enumerate(args):
arg_drawing_top = self._get_centered_top(
y, line_height, arg._drawing_height
)
right = self._handle(arg, command, right, arg_drawing_top)[0]
if i + 1 < len(args):
right = command(DrtTokens.COMMA + "" "", right, centred_string_top)[0]
right = command(DrtTokens.CLOSE, right, centred_string_top)[0]
return (right, max_bottom)
",[],0,[],/sem/drt.py__handle_ApplicationExpression
2691,/home/amandapotts/git/nltk/nltk/sem/drt.py__handle_BinaryExpression,"def _handle_BinaryExpression(self, expression, command, x, y):
first_height = self._visit(expression.first, 0, 0)[1]
second_height = self._visit(expression.second, 0, 0)[1]
line_height = max(first_height, second_height)
centred_string_top = self._get_centered_top(
y, line_height, self._get_text_height()
)
right = command(DrtTokens.OPEN, x, centred_string_top)[0]
first_height = expression.first._drawing_height
(right, first_bottom) = self._handle(
expression.first,
command,
right,
self._get_centered_top(y, line_height, first_height),
)
right = command("" %s "" % expression.getOp(), right, centred_string_top)[0]
second_height = expression.second._drawing_height
(right, second_bottom) = self._handle(
expression.second,
command,
right,
self._get_centered_top(y, line_height, second_height),
)
right = command(DrtTokens.CLOSE, right, centred_string_top)[0]
return (right, max(first_bottom, second_bottom))
",[],0,[],/sem/drt.py__handle_BinaryExpression
2692,/home/amandapotts/git/nltk/nltk/sem/drt.py__handle_DrtProposition,"def _handle_DrtProposition(self, expression, command, x, y):
right = command(expression.variable, x, y)[0]
(right, bottom) = self._handle(expression.term, command, right, y)
return (right, bottom)
",[],0,[],/sem/drt.py__handle_DrtProposition
2693,/home/amandapotts/git/nltk/nltk/sem/drt.py__get_centered_top,"def _get_centered_top(self, top, full_height, item_height):
""""""Get the y-coordinate of the point that a figure should start at if
its height is 'item_height' and it needs to be centered in an area that
starts at 'top' and is 'full_height' tall.""""""
return top + (full_height - item_height) / 2
",[],0,[],/sem/drt.py__get_centered_top
2694,/home/amandapotts/git/nltk/nltk/sem/drt.py_test_draw,"def test_draw():
try:
from tkinter import Tk
except ImportError as e:
raise ValueError(""tkinter is required, but it's not available."")
expressions = [
r""x"",
r""([],[])"",
r""([x],[])"",
r""([x],[man(x)])"",
r""([x,y],[sees(x,y)])"",
r""([x],[man(x), walks(x)])"",
r""\x.([],[man(x), walks(x)])"",
r""\x y.([],[sees(x,y)])"",
r""([],[(([],[walks(x)]) + ([],[runs(x)]))])"",
r""([x],[man(x), -([],[walks(x)])])"",
r""([],[(([x],[man(x)]) -> ([],[walks(x)]))])"",
]
for e in expressions:
d = DrtExpression.fromstring(e)
d.draw()
",[],0,[],/sem/drt.py_test_draw
2695,/home/amandapotts/git/nltk/nltk/inference/tableau.py__prove,"def _prove(self, goal=None, assumptions=None, verbose=False):
if not assumptions:
assumptions = []
result = None
try:
agenda = Agenda()
if goal:
agenda.put(-goal)
agenda.put_all(assumptions)
debugger = Debug(verbose)
result = self._attempt_proof(agenda, set(), set(), debugger)
except RuntimeError as e:
if self._assume_false and str(e).startswith(
""maximum recursion depth exceeded""
):
result = False
else:
if verbose:
print(e)
else:
raise e
return (result, ""\n"".join(debugger.lines))
",[],0,[],/inference/tableau.py__prove
2696,/home/amandapotts/git/nltk/nltk/inference/tableau.py__attempt_proof,"def _attempt_proof(self, agenda, accessible_vars, atoms, debug):
(current, context), category = agenda.pop_first()
if not current:
debug.line(""AGENDA EMPTY"")
return False
proof_method = {
Categories.ATOM: self._attempt_proof_atom,
Categories.PROP: self._attempt_proof_prop,
Categories.N_ATOM: self._attempt_proof_n_atom,
Categories.N_PROP: self._attempt_proof_n_prop,
Categories.APP: self._attempt_proof_app,
Categories.N_APP: self._attempt_proof_n_app,
Categories.N_EQ: self._attempt_proof_n_eq,
Categories.D_NEG: self._attempt_proof_d_neg,
Categories.N_ALL: self._attempt_proof_n_all,
Categories.N_EXISTS: self._attempt_proof_n_some,
Categories.AND: self._attempt_proof_and,
Categories.N_OR: self._attempt_proof_n_or,
Categories.N_IMP: self._attempt_proof_n_imp,
Categories.OR: self._attempt_proof_or,
Categories.IMP: self._attempt_proof_imp,
Categories.N_AND: self._attempt_proof_n_and,
Categories.IFF: self._attempt_proof_iff,
Categories.N_IFF: self._attempt_proof_n_iff,
Categories.EQ: self._attempt_proof_eq,
Categories.EXISTS: self._attempt_proof_some,
Categories.ALL: self._attempt_proof_all,
}[category]
debug.line((current, context))
return proof_method(current, context, agenda, accessible_vars, atoms, debug)
",[],0,[],/inference/tableau.py__attempt_proof
2697,/home/amandapotts/git/nltk/nltk/inference/tableau.py__attempt_proof_atom,"def _attempt_proof_atom(
self, current, context, agenda, accessible_vars, atoms, debug
",[],0,[],/inference/tableau.py__attempt_proof_atom
2698,/home/amandapotts/git/nltk/nltk/inference/tableau.py__attempt_proof_n_atom,"def _attempt_proof_n_atom(
self, current, context, agenda, accessible_vars, atoms, debug
",[],0,[],/inference/tableau.py__attempt_proof_n_atom
2699,/home/amandapotts/git/nltk/nltk/inference/tableau.py__attempt_proof_prop,"def _attempt_proof_prop(
self, current, context, agenda, accessible_vars, atoms, debug
",[],0,[],/inference/tableau.py__attempt_proof_prop
2700,/home/amandapotts/git/nltk/nltk/inference/tableau.py__attempt_proof_n_prop,"def _attempt_proof_n_prop(
self, current, context, agenda, accessible_vars, atoms, debug
",[],0,[],/inference/tableau.py__attempt_proof_n_prop
2701,/home/amandapotts/git/nltk/nltk/inference/tableau.py__attempt_proof_app,"def _attempt_proof_app(
self, current, context, agenda, accessible_vars, atoms, debug
",[],0,[],/inference/tableau.py__attempt_proof_app
2702,/home/amandapotts/git/nltk/nltk/inference/tableau.py__attempt_proof_n_app,"def _attempt_proof_n_app(
self, current, context, agenda, accessible_vars, atoms, debug
",[],0,[],/inference/tableau.py__attempt_proof_n_app
2703,/home/amandapotts/git/nltk/nltk/inference/tableau.py__attempt_proof_n_eq,"def _attempt_proof_n_eq(
self, current, context, agenda, accessible_vars, atoms, debug
",[],0,[],/inference/tableau.py__attempt_proof_n_eq
2704,/home/amandapotts/git/nltk/nltk/inference/tableau.py__attempt_proof_d_neg,"def _attempt_proof_d_neg(
self, current, context, agenda, accessible_vars, atoms, debug
",[],0,[],/inference/tableau.py__attempt_proof_d_neg
2705,/home/amandapotts/git/nltk/nltk/inference/tableau.py__attempt_proof_n_all,"def _attempt_proof_n_all(
self, current, context, agenda, accessible_vars, atoms, debug
",[],0,[],/inference/tableau.py__attempt_proof_n_all
2706,/home/amandapotts/git/nltk/nltk/inference/tableau.py__attempt_proof_n_some,"def _attempt_proof_n_some(
self, current, context, agenda, accessible_vars, atoms, debug
",[],0,[],/inference/tableau.py__attempt_proof_n_some
2707,/home/amandapotts/git/nltk/nltk/inference/tableau.py__attempt_proof_and,"def _attempt_proof_and(
self, current, context, agenda, accessible_vars, atoms, debug
",[],0,[],/inference/tableau.py__attempt_proof_and
2708,/home/amandapotts/git/nltk/nltk/inference/tableau.py__attempt_proof_n_or,"def _attempt_proof_n_or(
self, current, context, agenda, accessible_vars, atoms, debug
",[],0,[],/inference/tableau.py__attempt_proof_n_or
2709,/home/amandapotts/git/nltk/nltk/inference/tableau.py__attempt_proof_n_imp,"def _attempt_proof_n_imp(
self, current, context, agenda, accessible_vars, atoms, debug
",[],0,[],/inference/tableau.py__attempt_proof_n_imp
2710,/home/amandapotts/git/nltk/nltk/inference/tableau.py__attempt_proof_or,"def _attempt_proof_or(
self, current, context, agenda, accessible_vars, atoms, debug
",[],0,[],/inference/tableau.py__attempt_proof_or
2711,/home/amandapotts/git/nltk/nltk/inference/tableau.py__attempt_proof_imp,"def _attempt_proof_imp(
self, current, context, agenda, accessible_vars, atoms, debug
",[],0,[],/inference/tableau.py__attempt_proof_imp
2712,/home/amandapotts/git/nltk/nltk/inference/tableau.py__attempt_proof_n_and,"def _attempt_proof_n_and(
self, current, context, agenda, accessible_vars, atoms, debug
",[],0,[],/inference/tableau.py__attempt_proof_n_and
2713,/home/amandapotts/git/nltk/nltk/inference/tableau.py__attempt_proof_iff,"def _attempt_proof_iff(
self, current, context, agenda, accessible_vars, atoms, debug
",[],0,[],/inference/tableau.py__attempt_proof_iff
2714,/home/amandapotts/git/nltk/nltk/inference/tableau.py__attempt_proof_n_iff,"def _attempt_proof_n_iff(
self, current, context, agenda, accessible_vars, atoms, debug
",[],0,[],/inference/tableau.py__attempt_proof_n_iff
2715,/home/amandapotts/git/nltk/nltk/inference/tableau.py__attempt_proof_eq,"def _attempt_proof_eq(
self, current, context, agenda, accessible_vars, atoms, debug
",[],0,[],/inference/tableau.py__attempt_proof_eq
2716,/home/amandapotts/git/nltk/nltk/inference/tableau.py__attempt_proof_some,"def _attempt_proof_some(
self, current, context, agenda, accessible_vars, atoms, debug
",[],0,[],/inference/tableau.py__attempt_proof_some
2717,/home/amandapotts/git/nltk/nltk/inference/tableau.py__attempt_proof_all,"def _attempt_proof_all(
self, current, context, agenda, accessible_vars, atoms, debug
",[],0,[],/inference/tableau.py__attempt_proof_all
2718,/home/amandapotts/git/nltk/nltk/inference/tableau.py_is_atom,"def is_atom(e):
if isinstance(e, NegatedExpression):
e = e.term
if isinstance(e, ApplicationExpression):
for arg in e.args:
if not TableauProver.is_atom(arg):
return False
return True
elif isinstance(e, AbstractVariableExpression) or isinstance(
e, LambdaExpression
):
return True
else:
return False
",[],0,[],/inference/tableau.py_is_atom
2719,/home/amandapotts/git/nltk/nltk/inference/tableau.py___init__,"def __init__(self, goal=None, assumptions=None, prover=None):
""""""
:param goal: Input expression to prove
:type goal: sem.Expression
:param assumptions: Input expressions to use as assumptions in
the proof.
:type assumptions: list(sem.Expression)
""""""
if prover is not None:
assert isinstance(prover, TableauProver)
else:
prover = TableauProver()
BaseProverCommand.__init__(self, prover, goal, assumptions)
",[],0,[],/inference/tableau.py___init__
2720,/home/amandapotts/git/nltk/nltk/inference/tableau.py___init__,"def __init__(self):
self.sets = tuple(set() for i in range(21))
",[],0,[],/inference/tableau.py___init__
2721,/home/amandapotts/git/nltk/nltk/inference/tableau.py_clone,"def clone(self):
new_agenda = Agenda()
set_list = [s.copy() for s in self.sets]
new_allExs = set()
for allEx, _ in set_list[Categories.ALL]:
new_allEx = AllExpression(allEx.variable, allEx.term)
try:
new_allEx._used_vars = {used for used in allEx._used_vars}
except AttributeError:
new_allEx._used_vars = set()
new_allExs.add((new_allEx, None))
set_list[Categories.ALL] = new_allExs
set_list[Categories.N_EQ] = {
(NegatedExpression(n_eq.term), ctx)
for (n_eq, ctx) in set_list[Categories.N_EQ]
}
new_agenda.sets = tuple(set_list)
return new_agenda
",[],0,[],/inference/tableau.py_clone
2722,/home/amandapotts/git/nltk/nltk/inference/tableau.py___getitem__,"def __getitem__(self, index):
return self.sets[index]
",[],0,[],/inference/tableau.py___getitem__
2723,/home/amandapotts/git/nltk/nltk/inference/tableau.py_put,"def put(self, expression, context=None):
if isinstance(expression, AllExpression):
ex_to_add = AllExpression(expression.variable, expression.term)
try:
ex_to_add._used_vars = {used for used in expression._used_vars}
except AttributeError:
ex_to_add._used_vars = set()
else:
ex_to_add = expression
self.sets[self._categorize_expression(ex_to_add)].add((ex_to_add, context))
",[],0,[],/inference/tableau.py_put
2724,/home/amandapotts/git/nltk/nltk/inference/tableau.py_put_all,"def put_all(self, expressions):
for expression in expressions:
self.put(expression)
",[],0,[],/inference/tableau.py_put_all
2725,/home/amandapotts/git/nltk/nltk/inference/tableau.py_put_atoms,"def put_atoms(self, atoms):
for atom, neg in atoms:
if neg:
self[Categories.N_ATOM].add((-atom, None))
else:
self[Categories.ATOM].add((atom, None))
",[],0,[],/inference/tableau.py_put_atoms
2726,/home/amandapotts/git/nltk/nltk/inference/tableau.py_pop_first,"def pop_first(self):
""""""Pop the first expression that appears in the agenda""""""
for i, s in enumerate(self.sets):
if s:
if i in [Categories.N_EQ, Categories.ALL]:
for ex in s:
try:
if not ex[0]._exhausted:
s.remove(ex)
return (ex, i)
except AttributeError:
s.remove(ex)
return (ex, i)
else:
return (s.pop(), i)
return ((None, None), None)
",[],0,[],/inference/tableau.py_pop_first
2727,/home/amandapotts/git/nltk/nltk/inference/tableau.py_replace_all,"def replace_all(self, old, new):
for s in self.sets:
for ex, ctx in s:
ex.replace(old.variable, new)
if ctx is not None:
ctx.replace(old.variable, new)
",[],0,[],/inference/tableau.py_replace_all
2728,/home/amandapotts/git/nltk/nltk/inference/tableau.py_mark_alls_fresh,"def mark_alls_fresh(self):
for u, _ in self.sets[Categories.ALL]:
u._exhausted = False
",[],0,[],/inference/tableau.py_mark_alls_fresh
2729,/home/amandapotts/git/nltk/nltk/inference/tableau.py_mark_neqs_fresh,"def mark_neqs_fresh(self):
for neq, _ in self.sets[Categories.N_EQ]:
neq._exhausted = False
",[],0,[],/inference/tableau.py_mark_neqs_fresh
2730,/home/amandapotts/git/nltk/nltk/inference/tableau.py__categorize_expression,"def _categorize_expression(self, current):
if isinstance(current, NegatedExpression):
return self._categorize_NegatedExpression(current)
elif isinstance(current, FunctionVariableExpression):
return Categories.PROP
elif TableauProver.is_atom(current):
return Categories.ATOM
elif isinstance(current, AllExpression):
return Categories.ALL
elif isinstance(current, AndExpression):
return Categories.AND
elif isinstance(current, OrExpression):
return Categories.OR
elif isinstance(current, ImpExpression):
return Categories.IMP
elif isinstance(current, IffExpression):
return Categories.IFF
elif isinstance(current, EqualityExpression):
return Categories.EQ
elif isinstance(current, ExistsExpression):
return Categories.EXISTS
elif isinstance(current, ApplicationExpression):
return Categories.APP
else:
raise ProverParseError(""cannot categorize %s"" % current.__class__.__name__)
",[],0,[],/inference/tableau.py__categorize_expression
2731,/home/amandapotts/git/nltk/nltk/inference/tableau.py__categorize_NegatedExpression,"def _categorize_NegatedExpression(self, current):
negated = current.term
if isinstance(negated, NegatedExpression):
return Categories.D_NEG
elif isinstance(negated, FunctionVariableExpression):
return Categories.N_PROP
elif TableauProver.is_atom(negated):
return Categories.N_ATOM
elif isinstance(negated, AllExpression):
return Categories.N_ALL
elif isinstance(negated, AndExpression):
return Categories.N_AND
elif isinstance(negated, OrExpression):
return Categories.N_OR
elif isinstance(negated, ImpExpression):
return Categories.N_IMP
elif isinstance(negated, IffExpression):
return Categories.N_IFF
elif isinstance(negated, EqualityExpression):
return Categories.N_EQ
elif isinstance(negated, ExistsExpression):
return Categories.N_EXISTS
elif isinstance(negated, ApplicationExpression):
return Categories.N_APP
else:
raise ProverParseError(""cannot categorize %s"" % negated.__class__.__name__)
",[],0,[],/inference/tableau.py__categorize_NegatedExpression
2732,/home/amandapotts/git/nltk/nltk/inference/tableau.py___init__,"def __init__(self, verbose, indent=0, lines=None):
self.verbose = verbose
self.indent = indent
if not lines:
lines = []
self.lines = lines
",[],0,[],/inference/tableau.py___init__
2733,/home/amandapotts/git/nltk/nltk/inference/tableau.py___add__,"def __add__(self, increment):
return Debug(self.verbose, self.indent + 1, self.lines)
",[],0,[],/inference/tableau.py___add__
2734,/home/amandapotts/git/nltk/nltk/inference/tableau.py_line,"def line(self, data, indent=0):
if isinstance(data, tuple):
ex, ctx = data
if ctx:
data = f""{ex}, {ctx}""
else:
data = ""%s"" % ex
if isinstance(ex, AllExpression):
try:
used_vars = ""[%s]"" % (
"","".join(""%s"" % ve.variable.name for ve in ex._used_vars)
)
data += "":   %s"" % used_vars
except AttributeError:
data += "":   []""
newline = ""{}{}"".format(""   "" * (self.indent + indent), data)
self.lines.append(newline)
if self.verbose:
print(newline)
",[],0,[],/inference/tableau.py_line
2735,/home/amandapotts/git/nltk/nltk/inference/tableau.py_testTableauProver,"def testTableauProver():
tableau_test(""P | -P"")
tableau_test(""P & -P"")
tableau_test(""Q"", [""P"", ""(P -> Q)""])
tableau_test(""man(x)"")
tableau_test(""(man(x) -> man(x))"")
tableau_test(""(man(x) -> --man(x))"")
tableau_test(""-(man(x) and -man(x))"")
tableau_test(""(man(x) or -man(x))"")
tableau_test(""(man(x) -> man(x))"")
tableau_test(""-(man(x) and -man(x))"")
tableau_test(""(man(x) or -man(x))"")
tableau_test(""(man(x) -> man(x))"")
tableau_test(""(man(x) iff man(x))"")
tableau_test(""-(man(x) iff -man(x))"")
tableau_test(""all x.man(x)"")
tableau_test(""all x.all y.((x = y) -> (y = x))"")
tableau_test(""all x.all y.all z.(((x = y) & (y = z)) -> (x = z))"")
p1 = ""all x.(man(x) -> mortal(x))""
p2 = ""man(Socrates)""
c = ""mortal(Socrates)""
tableau_test(c, [p1, p2])
p1 = ""all x.(man(x) -> walks(x))""
p2 = ""man(John)""
c = ""some y.walks(y)""
tableau_test(c, [p1, p2])
p = ""((x = y) & walks(y))""
c = ""walks(x)""
tableau_test(c, [p])
p = ""((x = y) & ((y = z) & (z = w)))""
c = ""(x = w)""
tableau_test(c, [p])
p = ""some e1.some e2.(believe(e1,john,e2) & walk(e2,mary))""
c = ""some e0.walk(e0,mary)""
tableau_test(c, [p])
c = ""(exists x.exists z3.((x = Mary) & ((z3 = John) & sees(z3,x))) <-> exists x.exists z4.((x = John) & ((z4 = Mary) & sees(x,z4))))""
tableau_test(c)
",[],0,[],/inference/tableau.py_testTableauProver
2736,/home/amandapotts/git/nltk/nltk/inference/tableau.py_testHigherOrderTableauProver,"def testHigherOrderTableauProver():
tableau_test(""believe(j, -lie(b))"", [""believe(j, -lie(b) & -cheat(b))""])
tableau_test(""believe(j, lie(b) & cheat(b))"", [""believe(j, lie(b))""])
tableau_test(
""believe(j, lie(b))"", [""lie(b)""]
)  # how do we capture that John believes all things that are true
tableau_test(
""believe(j, know(b, cheat(b)))"",
[""believe(j, know(b, lie(b)) & know(b, steals(b) & cheat(b)))""],
)
tableau_test(""P(Q(y), R(y) & R(z))"", [""P(Q(x) & Q(y), R(y) & R(z))""])
tableau_test(""believe(j, cheat(b) & lie(b))"", [""believe(j, lie(b) & cheat(b))""])
tableau_test(""believe(j, -cheat(b) & -lie(b))"", [""believe(j, -lie(b) & -cheat(b))""])
",[],0,[],/inference/tableau.py_testHigherOrderTableauProver
2737,/home/amandapotts/git/nltk/nltk/inference/tableau.py_tableau_test,"def tableau_test(c, ps=None, verbose=False):
pc = Expression.fromstring(c)
pps = [Expression.fromstring(p) for p in ps] if ps else []
if not ps:
ps = []
print(
""%s |- %s: %s""
% ("", "".join(ps), pc, TableauProver().prove(pc, pps, verbose=verbose))
)
",[],0,[],/inference/tableau.py_tableau_test
2738,/home/amandapotts/git/nltk/nltk/inference/tableau.py_demo,"def demo():
testTableauProver()
testHigherOrderTableauProver()
",[],0,[],/inference/tableau.py_demo
2739,/home/amandapotts/git/nltk/nltk/inference/discourse.py_parse_to_readings,"def parse_to_readings(self, sentence):
""""""
:param sentence: the sentence to read
:type sentence: str
""""""
",[],0,[],/inference/discourse.py_parse_to_readings
2740,/home/amandapotts/git/nltk/nltk/inference/discourse.py_process_thread,"def process_thread(self, sentence_readings):
""""""
This method should be used to handle dependencies between readings such
as resolving anaphora.
:param sentence_readings: readings to process
:type sentence_readings: list(Expression)
:return: the list of readings after processing
:rtype: list(Expression)
""""""
return sentence_readings
",[],0,[],/inference/discourse.py_process_thread
2741,/home/amandapotts/git/nltk/nltk/inference/discourse.py_combine_readings,"def combine_readings(self, readings):
""""""
:param readings: readings to combine
:type readings: list(Expression)
:return: one combined reading
:rtype: Expression
""""""
",[],0,[],/inference/discourse.py_combine_readings
2742,/home/amandapotts/git/nltk/nltk/inference/discourse.py_to_fol,"def to_fol(self, expression):
""""""
Convert this expression into a First-Order Logic expression.
:param expression: an expression
:type expression: Expression
:return: a FOL version of the input expression
:rtype: Expression
""""""
",[],0,[],/inference/discourse.py_to_fol
2743,/home/amandapotts/git/nltk/nltk/inference/discourse.py___init__,"def __init__(self, gramfile=None):
""""""
:param gramfile: name of file where grammar can be loaded
:type gramfile: str
""""""
self._gramfile = (
gramfile if gramfile else ""grammars/book_grammars/discourse.fcfg""
)
self._parser = load_parser(self._gramfile)
",[],0,[],/inference/discourse.py___init__
2744,/home/amandapotts/git/nltk/nltk/inference/discourse.py_parse_to_readings,"def parse_to_readings(self, sentence):
"""""":see: ReadingCommand.parse_to_readings()""""""
from nltk.sem import root_semrep
tokens = sentence.split()
trees = self._parser.parse(tokens)
return [root_semrep(tree) for tree in trees]
",[],0,[],/inference/discourse.py_parse_to_readings
2745,/home/amandapotts/git/nltk/nltk/inference/discourse.py_combine_readings,"def combine_readings(self, readings):
"""""":see: ReadingCommand.combine_readings()""""""
return reduce(and_, readings)
",[],0,[],/inference/discourse.py_combine_readings
2746,/home/amandapotts/git/nltk/nltk/inference/discourse.py_to_fol,"def to_fol(self, expression):
"""""":see: ReadingCommand.to_fol()""""""
return expression
",[],0,[],/inference/discourse.py_to_fol
2747,/home/amandapotts/git/nltk/nltk/inference/discourse.py___init__,"def __init__(self, semtype_file=None, remove_duplicates=False, depparser=None):
""""""
:param semtype_file: name of file where grammar can be loaded
:param remove_duplicates: should duplicates be removed?
:param depparser: the dependency parser
""""""
if semtype_file is None:
semtype_file = os.path.join(
""grammars"", ""sample_grammars"", ""drt_glue.semtype""
)
self._glue = DrtGlue(
semtype_file=semtype_file,
remove_duplicates=remove_duplicates,
depparser=depparser,
)
",[],0,[],/inference/discourse.py___init__
2748,/home/amandapotts/git/nltk/nltk/inference/discourse.py_parse_to_readings,"def parse_to_readings(self, sentence):
"""""":see: ReadingCommand.parse_to_readings()""""""
return self._glue.parse_to_meaning(sentence)
",[],0,[],/inference/discourse.py_parse_to_readings
2749,/home/amandapotts/git/nltk/nltk/inference/discourse.py_process_thread,"def process_thread(self, sentence_readings):
"""""":see: ReadingCommand.process_thread()""""""
try:
return [self.combine_readings(sentence_readings)]
except AnaphoraResolutionException:
return []
",[],0,[],/inference/discourse.py_process_thread
2750,/home/amandapotts/git/nltk/nltk/inference/discourse.py_combine_readings,"def combine_readings(self, readings):
"""""":see: ReadingCommand.combine_readings()""""""
thread_reading = reduce(add, readings)
return resolve_anaphora(thread_reading.simplify())
",[],0,[],/inference/discourse.py_combine_readings
2751,/home/amandapotts/git/nltk/nltk/inference/discourse.py_to_fol,"def to_fol(self, expression):
"""""":see: ReadingCommand.to_fol()""""""
return expression.fol()
",[],0,[],/inference/discourse.py_to_fol
2752,/home/amandapotts/git/nltk/nltk/inference/discourse.py___init__,"def __init__(self, input, reading_command=None, background=None):
""""""
Initialize a ``DiscourseTester``.
:param input: the discourse sentences
:type input: list of str
:param background: Formulas which express background assumptions
:type background: list(Expression)
""""""
self._input = input
self._sentences = {""s%s"" % i: sent for i, sent in enumerate(input)}
self._models = None
self._readings = {}
self._reading_command = (
reading_command if reading_command else CfgReadingCommand()
)
self._threads = {}
self._filtered_threads = {}
if background is not None:
from nltk.sem.logic import Expression
for e in background:
assert isinstance(e, Expression)
self._background = background
else:
self._background = []
",[],0,[],/inference/discourse.py___init__
2753,/home/amandapotts/git/nltk/nltk/inference/discourse.py_sentences,"def sentences(self):
""""""
Display the list of sentences in the current discourse.
""""""
for id in sorted(self._sentences):
print(f""{id}: {self._sentences[id]}"")
",[],0,[],/inference/discourse.py_sentences
2754,/home/amandapotts/git/nltk/nltk/inference/discourse.py_add_sentence,"def add_sentence(self, sentence, informchk=False, consistchk=False):
""""""
Add a sentence to the current discourse.
Updates ``self._input`` and ``self._sentences``.
:param sentence: An input sentence
:type sentence: str
:param informchk: if ``True``, check that the result of adding the sentence is thread-informative. Updates ``self._readings``.
:param consistchk: if ``True``, check that the result of adding the sentence is thread-consistent. Updates ``self._readings``.
""""""
if informchk:
self.readings(verbose=False)
for tid in sorted(self._threads):
assumptions = [reading for (rid, reading) in self.expand_threads(tid)]
assumptions += self._background
for sent_reading in self._get_readings(sentence):
tp = Prover9Command(goal=sent_reading, assumptions=assumptions)
if tp.prove():
print(
""Sentence '%s' under reading '%s':""
% (sentence, str(sent_reading))
)
print(""Not informative relative to thread '%s'"" % tid)
self._input.append(sentence)
self._sentences = {""s%s"" % i: sent for i, sent in enumerate(self._input)}
if consistchk:
self.readings(verbose=False)
self.models(show=False)
",[],0,[],/inference/discourse.py_add_sentence
2755,/home/amandapotts/git/nltk/nltk/inference/discourse.py_retract_sentence,"def retract_sentence(self, sentence, verbose=True):
""""""
Remove a sentence from the current discourse.
Updates ``self._input``, ``self._sentences`` and ``self._readings``.
:param sentence: An input sentence
:type sentence: str
:param verbose: If ``True``,  report on the updated list of sentences.
""""""
try:
self._input.remove(sentence)
except ValueError:
print(
""Retraction failed. The sentence '%s' is not part of the current discourse:""
% sentence
)
self.sentences()
return None
self._sentences = {""s%s"" % i: sent for i, sent in enumerate(self._input)}
self.readings(verbose=False)
if verbose:
print(""Current sentences are "")
self.sentences()
",[],0,[],/inference/discourse.py_retract_sentence
2756,/home/amandapotts/git/nltk/nltk/inference/discourse.py_grammar,"def grammar(self):
""""""
Print out the grammar in use for parsing input sentences
""""""
show_cfg(self._reading_command._gramfile)
",[],0,[],/inference/discourse.py_grammar
2757,/home/amandapotts/git/nltk/nltk/inference/discourse.py__get_readings,"def _get_readings(self, sentence):
""""""
Build a list of semantic readings for a sentence.
:rtype: list(Expression)
""""""
return self._reading_command.parse_to_readings(sentence)
",[],0,[],/inference/discourse.py__get_readings
2758,/home/amandapotts/git/nltk/nltk/inference/discourse.py__construct_readings,"def _construct_readings(self):
""""""
Use ``self._sentences`` to construct a value for ``self._readings``.
""""""
self._readings = {}
for sid in sorted(self._sentences):
sentence = self._sentences[sid]
readings = self._get_readings(sentence)
self._readings[sid] = {
f""{sid}-r{rid}"": reading.simplify()
for rid, reading in enumerate(sorted(readings, key=str))
}
",[],0,[],/inference/discourse.py__construct_readings
2759,/home/amandapotts/git/nltk/nltk/inference/discourse.py__construct_threads,"def _construct_threads(self):
""""""
Use ``self._readings`` to construct a value for ``self._threads``
and use the model builder to construct a value for ``self._filtered_threads``
""""""
thread_list = [[]]
for sid in sorted(self._readings):
thread_list = self.multiply(thread_list, sorted(self._readings[sid]))
self._threads = {""d%s"" % tid: thread for tid, thread in enumerate(thread_list)}
self._filtered_threads = {}
consistency_checked = self._check_consistency(self._threads)
for tid, thread in self._threads.items():
if (tid, True) in consistency_checked:
self._filtered_threads[tid] = thread
",[],0,[],/inference/discourse.py__construct_threads
2760,/home/amandapotts/git/nltk/nltk/inference/discourse.py__show_readings,"def _show_readings(self, sentence=None):
""""""
Print out the readings for  the discourse (or a single sentence).
""""""
if sentence is not None:
print(""The sentence '%s' has these readings:"" % sentence)
for r in [str(reading) for reading in (self._get_readings(sentence))]:
print(""    %s"" % r)
else:
for sid in sorted(self._readings):
print()
print(""%s readings:"" % sid)
print()  #'-' * 30
for rid in sorted(self._readings[sid]):
lf = self._readings[sid][rid]
print(f""{rid}: {lf.normalize()}"")
",[],0,[],/inference/discourse.py__show_readings
2761,/home/amandapotts/git/nltk/nltk/inference/discourse.py__show_threads,"def _show_threads(self, filter=False, show_thread_readings=False):
""""""
Print out the value of ``self._threads`` or ``self._filtered_hreads``
""""""
threads = self._filtered_threads if filter else self._threads
for tid in sorted(threads):
if show_thread_readings:
readings = [
self._readings[rid.split(""-"")[0]][rid] for rid in self._threads[tid]
]
try:
thread_reading = (
"": %s""
% self._reading_command.combine_readings(readings).normalize()
)
except Exception as e:
thread_reading = "": INVALID: %s"" % e.__class__.__name__
else:
thread_reading = """"
print(""%s:"" % tid, self._threads[tid], thread_reading)
",[],0,[],/inference/discourse.py__show_threads
2762,/home/amandapotts/git/nltk/nltk/inference/discourse.py_readings,"def readings(
self,
sentence=None,
threaded=False,
verbose=True,
filter=False,
show_thread_readings=False,
",[],0,[],/inference/discourse.py_readings
2763,/home/amandapotts/git/nltk/nltk/inference/discourse.py_expand_threads,"def expand_threads(self, thread_id, threads=None):
""""""
Given a thread ID, find the list of ``logic.Expression`` objects corresponding to the reading IDs in that thread.
:param thread_id: thread ID
:type thread_id: str
:param threads: a mapping from thread IDs to lists of reading IDs
:type threads: dict
:return: A list of pairs ``(rid, reading)`` where reading is the ``logic.Expression`` associated with a reading ID
:rtype: list of tuple
""""""
if threads is None:
threads = self._threads
return [
(rid, self._readings[sid][rid])
for rid in threads[thread_id]
for sid in rid.split(""-"")[:1]
]
",[],0,[],/inference/discourse.py_expand_threads
2764,/home/amandapotts/git/nltk/nltk/inference/discourse.py__check_consistency,"def _check_consistency(self, threads, show=False, verbose=False):
results = []
for tid in sorted(threads):
assumptions = [
reading for (rid, reading) in self.expand_threads(tid, threads=threads)
]
assumptions = list(
map(
self._reading_command.to_fol,
self._reading_command.process_thread(assumptions),
)
)
if assumptions:
assumptions += self._background
mb = MaceCommand(None, assumptions, max_models=20)
modelfound = mb.build_model()
else:
modelfound = False
results.append((tid, modelfound))
if show:
spacer(80)
print(""Model for Discourse Thread %s"" % tid)
spacer(80)
if verbose:
for a in assumptions:
print(a)
spacer(80)
if modelfound:
print(mb.model(format=""cooked""))
else:
print(""No model found!\n"")
return results
",[],0,[],/inference/discourse.py__check_consistency
2765,/home/amandapotts/git/nltk/nltk/inference/discourse.py_models,"def models(self, thread_id=None, show=True, verbose=False):
""""""
Call Mace4 to build a model for each current discourse thread.
:param thread_id: thread ID
:type thread_id: str
:param show: If ``True``, display the model that has been found.
""""""
self._construct_readings()
self._construct_threads()
threads = {thread_id: self._threads[thread_id]} if thread_id else self._threads
for tid, modelfound in self._check_consistency(
threads, show=show, verbose=verbose
):
idlist = [rid for rid in threads[tid]]
if not modelfound:
print(f""Inconsistent discourse: {tid} {idlist}:"")
for rid, reading in self.expand_threads(tid):
print(f""    {rid}: {reading.normalize()}"")
print()
else:
print(f""Consistent discourse: {tid} {idlist}:"")
for rid, reading in self.expand_threads(tid):
print(f""    {rid}: {reading.normalize()}"")
print()
",[],0,[],/inference/discourse.py_models
2766,/home/amandapotts/git/nltk/nltk/inference/discourse.py_add_background,"def add_background(self, background, verbose=False):
""""""
Add a list of background assumptions for reasoning about the discourse.
When called,  this method also updates the discourse model's set of readings and threads.
:param background: Formulas which contain background information
:type background: list(Expression)
""""""
from nltk.sem.logic import Expression
for count, e in enumerate(background):
assert isinstance(e, Expression)
if verbose:
print(""Adding assumption %s to background"" % count)
self._background.append(e)
self._construct_readings()
self._construct_threads()
",[],0,[],/inference/discourse.py_add_background
2767,/home/amandapotts/git/nltk/nltk/inference/discourse.py_background,"def background(self):
""""""
Show the current background assumptions.
""""""
for e in self._background:
print(str(e))
",[],0,[],/inference/discourse.py_background
2768,/home/amandapotts/git/nltk/nltk/inference/discourse.py_multiply,"def multiply(discourse, readings):
""""""
Multiply every thread in ``discourse`` by every reading in ``readings``.
Given discourse = [['A'], ['B']], readings = ['a', 'b', 'c'] , returns
[['A', 'a'], ['A', 'b'], ['A', 'c'], ['B', 'a'], ['B', 'b'], ['B', 'c']]
:param discourse: the current list of readings
:type discourse: list of lists
:param readings: an additional list of readings
:type readings: list(Expression)
:rtype: A list of lists
""""""
result = []
for sublist in discourse:
for r in readings:
new = []
new += sublist
new.append(r)
result.append(new)
return result
",[],0,[],/inference/discourse.py_multiply
2769,/home/amandapotts/git/nltk/nltk/inference/discourse.py_load_fol,"def load_fol(s):
""""""
Temporarily duplicated from ``nltk.sem.util``.
Convert a  file of first order formulas into a list of ``Expression`` objects.
:param s: the contents of the file
:type s: str
:return: a list of parsed formulas.
:rtype: list(Expression)
""""""
statements = []
for linenum, line in enumerate(s.splitlines()):
line = line.strip()
if line.startswith(""#"") or line == """":
continue
try:
statements.append(Expression.fromstring(line))
except Exception as e:
raise ValueError(f""Unable to parse line {linenum}: {line}"") from e
return statements
",[],0,[],/inference/discourse.py_load_fol
2770,/home/amandapotts/git/nltk/nltk/inference/discourse.py_discourse_demo,"def discourse_demo(reading_command=None):
""""""
Illustrate the various methods of ``DiscourseTester``
""""""
dt = DiscourseTester(
[""A boxer walks"", ""Every boxer chases a girl""], reading_command
)
dt.models()
print()
print()
dt.sentences()
print()
dt.readings()
print()
dt.readings(threaded=True)
print()
dt.models(""d1"")
dt.add_sentence(""John is a boxer"")
print()
dt.sentences()
print()
dt.readings(threaded=True)
print()
dt = DiscourseTester(
[""A student dances"", ""Every student is a person""], reading_command
)
print()
dt.add_sentence(""No person dances"", consistchk=True)
print()
dt.readings()
print()
dt.retract_sentence(""No person dances"", verbose=True)
print()
dt.models()
print()
dt.readings(""A person dances"")
print()
dt.add_sentence(""A person dances"", informchk=True)
dt = DiscourseTester(
[""Vincent is a boxer"", ""Fido is a boxer"", ""Vincent is married"", ""Fido barks""],
reading_command,
)
dt.readings(filter=True)
import nltk.data
background_file = os.path.join(""grammars"", ""book_grammars"", ""background.fol"")
background = nltk.data.load(background_file)
print()
dt.add_background(background, verbose=False)
dt.background()
print()
dt.readings(filter=True)
print()
dt.models()
",[],0,[],/inference/discourse.py_discourse_demo
2771,/home/amandapotts/git/nltk/nltk/inference/discourse.py_drt_discourse_demo,"def drt_discourse_demo(reading_command=None):
""""""
Illustrate the various methods of ``DiscourseTester``
""""""
dt = DiscourseTester([""every dog chases a boy"", ""he runs""], reading_command)
dt.models()
print()
dt.sentences()
print()
dt.readings()
print()
dt.readings(show_thread_readings=True)
print()
dt.readings(filter=True, show_thread_readings=True)
",[],0,[],/inference/discourse.py_drt_discourse_demo
2772,/home/amandapotts/git/nltk/nltk/inference/discourse.py_spacer,"def spacer(num=30):
print(""-"" * num)
",[],0,[],/inference/discourse.py_spacer
2773,/home/amandapotts/git/nltk/nltk/inference/discourse.py_demo,"def demo():
discourse_demo()
tagger = RegexpTagger(
[
(""^(chases|runs)$"", ""VB""),
(""^(a)$"", ""ex_quant""),
(""^(every)$"", ""univ_quant""),
(""^(dog|boy)$"", ""NN""),
(""^(he)$"", ""PRP""),
]
)
depparser = MaltParser(tagger=tagger)
drt_discourse_demo(
DrtGlueReadingCommand(remove_duplicates=False, depparser=depparser)
)
",[],0,[],/inference/discourse.py_demo
2774,/home/amandapotts/git/nltk/nltk/inference/mace.py___init__,"def __init__(self, goal=None, assumptions=None, max_models=500, model_builder=None):
""""""
:param goal: Input expression to prove
:type goal: sem.Expression
:param assumptions: Input expressions to use as assumptions in
the proof.
:type assumptions: list(sem.Expression)
:param max_models: The maximum number of models that Mace will try before
simply returning false. (Use 0 for no maximum.)
:type max_models: int
""""""
if model_builder is not None:
assert isinstance(model_builder, Mace)
else:
model_builder = Mace(max_models)
BaseModelBuilderCommand.__init__(self, model_builder, goal, assumptions)
",[],0,[],/inference/mace.py___init__
2775,/home/amandapotts/git/nltk/nltk/inference/mace.py_valuation,"def valuation(mbc):
return mbc.model(""valuation"")
",[],0,[],/inference/mace.py_valuation
2776,/home/amandapotts/git/nltk/nltk/inference/mace.py__convert2val,"def _convert2val(self, valuation_str):
""""""
Transform the output file into an NLTK-style Valuation.
:return: A model if one is generated
:rtype: sem.Valuation
""""""
valuation_standard_format = self._transform_output(valuation_str, ""standard"")
val = []
for line in valuation_standard_format.splitlines(False):
l = line.strip()
if l.startswith(""interpretation""):
num_entities = int(l[l.index(""("") + 1 : l.index("","")].strip())
elif l.startswith(""function"") and l.find(""_"") == -1:
name = l[l.index(""("") + 1 : l.index("","")].strip()
if is_indvar(name):
name = name.upper()
value = int(l[l.index(""["") + 1 : l.index(""]"")].strip())
val.append((name, MaceCommand._make_model_var(value)))
elif l.startswith(""relation""):
l = l[l.index(""("") + 1 :]
if ""("" in l:
name = l[: l.index(""("")].strip()
values = [
int(v.strip())
for v in l[l.index(""["") + 1 : l.index(""]"")].split("","")
]
val.append(
(name, MaceCommand._make_relation_set(num_entities, values))
)
else:
name = l[: l.index("","")].strip()
value = int(l[l.index(""["") + 1 : l.index(""]"")].strip())
val.append((name, value == 1))
return Valuation(val)
",[],0,[],/inference/mace.py__convert2val
2777,/home/amandapotts/git/nltk/nltk/inference/mace.py__make_relation_set,"def _make_relation_set(num_entities, values):
""""""
Convert a Mace4-style relation table into a dictionary.
:param num_entities: the number of entities in the model
:type num_entities: int
:param values: a list of 1's and 0's that represent whether a relation holds in a Mace4 model.
:type values: list of int
""""""
r = set()
for position in [pos for (pos, v) in enumerate(values) if v == 1]:
r.add(
tuple(MaceCommand._make_relation_tuple(position, values, num_entities))
)
return r
",[],0,[],/inference/mace.py__make_relation_set
2778,/home/amandapotts/git/nltk/nltk/inference/mace.py__make_relation_tuple,"def _make_relation_tuple(position, values, num_entities):
if len(values) == 1:
return []
else:
sublist_size = len(values) // num_entities
sublist_start = position // sublist_size
sublist_position = int(position % sublist_size)
sublist = values[
sublist_start * sublist_size : (sublist_start + 1) * sublist_size
]
return [
MaceCommand._make_model_var(sublist_start)
] + MaceCommand._make_relation_tuple(
sublist_position, sublist, num_entities
)
",[],0,[],/inference/mace.py__make_relation_tuple
2779,/home/amandapotts/git/nltk/nltk/inference/mace.py__make_model_var,"def _make_model_var(value):
""""""
Pick an alphabetic character as identifier for an entity in the model.
:param value: where to index into the list of characters
:type value: int
""""""
letter = [
""a"",
""b"",
""c"",
""d"",
""e"",
""f"",
""g"",
""h"",
""i"",
""j"",
""k"",
""l"",
""m"",
""n"",
""o"",
""p"",
""q"",
""r"",
""s"",
""t"",
""u"",
""v"",
""w"",
""x"",
""y"",
""z"",
][value]
num = value // 26
return letter + str(num) if num > 0 else letter
",[],0,[],/inference/mace.py__make_model_var
2780,/home/amandapotts/git/nltk/nltk/inference/mace.py__decorate_model,"def _decorate_model(self, valuation_str, format):
""""""
Print out a Mace4 model using any Mace4 ``interpformat`` format.
See https://www.cs.unm.edu/~mccune/mace4/manual/ for details.
:param valuation_str: str with the model builder's output
:param format: str indicating the format for displaying
models. Defaults to 'standard' format.
:return: str
""""""
if not format:
return valuation_str
elif format == ""valuation"":
return self._convert2val(valuation_str)
else:
return self._transform_output(valuation_str, format)
",[],0,[],/inference/mace.py__decorate_model
2781,/home/amandapotts/git/nltk/nltk/inference/mace.py__transform_output,"def _transform_output(self, valuation_str, format):
""""""
Transform the output file into any Mace4 ``interpformat`` format.
:param format: Output format for displaying models.
:type format: str
""""""
if format in [
""standard"",
""standard2"",
""portable"",
""tabular"",
""raw"",
""cooked"",
""xml"",
""tex"",
]:
return self._call_interpformat(valuation_str, [format])[0]
else:
raise LookupError(""The specified format does not exist"")
",[],0,[],/inference/mace.py__transform_output
2782,/home/amandapotts/git/nltk/nltk/inference/mace.py__call_interpformat,"def _call_interpformat(self, input_str, args=[], verbose=False):
""""""
Call the ``interpformat`` binary with the given input.
:param input_str: A string whose contents are used as stdin.
:param args: A list of command-line arguments.
:return: A tuple (stdout, returncode)
:see: ``config_prover9``
""""""
if self._interpformat_bin is None:
self._interpformat_bin = self._modelbuilder._find_binary(
""interpformat"", verbose
)
return self._modelbuilder._call(
input_str, self._interpformat_bin, args, verbose
)
",[],0,[],/inference/mace.py__call_interpformat
2783,/home/amandapotts/git/nltk/nltk/inference/mace.py___init__,"def __init__(self, end_size=500):
self._end_size = end_size
""""""The maximum model size that Mace will try before
simply returning false. (Use -1 for no maximum.)""""""
",[],0,[],/inference/mace.py___init__
2784,/home/amandapotts/git/nltk/nltk/inference/mace.py__build_model,"def _build_model(self, goal=None, assumptions=None, verbose=False):
""""""
Use Mace4 to build a first order model.
:return: ``True`` if a model was found (i.e. Mace returns value of 0),
else ``False``
""""""
if not assumptions:
assumptions = []
stdout, returncode = self._call_mace4(
self.prover9_input(goal, assumptions), verbose=verbose
)
return (returncode == 0, stdout)
",[],0,[],/inference/mace.py__build_model
2785,/home/amandapotts/git/nltk/nltk/inference/mace.py__call_mace4,"def _call_mace4(self, input_str, args=[], verbose=False):
""""""
Call the ``mace4`` binary with the given input.
:param input_str: A string whose contents are used as stdin.
:param args: A list of command-line arguments.
:return: A tuple (stdout, returncode)
:see: ``config_prover9``
""""""
if self._mace4_bin is None:
self._mace4_bin = self._find_binary(""mace4"", verbose)
updated_input_str = """"
if self._end_size > 0:
updated_input_str += ""assign(end_size, %d).\n\n"" % self._end_size
updated_input_str += input_str
return self._call(updated_input_str, self._mace4_bin, args, verbose)
",[],0,[],/inference/mace.py__call_mace4
2786,/home/amandapotts/git/nltk/nltk/inference/mace.py_spacer,"def spacer(num=30):
print(""-"" * num)
",[],0,[],/inference/mace.py_spacer
2787,/home/amandapotts/git/nltk/nltk/inference/mace.py_decode_result,"def decode_result(found):
""""""
Decode the result of model_found()
:param found: The output of model_found()
:type found: bool
""""""
return {True: ""Countermodel found"", False: ""No countermodel found"", None: ""None""}[
found
]
",[],0,[],/inference/mace.py_decode_result
2788,/home/amandapotts/git/nltk/nltk/inference/mace.py_test_model_found,"def test_model_found(arguments):
""""""
Try some proofs and exhibit the results.
""""""
for goal, assumptions in arguments:
g = Expression.fromstring(goal)
alist = [lp.parse(a) for a in assumptions]
m = MaceCommand(g, assumptions=alist, max_models=50)
found = m.build_model()
for a in alist:
print(""   %s"" % a)
print(f""|- {g}: {decode_result(found)}\n"")
",[],0,[],/inference/mace.py_test_model_found
2789,/home/amandapotts/git/nltk/nltk/inference/mace.py_test_build_model,"def test_build_model(arguments):
""""""
Try to build a ``nltk.sem.Valuation``.
""""""
g = Expression.fromstring(""all x.man(x)"")
alist = [
Expression.fromstring(a)
for a in [
""man(John)"",
""man(Socrates)"",
""man(Bill)"",
""some x.(-(x = John) & man(x) & sees(John,x))"",
""some x.(-(x = Bill) & man(x))"",
""all x.some y.(man(x) -> gives(Socrates,x,y))"",
]
]
m = MaceCommand(g, assumptions=alist)
m.build_model()
spacer()
print(""Assumptions and Goal"")
spacer()
for a in alist:
print(""   %s"" % a)
print(f""|- {g}: {decode_result(m.build_model())}\n"")
spacer()
print(""Valuation"")
spacer()
print(m.valuation, ""\n"")
",[],0,[],/inference/mace.py_test_build_model
2790,/home/amandapotts/git/nltk/nltk/inference/mace.py_test_transform_output,"def test_transform_output(argument_pair):
""""""
Transform the model into various Mace4 ``interpformat`` formats.
""""""
g = Expression.fromstring(argument_pair[0])
alist = [lp.parse(a) for a in argument_pair[1]]
m = MaceCommand(g, assumptions=alist)
m.build_model()
for a in alist:
print(""   %s"" % a)
print(f""|- {g}: {m.build_model()}\n"")
for format in [""standard"", ""portable"", ""xml"", ""cooked""]:
spacer()
print(""Using '%s' format"" % format)
spacer()
print(m.model(format=format))
",[],0,[],/inference/mace.py_test_transform_output
2791,/home/amandapotts/git/nltk/nltk/inference/mace.py_test_make_relation_set,"def test_make_relation_set():
print(
MaceCommand._make_relation_set(num_entities=3, values=[1, 0, 1])
== {(""c"",), (""a"",)}
)
print(
MaceCommand._make_relation_set(
num_entities=3, values=[0, 0, 0, 0, 0, 0, 1, 0, 0]
)
== {(""c"", ""a"")}
)
print(
MaceCommand._make_relation_set(num_entities=2, values=[0, 0, 1, 0, 0, 0, 1, 0])
== {(""a"", ""b"", ""a""), (""b"", ""b"", ""a"")}
)
",[],0,[],/inference/mace.py_test_make_relation_set
2792,/home/amandapotts/git/nltk/nltk/inference/mace.py_demo,"def demo():
test_model_found(arguments)
test_build_model(arguments)
test_transform_output(arguments[1])
",[],0,[],/inference/mace.py_demo
2793,/home/amandapotts/git/nltk/nltk/inference/api.py_prove,"def prove(self, goal=None, assumptions=None, verbose=False):
""""""
:return: Whether the proof was successful or not.
:rtype: bool
""""""
return self._prove(goal, assumptions, verbose)[0]
",[],0,[],/inference/api.py_prove
2794,/home/amandapotts/git/nltk/nltk/inference/api.py__prove,"def _prove(self, goal=None, assumptions=None, verbose=False):
""""""
:return: Whether the proof was successful or not, along with the proof
:rtype: tuple: (bool, str)
""""""
",[],0,[],/inference/api.py__prove
2795,/home/amandapotts/git/nltk/nltk/inference/api.py_build_model,"def build_model(self, goal=None, assumptions=None, verbose=False):
""""""
Perform the actual model building.
:return: Whether a model was generated
:rtype: bool
""""""
return self._build_model(goal, assumptions, verbose)[0]
",[],0,[],/inference/api.py_build_model
2796,/home/amandapotts/git/nltk/nltk/inference/api.py__build_model,"def _build_model(self, goal=None, assumptions=None, verbose=False):
""""""
Perform the actual model building.
:return: Whether a model was generated, and the model itself
:rtype: tuple(bool, sem.Valuation)
""""""
",[],0,[],/inference/api.py__build_model
2797,/home/amandapotts/git/nltk/nltk/inference/api.py_add_assumptions,"def add_assumptions(self, new_assumptions):
""""""
Add new assumptions to the assumption list.
:param new_assumptions: new assumptions
:type new_assumptions: list(sem.Expression)
""""""
",[],0,[],/inference/api.py_add_assumptions
2798,/home/amandapotts/git/nltk/nltk/inference/api.py_retract_assumptions,"def retract_assumptions(self, retracted, debug=False):
""""""
Retract assumptions from the assumption list.
:param debug: If True, give warning when ``retracted`` is not present on
assumptions list.
:type debug: bool
:param retracted: assumptions to be retracted
:type retracted: list(sem.Expression)
""""""
",[],0,[],/inference/api.py_retract_assumptions
2799,/home/amandapotts/git/nltk/nltk/inference/api.py_assumptions,"def assumptions(self):
""""""
List the current assumptions.
:return: list of ``Expression``
""""""
",[],0,[],/inference/api.py_assumptions
2800,/home/amandapotts/git/nltk/nltk/inference/api.py_goal,"def goal(self):
""""""
Return the goal
:return: ``Expression``
""""""
",[],0,[],/inference/api.py_goal
2801,/home/amandapotts/git/nltk/nltk/inference/api.py_print_assumptions,"def print_assumptions(self):
""""""
Print the list of the current assumptions.
""""""
",[],0,[],/inference/api.py_print_assumptions
2802,/home/amandapotts/git/nltk/nltk/inference/api.py_prove,"def prove(self, verbose=False):
""""""
Perform the actual proof.
""""""
",[],0,[],/inference/api.py_prove
2803,/home/amandapotts/git/nltk/nltk/inference/api.py_proof,"def proof(self, simplify=True):
""""""
Return the proof string
:param simplify: bool simplify the proof?
:return: str
""""""
",[],0,[],/inference/api.py_proof
2804,/home/amandapotts/git/nltk/nltk/inference/api.py_get_prover,"def get_prover(self):
""""""
Return the prover object
:return: ``Prover``
""""""
",[],0,[],/inference/api.py_get_prover
2805,/home/amandapotts/git/nltk/nltk/inference/api.py_build_model,"def build_model(self, verbose=False):
""""""
Perform the actual model building.
:return: A model if one is generated
:rtype: sem.Valuation
""""""
",[],0,[],/inference/api.py_build_model
2806,/home/amandapotts/git/nltk/nltk/inference/api.py_model,"def model(self, format=None):
""""""
Return a string representation of the model
:param simplify: bool simplify the proof?
:return: str
""""""
",[],0,[],/inference/api.py_model
2807,/home/amandapotts/git/nltk/nltk/inference/api.py_get_model_builder,"def get_model_builder(self):
""""""
Return the model builder object
:return: ``ModelBuilder``
""""""
",[],0,[],/inference/api.py_get_model_builder
2808,/home/amandapotts/git/nltk/nltk/inference/api.py___init__,"def __init__(self, goal=None, assumptions=None):
""""""
:param goal: Input expression to prove
:type goal: sem.Expression
:param assumptions: Input expressions to use as assumptions in
the proof.
:type assumptions: list(sem.Expression)
""""""
self._goal = goal
if not assumptions:
self._assumptions = []
else:
self._assumptions = list(assumptions)
self._result = None
""""""A holder for the result, to prevent unnecessary re-proving""""""
",[],0,[],/inference/api.py___init__
2809,/home/amandapotts/git/nltk/nltk/inference/api.py_add_assumptions,"def add_assumptions(self, new_assumptions):
""""""
Add new assumptions to the assumption list.
:param new_assumptions: new assumptions
:type new_assumptions: list(sem.Expression)
""""""
self._assumptions.extend(new_assumptions)
self._result = None
",[],0,[],/inference/api.py_add_assumptions
2810,/home/amandapotts/git/nltk/nltk/inference/api.py_assumptions,"def assumptions(self):
""""""
List the current assumptions.
:return: list of ``Expression``
""""""
return self._assumptions
",[],0,[],/inference/api.py_assumptions
2811,/home/amandapotts/git/nltk/nltk/inference/api.py_goal,"def goal(self):
""""""
Return the goal
:return: ``Expression``
""""""
return self._goal
",[],0,[],/inference/api.py_goal
2812,/home/amandapotts/git/nltk/nltk/inference/api.py_print_assumptions,"def print_assumptions(self):
""""""
Print the list of the current assumptions.
""""""
for a in self.assumptions():
print(a)
",[],0,[],/inference/api.py_print_assumptions
2813,/home/amandapotts/git/nltk/nltk/inference/api.py___init__,"def __init__(self, prover, goal=None, assumptions=None):
""""""
:param prover: The theorem tool to execute with the assumptions
:type prover: Prover
:see: ``BaseTheoremToolCommand``
""""""
self._prover = prover
""""""The theorem tool to execute with the assumptions""""""
BaseTheoremToolCommand.__init__(self, goal, assumptions)
self._proof = None
",[],0,[],/inference/api.py___init__
2814,/home/amandapotts/git/nltk/nltk/inference/api.py_prove,"def prove(self, verbose=False):
""""""
Perform the actual proof.  Store the result to prevent unnecessary
re-proving.
""""""
if self._result is None:
self._result, self._proof = self._prover._prove(
self.goal(), self.assumptions(), verbose
)
return self._result
",[],0,[],/inference/api.py_prove
2815,/home/amandapotts/git/nltk/nltk/inference/api.py_proof,"def proof(self, simplify=True):
""""""
Return the proof string
:param simplify: bool simplify the proof?
:return: str
""""""
if self._result is None:
raise LookupError(""You have to call prove() first to get a proof!"")
else:
return self.decorate_proof(self._proof, simplify)
",[],0,[],/inference/api.py_proof
2816,/home/amandapotts/git/nltk/nltk/inference/api.py_decorate_proof,"def decorate_proof(self, proof_string, simplify=True):
""""""
Modify and return the proof string
:param proof_string: str the proof to decorate
:param simplify: bool simplify the proof?
:return: str
""""""
return proof_string
",[],0,[],/inference/api.py_decorate_proof
2817,/home/amandapotts/git/nltk/nltk/inference/api.py_get_prover,"def get_prover(self):
return self._prover
",[],0,[],/inference/api.py_get_prover
2818,/home/amandapotts/git/nltk/nltk/inference/api.py___init__,"def __init__(self, modelbuilder, goal=None, assumptions=None):
""""""
:param modelbuilder: The theorem tool to execute with the assumptions
:type modelbuilder: ModelBuilder
:see: ``BaseTheoremToolCommand``
""""""
self._modelbuilder = modelbuilder
""""""The theorem tool to execute with the assumptions""""""
BaseTheoremToolCommand.__init__(self, goal, assumptions)
self._model = None
",[],0,[],/inference/api.py___init__
2819,/home/amandapotts/git/nltk/nltk/inference/api.py_build_model,"def build_model(self, verbose=False):
""""""
Attempt to build a model.  Store the result to prevent unnecessary
re-building.
""""""
if self._result is None:
self._result, self._model = self._modelbuilder._build_model(
self.goal(), self.assumptions(), verbose
)
return self._result
",[],0,[],/inference/api.py_build_model
2820,/home/amandapotts/git/nltk/nltk/inference/api.py_model,"def model(self, format=None):
""""""
Return a string representation of the model
:param simplify: bool simplify the proof?
:return: str
""""""
if self._result is None:
raise LookupError(""You have to call build_model() first to "" ""get a model!"")
else:
return self._decorate_model(self._model, format)
",[],0,[],/inference/api.py_model
2821,/home/amandapotts/git/nltk/nltk/inference/api.py__decorate_model,"def _decorate_model(self, valuation_str, format=None):
""""""
:param valuation_str: str with the model builder's output
:param format: str indicating the format for displaying
:return: str
""""""
return valuation_str
",[],0,[],/inference/api.py__decorate_model
2822,/home/amandapotts/git/nltk/nltk/inference/api.py_get_model_builder,"def get_model_builder(self):
return self._modelbuilder
",[],0,[],/inference/api.py_get_model_builder
2823,/home/amandapotts/git/nltk/nltk/inference/api.py___init__,"def __init__(self, command):
""""""
:param command: ``TheoremToolCommand`` to decorate
""""""
self._command = command
self._result = None
",[],0,[],/inference/api.py___init__
2824,/home/amandapotts/git/nltk/nltk/inference/api.py_assumptions,"def assumptions(self):
return self._command.assumptions()
",[],0,[],/inference/api.py_assumptions
2825,/home/amandapotts/git/nltk/nltk/inference/api.py_goal,"def goal(self):
return self._command.goal()
",[],0,[],/inference/api.py_goal
2826,/home/amandapotts/git/nltk/nltk/inference/api.py_add_assumptions,"def add_assumptions(self, new_assumptions):
self._command.add_assumptions(new_assumptions)
self._result = None
",[],0,[],/inference/api.py_add_assumptions
2827,/home/amandapotts/git/nltk/nltk/inference/api.py_retract_assumptions,"def retract_assumptions(self, retracted, debug=False):
self._command.retract_assumptions(retracted, debug)
self._result = None
",[],0,[],/inference/api.py_retract_assumptions
2828,/home/amandapotts/git/nltk/nltk/inference/api.py_print_assumptions,"def print_assumptions(self):
self._command.print_assumptions()
",[],0,[],/inference/api.py_print_assumptions
2829,/home/amandapotts/git/nltk/nltk/inference/api.py___init__,"def __init__(self, proverCommand):
""""""
:param proverCommand: ``ProverCommand`` to decorate
""""""
TheoremToolCommandDecorator.__init__(self, proverCommand)
self._proof = None
",[],0,[],/inference/api.py___init__
2830,/home/amandapotts/git/nltk/nltk/inference/api.py_prove,"def prove(self, verbose=False):
if self._result is None:
prover = self.get_prover()
self._result, self._proof = prover._prove(
self.goal(), self.assumptions(), verbose
)
return self._result
",[],0,[],/inference/api.py_prove
2831,/home/amandapotts/git/nltk/nltk/inference/api.py_proof,"def proof(self, simplify=True):
""""""
Return the proof string
:param simplify: bool simplify the proof?
:return: str
""""""
if self._result is None:
raise LookupError(""You have to call prove() first to get a proof!"")
else:
return self.decorate_proof(self._proof, simplify)
",[],0,[],/inference/api.py_proof
2832,/home/amandapotts/git/nltk/nltk/inference/api.py_decorate_proof,"def decorate_proof(self, proof_string, simplify=True):
""""""
Modify and return the proof string
:param proof_string: str the proof to decorate
:param simplify: bool simplify the proof?
:return: str
""""""
return self._command.decorate_proof(proof_string, simplify)
",[],0,[],/inference/api.py_decorate_proof
2833,/home/amandapotts/git/nltk/nltk/inference/api.py_get_prover,"def get_prover(self):
return self._command.get_prover()
",[],0,[],/inference/api.py_get_prover
2834,/home/amandapotts/git/nltk/nltk/inference/api.py___init__,"def __init__(self, modelBuilderCommand):
""""""
:param modelBuilderCommand: ``ModelBuilderCommand`` to decorate
""""""
TheoremToolCommandDecorator.__init__(self, modelBuilderCommand)
self._model = None
",[],0,[],/inference/api.py___init__
2835,/home/amandapotts/git/nltk/nltk/inference/api.py_build_model,"def build_model(self, verbose=False):
""""""
Attempt to build a model.  Store the result to prevent unnecessary
re-building.
""""""
if self._result is None:
modelbuilder = self.get_model_builder()
self._result, self._model = modelbuilder._build_model(
self.goal(), self.assumptions(), verbose
)
return self._result
",[],0,[],/inference/api.py_build_model
2836,/home/amandapotts/git/nltk/nltk/inference/api.py_model,"def model(self, format=None):
""""""
Return a string representation of the model
:param simplify: bool simplify the proof?
:return: str
""""""
if self._result is None:
raise LookupError(""You have to call build_model() first to "" ""get a model!"")
else:
return self._decorate_model(self._model, format)
",[],0,[],/inference/api.py_model
2837,/home/amandapotts/git/nltk/nltk/inference/api.py__decorate_model,"def _decorate_model(self, valuation_str, format=None):
""""""
Modify and return the proof string
:param valuation_str: str with the model builder's output
:param format: str indicating the format for displaying
:return: str
""""""
return self._command._decorate_model(valuation_str, format)
",[],0,[],/inference/api.py__decorate_model
2838,/home/amandapotts/git/nltk/nltk/inference/api.py_get_model_builder,"def get_model_builder(self):
return self._command.get_prover()
",[],0,[],/inference/api.py_get_model_builder
2839,/home/amandapotts/git/nltk/nltk/inference/api.py___init__,"def __init__(self, prover, modelbuilder):
self._prover = prover
self._modelbuilder = modelbuilder
",[],0,[],/inference/api.py___init__
2840,/home/amandapotts/git/nltk/nltk/inference/api.py__prove,"def _prove(self, goal=None, assumptions=None, verbose=False):
return self._run(goal, assumptions, verbose), """"
",[],0,[],/inference/api.py__prove
2841,/home/amandapotts/git/nltk/nltk/inference/api.py__build_model,"def _build_model(self, goal=None, assumptions=None, verbose=False):
return not self._run(goal, assumptions, verbose), """"
",[],0,[],/inference/api.py__build_model
2842,/home/amandapotts/git/nltk/nltk/inference/api.py___init__,"def __init__(self, prover, modelbuilder, goal=None, assumptions=None):
BaseProverCommand.__init__(self, prover, goal, assumptions)
BaseModelBuilderCommand.__init__(self, modelbuilder, goal, assumptions)
",[],0,[],/inference/api.py___init__
2843,/home/amandapotts/git/nltk/nltk/inference/api.py_prove,"def prove(self, verbose=False):
return self._run(verbose)
",[],0,[],/inference/api.py_prove
2844,/home/amandapotts/git/nltk/nltk/inference/api.py_build_model,"def build_model(self, verbose=False):
return not self._run(verbose)
",[],0,[],/inference/api.py_build_model
2845,/home/amandapotts/git/nltk/nltk/inference/api.py___init__,"def __init__(self, command, verbose, name=None):
threading.Thread.__init__(self)
self._command = command
self._result = None
self._verbose = verbose
self._name = name
",[],0,[],/inference/api.py___init__
2846,/home/amandapotts/git/nltk/nltk/inference/api.py_run,"def run(self):
try:
self._result = self._command()
if self._verbose:
print(
""Thread %s finished with result %s at %s""
% (self._name, self._result, time.localtime(time.time()))
)
except Exception as e:
print(e)
print(""Thread %s completed abnormally"" % (self._name))
",[],0,[],/inference/api.py_run
2847,/home/amandapotts/git/nltk/nltk/inference/api.py_result,"def result(self):
return self._result
",[],0,[],/inference/api.py_result
2848,/home/amandapotts/git/nltk/nltk/inference/resolution.py__prove,"def _prove(self, goal=None, assumptions=None, verbose=False):
""""""
:param goal: Input expression to prove
:type goal: sem.Expression
:param assumptions: Input expressions to use as assumptions in the proof
:type assumptions: list(sem.Expression)
""""""
if not assumptions:
assumptions = []
result = None
try:
clauses = []
if goal:
clauses.extend(clausify(-goal))
for a in assumptions:
clauses.extend(clausify(a))
result, clauses = self._attempt_proof(clauses)
if verbose:
print(ResolutionProverCommand._decorate_clauses(clauses))
except RuntimeError as e:
if self._assume_false and str(e).startswith(
""maximum recursion depth exceeded""
):
result = False
clauses = []
else:
if verbose:
print(e)
else:
raise e
return (result, clauses)
",[],0,[],/inference/resolution.py__prove
2849,/home/amandapotts/git/nltk/nltk/inference/resolution.py__attempt_proof,"def _attempt_proof(self, clauses):
tried = defaultdict(list)
i = 0
while i < len(clauses):
if not clauses[i].is_tautology():
if tried[i]:
j = tried[i][-1] + 1
else:
j = i + 1  # nothing tried yet for 'i', so start with the next
while j < len(clauses):
if i != j and j and not clauses[j].is_tautology():
tried[i].append(j)
newclauses = clauses[i].unify(clauses[j])
if newclauses:
for newclause in newclauses:
newclause._parents = (i + 1, j + 1)
clauses.append(newclause)
if not len(newclause):  # if there's an empty clause
return (True, clauses)
i = -1  # since we added a new clause, restart from the top
break
j += 1
i += 1
return (False, clauses)
",[],0,[],/inference/resolution.py__attempt_proof
2850,/home/amandapotts/git/nltk/nltk/inference/resolution.py___init__,"def __init__(self, goal=None, assumptions=None, prover=None):
""""""
:param goal: Input expression to prove
:type goal: sem.Expression
:param assumptions: Input expressions to use as assumptions in
the proof.
:type assumptions: list(sem.Expression)
""""""
if prover is not None:
assert isinstance(prover, ResolutionProver)
else:
prover = ResolutionProver()
BaseProverCommand.__init__(self, prover, goal, assumptions)
self._clauses = None
",[],0,[],/inference/resolution.py___init__
2851,/home/amandapotts/git/nltk/nltk/inference/resolution.py_prove,"def prove(self, verbose=False):
""""""
Perform the actual proof.  Store the result to prevent unnecessary
re-proving.
""""""
if self._result is None:
self._result, clauses = self._prover._prove(
self.goal(), self.assumptions(), verbose
)
self._clauses = clauses
self._proof = ResolutionProverCommand._decorate_clauses(clauses)
return self._result
",[],0,[],/inference/resolution.py_prove
2852,/home/amandapotts/git/nltk/nltk/inference/resolution.py_find_answers,"def find_answers(self, verbose=False):
self.prove(verbose)
answers = set()
answer_ex = VariableExpression(Variable(ResolutionProver.ANSWER_KEY))
for clause in self._clauses:
for term in clause:
if (
isinstance(term, ApplicationExpression)
and term.function == answer_ex
and not isinstance(term.argument, IndividualVariableExpression)
):
answers.add(term.argument)
return answers
",[],0,[],/inference/resolution.py_find_answers
2853,/home/amandapotts/git/nltk/nltk/inference/resolution.py__decorate_clauses,"def _decorate_clauses(clauses):
""""""
Decorate the proof output.
""""""
out = """"
max_clause_len = max(len(str(clause)) for clause in clauses)
max_seq_len = len(str(len(clauses)))
for i in range(len(clauses)):
parents = ""A""
taut = """"
if clauses[i].is_tautology():
taut = ""Tautology""
if clauses[i]._parents:
parents = str(clauses[i]._parents)
parents = "" "" * (max_clause_len - len(str(clauses[i])) + 1) + parents
seq = "" "" * (max_seq_len - len(str(i + 1))) + str(i + 1)
out += f""[{seq}] {clauses[i]} {parents} {taut}\n""
return out
",[],0,[],/inference/resolution.py__decorate_clauses
2854,/home/amandapotts/git/nltk/nltk/inference/resolution.py___init__,"def __init__(self, data):
list.__init__(self, data)
self._is_tautology = None
self._parents = None
",[],0,[],/inference/resolution.py___init__
2855,/home/amandapotts/git/nltk/nltk/inference/resolution.py_unify,"def unify(self, other, bindings=None, used=None, skipped=None, debug=False):
""""""
Attempt to unify this Clause with the other, returning a list of
resulting, unified, Clauses.
:param other: ``Clause`` with which to unify
:param bindings: ``BindingDict`` containing bindings that should be used
during the unification
:param used: tuple of two lists of atoms.  The first lists the
atoms from 'self' that were successfully unified with atoms from
'other'.  The second lists the atoms from 'other' that were successfully
unified with atoms from 'self'.
:param skipped: tuple of two ``Clause`` objects.  The first is a list of all
the atoms from the 'self' Clause that have not been unified with
anything on the path.  The second is same thing for the 'other' Clause.
:param debug: bool indicating whether debug statements should print
:return: list containing all the resulting ``Clause`` objects that could be
obtained by unification
""""""
if bindings is None:
bindings = BindingDict()
if used is None:
used = ([], [])
if skipped is None:
skipped = ([], [])
if isinstance(debug, bool):
debug = DebugObject(debug)
newclauses = _iterate_first(
self, other, bindings, used, skipped, _complete_unify_path, debug
)
subsumed = []
for i, c1 in enumerate(newclauses):
if i not in subsumed:
for j, c2 in enumerate(newclauses):
if i != j and j not in subsumed and c1.subsumes(c2):
subsumed.append(j)
result = []
for i in range(len(newclauses)):
if i not in subsumed:
result.append(newclauses[i])
return result
",[],0,[],/inference/resolution.py_unify
2856,/home/amandapotts/git/nltk/nltk/inference/resolution.py_isSubsetOf,"def isSubsetOf(self, other):
""""""
Return True iff every term in 'self' is a term in 'other'.
:param other: ``Clause``
:return: bool
""""""
for a in self:
if a not in other:
return False
return True
",[],0,[],/inference/resolution.py_isSubsetOf
2857,/home/amandapotts/git/nltk/nltk/inference/resolution.py_subsumes,"def subsumes(self, other):
""""""
Return True iff 'self' subsumes 'other', this is, if there is a
substitution such that every term in 'self' can be unified with a term
in 'other'.
:param other: ``Clause``
:return: bool
""""""
negatedother = []
for atom in other:
if isinstance(atom, NegatedExpression):
negatedother.append(atom.term)
else:
negatedother.append(-atom)
negatedotherClause = Clause(negatedother)
bindings = BindingDict()
used = ([], [])
skipped = ([], [])
debug = DebugObject(False)
return (
len(
_iterate_first(
self,
negatedotherClause,
bindings,
used,
skipped,
_subsumes_finalize,
debug,
)
)
> 0
)
",[],0,[],/inference/resolution.py_subsumes
2858,/home/amandapotts/git/nltk/nltk/inference/resolution.py___getslice__,"def __getslice__(self, start, end):
return Clause(list.__getslice__(self, start, end))
",[],0,[],/inference/resolution.py___getslice__
2859,/home/amandapotts/git/nltk/nltk/inference/resolution.py___sub__,"def __sub__(self, other):
return Clause([a for a in self if a not in other])
",[],0,[],/inference/resolution.py___sub__
2860,/home/amandapotts/git/nltk/nltk/inference/resolution.py___add__,"def __add__(self, other):
return Clause(list.__add__(self, other))
",[],0,[],/inference/resolution.py___add__
2861,/home/amandapotts/git/nltk/nltk/inference/resolution.py_is_tautology,"def is_tautology(self):
""""""
Self is a tautology if it contains ground terms P and -P.  The ground
term, P, must be an exact match, ie, not using unification.
""""""
if self._is_tautology is not None:
return self._is_tautology
for i, a in enumerate(self):
if not isinstance(a, EqualityExpression):
j = len(self) - 1
while j > i:
b = self[j]
if isinstance(a, NegatedExpression):
if a.term == b:
self._is_tautology = True
return True
elif isinstance(b, NegatedExpression):
if a == b.term:
self._is_tautology = True
return True
j -= 1
self._is_tautology = False
return False
",[],0,[],/inference/resolution.py_is_tautology
2862,/home/amandapotts/git/nltk/nltk/inference/resolution.py_free,"def free(self):
return reduce(operator.or_, ((atom.free() | atom.constants()) for atom in self))
",[],0,[],/inference/resolution.py_free
2863,/home/amandapotts/git/nltk/nltk/inference/resolution.py_replace,"def replace(self, variable, expression):
""""""
Replace every instance of variable with expression across every atom
in the clause
:param variable: ``Variable``
:param expression: ``Expression``
""""""
return Clause([atom.replace(variable, expression) for atom in self])
",[],0,[],/inference/resolution.py_replace
2864,/home/amandapotts/git/nltk/nltk/inference/resolution.py_substitute_bindings,"def substitute_bindings(self, bindings):
""""""
Replace every binding
:param bindings: A list of tuples mapping Variable Expressions to the
Expressions to which they are bound.
:return: ``Clause``
""""""
return Clause([atom.substitute_bindings(bindings) for atom in self])
",[],0,[],/inference/resolution.py_substitute_bindings
2865,/home/amandapotts/git/nltk/nltk/inference/resolution.py___str__,"def __str__(self):
return ""{"" + "", "".join(""%s"" % item for item in self) + ""}""
",[],0,[],/inference/resolution.py___str__
2866,/home/amandapotts/git/nltk/nltk/inference/resolution.py___repr__,"def __repr__(self):
return ""%s"" % self
",[],0,[],/inference/resolution.py___repr__
2867,/home/amandapotts/git/nltk/nltk/inference/resolution.py__iterate_first,"def _iterate_first(first, second, bindings, used, skipped, finalize_method, debug):
""""""
This method facilitates movement through the terms of 'self'
""""""
debug.line(f""unify({first},{second}) {bindings}"")
if not len(first) or not len(second):  # if no more recursions can be performed
return finalize_method(first, second, bindings, used, skipped, debug)
else:
result = _iterate_second(
first, second, bindings, used, skipped, finalize_method, debug + 1
)
newskipped = (skipped[0] + [first[0]], skipped[1])
result += _iterate_first(
first[1:], second, bindings, used, newskipped, finalize_method, debug + 1
)
try:
newbindings, newused, unused = _unify_terms(
first[0], second[0], bindings, used
)
newfirst = first[1:] + skipped[0] + unused[0]
newsecond = second[1:] + skipped[1] + unused[1]
result += _iterate_first(
newfirst,
newsecond,
newbindings,
newused,
([], []),
finalize_method,
debug + 1,
)
except BindingException:
pass
return result
",[],0,[],/inference/resolution.py__iterate_first
2868,/home/amandapotts/git/nltk/nltk/inference/resolution.py__iterate_second,"def _iterate_second(first, second, bindings, used, skipped, finalize_method, debug):
""""""
This method facilitates movement through the terms of 'other'
""""""
debug.line(f""unify({first},{second}) {bindings}"")
if not len(first) or not len(second):  # if no more recursions can be performed
return finalize_method(first, second, bindings, used, skipped, debug)
else:
newskipped = (skipped[0], skipped[1] + [second[0]])
result = _iterate_second(
first, second[1:], bindings, used, newskipped, finalize_method, debug + 1
)
try:
newbindings, newused, unused = _unify_terms(
first[0], second[0], bindings, used
)
newfirst = first[1:] + skipped[0] + unused[0]
newsecond = second[1:] + skipped[1] + unused[1]
result += _iterate_second(
newfirst,
newsecond,
newbindings,
newused,
([], []),
finalize_method,
debug + 1,
)
except BindingException:
pass
return result
",[],0,[],/inference/resolution.py__iterate_second
2869,/home/amandapotts/git/nltk/nltk/inference/resolution.py__unify_terms,"def _unify_terms(a, b, bindings=None, used=None):
""""""
This method attempts to unify two terms.  Two expressions are unifiable
if there exists a substitution function S such that S(a) == S(-b).
:param a: ``Expression``
:param b: ``Expression``
:param bindings: ``BindingDict`` a starting set of bindings with which
the unification must be consistent
:return: ``BindingDict`` A dictionary of the bindings required to unify
:raise ``BindingException``: If the terms cannot be unified
""""""
assert isinstance(a, Expression)
assert isinstance(b, Expression)
if bindings is None:
bindings = BindingDict()
if used is None:
used = ([], [])
if isinstance(a, NegatedExpression) and isinstance(b, ApplicationExpression):
newbindings = most_general_unification(a.term, b, bindings)
newused = (used[0] + [a], used[1] + [b])
unused = ([], [])
elif isinstance(a, ApplicationExpression) and isinstance(b, NegatedExpression):
newbindings = most_general_unification(a, b.term, bindings)
newused = (used[0] + [a], used[1] + [b])
unused = ([], [])
elif isinstance(a, EqualityExpression):
newbindings = BindingDict([(a.first.variable, a.second)])
newused = (used[0] + [a], used[1])
unused = ([], [b])
elif isinstance(b, EqualityExpression):
newbindings = BindingDict([(b.first.variable, b.second)])
newused = (used[0], used[1] + [b])
unused = ([a], [])
else:
raise BindingException((a, b))
return newbindings, newused, unused
",[],0,[],/inference/resolution.py__unify_terms
2870,/home/amandapotts/git/nltk/nltk/inference/resolution.py__complete_unify_path,"def _complete_unify_path(first, second, bindings, used, skipped, debug):
if used[0] or used[1]:  # if bindings were made along the path
newclause = Clause(skipped[0] + skipped[1] + first + second)
debug.line(""  -> New Clause: %s"" % newclause)
return [newclause.substitute_bindings(bindings)]
else:  # no bindings made means no unification occurred.  so no result
debug.line(""  -> End"")
return []
",[],0,[],/inference/resolution.py__complete_unify_path
2871,/home/amandapotts/git/nltk/nltk/inference/resolution.py__subsumes_finalize,"def _subsumes_finalize(first, second, bindings, used, skipped, debug):
if not len(skipped[0]) and not len(first):
return [True]
else:
return []
",[],0,[],/inference/resolution.py__subsumes_finalize
2872,/home/amandapotts/git/nltk/nltk/inference/resolution.py_clausify,"def clausify(expression):
""""""
Skolemize, clausify, and standardize the variables apart.
""""""
clause_list = []
for clause in _clausify(skolemize(expression)):
for free in clause.free():
if is_indvar(free.name):
newvar = VariableExpression(unique_variable())
clause = clause.replace(free, newvar)
clause_list.append(clause)
return clause_list
",[],0,[],/inference/resolution.py_clausify
2873,/home/amandapotts/git/nltk/nltk/inference/resolution.py__clausify,"def _clausify(expression):
""""""
:param expression: a skolemized expression in CNF
""""""
if isinstance(expression, AndExpression):
return _clausify(expression.first) + _clausify(expression.second)
elif isinstance(expression, OrExpression):
first = _clausify(expression.first)
second = _clausify(expression.second)
assert len(first) == 1
assert len(second) == 1
return [first[0] + second[0]]
elif isinstance(expression, EqualityExpression):
return [Clause([expression])]
elif isinstance(expression, ApplicationExpression):
return [Clause([expression])]
elif isinstance(expression, NegatedExpression):
if isinstance(expression.term, ApplicationExpression):
return [Clause([expression])]
elif isinstance(expression.term, EqualityExpression):
return [Clause([expression])]
raise ProverParseError()
",[],0,[],/inference/resolution.py__clausify
2874,/home/amandapotts/git/nltk/nltk/inference/resolution.py___init__,"def __init__(self, binding_list=None):
""""""
:param binding_list: list of (``AbstractVariableExpression``, ``AtomicExpression``) to initialize the dictionary
""""""
self.d = {}
if binding_list:
for v, b in binding_list:
self[v] = b
",[],0,[],/inference/resolution.py___init__
2875,/home/amandapotts/git/nltk/nltk/inference/resolution.py___setitem__,"def __setitem__(self, variable, binding):
""""""
A binding is consistent with the dict if its variable is not already bound, OR if its
variable is already bound to its argument.
:param variable: ``Variable`` The variable to bind
:param binding: ``Expression`` The atomic to which 'variable' should be bound
:raise BindingException: If the variable cannot be bound in this dictionary
""""""
assert isinstance(variable, Variable)
assert isinstance(binding, Expression)
try:
existing = self[variable]
except KeyError:
existing = None
if not existing or binding == existing:
self.d[variable] = binding
elif isinstance(binding, IndividualVariableExpression):
try:
existing = self[binding.variable]
except KeyError:
existing = None
binding2 = VariableExpression(variable)
if not existing or binding2 == existing:
self.d[binding.variable] = binding2
else:
raise BindingException(
""Variable %s already bound to another "" ""value"" % (variable)
)
else:
raise BindingException(
""Variable %s already bound to another "" ""value"" % (variable)
)
",[],0,[],/inference/resolution.py___setitem__
2876,/home/amandapotts/git/nltk/nltk/inference/resolution.py___getitem__,"def __getitem__(self, variable):
""""""
Return the expression to which 'variable' is bound
""""""
assert isinstance(variable, Variable)
intermediate = self.d[variable]
while intermediate:
try:
intermediate = self.d[intermediate]
except KeyError:
return intermediate
",[],0,[],/inference/resolution.py___getitem__
2877,/home/amandapotts/git/nltk/nltk/inference/resolution.py___contains__,"def __contains__(self, item):
return item in self.d
",[],0,[],/inference/resolution.py___contains__
2878,/home/amandapotts/git/nltk/nltk/inference/resolution.py___add__,"def __add__(self, other):
""""""
:param other: ``BindingDict`` The dict with which to combine self
:return: ``BindingDict`` A new dict containing all the elements of both parameters
:raise BindingException: If the parameter dictionaries are not consistent with each other
""""""
try:
combined = BindingDict()
for v in self.d:
combined[v] = self.d[v]
for v in other.d:
combined[v] = other.d[v]
return combined
except BindingException as e:
raise BindingException(
""Attempting to add two contradicting ""
""BindingDicts: '%s' and '%s'"" % (self, other)
) from e
",[],0,[],/inference/resolution.py___add__
2879,/home/amandapotts/git/nltk/nltk/inference/resolution.py___len__,"def __len__(self):
return len(self.d)
",[],0,[],/inference/resolution.py___len__
2880,/home/amandapotts/git/nltk/nltk/inference/resolution.py___str__,"def __str__(self):
data_str = "", "".join(f""{v}: {self.d[v]}"" for v in sorted(self.d.keys()))
return ""{"" + data_str + ""}""
",[],0,[],/inference/resolution.py___str__
2881,/home/amandapotts/git/nltk/nltk/inference/resolution.py___repr__,"def __repr__(self):
return ""%s"" % self
",[],0,[],/inference/resolution.py___repr__
2882,/home/amandapotts/git/nltk/nltk/inference/resolution.py_most_general_unification,"def most_general_unification(a, b, bindings=None):
""""""
Find the most general unification of the two given expressions
:param a: ``Expression``
:param b: ``Expression``
:param bindings: ``BindingDict`` a starting set of bindings with which the
unification must be consistent
:return: a list of bindings
:raise BindingException: if the Expressions cannot be unified
""""""
if bindings is None:
bindings = BindingDict()
if a == b:
return bindings
elif isinstance(a, IndividualVariableExpression):
return _mgu_var(a, b, bindings)
elif isinstance(b, IndividualVariableExpression):
return _mgu_var(b, a, bindings)
elif isinstance(a, ApplicationExpression) and isinstance(b, ApplicationExpression):
return most_general_unification(
a.function, b.function, bindings
) + most_general_unification(a.argument, b.argument, bindings)
raise BindingException((a, b))
",[],0,[],/inference/resolution.py_most_general_unification
2883,/home/amandapotts/git/nltk/nltk/inference/resolution.py__mgu_var,"def _mgu_var(var, expression, bindings):
if var.variable in expression.free() | expression.constants():
raise BindingException((var, expression))
else:
return BindingDict([(var.variable, expression)]) + bindings
",[],0,[],/inference/resolution.py__mgu_var
2884,/home/amandapotts/git/nltk/nltk/inference/resolution.py___init__,"def __init__(self, arg):
if isinstance(arg, tuple):
Exception.__init__(self, ""'%s' cannot be bound to '%s'"" % arg)
else:
Exception.__init__(self, arg)
",[],0,[],/inference/resolution.py___init__
2885,/home/amandapotts/git/nltk/nltk/inference/resolution.py___init__,"def __init__(self, a, b):
Exception.__init__(self, f""'{a}' cannot unify with '{b}'"")
",[],0,[],/inference/resolution.py___init__
2886,/home/amandapotts/git/nltk/nltk/inference/resolution.py___init__,"def __init__(self, enabled=True, indent=0):
self.enabled = enabled
self.indent = indent
",[],0,[],/inference/resolution.py___init__
2887,/home/amandapotts/git/nltk/nltk/inference/resolution.py___add__,"def __add__(self, i):
return DebugObject(self.enabled, self.indent + i)
",[],0,[],/inference/resolution.py___add__
2888,/home/amandapotts/git/nltk/nltk/inference/resolution.py_line,"def line(self, line):
if self.enabled:
print(""    "" * self.indent + line)
",[],0,[],/inference/resolution.py_line
2889,/home/amandapotts/git/nltk/nltk/inference/resolution.py_testResolutionProver,"def testResolutionProver():
resolution_test(r""man(x)"")
resolution_test(r""(man(x) -> man(x))"")
resolution_test(r""(man(x) -> --man(x))"")
resolution_test(r""-(man(x) and -man(x))"")
resolution_test(r""(man(x) or -man(x))"")
resolution_test(r""(man(x) -> man(x))"")
resolution_test(r""-(man(x) and -man(x))"")
resolution_test(r""(man(x) or -man(x))"")
resolution_test(r""(man(x) -> man(x))"")
resolution_test(r""(man(x) iff man(x))"")
resolution_test(r""-(man(x) iff -man(x))"")
resolution_test(""all x.man(x)"")
resolution_test(""-all x.some y.F(x,y) & some x.all y.(-F(x,y))"")
resolution_test(""some x.all y.sees(x,y)"")
p1 = Expression.fromstring(r""all x.(man(x) -> mortal(x))"")
p2 = Expression.fromstring(r""man(Socrates)"")
c = Expression.fromstring(r""mortal(Socrates)"")
print(f""{p1}, {p2} |- {c}: {ResolutionProver().prove(c, [p1, p2])}"")
p1 = Expression.fromstring(r""all x.(man(x) -> walks(x))"")
p2 = Expression.fromstring(r""man(John)"")
c = Expression.fromstring(r""some y.walks(y)"")
print(f""{p1}, {p2} |- {c}: {ResolutionProver().prove(c, [p1, p2])}"")
p = Expression.fromstring(r""some e1.some e2.(believe(e1,john,e2) & walk(e2,mary))"")
c = Expression.fromstring(r""some e0.walk(e0,mary)"")
print(f""{p} |- {c}: {ResolutionProver().prove(c, [p])}"")
",[],0,[],/inference/resolution.py_testResolutionProver
2890,/home/amandapotts/git/nltk/nltk/inference/resolution.py_resolution_test,"def resolution_test(e):
f = Expression.fromstring(e)
t = ResolutionProver().prove(f)
print(f""|- {f}: {t}"")
",[],0,[],/inference/resolution.py_resolution_test
2891,/home/amandapotts/git/nltk/nltk/inference/resolution.py_test_clausify,"def test_clausify():
lexpr = Expression.fromstring
print(clausify(lexpr(""P(x) | Q(x)"")))
print(clausify(lexpr(""(P(x) & Q(x)) | R(x)"")))
print(clausify(lexpr(""P(x) | (Q(x) & R(x))"")))
print(clausify(lexpr(""(P(x) & Q(x)) | (R(x) & S(x))"")))
print(clausify(lexpr(""P(x) | Q(x) | R(x)"")))
print(clausify(lexpr(""P(x) | (Q(x) & R(x)) | S(x)"")))
print(clausify(lexpr(""exists x.P(x) | Q(x)"")))
print(clausify(lexpr(""-(-P(x) & Q(x))"")))
print(clausify(lexpr(""P(x) <-> Q(x)"")))
print(clausify(lexpr(""-(P(x) <-> Q(x))"")))
print(clausify(lexpr(""-(all x.P(x))"")))
print(clausify(lexpr(""-(some x.P(x))"")))
print(clausify(lexpr(""some x.P(x)"")))
print(clausify(lexpr(""some x.all y.P(x,y)"")))
print(clausify(lexpr(""all y.some x.P(x,y)"")))
print(clausify(lexpr(""all z.all y.some x.P(x,y,z)"")))
print(clausify(lexpr(""all x.(all y.P(x,y) -> -all y.(Q(x,y) -> R(x,y)))"")))
",[],0,[],/inference/resolution.py_test_clausify
2892,/home/amandapotts/git/nltk/nltk/inference/resolution.py_demo,"def demo():
test_clausify()
print()
testResolutionProver()
print()
p = Expression.fromstring(""man(x)"")
print(ResolutionProverCommand(p, [p]).prove())
",[],0,[],/inference/resolution.py_demo
2893,/home/amandapotts/git/nltk/nltk/inference/prover9.py_print_assumptions,"def print_assumptions(self, output_format=""nltk""):
""""""
Print the list of the current assumptions.
""""""
if output_format.lower() == ""nltk"":
for a in self.assumptions():
print(a)
elif output_format.lower() == ""prover9"":
for a in convert_to_prover9(self.assumptions()):
print(a)
else:
raise NameError(
""Unrecognized value for 'output_format': %s"" % output_format
)
",[],0,[],/inference/prover9.py_print_assumptions
2894,/home/amandapotts/git/nltk/nltk/inference/prover9.py___init__,"def __init__(self, goal=None, assumptions=None, timeout=60, prover=None):
""""""
:param goal: Input expression to prove
:type goal: sem.Expression
:param assumptions: Input expressions to use as assumptions in
the proof.
:type assumptions: list(sem.Expression)
:param timeout: number of seconds before timeout
no timeout.
:type timeout: int
:param prover: a prover.  If not set, one will be created.
:type prover: Prover9
""""""
if not assumptions:
assumptions = []
if prover is not None:
assert isinstance(prover, Prover9)
else:
prover = Prover9(timeout)
BaseProverCommand.__init__(self, prover, goal, assumptions)
",[],0,[],/inference/prover9.py___init__
2895,/home/amandapotts/git/nltk/nltk/inference/prover9.py_decorate_proof,"def decorate_proof(self, proof_string, simplify=True):
""""""
:see BaseProverCommand.decorate_proof()
""""""
if simplify:
return self._prover._call_prooftrans(proof_string, [""striplabels""])[
0
].rstrip()
else:
return proof_string.rstrip()
",[],0,[],/inference/prover9.py_decorate_proof
2896,/home/amandapotts/git/nltk/nltk/inference/prover9.py_config_prover9,"def config_prover9(self, binary_location, verbose=False):
if binary_location is None:
self._binary_location = None
self._prover9_bin = None
else:
name = ""prover9""
self._prover9_bin = nltk.internals.find_binary(
name,
path_to_bin=binary_location,
env_vars=[""PROVER9""],
url=""https://www.cs.unm.edu/~mccune/prover9/"",
binary_names=[name, name + "".exe""],
verbose=verbose,
)
self._binary_location = self._prover9_bin.rsplit(os.path.sep, 1)
",[],0,[],/inference/prover9.py_config_prover9
2897,/home/amandapotts/git/nltk/nltk/inference/prover9.py_prover9_input,"def prover9_input(self, goal, assumptions):
""""""
:return: The input string that should be provided to the
prover9 binary.  This string is formed based on the goal,
assumptions, and timeout value of this object.
""""""
s = """"
if assumptions:
s += ""formulas(assumptions).\n""
for p9_assumption in convert_to_prover9(assumptions):
s += ""    %s.\n"" % p9_assumption
s += ""end_of_list.\n\n""
if goal:
s += ""formulas(goals).\n""
s += ""    %s.\n"" % convert_to_prover9(goal)
s += ""end_of_list.\n\n""
return s
",[],0,[],/inference/prover9.py_prover9_input
2898,/home/amandapotts/git/nltk/nltk/inference/prover9.py_binary_locations,"def binary_locations(self):
""""""
A list of directories that should be searched for the prover9
executables.  This list is used by ``config_prover9`` when searching
for the prover9 executables.
""""""
return [
""/usr/local/bin/prover9"",
""/usr/local/bin/prover9/bin"",
""/usr/local/bin"",
""/usr/bin"",
""/usr/local/prover9"",
""/usr/local/share/prover9"",
]
",[],0,[],/inference/prover9.py_binary_locations
2899,/home/amandapotts/git/nltk/nltk/inference/prover9.py__find_binary,"def _find_binary(self, name, verbose=False):
binary_locations = self.binary_locations()
if self._binary_location is not None:
binary_locations += [self._binary_location]
return nltk.internals.find_binary(
name,
searchpath=binary_locations,
env_vars=[""PROVER9""],
url=""https://www.cs.unm.edu/~mccune/prover9/"",
binary_names=[name, name + "".exe""],
verbose=verbose,
)
",[],0,[],/inference/prover9.py__find_binary
2900,/home/amandapotts/git/nltk/nltk/inference/prover9.py__call,"def _call(self, input_str, binary, args=[], verbose=False):
""""""
Call the binary with the given input.
:param input_str: A string whose contents are used as stdin.
:param binary: The location of the binary to call
:param args: A list of command-line arguments.
:return: A tuple (stdout, returncode)
:see: ``config_prover9``
""""""
if verbose:
print(""Calling:"", binary)
print(""Args:"", args)
print(""Input:\n"", input_str, ""\n"")
cmd = [binary] + args
try:
input_str = input_str.encode(""utf8"")
except AttributeError:
pass
p = subprocess.Popen(
cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, stdin=subprocess.PIPE
)
(stdout, stderr) = p.communicate(input=input_str)
if verbose:
print(""Return code:"", p.returncode)
if stdout:
print(""stdout:\n"", stdout, ""\n"")
if stderr:
print(""stderr:\n"", stderr, ""\n"")
return (stdout.decode(""utf-8""), p.returncode)
",[],0,[],/inference/prover9.py__call
2901,/home/amandapotts/git/nltk/nltk/inference/prover9.py_convert_to_prover9,"def convert_to_prover9(input):
""""""
Convert a ``logic.Expression`` to Prover9 format.
""""""
if isinstance(input, list):
result = []
for s in input:
try:
result.append(_convert_to_prover9(s.simplify()))
except:
print(""input %s cannot be converted to Prover9 input syntax"" % input)
raise
return result
else:
try:
return _convert_to_prover9(input.simplify())
except:
print(""input %s cannot be converted to Prover9 input syntax"" % input)
raise
",[],0,[],/inference/prover9.py_convert_to_prover9
2902,/home/amandapotts/git/nltk/nltk/inference/prover9.py__convert_to_prover9,"def _convert_to_prover9(expression):
""""""
Convert ``logic.Expression`` to Prover9 formatted string.
""""""
if isinstance(expression, ExistsExpression):
return (
""exists ""
+ str(expression.variable)
+ "" ""
+ _convert_to_prover9(expression.term)
)
elif isinstance(expression, AllExpression):
return (
""all ""
+ str(expression.variable)
+ "" ""
+ _convert_to_prover9(expression.term)
)
elif isinstance(expression, NegatedExpression):
return ""-("" + _convert_to_prover9(expression.term) + "")""
elif isinstance(expression, AndExpression):
return (
""(""
+ _convert_to_prover9(expression.first)
+ "" & ""
+ _convert_to_prover9(expression.second)
+ "")""
)
elif isinstance(expression, OrExpression):
return (
""(""
+ _convert_to_prover9(expression.first)
+ "" | ""
+ _convert_to_prover9(expression.second)
+ "")""
)
elif isinstance(expression, ImpExpression):
return (
""(""
+ _convert_to_prover9(expression.first)
+ "" -> ""
+ _convert_to_prover9(expression.second)
+ "")""
)
elif isinstance(expression, IffExpression):
return (
""(""
+ _convert_to_prover9(expression.first)
+ "" <-> ""
+ _convert_to_prover9(expression.second)
+ "")""
)
elif isinstance(expression, EqualityExpression):
return (
""(""
+ _convert_to_prover9(expression.first)
+ "" = ""
+ _convert_to_prover9(expression.second)
+ "")""
)
else:
return str(expression)
",[],0,[],/inference/prover9.py__convert_to_prover9
2903,/home/amandapotts/git/nltk/nltk/inference/prover9.py___init__,"def __init__(self, timeout=60):
self._timeout = timeout
""""""The timeout value for prover9.  If a proof can not be found
in this amount of time, then prover9 will return false.
(Use 0 for no timeout.)""""""
",[],0,[],/inference/prover9.py___init__
2904,/home/amandapotts/git/nltk/nltk/inference/prover9.py__prove,"def _prove(self, goal=None, assumptions=None, verbose=False):
""""""
Use Prover9 to prove a theorem.
:return: A pair whose first element is a boolean indicating if the
proof was successful (i.e. returns value of 0) and whose second element
is the output of the prover.
""""""
if not assumptions:
assumptions = []
stdout, returncode = self._call_prover9(
self.prover9_input(goal, assumptions), verbose=verbose
)
return (returncode == 0, stdout)
",[],0,[],/inference/prover9.py__prove
2905,/home/amandapotts/git/nltk/nltk/inference/prover9.py_prover9_input,"def prover9_input(self, goal, assumptions):
""""""
:see: Prover9Parent.prover9_input
""""""
s = ""clear(auto_denials).\n""  # only one proof required
return s + Prover9Parent.prover9_input(self, goal, assumptions)
",[],0,[],/inference/prover9.py_prover9_input
2906,/home/amandapotts/git/nltk/nltk/inference/prover9.py__call_prover9,"def _call_prover9(self, input_str, args=[], verbose=False):
""""""
Call the ``prover9`` binary with the given input.
:param input_str: A string whose contents are used as stdin.
:param args: A list of command-line arguments.
:return: A tuple (stdout, returncode)
:see: ``config_prover9``
""""""
if self._prover9_bin is None:
self._prover9_bin = self._find_binary(""prover9"", verbose)
updated_input_str = """"
if self._timeout > 0:
updated_input_str += ""assign(max_seconds, %d).\n\n"" % self._timeout
updated_input_str += input_str
stdout, returncode = self._call(
updated_input_str, self._prover9_bin, args, verbose
)
if returncode not in [0, 2]:
errormsgprefix = ""%%ERROR:""
if errormsgprefix in stdout:
msgstart = stdout.index(errormsgprefix)
errormsg = stdout[msgstart:].strip()
else:
errormsg = None
if returncode in [3, 4, 5, 6]:
raise Prover9LimitExceededException(returncode, errormsg)
else:
raise Prover9FatalException(returncode, errormsg)
return stdout, returncode
",[],0,[],/inference/prover9.py__call_prover9
2907,/home/amandapotts/git/nltk/nltk/inference/prover9.py__call_prooftrans,"def _call_prooftrans(self, input_str, args=[], verbose=False):
""""""
Call the ``prooftrans`` binary with the given input.
:param input_str: A string whose contents are used as stdin.
:param args: A list of command-line arguments.
:return: A tuple (stdout, returncode)
:see: ``config_prover9``
""""""
if self._prooftrans_bin is None:
self._prooftrans_bin = self._find_binary(""prooftrans"", verbose)
return self._call(input_str, self._prooftrans_bin, args, verbose)
",[],0,[],/inference/prover9.py__call_prooftrans
2908,/home/amandapotts/git/nltk/nltk/inference/prover9.py___init__,"def __init__(self, returncode, message):
msg = p9_return_codes[returncode]
if message:
msg += ""\n%s"" % message
Exception.__init__(self, msg)
",[],0,[],/inference/prover9.py___init__
2909,/home/amandapotts/git/nltk/nltk/inference/prover9.py_test_config,"def test_config():
a = Expression.fromstring(""(walk(j) & sing(j))"")
g = Expression.fromstring(""walk(j)"")
p = Prover9Command(g, assumptions=[a])
p._executable_path = None
p.prover9_search = []
p.prove()
print(p.prove())
print(p.proof())
",[],0,[],/inference/prover9.py_test_config
2910,/home/amandapotts/git/nltk/nltk/inference/prover9.py_test_convert_to_prover9,"def test_convert_to_prover9(expr):
""""""
Test that parsing works OK.
""""""
for t in expr:
e = Expression.fromstring(t)
print(convert_to_prover9(e))
",[],0,[],/inference/prover9.py_test_convert_to_prover9
2911,/home/amandapotts/git/nltk/nltk/inference/prover9.py_test_prove,"def test_prove(arguments):
""""""
Try some proofs and exhibit the results.
""""""
for goal, assumptions in arguments:
g = Expression.fromstring(goal)
alist = [Expression.fromstring(a) for a in assumptions]
p = Prover9Command(g, assumptions=alist).prove()
for a in alist:
print(""   %s"" % a)
print(f""|- {g}: {p}\n"")
",[],0,[],/inference/prover9.py_test_prove
2912,/home/amandapotts/git/nltk/nltk/inference/prover9.py_spacer,"def spacer(num=45):
print(""-"" * num)
",[],0,[],/inference/prover9.py_spacer
2913,/home/amandapotts/git/nltk/nltk/inference/prover9.py_demo,"def demo():
print(""Testing configuration"")
spacer()
test_config()
print()
print(""Testing conversion to Prover9 format"")
spacer()
test_convert_to_prover9(expressions)
print()
print(""Testing proofs"")
spacer()
test_prove(arguments)
",[],0,[],/inference/prover9.py_demo
2914,/home/amandapotts/git/nltk/nltk/inference/nonmonotonic.py_get_domain,"def get_domain(goal, assumptions):
if goal is None:
all_expressions = assumptions
else:
all_expressions = assumptions + [-goal]
return reduce(operator.or_, (a.constants() for a in all_expressions), set())
",[],0,[],/inference/nonmonotonic.py_get_domain
2915,/home/amandapotts/git/nltk/nltk/inference/nonmonotonic.py_assumptions,"def assumptions(self):
assumptions = [a for a in self._command.assumptions()]
goal = self._command.goal()
domain = get_domain(goal, assumptions)
return [self.replace_quants(ex, domain) for ex in assumptions]
",[],0,[],/inference/nonmonotonic.py_assumptions
2916,/home/amandapotts/git/nltk/nltk/inference/nonmonotonic.py_goal,"def goal(self):
goal = self._command.goal()
domain = get_domain(goal, self._command.assumptions())
return self.replace_quants(goal, domain)
",[],0,[],/inference/nonmonotonic.py_goal
2917,/home/amandapotts/git/nltk/nltk/inference/nonmonotonic.py_assumptions,"def assumptions(self):
""""""
- Domain = union([e.free()|e.constants() for e in all_expressions])
- if ""d1 = d2"" cannot be proven from the premises, then add ""d1 != d2""
""""""
assumptions = self._command.assumptions()
domain = list(get_domain(self._command.goal(), assumptions))
eq_sets = SetHolder()
for a in assumptions:
if isinstance(a, EqualityExpression):
av = a.first.variable
bv = a.second.variable
eq_sets[av].add(bv)
new_assumptions = []
for i, a in enumerate(domain):
for b in domain[i + 1 :]:
if b not in eq_sets[a]:
newEqEx = EqualityExpression(
VariableExpression(a), VariableExpression(b)
)
if Prover9().prove(newEqEx, assumptions):
eq_sets[a].add(b)
else:
new_assumptions.append(-newEqEx)
return assumptions + new_assumptions
",[],0,[],/inference/nonmonotonic.py_assumptions
2918,/home/amandapotts/git/nltk/nltk/inference/nonmonotonic.py___getitem__,"def __getitem__(self, item):
""""""
:param item: ``Variable``
:return: the set containing 'item'
""""""
assert isinstance(item, Variable)
for s in self:
if item in s:
return s
new = {item}
self.append(new)
return new
",[],0,[],/inference/nonmonotonic.py___getitem__
2919,/home/amandapotts/git/nltk/nltk/inference/nonmonotonic.py__make_unique_signature,"def _make_unique_signature(self, predHolder):
""""""
This method figures out how many arguments the predicate takes and
returns a tuple containing that number of unique variables.
""""""
return tuple(unique_variable() for i in range(predHolder.signature_len))
",[],0,[],/inference/nonmonotonic.py__make_unique_signature
2920,/home/amandapotts/git/nltk/nltk/inference/nonmonotonic.py__make_antecedent,"def _make_antecedent(self, predicate, signature):
""""""
Return an application expression with 'predicate' as the predicate
and 'signature' as the list of arguments.
""""""
antecedent = predicate
for v in signature:
antecedent = antecedent(VariableExpression(v))
return antecedent
",[],0,[],/inference/nonmonotonic.py__make_antecedent
2921,/home/amandapotts/git/nltk/nltk/inference/nonmonotonic.py__make_predicate_dict,"def _make_predicate_dict(self, assumptions):
""""""
Create a dictionary of predicates from the assumptions.
:param assumptions: a list of ``Expression``s
:return: dict mapping ``AbstractVariableExpression`` to ``PredHolder``
""""""
predicates = defaultdict(PredHolder)
for a in assumptions:
self._map_predicates(a, predicates)
return predicates
",[],0,[],/inference/nonmonotonic.py__make_predicate_dict
2922,/home/amandapotts/git/nltk/nltk/inference/nonmonotonic.py__map_predicates,"def _map_predicates(self, expression, predDict):
if isinstance(expression, ApplicationExpression):
func, args = expression.uncurry()
if isinstance(func, AbstractVariableExpression):
predDict[func].append_sig(tuple(args))
elif isinstance(expression, AndExpression):
self._map_predicates(expression.first, predDict)
self._map_predicates(expression.second, predDict)
elif isinstance(expression, AllExpression):
sig = [expression.variable]
term = expression.term
while isinstance(term, AllExpression):
sig.append(term.variable)
term = term.term
if isinstance(term, ImpExpression):
if isinstance(term.first, ApplicationExpression) and isinstance(
term.second, ApplicationExpression
):
func1, args1 = term.first.uncurry()
func2, args2 = term.second.uncurry()
if (
isinstance(func1, AbstractVariableExpression)
and isinstance(func2, AbstractVariableExpression)
and sig == [v.variable for v in args1]
and sig == [v.variable for v in args2]
):
predDict[func2].append_prop((tuple(sig), term.first))
predDict[func1].validate_sig_len(sig)
",[],0,[],/inference/nonmonotonic.py__map_predicates
2923,/home/amandapotts/git/nltk/nltk/inference/nonmonotonic.py___init__,"def __init__(self):
self.signatures = []
self.properties = []
self.signature_len = None
",[],0,[],/inference/nonmonotonic.py___init__
2924,/home/amandapotts/git/nltk/nltk/inference/nonmonotonic.py_append_sig,"def append_sig(self, new_sig):
self.validate_sig_len(new_sig)
self.signatures.append(new_sig)
",[],0,[],/inference/nonmonotonic.py_append_sig
2925,/home/amandapotts/git/nltk/nltk/inference/nonmonotonic.py_append_prop,"def append_prop(self, new_prop):
self.validate_sig_len(new_prop[0])
self.properties.append(new_prop)
",[],0,[],/inference/nonmonotonic.py_append_prop
2926,/home/amandapotts/git/nltk/nltk/inference/nonmonotonic.py_validate_sig_len,"def validate_sig_len(self, new_sig):
if self.signature_len is None:
self.signature_len = len(new_sig)
elif self.signature_len != len(new_sig):
raise Exception(""Signature lengths do not match"")
",[],0,[],/inference/nonmonotonic.py_validate_sig_len
2927,/home/amandapotts/git/nltk/nltk/inference/nonmonotonic.py___str__,"def __str__(self):
return f""({self.signatures},{self.properties},{self.signature_len})""
",[],0,[],/inference/nonmonotonic.py___str__
2928,/home/amandapotts/git/nltk/nltk/inference/nonmonotonic.py___repr__,"def __repr__(self):
return ""%s"" % self
",[],0,[],/inference/nonmonotonic.py___repr__
2929,/home/amandapotts/git/nltk/nltk/inference/nonmonotonic.py_closed_domain_demo,"def closed_domain_demo():
lexpr = Expression.fromstring
p1 = lexpr(r""exists x.walk(x)"")
p2 = lexpr(r""man(Socrates)"")
c = lexpr(r""walk(Socrates)"")
prover = Prover9Command(c, [p1, p2])
print(prover.prove())
cdp = ClosedDomainProver(prover)
print(""assumptions:"")
for a in cdp.assumptions():
print(""   "", a)
print(""goal:"", cdp.goal())
print(cdp.prove())
p1 = lexpr(r""exists x.walk(x)"")
p2 = lexpr(r""man(Socrates)"")
p3 = lexpr(r""-walk(Bill)"")
c = lexpr(r""walk(Socrates)"")
prover = Prover9Command(c, [p1, p2, p3])
print(prover.prove())
cdp = ClosedDomainProver(prover)
print(""assumptions:"")
for a in cdp.assumptions():
print(""   "", a)
print(""goal:"", cdp.goal())
print(cdp.prove())
p1 = lexpr(r""exists x.walk(x)"")
p2 = lexpr(r""man(Socrates)"")
p3 = lexpr(r""-walk(Bill)"")
c = lexpr(r""walk(Socrates)"")
prover = Prover9Command(c, [p1, p2, p3])
print(prover.prove())
cdp = ClosedDomainProver(prover)
print(""assumptions:"")
for a in cdp.assumptions():
print(""   "", a)
print(""goal:"", cdp.goal())
print(cdp.prove())
p1 = lexpr(r""walk(Socrates)"")
p2 = lexpr(r""walk(Bill)"")
c = lexpr(r""all x.walk(x)"")
prover = Prover9Command(c, [p1, p2])
print(prover.prove())
cdp = ClosedDomainProver(prover)
print(""assumptions:"")
for a in cdp.assumptions():
print(""   "", a)
print(""goal:"", cdp.goal())
print(cdp.prove())
p1 = lexpr(r""girl(mary)"")
p2 = lexpr(r""dog(rover)"")
p3 = lexpr(r""all x.(girl(x) -> -dog(x))"")
p4 = lexpr(r""all x.(dog(x) -> -girl(x))"")
p5 = lexpr(r""chase(mary, rover)"")
c = lexpr(r""exists y.(dog(y) & all x.(girl(x) -> chase(x,y)))"")
prover = Prover9Command(c, [p1, p2, p3, p4, p5])
print(prover.prove())
cdp = ClosedDomainProver(prover)
print(""assumptions:"")
for a in cdp.assumptions():
print(""   "", a)
print(""goal:"", cdp.goal())
print(cdp.prove())
",[],0,[],/inference/nonmonotonic.py_closed_domain_demo
2930,/home/amandapotts/git/nltk/nltk/inference/nonmonotonic.py_unique_names_demo,"def unique_names_demo():
lexpr = Expression.fromstring
p1 = lexpr(r""man(Socrates)"")
p2 = lexpr(r""man(Bill)"")
c = lexpr(r""exists x.exists y.(x != y)"")
prover = Prover9Command(c, [p1, p2])
print(prover.prove())
unp = UniqueNamesProver(prover)
print(""assumptions:"")
for a in unp.assumptions():
print(""   "", a)
print(""goal:"", unp.goal())
print(unp.prove())
p1 = lexpr(r""all x.(walk(x) -> (x = Socrates))"")
p2 = lexpr(r""Bill = William"")
p3 = lexpr(r""Bill = Billy"")
c = lexpr(r""-walk(William)"")
prover = Prover9Command(c, [p1, p2, p3])
print(prover.prove())
unp = UniqueNamesProver(prover)
print(""assumptions:"")
for a in unp.assumptions():
print(""   "", a)
print(""goal:"", unp.goal())
print(unp.prove())
","['assumptions', 'goal', 'prove', 'assumptions', 'goal', 'prove']",6,"['assumptions()', 'goal())', 'prove())', 'assumptions()', 'goal())', 'prove())']",/inference/nonmonotonic.py_unique_names_demo
2931,/home/amandapotts/git/nltk/nltk/inference/nonmonotonic.py_closed_world_demo,"def closed_world_demo():
lexpr = Expression.fromstring
p1 = lexpr(r""walk(Socrates)"")
p2 = lexpr(r""(Socrates != Bill)"")
c = lexpr(r""-walk(Bill)"")
prover = Prover9Command(c, [p1, p2])
print(prover.prove())
cwp = ClosedWorldProver(prover)
print(""assumptions:"")
for a in cwp.assumptions():
print(""   "", a)
print(""goal:"", cwp.goal())
print(cwp.prove())
p1 = lexpr(r""see(Socrates, John)"")
p2 = lexpr(r""see(John, Mary)"")
p3 = lexpr(r""(Socrates != John)"")
p4 = lexpr(r""(John != Mary)"")
c = lexpr(r""-see(Socrates, Mary)"")
prover = Prover9Command(c, [p1, p2, p3, p4])
print(prover.prove())
cwp = ClosedWorldProver(prover)
print(""assumptions:"")
for a in cwp.assumptions():
print(""   "", a)
print(""goal:"", cwp.goal())
print(cwp.prove())
p1 = lexpr(r""all x.(ostrich(x) -> bird(x))"")
p2 = lexpr(r""bird(Tweety)"")
p3 = lexpr(r""-ostrich(Sam)"")
p4 = lexpr(r""Sam != Tweety"")
c = lexpr(r""-bird(Sam)"")
prover = Prover9Command(c, [p1, p2, p3, p4])
print(prover.prove())
cwp = ClosedWorldProver(prover)
print(""assumptions:"")
for a in cwp.assumptions():
print(""   "", a)
print(""goal:"", cwp.goal())
print(cwp.prove())
",[],0,[],/inference/nonmonotonic.py_closed_world_demo
2932,/home/amandapotts/git/nltk/nltk/inference/nonmonotonic.py_combination_prover_demo,"def combination_prover_demo():
lexpr = Expression.fromstring
p1 = lexpr(r""see(Socrates, John)"")
p2 = lexpr(r""see(John, Mary)"")
c = lexpr(r""-see(Socrates, Mary)"")
prover = Prover9Command(c, [p1, p2])
print(prover.prove())
command = ClosedDomainProver(UniqueNamesProver(ClosedWorldProver(prover)))
for a in command.assumptions():
print(a)
print(command.prove())
",[],0,[],/inference/nonmonotonic.py_combination_prover_demo
2933,/home/amandapotts/git/nltk/nltk/inference/nonmonotonic.py_default_reasoning_demo,"def default_reasoning_demo():
lexpr = Expression.fromstring
premises = []
premises.append(lexpr(r""all x.(elephant(x)        -> animal(x))""))
premises.append(lexpr(r""all x.(bird(x)            -> animal(x))""))
premises.append(lexpr(r""all x.(dove(x)            -> bird(x))""))
premises.append(lexpr(r""all x.(ostrich(x)         -> bird(x))""))
premises.append(lexpr(r""all x.(flying_ostrich(x)  -> ostrich(x))""))
premises.append(
lexpr(r""all x.((animal(x)  & -Ab1(x)) -> -fly(x))"")
)  # normal animals don't fly
premises.append(
lexpr(r""all x.((bird(x)    & -Ab2(x)) -> fly(x))"")
)  # normal birds fly
premises.append(
lexpr(r""all x.((ostrich(x) & -Ab3(x)) -> -fly(x))"")
)  # normal ostriches don't fly
premises.append(lexpr(r""all x.(bird(x)           -> Ab1(x))""))  # flight
premises.append(lexpr(r""all x.(ostrich(x)        -> Ab2(x))""))  # non-flying bird
premises.append(lexpr(r""all x.(flying_ostrich(x) -> Ab3(x))""))  # flying ostrich
premises.append(lexpr(r""elephant(E)""))
premises.append(lexpr(r""dove(D)""))
premises.append(lexpr(r""ostrich(O)""))
prover = Prover9Command(None, premises)
command = UniqueNamesProver(ClosedWorldProver(prover))
for a in command.assumptions():
print(a)
print_proof(""-fly(E)"", premises)
print_proof(""fly(D)"", premises)
print_proof(""-fly(O)"", premises)
",[],0,[],/inference/nonmonotonic.py_default_reasoning_demo
2934,/home/amandapotts/git/nltk/nltk/inference/nonmonotonic.py_print_proof,"def print_proof(goal, premises):
lexpr = Expression.fromstring
prover = Prover9Command(lexpr(goal), premises)
command = UniqueNamesProver(ClosedWorldProver(prover))
print(goal, prover.prove(), command.prove())
",[],0,[],/inference/nonmonotonic.py_print_proof
2935,/home/amandapotts/git/nltk/nltk/inference/nonmonotonic.py_demo,"def demo():
closed_domain_demo()
unique_names_demo()
closed_world_demo()
combination_prover_demo()
default_reasoning_demo()
",[],0,[],/inference/nonmonotonic.py_demo
2936,/home/amandapotts/git/nltk/nltk/sentiment/util.py_timer,"def timer(method):
""""""
A timer decorator to measure execution performance of methods.
""""""
",[],0,[],/sentiment/util.py_timer
2937,/home/amandapotts/git/nltk/nltk/sentiment/util.py_timed,"def timed(*args, **kw):
start = time.time()
result = method(*args, **kw)
end = time.time()
tot_time = end - start
hours = tot_time // 3600
mins = tot_time // 60 % 60
secs = int(round(tot_time % 60))
if hours == 0 and mins == 0 and secs < 10:
print(f""[TIMER] {method.__name__}(): {method.__name__:.3f} seconds"")
else:
print(f""[TIMER] {method.__name__}(): {hours}h {mins}m {secs}s"")
return result
",[],0,[],/sentiment/util.py_timed
2938,/home/amandapotts/git/nltk/nltk/sentiment/util.py_extract_unigram_feats,"def extract_unigram_feats(document, unigrams, handle_negation=False):
""""""
Populate a dictionary of unigram features, reflecting the presence/absence in
the document of each of the tokens in `unigrams`.
:param document: a list of words/tokens.
:param unigrams: a list of words/tokens whose presence/absence has to be
checked in `document`.
:param handle_negation: if `handle_negation == True` apply `mark_negation`
method to `document` before checking for unigram presence/absence.
:return: a dictionary of unigram features {unigram : boolean}.
>>> words = ['ice', 'police', 'riot']
>>> document = 'ice is melting due to global warming'.split()
>>> sorted(extract_unigram_feats(document, words).items())
[('contains(ice)', True), ('contains(police)', False), ('contains(riot)', False)]
""""""
features = {}
if handle_negation:
document = mark_negation(document)
for word in unigrams:
features[f""contains({word})""] = word in set(document)
return features
",[],0,[],/sentiment/util.py_extract_unigram_feats
2939,/home/amandapotts/git/nltk/nltk/sentiment/util.py_extract_bigram_feats,"def extract_bigram_feats(document, bigrams):
""""""
Populate a dictionary of bigram features, reflecting the presence/absence in
the document of each of the tokens in `bigrams`. This extractor function only
considers contiguous bigrams obtained by `nltk.bigrams`.
:param document: a list of words/tokens.
:param unigrams: a list of bigrams whose presence/absence has to be
checked in `document`.
:return: a dictionary of bigram features {bigram : boolean}.
>>> bigrams = [('global', 'warming'), ('police', 'prevented'), ('love', 'you')]
>>> document = 'ice is melting due to global warming'.split()
>>> sorted(extract_bigram_feats(document, bigrams).items()) # doctest: +NORMALIZE_WHITESPACE
[('contains(global - warming)', True), ('contains(love - you)', False),
('contains(police - prevented)', False)]
""""""
features = {}
for bigr in bigrams:
features[f""contains({bigr[0]} - {bigr[1]})""] = bigr in nltk.bigrams(document)
return features
",[],0,[],/sentiment/util.py_extract_bigram_feats
2940,/home/amandapotts/git/nltk/nltk/sentiment/util.py_mark_negation,"def mark_negation(document, double_neg_flip=False, shallow=False):
""""""
Append _NEG suffix to words that appear in the scope between a negation
and a punctuation mark.
:param document: a list of words/tokens, or a tuple (words, label).
:param shallow: if True, the method will modify the original document in place.
:param double_neg_flip: if True, double negation is considered affirmation
(we activate/deactivate negation scope every time we find a negation).
:return: if `shallow == True` the method will modify the original document
and return it. If `shallow == False` the method will return a modified
document, leaving the original unmodified.
>>> sent = ""I didn't like this movie . It was bad ."".split()
>>> mark_negation(sent)
['I', ""didn't"", 'like_NEG', 'this_NEG', 'movie_NEG', '.', 'It', 'was', 'bad', '.']
""""""
if not shallow:
document = deepcopy(document)
labeled = document and isinstance(document[0], (tuple, list))
if labeled:
doc = document[0]
else:
doc = document
neg_scope = False
for i, word in enumerate(doc):
if NEGATION_RE.search(word):
if not neg_scope or (neg_scope and double_neg_flip):
neg_scope = not neg_scope
continue
else:
doc[i] += ""_NEG""
elif neg_scope and CLAUSE_PUNCT_RE.search(word):
neg_scope = not neg_scope
elif neg_scope and not CLAUSE_PUNCT_RE.search(word):
doc[i] += ""_NEG""
return document
",[],0,[],/sentiment/util.py_mark_negation
2941,/home/amandapotts/git/nltk/nltk/sentiment/util.py_output_markdown,"def output_markdown(filename, **kwargs):
""""""
Write the output of an analysis to a file.
""""""
with codecs.open(filename, ""at"") as outfile:
text = ""\n*** \n\n""
text += ""{} \n\n"".format(time.strftime(""%d/%m/%Y, %H:%M""))
for k in sorted(kwargs):
if isinstance(kwargs[k], dict):
dictionary = kwargs[k]
text += f""  - **{k}:**\n""
for entry in sorted(dictionary):
text += f""    - {entry}: {dictionary[entry]} \n""
elif isinstance(kwargs[k], list):
text += f""  - **{k}:**\n""
for entry in kwargs[k]:
text += f""    - {entry}\n""
else:
text += f""  - **{k}:** {kwargs[k]} \n""
outfile.write(text)
",[],0,[],/sentiment/util.py_output_markdown
2942,/home/amandapotts/git/nltk/nltk/sentiment/util.py_split_train_test,"def split_train_test(all_instances, n=None):
""""""
Randomly split `n` instances of the dataset into train and test sets.
:param all_instances: a list of instances (e.g. documents) that will be split.
:param n: the number of instances to consider (in case we want to use only a
subset).
:return: two lists of instances. Train set is 8/10 of the total and test set
is 2/10 of the total.
""""""
random.seed(12345)
random.shuffle(all_instances)
if not n or n > len(all_instances):
n = len(all_instances)
train_set = all_instances[: int(0.8 * n)]
test_set = all_instances[int(0.8 * n) : n]
return train_set, test_set
",[],0,[],/sentiment/util.py_split_train_test
2943,/home/amandapotts/git/nltk/nltk/sentiment/util.py__show_plot,"def _show_plot(x_values, y_values, x_labels=None, y_labels=None):
try:
import matplotlib.pyplot as plt
except ImportError as e:
raise ImportError(
""The plot function requires matplotlib to be installed.""
""See https://matplotlib.org/""
) from e
plt.locator_params(axis=""y"", nbins=3)
axes = plt.axes()
axes.yaxis.grid()
plt.plot(x_values, y_values, ""ro"", color=""red"")
plt.ylim(ymin=-1.2, ymax=1.2)
plt.tight_layout(pad=5)
if x_labels:
plt.xticks(x_values, x_labels, rotation=""vertical"")
if y_labels:
plt.yticks([-1, 0, 1], y_labels, rotation=""horizontal"")
plt.margins(0.2)
plt.show()
",[],0,[],/sentiment/util.py__show_plot
2944,/home/amandapotts/git/nltk/nltk/sentiment/util.py_json2csv_preprocess,"def json2csv_preprocess(
json_file,
outfile,
fields,
encoding=""utf8"",
errors=""replace"",
gzip_compress=False,
skip_retweets=True,
skip_tongue_tweets=True,
skip_ambiguous_tweets=True,
strip_off_emoticons=True,
remove_duplicates=True,
limit=None,
",[],0,[],/sentiment/util.py_json2csv_preprocess
2945,/home/amandapotts/git/nltk/nltk/sentiment/util.py_parse_tweets_set,"def parse_tweets_set(
filename, label, word_tokenizer=None, sent_tokenizer=None, skip_header=True
",[],0,[],/sentiment/util.py_parse_tweets_set
2946,/home/amandapotts/git/nltk/nltk/sentiment/util.py_demo_tweets,"def demo_tweets(trainer, n_instances=None, output=None):
""""""
Train and test Naive Bayes classifier on 10000 tweets, tokenized using
TweetTokenizer.
Features are composed of:
- 1000 most frequent unigrams
- 100 top bigrams (using BigramAssocMeasures.pmi)
:param trainer: `train` method of a classifier.
:param n_instances: the number of total tweets that have to be used for
training and testing. Tweets will be equally split between positive and
negative.
:param output: the output file where results have to be reported.
""""""
from nltk.corpus import stopwords, twitter_samples
from nltk.sentiment import SentimentAnalyzer
from nltk.tokenize import TweetTokenizer
tokenizer = TweetTokenizer(preserve_case=False)
if n_instances is not None:
n_instances = int(n_instances / 2)
fields = [""id"", ""text""]
positive_json = twitter_samples.abspath(""positive_tweets.json"")
positive_csv = ""positive_tweets.csv""
json2csv_preprocess(positive_json, positive_csv, fields, limit=n_instances)
negative_json = twitter_samples.abspath(""negative_tweets.json"")
negative_csv = ""negative_tweets.csv""
json2csv_preprocess(negative_json, negative_csv, fields, limit=n_instances)
neg_docs = parse_tweets_set(negative_csv, label=""neg"", word_tokenizer=tokenizer)
pos_docs = parse_tweets_set(positive_csv, label=""pos"", word_tokenizer=tokenizer)
train_pos_docs, test_pos_docs = split_train_test(pos_docs)
train_neg_docs, test_neg_docs = split_train_test(neg_docs)
training_tweets = train_pos_docs + train_neg_docs
testing_tweets = test_pos_docs + test_neg_docs
sentim_analyzer = SentimentAnalyzer()
all_words = [word for word in sentim_analyzer.all_words(training_tweets)]
unigram_feats = sentim_analyzer.unigram_word_feats(all_words, top_n=1000)
sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)
bigram_collocs_feats = sentim_analyzer.bigram_collocation_feats(
[tweet[0] for tweet in training_tweets], top_n=100, min_freq=12
)
sentim_analyzer.add_feat_extractor(
extract_bigram_feats, bigrams=bigram_collocs_feats
)
training_set = sentim_analyzer.apply_features(training_tweets)
test_set = sentim_analyzer.apply_features(testing_tweets)
classifier = sentim_analyzer.train(trainer, training_set)
try:
classifier.show_most_informative_features()
except AttributeError:
print(
""Your classifier does not provide a show_most_informative_features() method.""
)
results = sentim_analyzer.evaluate(test_set)
if output:
extr = [f.__name__ for f in sentim_analyzer.feat_extractors]
output_markdown(
output,
Dataset=""labeled_tweets"",
Classifier=type(classifier).__name__,
Tokenizer=tokenizer.__class__.__name__,
Feats=extr,
Results=results,
Instances=n_instances,
)
",[],0,[],/sentiment/util.py_demo_tweets
2947,/home/amandapotts/git/nltk/nltk/sentiment/util.py_demo_movie_reviews,"def demo_movie_reviews(trainer, n_instances=None, output=None):
""""""
Train classifier on all instances of the Movie Reviews dataset.
The corpus has been preprocessed using the default sentence tokenizer and
WordPunctTokenizer.
Features are composed of:
- most frequent unigrams
:param trainer: `train` method of a classifier.
:param n_instances: the number of total reviews that have to be used for
training and testing. Reviews will be equally split between positive and
negative.
:param output: the output file where results have to be reported.
""""""
from nltk.corpus import movie_reviews
from nltk.sentiment import SentimentAnalyzer
if n_instances is not None:
n_instances = int(n_instances / 2)
pos_docs = [
(list(movie_reviews.words(pos_id)), ""pos"")
for pos_id in movie_reviews.fileids(""pos"")[:n_instances]
]
neg_docs = [
(list(movie_reviews.words(neg_id)), ""neg"")
for neg_id in movie_reviews.fileids(""neg"")[:n_instances]
]
train_pos_docs, test_pos_docs = split_train_test(pos_docs)
train_neg_docs, test_neg_docs = split_train_test(neg_docs)
training_docs = train_pos_docs + train_neg_docs
testing_docs = test_pos_docs + test_neg_docs
sentim_analyzer = SentimentAnalyzer()
all_words = sentim_analyzer.all_words(training_docs)
unigram_feats = sentim_analyzer.unigram_word_feats(all_words, min_freq=4)
sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)
training_set = sentim_analyzer.apply_features(training_docs)
test_set = sentim_analyzer.apply_features(testing_docs)
classifier = sentim_analyzer.train(trainer, training_set)
try:
classifier.show_most_informative_features()
except AttributeError:
print(
""Your classifier does not provide a show_most_informative_features() method.""
)
results = sentim_analyzer.evaluate(test_set)
if output:
extr = [f.__name__ for f in sentim_analyzer.feat_extractors]
output_markdown(
output,
Dataset=""Movie_reviews"",
Classifier=type(classifier).__name__,
Tokenizer=""WordPunctTokenizer"",
Feats=extr,
Results=results,
Instances=n_instances,
)
",[],0,[],/sentiment/util.py_demo_movie_reviews
2948,/home/amandapotts/git/nltk/nltk/sentiment/util.py_demo_subjectivity,"def demo_subjectivity(trainer, save_analyzer=False, n_instances=None, output=None):
""""""
Train and test a classifier on instances of the Subjective Dataset by Pang and
Lee. The dataset is made of 5000 subjective and 5000 objective sentences.
All tokens (words and punctuation marks) are separated by a whitespace, so
we use the basic WhitespaceTokenizer to parse the data.
:param trainer: `train` method of a classifier.
:param save_analyzer: if `True`, store the SentimentAnalyzer in a pickle file.
:param n_instances: the number of total sentences that have to be used for
training and testing. Sentences will be equally split between positive
and negative.
:param output: the output file where results have to be reported.
""""""
from nltk.corpus import subjectivity
from nltk.sentiment import SentimentAnalyzer
if n_instances is not None:
n_instances = int(n_instances / 2)
subj_docs = [
(sent, ""subj"") for sent in subjectivity.sents(categories=""subj"")[:n_instances]
]
obj_docs = [
(sent, ""obj"") for sent in subjectivity.sents(categories=""obj"")[:n_instances]
]
train_subj_docs, test_subj_docs = split_train_test(subj_docs)
train_obj_docs, test_obj_docs = split_train_test(obj_docs)
training_docs = train_subj_docs + train_obj_docs
testing_docs = test_subj_docs + test_obj_docs
sentim_analyzer = SentimentAnalyzer()
all_words_neg = sentim_analyzer.all_words(
[mark_negation(doc) for doc in training_docs]
)
unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)
sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)
training_set = sentim_analyzer.apply_features(training_docs)
test_set = sentim_analyzer.apply_features(testing_docs)
classifier = sentim_analyzer.train(trainer, training_set)
try:
classifier.show_most_informative_features()
except AttributeError:
print(
""Your classifier does not provide a show_most_informative_features() method.""
)
results = sentim_analyzer.evaluate(test_set)
if save_analyzer == True:
sentim_analyzer.save_file(sentim_analyzer, ""sa_subjectivity.pickle"")
if output:
extr = [f.__name__ for f in sentim_analyzer.feat_extractors]
output_markdown(
output,
Dataset=""subjectivity"",
Classifier=type(classifier).__name__,
Tokenizer=""WhitespaceTokenizer"",
Feats=extr,
Instances=n_instances,
Results=results,
)
return sentim_analyzer
",[],0,[],/sentiment/util.py_demo_subjectivity
2949,/home/amandapotts/git/nltk/nltk/sentiment/util.py_demo_sent_subjectivity,"def demo_sent_subjectivity(text):
""""""
Classify a single sentence as subjective or objective using a stored
SentimentAnalyzer.
:param text: a sentence whose subjectivity has to be classified.
""""""
from nltk.classify import NaiveBayesClassifier
from nltk.tokenize import regexp
word_tokenizer = regexp.WhitespaceTokenizer()
try:
sentim_analyzer = load(""sa_subjectivity.pickle"")
except LookupError:
print(""Cannot find the sentiment analyzer you want to load."")
print(""Training a new one using NaiveBayesClassifier."")
sentim_analyzer = demo_subjectivity(NaiveBayesClassifier.train, True)
tokenized_text = [word.lower() for word in word_tokenizer.tokenize(text)]
print(sentim_analyzer.classify(tokenized_text))
",[],0,[],/sentiment/util.py_demo_sent_subjectivity
2950,/home/amandapotts/git/nltk/nltk/sentiment/util.py_demo_liu_hu_lexicon,"def demo_liu_hu_lexicon(sentence, plot=False):
""""""
Basic example of sentiment classification using Liu and Hu opinion lexicon.
This function simply counts the number of positive, negative and neutral words
in the sentence and classifies it depending on which polarity is more represented.
Words that do not appear in the lexicon are considered as neutral.
:param sentence: a sentence whose polarity has to be classified.
:param plot: if True, plot a visual representation of the sentence polarity.
""""""
from nltk.corpus import opinion_lexicon
from nltk.tokenize import treebank
tokenizer = treebank.TreebankWordTokenizer()
pos_words = 0
neg_words = 0
tokenized_sent = [word.lower() for word in tokenizer.tokenize(sentence)]
x = list(range(len(tokenized_sent)))  # x axis for the plot
y = []
for word in tokenized_sent:
if word in opinion_lexicon.positive():
pos_words += 1
y.append(1)  # positive
elif word in opinion_lexicon.negative():
neg_words += 1
y.append(-1)  # negative
else:
y.append(0)  # neutral
if pos_words > neg_words:
print(""Positive"")
elif pos_words < neg_words:
print(""Negative"")
elif pos_words == neg_words:
print(""Neutral"")
if plot == True:
_show_plot(
x, y, x_labels=tokenized_sent, y_labels=[""Negative"", ""Neutral"", ""Positive""]
)
",[],0,[],/sentiment/util.py_demo_liu_hu_lexicon
2951,/home/amandapotts/git/nltk/nltk/sentiment/util.py_demo_vader_instance,"def demo_vader_instance(text):
""""""
Output polarity scores for a text using Vader approach.
:param text: a text whose polarity has to be evaluated.
""""""
from nltk.sentiment import SentimentIntensityAnalyzer
vader_analyzer = SentimentIntensityAnalyzer()
print(vader_analyzer.polarity_scores(text))
",[],0,[],/sentiment/util.py_demo_vader_instance
2952,/home/amandapotts/git/nltk/nltk/sentiment/util.py_demo_vader_tweets,"def demo_vader_tweets(n_instances=None, output=None):
""""""
Classify 10000 positive and negative tweets using Vader approach.
:param n_instances: the number of total tweets that have to be classified.
:param output: the output file where results have to be reported.
""""""
from collections import defaultdict
from nltk.corpus import twitter_samples
from nltk.metrics import accuracy as eval_accuracy
from nltk.metrics import f_measure as eval_f_measure
from nltk.metrics import precision as eval_precision
from nltk.metrics import recall as eval_recall
from nltk.sentiment import SentimentIntensityAnalyzer
if n_instances is not None:
n_instances = int(n_instances / 2)
fields = [""id"", ""text""]
positive_json = twitter_samples.abspath(""positive_tweets.json"")
positive_csv = ""positive_tweets.csv""
json2csv_preprocess(
positive_json,
positive_csv,
fields,
strip_off_emoticons=False,
limit=n_instances,
)
negative_json = twitter_samples.abspath(""negative_tweets.json"")
negative_csv = ""negative_tweets.csv""
json2csv_preprocess(
negative_json,
negative_csv,
fields,
strip_off_emoticons=False,
limit=n_instances,
)
pos_docs = parse_tweets_set(positive_csv, label=""pos"")
neg_docs = parse_tweets_set(negative_csv, label=""neg"")
train_pos_docs, test_pos_docs = split_train_test(pos_docs)
train_neg_docs, test_neg_docs = split_train_test(neg_docs)
training_tweets = train_pos_docs + train_neg_docs
testing_tweets = test_pos_docs + test_neg_docs
vader_analyzer = SentimentIntensityAnalyzer()
gold_results = defaultdict(set)
test_results = defaultdict(set)
acc_gold_results = []
acc_test_results = []
labels = set()
num = 0
for i, (text, label) in enumerate(testing_tweets):
labels.add(label)
gold_results[label].add(i)
acc_gold_results.append(label)
score = vader_analyzer.polarity_scores(text)[""compound""]
if score > 0:
observed = ""pos""
else:
observed = ""neg""
num += 1
acc_test_results.append(observed)
test_results[observed].add(i)
metrics_results = {}
for label in labels:
accuracy_score = eval_accuracy(acc_gold_results, acc_test_results)
metrics_results[""Accuracy""] = accuracy_score
precision_score = eval_precision(gold_results[label], test_results[label])
metrics_results[f""Precision [{label}]""] = precision_score
recall_score = eval_recall(gold_results[label], test_results[label])
metrics_results[f""Recall [{label}]""] = recall_score
f_measure_score = eval_f_measure(gold_results[label], test_results[label])
metrics_results[f""F-measure [{label}]""] = f_measure_score
for result in sorted(metrics_results):
print(f""{result}: {metrics_results[result]}"")
if output:
output_markdown(
output,
Approach=""Vader"",
Dataset=""labeled_tweets"",
Instances=n_instances,
Results=metrics_results,
)
",[],0,[],/sentiment/util.py_demo_vader_tweets
2953,/home/amandapotts/git/nltk/nltk/sentiment/vader.py___init__,"def __init__(self):
pass
",[],0,[],/sentiment/vader.py___init__
2954,/home/amandapotts/git/nltk/nltk/sentiment/vader.py_negated,"def negated(self, input_words, include_nt=True):
""""""
Determine if input contains negation words
""""""
neg_words = self.NEGATE
if any(word.lower() in neg_words for word in input_words):
return True
if include_nt:
if any(""n't"" in word.lower() for word in input_words):
return True
for first, second in pairwise(input_words):
if second.lower() == ""least"" and first.lower() != ""at"":
return True
return False
",[],0,[],/sentiment/vader.py_negated
2955,/home/amandapotts/git/nltk/nltk/sentiment/vader.py_normalize,"def normalize(self, score, alpha=15):
""""""
Normalize the score to be between -1 and 1 using an alpha that
approximates the max expected value
""""""
norm_score = score / math.sqrt((score * score) + alpha)
return norm_score
",[],0,[],/sentiment/vader.py_normalize
2956,/home/amandapotts/git/nltk/nltk/sentiment/vader.py_scalar_inc_dec,"def scalar_inc_dec(self, word, valence, is_cap_diff):
""""""
Check if the preceding words increase, decrease, or negate/nullify the
valence
""""""
scalar = 0.0
word_lower = word.lower()
if word_lower in self.BOOSTER_DICT:
scalar = self.BOOSTER_DICT[word_lower]
if valence < 0:
scalar *= -1
if word.isupper() and is_cap_diff:
if valence > 0:
scalar += self.C_INCR
else:
scalar -= self.C_INCR
return scalar
",[],0,[],/sentiment/vader.py_scalar_inc_dec
2957,/home/amandapotts/git/nltk/nltk/sentiment/vader.py___init__,"def __init__(self, text, punc_list, regex_remove_punctuation):
if not isinstance(text, str):
text = str(text.encode(""utf-8""))
self.text = text
self.PUNC_LIST = punc_list
self.REGEX_REMOVE_PUNCTUATION = regex_remove_punctuation
self.words_and_emoticons = self._words_and_emoticons()
self.is_cap_diff = self.allcap_differential(self.words_and_emoticons)
",[],0,[],/sentiment/vader.py___init__
2958,/home/amandapotts/git/nltk/nltk/sentiment/vader.py__words_plus_punc,"def _words_plus_punc(self):
""""""
Returns mapping of form:
{
'cat,': 'cat',
',cat': 'cat',
}
""""""
no_punc_text = self.REGEX_REMOVE_PUNCTUATION.sub("""", self.text)
words_only = no_punc_text.split()
words_only = {w for w in words_only if len(w) > 1}
punc_before = {"""".join(p): p[1] for p in product(self.PUNC_LIST, words_only)}
punc_after = {"""".join(p): p[0] for p in product(words_only, self.PUNC_LIST)}
words_punc_dict = punc_before
words_punc_dict.update(punc_after)
return words_punc_dict
",[],0,[],/sentiment/vader.py__words_plus_punc
2959,/home/amandapotts/git/nltk/nltk/sentiment/vader.py__words_and_emoticons,"def _words_and_emoticons(self):
""""""
Removes leading and trailing puncutation
Leaves contractions and most emoticons
Does not preserve punc-plus-letter emoticons (e.g. :D)
""""""
wes = self.text.split()
words_punc_dict = self._words_plus_punc()
wes = [we for we in wes if len(we) > 1]
for i, we in enumerate(wes):
if we in words_punc_dict:
wes[i] = words_punc_dict[we]
return wes
",[],0,[],/sentiment/vader.py__words_and_emoticons
2960,/home/amandapotts/git/nltk/nltk/sentiment/vader.py_allcap_differential,"def allcap_differential(self, words):
""""""
Check whether just some words in the input are ALL CAPS
:param list words: The words to inspect
:returns: `True` if some but not all items in `words` are ALL CAPS
""""""
is_different = False
allcap_words = 0
for word in words:
if word.isupper():
allcap_words += 1
cap_differential = len(words) - allcap_words
if 0 < cap_differential < len(words):
is_different = True
return is_different
",[],0,[],/sentiment/vader.py_allcap_differential
2961,/home/amandapotts/git/nltk/nltk/sentiment/vader.py___init__,"def __init__(
self,
lexicon_file=""sentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt"",
",[],0,[],/sentiment/vader.py___init__
2962,/home/amandapotts/git/nltk/nltk/sentiment/vader.py_make_lex_dict,"def make_lex_dict(self):
""""""
Convert lexicon file to a dictionary
""""""
lex_dict = {}
for line in self.lexicon_file.split(""\n""):
(word, measure) = line.strip().split(""\t"")[0:2]
lex_dict[word] = float(measure)
return lex_dict
",[],0,[],/sentiment/vader.py_make_lex_dict
2963,/home/amandapotts/git/nltk/nltk/sentiment/vader.py_polarity_scores,"def polarity_scores(self, text):
""""""
Return a float for sentiment strength based on the input text.
Positive values are positive valence, negative value are negative
valence.
:note: Hashtags are not taken into consideration (e.g. #BAD is neutral). If you
are interested in processing the text in the hashtags too, then we recommend
preprocessing your data to remove the #, after which the hashtag text may be
matched as if it was a normal word in the sentence.
""""""
sentitext = SentiText(
text, self.constants.PUNC_LIST, self.constants.REGEX_REMOVE_PUNCTUATION
)
sentiments = []
words_and_emoticons = sentitext.words_and_emoticons
for item in words_and_emoticons:
valence = 0
i = words_and_emoticons.index(item)
if (
i < len(words_and_emoticons) - 1
and item.lower() == ""kind""
and words_and_emoticons[i + 1].lower() == ""of""
) or item.lower() in self.constants.BOOSTER_DICT:
sentiments.append(valence)
continue
sentiments = self.sentiment_valence(valence, sentitext, item, i, sentiments)
sentiments = self._but_check(words_and_emoticons, sentiments)
return self.score_valence(sentiments, text)
",[],0,[],/sentiment/vader.py_polarity_scores
2964,/home/amandapotts/git/nltk/nltk/sentiment/vader.py_sentiment_valence,"def sentiment_valence(self, valence, sentitext, item, i, sentiments):
is_cap_diff = sentitext.is_cap_diff
words_and_emoticons = sentitext.words_and_emoticons
item_lowercase = item.lower()
if item_lowercase in self.lexicon:
valence = self.lexicon[item_lowercase]
if item.isupper() and is_cap_diff:
if valence > 0:
valence += self.constants.C_INCR
else:
valence -= self.constants.C_INCR
for start_i in range(0, 3):
if (
i > start_i
and words_and_emoticons[i - (start_i + 1)].lower()
not in self.lexicon
):
s = self.constants.scalar_inc_dec(
words_and_emoticons[i - (start_i + 1)], valence, is_cap_diff
)
if start_i == 1 and s != 0:
s = s * 0.95
if start_i == 2 and s != 0:
s = s * 0.9
valence = valence + s
valence = self._never_check(
valence, words_and_emoticons, start_i, i
)
if start_i == 2:
valence = self._idioms_check(valence, words_and_emoticons, i)
valence = self._least_check(valence, words_and_emoticons, i)
sentiments.append(valence)
return sentiments
",[],0,[],/sentiment/vader.py_sentiment_valence
2965,/home/amandapotts/git/nltk/nltk/sentiment/vader.py__least_check,"def _least_check(self, valence, words_and_emoticons, i):
if (
i > 1
and words_and_emoticons[i - 1].lower() not in self.lexicon
and words_and_emoticons[i - 1].lower() == ""least""
):
if (
words_and_emoticons[i - 2].lower() != ""at""
and words_and_emoticons[i - 2].lower() != ""very""
):
valence = valence * self.constants.N_SCALAR
elif (
i > 0
and words_and_emoticons[i - 1].lower() not in self.lexicon
and words_and_emoticons[i - 1].lower() == ""least""
):
valence = valence * self.constants.N_SCALAR
return valence
",[],0,[],/sentiment/vader.py__least_check
2966,/home/amandapotts/git/nltk/nltk/sentiment/vader.py__but_check,"def _but_check(self, words_and_emoticons, sentiments):
words_and_emoticons = [w_e.lower() for w_e in words_and_emoticons]
but = {""but""} & set(words_and_emoticons)
if but:
bi = words_and_emoticons.index(next(iter(but)))
for sidx, sentiment in enumerate(sentiments):
if sidx < bi:
sentiments[sidx] = sentiment * 0.5
elif sidx > bi:
sentiments[sidx] = sentiment * 1.5
return sentiments
",[],0,[],/sentiment/vader.py__but_check
2967,/home/amandapotts/git/nltk/nltk/sentiment/vader.py__idioms_check,"def _idioms_check(self, valence, words_and_emoticons, i):
onezero = f""{words_and_emoticons[i - 1]} {words_and_emoticons[i]}""
twoonezero = ""{} {} {}"".format(
words_and_emoticons[i - 2],
words_and_emoticons[i - 1],
words_and_emoticons[i],
)
twoone = f""{words_and_emoticons[i - 2]} {words_and_emoticons[i - 1]}""
threetwoone = ""{} {} {}"".format(
words_and_emoticons[i - 3],
words_and_emoticons[i - 2],
words_and_emoticons[i - 1],
)
threetwo = ""{} {}"".format(
words_and_emoticons[i - 3], words_and_emoticons[i - 2]
)
sequences = [onezero, twoonezero, twoone, threetwoone, threetwo]
for seq in sequences:
if seq in self.constants.SPECIAL_CASE_IDIOMS:
valence = self.constants.SPECIAL_CASE_IDIOMS[seq]
break
if len(words_and_emoticons) - 1 > i:
zeroone = f""{words_and_emoticons[i]} {words_and_emoticons[i + 1]}""
if zeroone in self.constants.SPECIAL_CASE_IDIOMS:
valence = self.constants.SPECIAL_CASE_IDIOMS[zeroone]
if len(words_and_emoticons) - 1 > i + 1:
zeroonetwo = ""{} {} {}"".format(
words_and_emoticons[i],
words_and_emoticons[i + 1],
words_and_emoticons[i + 2],
)
if zeroonetwo in self.constants.SPECIAL_CASE_IDIOMS:
valence = self.constants.SPECIAL_CASE_IDIOMS[zeroonetwo]
if (
threetwo in self.constants.BOOSTER_DICT
or twoone in self.constants.BOOSTER_DICT
):
valence = valence + self.constants.B_DECR
return valence
",[],0,[],/sentiment/vader.py__idioms_check
2968,/home/amandapotts/git/nltk/nltk/sentiment/vader.py__never_check,"def _never_check(self, valence, words_and_emoticons, start_i, i):
if start_i == 0:
if self.constants.negated([words_and_emoticons[i - 1]]):
valence = valence * self.constants.N_SCALAR
if start_i == 1:
if words_and_emoticons[i - 2] == ""never"" and (
words_and_emoticons[i - 1] == ""so""
or words_and_emoticons[i - 1] == ""this""
):
valence = valence * 1.5
elif self.constants.negated([words_and_emoticons[i - (start_i + 1)]]):
valence = valence * self.constants.N_SCALAR
if start_i == 2:
if (
words_and_emoticons[i - 3] == ""never""
and (
words_and_emoticons[i - 2] == ""so""
or words_and_emoticons[i - 2] == ""this""
)
or (
words_and_emoticons[i - 1] == ""so""
or words_and_emoticons[i - 1] == ""this""
)
):
valence = valence * 1.25
elif self.constants.negated([words_and_emoticons[i - (start_i + 1)]]):
valence = valence * self.constants.N_SCALAR
return valence
",[],0,[],/sentiment/vader.py__never_check
2969,/home/amandapotts/git/nltk/nltk/sentiment/vader.py__punctuation_emphasis,"def _punctuation_emphasis(self, sum_s, text):
ep_amplifier = self._amplify_ep(text)
qm_amplifier = self._amplify_qm(text)
punct_emph_amplifier = ep_amplifier + qm_amplifier
return punct_emph_amplifier
",[],0,[],/sentiment/vader.py__punctuation_emphasis
2970,/home/amandapotts/git/nltk/nltk/sentiment/vader.py__amplify_ep,"def _amplify_ep(self, text):
ep_count = text.count(""!"")
if ep_count > 4:
ep_count = 4
ep_amplifier = ep_count * 0.292
return ep_amplifier
",[],0,[],/sentiment/vader.py__amplify_ep
2971,/home/amandapotts/git/nltk/nltk/sentiment/vader.py__amplify_qm,"def _amplify_qm(self, text):
qm_count = text.count(""?"")
qm_amplifier = 0
if qm_count > 1:
if qm_count <= 3:
qm_amplifier = qm_count * 0.18
else:
qm_amplifier = 0.96
return qm_amplifier
",[],0,[],/sentiment/vader.py__amplify_qm
2972,/home/amandapotts/git/nltk/nltk/sentiment/vader.py__sift_sentiment_scores,"def _sift_sentiment_scores(self, sentiments):
pos_sum = 0.0
neg_sum = 0.0
neu_count = 0
for sentiment_score in sentiments:
if sentiment_score > 0:
pos_sum += (
float(sentiment_score) + 1
)  # compensates for neutral words that are counted as 1
if sentiment_score < 0:
neg_sum += (
float(sentiment_score) - 1
)  # when used with math.fabs(), compensates for neutrals
if sentiment_score == 0:
neu_count += 1
return pos_sum, neg_sum, neu_count
",[],0,[],/sentiment/vader.py__sift_sentiment_scores
2973,/home/amandapotts/git/nltk/nltk/sentiment/vader.py_score_valence,"def score_valence(self, sentiments, text):
if sentiments:
sum_s = float(sum(sentiments))
punct_emph_amplifier = self._punctuation_emphasis(sum_s, text)
if sum_s > 0:
sum_s += punct_emph_amplifier
elif sum_s < 0:
sum_s -= punct_emph_amplifier
compound = self.constants.normalize(sum_s)
pos_sum, neg_sum, neu_count = self._sift_sentiment_scores(sentiments)
if pos_sum > math.fabs(neg_sum):
pos_sum += punct_emph_amplifier
elif pos_sum < math.fabs(neg_sum):
neg_sum -= punct_emph_amplifier
total = pos_sum + math.fabs(neg_sum) + neu_count
pos = math.fabs(pos_sum / total)
neg = math.fabs(neg_sum / total)
neu = math.fabs(neu_count / total)
else:
compound = 0.0
pos = 0.0
neg = 0.0
neu = 0.0
sentiment_dict = {
""neg"": round(neg, 3),
""neu"": round(neu, 3),
""pos"": round(pos, 3),
""compound"": round(compound, 4),
}
return sentiment_dict
",[],0,[],/sentiment/vader.py_score_valence
2974,/home/amandapotts/git/nltk/nltk/sentiment/sentiment_analyzer.py___init__,"def __init__(self, classifier=None):
self.feat_extractors = defaultdict(list)
self.classifier = classifier
",[],0,[],/sentiment/sentiment_analyzer.py___init__
2975,/home/amandapotts/git/nltk/nltk/sentiment/sentiment_analyzer.py_all_words,"def all_words(self, documents, labeled=None):
""""""
Return all words/tokens from the documents (with duplicates).
:param documents: a list of (words, label) tuples.
:param labeled: if `True`, assume that each document is represented by a
(words, label) tuple: (list(str), str). If `False`, each document is
considered as being a simple list of strings: list(str).
:rtype: list(str)
:return: A list of all words/tokens in `documents`.
""""""
all_words = []
if labeled is None:
labeled = documents and isinstance(documents[0], tuple)
if labeled:
for words, _sentiment in documents:
all_words.extend(words)
elif not labeled:
for words in documents:
all_words.extend(words)
return all_words
",[],0,[],/sentiment/sentiment_analyzer.py_all_words
2976,/home/amandapotts/git/nltk/nltk/sentiment/sentiment_analyzer.py_apply_features,"def apply_features(self, documents, labeled=None):
""""""
Apply all feature extractor functions to the documents. This is a wrapper
around `nltk.classify.util.apply_features`.
If `labeled=False`, return featuresets as:
[feature_func(doc) for doc in documents]
If `labeled=True`, return featuresets as:
[(feature_func(tok), label) for (tok, label) in toks]
:param documents: a list of documents. `If labeled=True`, the method expects
a list of (words, label) tuples.
:rtype: LazyMap
""""""
return apply_features(self.extract_features, documents, labeled)
",[],0,[],/sentiment/sentiment_analyzer.py_apply_features
2977,/home/amandapotts/git/nltk/nltk/sentiment/sentiment_analyzer.py_unigram_word_feats,"def unigram_word_feats(self, words, top_n=None, min_freq=0):
""""""
Return most common top_n word features.
:param words: a list of words/tokens.
:param top_n: number of best words/tokens to use, sorted by frequency.
:rtype: list(str)
:return: A list of `top_n` words/tokens (with no duplicates) sorted by
frequency.
""""""
unigram_feats_freqs = FreqDist(word for word in words)
return [
w
for w, f in unigram_feats_freqs.most_common(top_n)
if unigram_feats_freqs[w] > min_freq
]
",[],0,[],/sentiment/sentiment_analyzer.py_unigram_word_feats
2978,/home/amandapotts/git/nltk/nltk/sentiment/sentiment_analyzer.py_bigram_collocation_feats,"def bigram_collocation_feats(
self, documents, top_n=None, min_freq=3, assoc_measure=BigramAssocMeasures.pmi
",[],0,[],/sentiment/sentiment_analyzer.py_bigram_collocation_feats
2979,/home/amandapotts/git/nltk/nltk/sentiment/sentiment_analyzer.py_classify,"def classify(self, instance):
""""""
Classify a single instance applying the features that have already been
stored in the SentimentAnalyzer.
:param instance: a list (or iterable) of tokens.
:return: the classification result given by applying the classifier.
""""""
instance_feats = self.apply_features([instance], labeled=False)
return self.classifier.classify(instance_feats[0])
",[],0,[],/sentiment/sentiment_analyzer.py_classify
2980,/home/amandapotts/git/nltk/nltk/sentiment/sentiment_analyzer.py_add_feat_extractor,"def add_feat_extractor(self, function, **kwargs):
""""""
Add a new function to extract features from a document. This function will
be used in extract_features().
Important: in this step our kwargs are only representing additional parameters,
and NOT the document we have to parse. The document will always be the first
parameter in the parameter list, and it will be added in the extract_features()
function.
:param function: the extractor function to add to the list of feature extractors.
:param kwargs: additional parameters required by the `function` function.
""""""
self.feat_extractors[function].append(kwargs)
",[],0,[],/sentiment/sentiment_analyzer.py_add_feat_extractor
2981,/home/amandapotts/git/nltk/nltk/sentiment/sentiment_analyzer.py_extract_features,"def extract_features(self, document):
""""""
Apply extractor functions (and their parameters) to the present document.
We pass `document` as the first parameter of the extractor functions.
If we want to use the same extractor function multiple times, we have to
add it to the extractors with `add_feat_extractor` using multiple sets of
parameters (one for each call of the extractor function).
:param document: the document that will be passed as argument to the
feature extractor functions.
:return: A dictionary of populated features extracted from the document.
:rtype: dict
""""""
all_features = {}
for extractor in self.feat_extractors:
for param_set in self.feat_extractors[extractor]:
feats = extractor(document, **param_set)
all_features.update(feats)
return all_features
",[],0,[],/sentiment/sentiment_analyzer.py_extract_features
2982,/home/amandapotts/git/nltk/nltk/sentiment/sentiment_analyzer.py_train,"def train(self, trainer, training_set, save_classifier=None, **kwargs):
""""""
Train classifier on the training set, optionally saving the output in the
file specified by `save_classifier`.
Additional arguments depend on the specific trainer used. For example,
a MaxentClassifier can use `max_iter` parameter to specify the number
of iterations, while a NaiveBayesClassifier cannot.
:param trainer: `train` method of a classifier.
E.g.: NaiveBayesClassifier.train
:param training_set: the training set to be passed as argument to the
classifier `train` method.
:param save_classifier: the filename of the file where the classifier
will be stored (optional).
:param kwargs: additional parameters that will be passed as arguments to
the classifier `train` function.
:return: A classifier instance trained on the training set.
:rtype:
""""""
print(""Training classifier"")
self.classifier = trainer(training_set, **kwargs)
if save_classifier:
self.save_file(self.classifier, save_classifier)
return self.classifier
",[],0,[],/sentiment/sentiment_analyzer.py_train
2983,/home/amandapotts/git/nltk/nltk/sentiment/sentiment_analyzer.py_save_file,"def save_file(self, content, filename):
""""""
Store `content` in `filename`. Can be used to store a SentimentAnalyzer.
""""""
print(""Saving"", filename, file=sys.stderr)
with open(filename, ""wb"") as storage_file:
import pickle
pickle.dump(content, storage_file, protocol=2)
",[],0,[],/sentiment/sentiment_analyzer.py_save_file
2984,/home/amandapotts/git/nltk/nltk/sentiment/sentiment_analyzer.py_evaluate,"def evaluate(
self,
test_set,
classifier=None,
accuracy=True,
f_measure=True,
precision=True,
recall=True,
verbose=False,
",[],0,[],/sentiment/sentiment_analyzer.py_evaluate
2985,/home/amandapotts/git/nltk/nltk/translate/gale_church.py_erfcc,"def erfcc(x):
""""""Complementary error function.""""""
z = abs(x)
t = 1 / (1 + 0.5 * z)
r = t * math.exp(
-z * z
- 1.26551223
+ t
1.00002368
+ t
0.37409196
+ t
0.09678418
+ t
-0.18628806
+ t
0.27886807
+ t
-1.13520398
+ t
)
)
)
)
)
)
)
if x >= 0.0:
return r
else:
return 2.0 - r
",[],0,[],/translate/gale_church.py_erfcc
2986,/home/amandapotts/git/nltk/nltk/translate/gale_church.py_norm_cdf,"def norm_cdf(x):
""""""Return the area under the normal distribution from M{-∞..x}.""""""
return 1 - 0.5 * erfcc(x / math.sqrt(2))
",[],0,[],/translate/gale_church.py_norm_cdf
2987,/home/amandapotts/git/nltk/nltk/translate/gale_church.py_norm_logsf,"def norm_logsf(x):
try:
return math.log(1 - norm_cdf(x))
except ValueError:
return float(""-inf"")
",[],0,[],/translate/gale_church.py_norm_logsf
2988,/home/amandapotts/git/nltk/nltk/translate/gale_church.py_trace,"def trace(backlinks, source_sents_lens, target_sents_lens):
""""""
Traverse the alignment cost from the tracebacks and retrieves
appropriate sentence pairs.
:param backlinks: A dictionary where the key is the alignment points and value is the cost (referencing the LanguageIndependent.PRIORS)
:type backlinks: dict
:param source_sents_lens: A list of target sentences' lengths
:type source_sents_lens: list(int)
:param target_sents_lens: A list of target sentences' lengths
:type target_sents_lens: list(int)
""""""
links = []
position = (len(source_sents_lens), len(target_sents_lens))
while position != (0, 0) and all(p >= 0 for p in position):
try:
s, t = backlinks[position]
except TypeError:
position = (position[0] - 1, position[1] - 1)
continue
for i in range(s):
for j in range(t):
links.append((position[0] - i - 1, position[1] - j - 1))
position = (position[0] - s, position[1] - t)
return links[::-1]
",[],0,[],/translate/gale_church.py_trace
2989,/home/amandapotts/git/nltk/nltk/translate/gale_church.py_align_log_prob,"def align_log_prob(i, j, source_sents, target_sents, alignment, params):
""""""Returns the log probability of the two sentences C{source_sents[i]}, C{target_sents[j]}
being aligned with a specific C{alignment}.
@param i: The offset of the source sentence.
@param j: The offset of the target sentence.
@param source_sents: The list of source sentence lengths.
@param target_sents: The list of target sentence lengths.
@param alignment: The alignment type, a tuple of two integers.
@param params: The sentence alignment parameters.
@returns: The log probability of a specific alignment between the two sentences, given the parameters.
""""""
l_s = sum(source_sents[i - offset - 1] for offset in range(alignment[0]))
l_t = sum(target_sents[j - offset - 1] for offset in range(alignment[1]))
try:
m = (l_s + l_t / params.AVERAGE_CHARACTERS) / 2
delta = (l_s * params.AVERAGE_CHARACTERS - l_t) / math.sqrt(
m * params.VARIANCE_CHARACTERS
)
except ZeroDivisionError:
return float(""-inf"")
return -(LOG2 + norm_logsf(abs(delta)) + math.log(params.PRIORS[alignment]))
",[],0,[],/translate/gale_church.py_align_log_prob
2990,/home/amandapotts/git/nltk/nltk/translate/gale_church.py_align_blocks,"def align_blocks(source_sents_lens, target_sents_lens, params=LanguageIndependent):
""""""Return the sentence alignment of two text blocks (usually paragraphs).
>>> align_blocks([5,5,5], [7,7,7])
[(0, 0), (1, 1), (2, 2)]
>>> align_blocks([10,5,5], [12,20])
[(0, 0), (1, 1), (2, 1)]
>>> align_blocks([12,20], [10,5,5])
[(0, 0), (1, 1), (1, 2)]
>>> align_blocks([10,2,10,10,2,10], [12,3,20,3,12])
[(0, 0), (1, 1), (2, 2), (3, 2), (4, 3), (5, 4)]
@param source_sents_lens: The list of source sentence lengths.
@param target_sents_lens: The list of target sentence lengths.
@param params: the sentence alignment parameters.
@return: The sentence alignments, a list of index pairs.
""""""
alignment_types = list(params.PRIORS.keys())
D = [[]]
backlinks = {}
for i in range(len(source_sents_lens) + 1):
for j in range(len(target_sents_lens) + 1):
min_dist = float(""inf"")
min_align = None
for a in alignment_types:
prev_i = -1 - a[0]
prev_j = j - a[1]
if prev_i < -len(D) or prev_j < 0:
continue
p = D[prev_i][prev_j] + align_log_prob(
i, j, source_sents_lens, target_sents_lens, a, params
)
if p < min_dist:
min_dist = p
min_align = a
if min_dist == float(""inf""):
min_dist = 0
backlinks[(i, j)] = min_align
D[-1].append(min_dist)
if len(D) > 2:
D.pop(0)
D.append([])
return trace(backlinks, source_sents_lens, target_sents_lens)
",[],0,[],/translate/gale_church.py_align_blocks
2991,/home/amandapotts/git/nltk/nltk/translate/gale_church.py_align_texts,"def align_texts(source_blocks, target_blocks, params=LanguageIndependent):
""""""Creates the sentence alignment of two texts.
Texts can consist of several blocks. Block boundaries cannot be crossed by sentence
alignment links.
Each block consists of a list that contains the lengths (in characters) of the sentences
in this block.
@param source_blocks: The list of blocks in the source text.
@param target_blocks: The list of blocks in the target text.
@param params: the sentence alignment parameters.
@returns: A list of sentence alignment lists
""""""
if len(source_blocks) != len(target_blocks):
raise ValueError(
""Source and target texts do not have the same number of blocks.""
)
return [
align_blocks(source_block, target_block, params)
for source_block, target_block in zip(source_blocks, target_blocks)
]
",[],0,[],/translate/gale_church.py_align_texts
2992,/home/amandapotts/git/nltk/nltk/translate/gale_church.py_split_at,"def split_at(it, split_value):
""""""Splits an iterator C{it} at values of C{split_value}.
Each instance of C{split_value} is swallowed. The iterator produces
subiterators which need to be consumed fully before the next subiterator
can be used.
""""""
",[],0,[],/translate/gale_church.py_split_at
2993,/home/amandapotts/git/nltk/nltk/translate/gale_church.py__chunk_iterator,"def _chunk_iterator(first):
v = first
while v != split_value:
yield v
v = it.next()
",[],0,[],/translate/gale_church.py__chunk_iterator
2994,/home/amandapotts/git/nltk/nltk/translate/gale_church.py_parse_token_stream,"def parse_token_stream(stream, soft_delimiter, hard_delimiter):
""""""Parses a stream of tokens and splits it into sentences (using C{soft_delimiter} tokens)
and blocks (using C{hard_delimiter} tokens) for use with the L{align_texts} function.
""""""
return [
[
sum(len(token) for token in sentence_it)
for sentence_it in split_at(block_it, soft_delimiter)
]
for block_it in split_at(stream, hard_delimiter)
]
",[],0,[],/translate/gale_church.py_parse_token_stream
2995,/home/amandapotts/git/nltk/nltk/translate/metrics.py_alignment_error_rate,"def alignment_error_rate(reference, hypothesis, possible=None):
""""""
Return the Alignment Error Rate (AER) of an alignment
with respect to a ""gold standard"" reference alignment.
Return an error rate between 0.0 (perfect alignment) and 1.0 (no
alignment).
>>> from nltk.translate import Alignment
>>> ref = Alignment([(0, 0), (1, 1), (2, 2)])
>>> test = Alignment([(0, 0), (1, 2), (2, 1)])
>>> alignment_error_rate(ref, test) # doctest: +ELLIPSIS
0.6666666666666667
:type reference: Alignment
:param reference: A gold standard alignment (sure alignments)
:type hypothesis: Alignment
:param hypothesis: A hypothesis alignment (aka. candidate alignments)
:type possible: Alignment or None
:param possible: A gold standard reference of possible alignments
(defaults to *reference* if None)
:rtype: float or None
""""""
if possible is None:
possible = reference
else:
assert reference.issubset(possible)  # sanity check
return 1.0 - (len(hypothesis & reference) + len(hypothesis & possible)) / float(
len(hypothesis) + len(reference)
)
",[],0,[],/translate/metrics.py_alignment_error_rate
2996,/home/amandapotts/git/nltk/nltk/translate/ribes_score.py_sentence_ribes,"def sentence_ribes(references, hypothesis, alpha=0.25, beta=0.10):
""""""
The RIBES (Rank-based Intuitive Bilingual Evaluation Score) from
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito Sudoh and
Hajime Tsukada. 2010. ""Automatic Evaluation of Translation Quality for
Distant Language Pairs"". In Proceedings of EMNLP.
https://www.aclweb.org/anthology/D/D10/D10-1092.pdf
The generic RIBES scores used in shared task, e.g. Workshop for
Asian Translation (WAT) uses the following RIBES calculations:
RIBES = kendall_tau * (alpha**p1) * (beta**bp)
Please note that this re-implementation differs from the official
RIBES implementation and though it emulates the results as describe
in the original paper, there are further optimization implemented
in the official RIBES script.
Users are encouraged to use the official RIBES script instead of this
implementation when evaluating your machine translation system. Refer
to https://www.kecl.ntt.co.jp/icl/lirg/ribes/ for the official script.
:param references: a list of reference sentences
:type references: list(list(str))
:param hypothesis: a hypothesis sentence
:type hypothesis: list(str)
:param alpha: hyperparameter used as a prior for the unigram precision.
:type alpha: float
:param beta: hyperparameter used as a prior for the brevity penalty.
:type beta: float
:return: The best ribes score from one of the references.
:rtype: float
""""""
best_ribes = -1.0
for reference in references:
worder = word_rank_alignment(reference, hypothesis)
nkt = kendall_tau(worder)
bp = min(1.0, math.exp(1.0 - len(reference) / len(hypothesis)))
p1 = len(worder) / len(hypothesis)
_ribes = nkt * (p1**alpha) * (bp**beta)
if _ribes > best_ribes:  # Keeps the best score.
best_ribes = _ribes
return best_ribes
",[],0,[],/translate/ribes_score.py_sentence_ribes
2997,/home/amandapotts/git/nltk/nltk/translate/ribes_score.py_corpus_ribes,"def corpus_ribes(list_of_references, hypotheses, alpha=0.25, beta=0.10):
""""""
This function ""calculates RIBES for a system output (hypothesis) with
multiple references, and returns ""best"" score among multi-references and
individual scores. The scores are corpus-wise, i.e., averaged by the number
of sentences."" (c.f. RIBES version 1.03.1 code).
Different from BLEU's micro-average precision, RIBES calculates the
macro-average precision by averaging the best RIBES score for each pair of
hypothesis and its corresponding references
>>> hyp1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which',
...         'ensures', 'that', 'the', 'military', 'always',
...         'obeys', 'the', 'commands', 'of', 'the', 'party']
>>> ref1a = ['It', 'is', 'a', 'guide', 'to', 'action', 'that',
...          'ensures', 'that', 'the', 'military', 'will', 'forever',
...          'heed', 'Party', 'commands']
>>> ref1b = ['It', 'is', 'the', 'guiding', 'principle', 'which',
...          'guarantees', 'the', 'military', 'forces', 'always',
...          'being', 'under', 'the', 'command', 'of', 'the', 'Party']
>>> ref1c = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the',
...          'army', 'always', 'to', 'heed', 'the', 'directions',
...          'of', 'the', 'party']
>>> hyp2 = ['he', 'read', 'the', 'book', 'because', 'he', 'was',
...         'interested', 'in', 'world', 'history']
>>> ref2a = ['he', 'was', 'interested', 'in', 'world', 'history',
...          'because', 'he', 'read', 'the', 'book']
>>> list_of_references = [[ref1a, ref1b, ref1c], [ref2a]]
>>> hypotheses = [hyp1, hyp2]
>>> round(corpus_ribes(list_of_references, hypotheses),4)
0.3597
:param references: a corpus of lists of reference sentences, w.r.t. hypotheses
:type references: list(list(list(str)))
:param hypotheses: a list of hypothesis sentences
:type hypotheses: list(list(str))
:param alpha: hyperparameter used as a prior for the unigram precision.
:type alpha: float
:param beta: hyperparameter used as a prior for the brevity penalty.
:type beta: float
:return: The best ribes score from one of the references.
:rtype: float
""""""
corpus_best_ribes = 0.0
for references, hypothesis in zip(list_of_references, hypotheses):
corpus_best_ribes += sentence_ribes(references, hypothesis, alpha, beta)
return corpus_best_ribes / len(hypotheses)
",[],0,[],/translate/ribes_score.py_corpus_ribes
2998,/home/amandapotts/git/nltk/nltk/translate/ribes_score.py_position_of_ngram,"def position_of_ngram(ngram, sentence):
""""""
This function returns the position of the first instance of the ngram
appearing in a sentence.
Note that one could also use string as follows but the code is a little
convoluted with type casting back and forth:
char_pos = ' '.join(sent)[:' '.join(sent).index(' '.join(ngram))]
word_pos = char_pos.count(' ')
Another way to conceive this is:
return next(i for i, ng in enumerate(ngrams(sentence, len(ngram)))
if ng == ngram)
:param ngram: The ngram that needs to be searched
:type ngram: tuple
:param sentence: The list of tokens to search from.
:type sentence: list(str)
""""""
for i, sublist in enumerate(ngrams(sentence, len(ngram))):
if ngram == sublist:
return i
",[],0,[],/translate/ribes_score.py_position_of_ngram
2999,/home/amandapotts/git/nltk/nltk/translate/ribes_score.py_word_rank_alignment,"def word_rank_alignment(reference, hypothesis, character_based=False):
""""""
This is the word rank alignment algorithm described in the paper to produce
the *worder* list, i.e. a list of word indices of the hypothesis word orders
w.r.t. the list of reference words.
Below is (H0, R0) example from the Isozaki et al. 2010 paper,
note the examples are indexed from 1 but the results here are indexed from 0:
>>> ref = str('he was interested in world history because he '
... 'read the book').split()
>>> hyp = str('he read the book because he was interested in world '
... 'history').split()
>>> word_rank_alignment(ref, hyp)
[7, 8, 9, 10, 6, 0, 1, 2, 3, 4, 5]
The (H1, R1) example from the paper, note the 0th index:
>>> ref = 'John hit Bob yesterday'.split()
>>> hyp = 'Bob hit John yesterday'.split()
>>> word_rank_alignment(ref, hyp)
[2, 1, 0, 3]
Here is the (H2, R2) example from the paper, note the 0th index here too:
>>> ref = 'the boy read the book'.split()
>>> hyp = 'the book was read by the boy'.split()
>>> word_rank_alignment(ref, hyp)
[3, 4, 2, 0, 1]
:param reference: a reference sentence
:type reference: list(str)
:param hypothesis: a hypothesis sentence
:type hypothesis: list(str)
""""""
worder = []
hyp_len = len(hypothesis)
ref_ngrams = []
hyp_ngrams = []
for n in range(1, len(reference) + 1):
for ng in ngrams(reference, n):
ref_ngrams.append(ng)
for ng in ngrams(hypothesis, n):
hyp_ngrams.append(ng)
for i, h_word in enumerate(hypothesis):
if h_word not in reference:
continue
elif hypothesis.count(h_word) == reference.count(h_word) == 1:
worder.append(reference.index(h_word))
else:
max_window_size = max(i, hyp_len - i + 1)
for window in range(1, max_window_size):
if i + window < hyp_len:  # If searching the right context is possible.
right_context_ngram = tuple(islice(hypothesis, i, i + window + 1))
num_times_in_ref = ref_ngrams.count(right_context_ngram)
num_times_in_hyp = hyp_ngrams.count(right_context_ngram)
if num_times_in_ref == num_times_in_hyp == 1:
pos = position_of_ngram(right_context_ngram, reference)
worder.append(pos)  # Add the positions of the ngram.
break
if window <= i:  # If searching the left context is possible.
left_context_ngram = tuple(islice(hypothesis, i - window, i + 1))
num_times_in_ref = ref_ngrams.count(left_context_ngram)
num_times_in_hyp = hyp_ngrams.count(left_context_ngram)
if num_times_in_ref == num_times_in_hyp == 1:
pos = position_of_ngram(left_context_ngram, reference)
worder.append(pos + len(left_context_ngram) - 1)
break
return worder
",[],0,[],/translate/ribes_score.py_word_rank_alignment
3000,/home/amandapotts/git/nltk/nltk/translate/ribes_score.py_find_increasing_sequences,"def find_increasing_sequences(worder):
""""""
Given the *worder* list, this function groups monotonic +1 sequences.
>>> worder = [7, 8, 9, 10, 6, 0, 1, 2, 3, 4, 5]
>>> list(find_increasing_sequences(worder))
[(7, 8, 9, 10), (0, 1, 2, 3, 4, 5)]
:param worder: The worder list output from word_rank_alignment
:param type: list(int)
""""""
items = iter(worder)
a, b = None, next(items, None)
result = [b]
while b is not None:
a, b = b, next(items, None)
if b is not None and a + 1 == b:
result.append(b)
else:
if len(result) > 1:
yield tuple(result)
result = [b]
",[],0,[],/translate/ribes_score.py_find_increasing_sequences
3001,/home/amandapotts/git/nltk/nltk/translate/ribes_score.py_kendall_tau,"def kendall_tau(worder, normalize=True):
""""""
Calculates the Kendall's Tau correlation coefficient given the *worder*
list of word alignments from word_rank_alignment(), using the formula:
tau = 2 * num_increasing_pairs / num_possible_pairs -1
Note that the no. of increasing pairs can be discontinuous in the *worder*
list and each each increasing sequence can be tabulated as choose(len(seq), 2)
no. of increasing pairs, e.g.
>>> worder = [7, 8, 9, 10, 6, 0, 1, 2, 3, 4, 5]
>>> number_possible_pairs = choose(len(worder), 2)
>>> round(kendall_tau(worder, normalize=False),3)
-0.236
>>> round(kendall_tau(worder),3)
0.382
:param worder: The worder list output from word_rank_alignment
:type worder: list(int)
:param normalize: Flag to indicate normalization to between 0.0 and 1.0.
:type normalize: boolean
:return: The Kendall's Tau correlation coefficient.
:rtype: float
""""""
worder_len = len(worder)
if worder_len < 2:
tau = -1
else:
increasing_sequences = find_increasing_sequences(worder)
num_increasing_pairs = sum(choose(len(seq), 2) for seq in increasing_sequences)
num_possible_pairs = choose(worder_len, 2)
tau = 2 * num_increasing_pairs / num_possible_pairs - 1
if normalize:  # If normalized, the tau output falls between 0.0 to 1.0
return (tau + 1) / 2
else:  # Otherwise, the tau outputs falls between -1.0 to +1.0
return tau
",[],0,[],/translate/ribes_score.py_kendall_tau
3002,/home/amandapotts/git/nltk/nltk/translate/ribes_score.py_spearman_rho,"def spearman_rho(worder, normalize=True):
""""""
Calculates the Spearman's Rho correlation coefficient given the *worder*
list of word alignment from word_rank_alignment(), using the formula:
rho = 1 - sum(d**2) / choose(len(worder)+1, 3)
Given that d is the sum of difference between the *worder* list of indices
and the original word indices from the reference sentence.
Using the (H0,R0) and (H5, R5) example from the paper
>>> worder =  [7, 8, 9, 10, 6, 0, 1, 2, 3, 4, 5]
>>> round(spearman_rho(worder, normalize=False), 3)
-0.591
>>> round(spearman_rho(worder), 3)
0.205
:param worder: The worder list output from word_rank_alignment
:param type: list(int)
""""""
worder_len = len(worder)
sum_d_square = sum((wi - i) ** 2 for wi, i in zip(worder, range(worder_len)))
rho = 1 - sum_d_square / choose(worder_len + 1, 3)
if normalize:  # If normalized, the rho output falls between 0.0 to 1.0
return (rho + 1) / 2
else:  # Otherwise, the rho outputs falls between -1.0 to +1.0
return rho
",[],0,[],/translate/ribes_score.py_spearman_rho
3003,/home/amandapotts/git/nltk/nltk/translate/ibm5.py___init__,"def __init__(
self,
sentence_aligned_corpus,
iterations,
source_word_classes,
target_word_classes,
probability_tables=None,
",[],0,[],/translate/ibm5.py___init__
3004,/home/amandapotts/git/nltk/nltk/translate/ibm5.py_train,"def train(self, parallel_corpus):
counts = Model5Counts()
for aligned_sentence in parallel_corpus:
l = len(aligned_sentence.mots)
m = len(aligned_sentence.words)
sampled_alignments, best_alignment = self.sample(aligned_sentence)
aligned_sentence.alignment = Alignment(
best_alignment.zero_indexed_alignment()
)
total_count = self.prob_of_alignments(sampled_alignments)
for alignment_info in sampled_alignments:
count = self.prob_t_a_given_s(alignment_info)
normalized_count = count / total_count
for j in range(1, m + 1):
counts.update_lexical_translation(
normalized_count, alignment_info, j
)
slots = Slots(m)
for i in range(1, l + 1):
counts.update_vacancy(
normalized_count, alignment_info, i, self.trg_classes, slots
)
counts.update_null_generation(normalized_count, alignment_info)
counts.update_fertility(normalized_count, alignment_info)
existing_alignment_table = self.alignment_table
self.reset_probabilities()
self.alignment_table = existing_alignment_table  # don't retrain
self.maximize_lexical_translation_probabilities(counts)
self.maximize_vacancy_probabilities(counts)
self.maximize_fertility_probabilities(counts)
self.maximize_null_generation_probabilities(counts)
",[],0,[],/translate/ibm5.py_train
3005,/home/amandapotts/git/nltk/nltk/translate/ibm5.py_sample,"def sample(self, sentence_pair):
""""""
Sample the most probable alignments from the entire alignment
space according to Model 4
Note that Model 4 scoring is used instead of Model 5 because the
latter is too expensive to compute.
First, determine the best alignment according to IBM Model 2.
With this initial alignment, use hill climbing to determine the
best alignment according to a IBM Model 4. Add this
alignment and its neighbors to the sample set. Repeat this
process with other initial alignments obtained by pegging an
alignment point. Finally, prune alignments that have
substantially lower Model 4 scores than the best alignment.
:param sentence_pair: Source and target language sentence pair
to generate a sample of alignments from
:type sentence_pair: AlignedSent
:return: A set of best alignments represented by their ``AlignmentInfo``
and the best alignment of the set for convenience
:rtype: set(AlignmentInfo), AlignmentInfo
""""""
sampled_alignments, best_alignment = super().sample(sentence_pair)
return self.prune(sampled_alignments), best_alignment
",[],0,[],/translate/ibm5.py_sample
3006,/home/amandapotts/git/nltk/nltk/translate/ibm5.py_prune,"def prune(self, alignment_infos):
""""""
Removes alignments from ``alignment_infos`` that have
substantially lower Model 4 scores than the best alignment
:return: Pruned alignments
:rtype: set(AlignmentInfo)
""""""
alignments = []
best_score = 0
for alignment_info in alignment_infos:
score = IBMModel4.model4_prob_t_a_given_s(alignment_info, self)
best_score = max(score, best_score)
alignments.append((alignment_info, score))
threshold = IBMModel5.MIN_SCORE_FACTOR * best_score
alignments = [a[0] for a in alignments if a[1] > threshold]
return set(alignments)
",[],0,[],/translate/ibm5.py_prune
3007,/home/amandapotts/git/nltk/nltk/translate/ibm5.py_hillclimb,"def hillclimb(self, alignment_info, j_pegged=None):
""""""
Starting from the alignment in ``alignment_info``, look at
neighboring alignments iteratively for the best one, according
to Model 4
Note that Model 4 scoring is used instead of Model 5 because the
latter is too expensive to compute.
There is no guarantee that the best alignment in the alignment
space will be found, because the algorithm might be stuck in a
local maximum.
:param j_pegged: If specified, the search will be constrained to
alignments where ``j_pegged`` remains unchanged
:type j_pegged: int
:return: The best alignment found from hill climbing
:rtype: AlignmentInfo
""""""
alignment = alignment_info  # alias with shorter name
max_probability = IBMModel4.model4_prob_t_a_given_s(alignment, self)
while True:
old_alignment = alignment
for neighbor_alignment in self.neighboring(alignment, j_pegged):
neighbor_probability = IBMModel4.model4_prob_t_a_given_s(
neighbor_alignment, self
)
if neighbor_probability > max_probability:
alignment = neighbor_alignment
max_probability = neighbor_probability
if alignment == old_alignment:
break
alignment.score = max_probability
return alignment
",[],0,[],/translate/ibm5.py_hillclimb
3008,/home/amandapotts/git/nltk/nltk/translate/ibm5.py_prob_t_a_given_s,"def prob_t_a_given_s(self, alignment_info):
""""""
Probability of target sentence and an alignment given the
source sentence
""""""
probability = 1.0
MIN_PROB = IBMModel.MIN_PROB
slots = Slots(len(alignment_info.trg_sentence) - 1)
",[],0,[],/translate/ibm5.py_prob_t_a_given_s
3009,/home/amandapotts/git/nltk/nltk/translate/ibm5.py_null_generation_term,"def null_generation_term():
value = 1.0
p1 = self.p1
p0 = 1 - p1
null_fertility = alignment_info.fertility_of_i(0)
m = len(alignment_info.trg_sentence) - 1
value *= pow(p1, null_fertility) * pow(p0, m - 2 * null_fertility)
if value < MIN_PROB:
return MIN_PROB
for i in range(1, null_fertility + 1):
value *= (m - null_fertility - i + 1) / i
return value
",[],0,[],/translate/ibm5.py_null_generation_term
3010,/home/amandapotts/git/nltk/nltk/translate/ibm5.py_fertility_term,"def fertility_term():
value = 1.0
src_sentence = alignment_info.src_sentence
for i in range(1, len(src_sentence)):
fertility = alignment_info.fertility_of_i(i)
value *= (
factorial(fertility)
)
if value < MIN_PROB:
return MIN_PROB
return value
",[],0,[],/translate/ibm5.py_fertility_term
3011,/home/amandapotts/git/nltk/nltk/translate/ibm5.py_lexical_translation_term,"def lexical_translation_term(j):
t = alignment_info.trg_sentence[j]
i = alignment_info.alignment[j]
s = alignment_info.src_sentence[i]
return self.translation_table[t][s]
",[],0,[],/translate/ibm5.py_lexical_translation_term
3012,/home/amandapotts/git/nltk/nltk/translate/ibm5.py_vacancy_term,"def vacancy_term(i):
value = 1.0
tablet = alignment_info.cepts[i]
tablet_length = len(tablet)
total_vacancies = slots.vacancies_at(len(slots))
if tablet_length == 0:
return value
j = tablet[0]
previous_cept = alignment_info.previous_cept(j)
previous_center = alignment_info.center_of_cept(previous_cept)
dv = slots.vacancies_at(j) - slots.vacancies_at(previous_center)
max_v = total_vacancies - tablet_length + 1
trg_class = self.trg_classes[alignment_info.trg_sentence[j]]
value *= self.head_vacancy_table[dv][max_v][trg_class]
slots.occupy(j)  # mark position as occupied
total_vacancies -= 1
if value < MIN_PROB:
return MIN_PROB
for k in range(1, tablet_length):
previous_position = tablet[k - 1]
previous_vacancies = slots.vacancies_at(previous_position)
j = tablet[k]
dv = slots.vacancies_at(j) - previous_vacancies
max_v = total_vacancies - tablet_length + k + 1 - previous_vacancies
trg_class = self.trg_classes[alignment_info.trg_sentence[j]]
value *= self.non_head_vacancy_table[dv][max_v][trg_class]
slots.occupy(j)  # mark position as occupied
total_vacancies -= 1
if value < MIN_PROB:
return MIN_PROB
return value
",[],0,[],/translate/ibm5.py_vacancy_term
3013,/home/amandapotts/git/nltk/nltk/translate/ibm5.py_maximize_vacancy_probabilities,"def maximize_vacancy_probabilities(self, counts):
MIN_PROB = IBMModel.MIN_PROB
head_vacancy_table = self.head_vacancy_table
for dv, max_vs in counts.head_vacancy.items():
for max_v, trg_classes in max_vs.items():
for t_cls in trg_classes:
estimate = (
counts.head_vacancy[dv][max_v][t_cls]
/ counts.head_vacancy_for_any_dv[max_v][t_cls]
)
head_vacancy_table[dv][max_v][t_cls] = max(estimate, MIN_PROB)
non_head_vacancy_table = self.non_head_vacancy_table
for dv, max_vs in counts.non_head_vacancy.items():
for max_v, trg_classes in max_vs.items():
for t_cls in trg_classes:
estimate = (
counts.non_head_vacancy[dv][max_v][t_cls]
/ counts.non_head_vacancy_for_any_dv[max_v][t_cls]
)
non_head_vacancy_table[dv][max_v][t_cls] = max(estimate, MIN_PROB)
",[],0,[],/translate/ibm5.py_maximize_vacancy_probabilities
3014,/home/amandapotts/git/nltk/nltk/translate/ibm5.py_update_vacancy,"def update_vacancy(self, count, alignment_info, i, trg_classes, slots):
""""""
:param count: Value to add to the vacancy counts
:param alignment_info: Alignment under consideration
:param i: Source word position under consideration
:param trg_classes: Target word classes
:param slots: Vacancy states of the slots in the target sentence.
Output parameter that will be modified as new words are placed
in the target sentence.
""""""
tablet = alignment_info.cepts[i]
tablet_length = len(tablet)
total_vacancies = slots.vacancies_at(len(slots))
if tablet_length == 0:
return  # ignore zero fertility words
j = tablet[0]
previous_cept = alignment_info.previous_cept(j)
previous_center = alignment_info.center_of_cept(previous_cept)
dv = slots.vacancies_at(j) - slots.vacancies_at(previous_center)
max_v = total_vacancies - tablet_length + 1
trg_class = trg_classes[alignment_info.trg_sentence[j]]
self.head_vacancy[dv][max_v][trg_class] += count
self.head_vacancy_for_any_dv[max_v][trg_class] += count
slots.occupy(j)  # mark position as occupied
total_vacancies -= 1
for k in range(1, tablet_length):
previous_position = tablet[k - 1]
previous_vacancies = slots.vacancies_at(previous_position)
j = tablet[k]
dv = slots.vacancies_at(j) - previous_vacancies
max_v = total_vacancies - tablet_length + k + 1 - previous_vacancies
trg_class = trg_classes[alignment_info.trg_sentence[j]]
self.non_head_vacancy[dv][max_v][trg_class] += count
self.non_head_vacancy_for_any_dv[max_v][trg_class] += count
slots.occupy(j)  # mark position as occupied
total_vacancies -= 1
",[],0,[],/translate/ibm5.py_update_vacancy
3015,/home/amandapotts/git/nltk/nltk/translate/ibm5.py___init__,"def __init__(self, target_sentence_length):
self._slots = [False] * (target_sentence_length + 1)  # 1-indexed
",[],0,[],/translate/ibm5.py___init__
3016,/home/amandapotts/git/nltk/nltk/translate/ibm5.py_occupy,"def occupy(self, position):
""""""
:return: Mark slot at ``position`` as occupied
""""""
self._slots[position] = True
",[],0,[],/translate/ibm5.py_occupy
3017,/home/amandapotts/git/nltk/nltk/translate/ibm5.py_vacancies_at,"def vacancies_at(self, position):
""""""
:return: Number of vacant slots up to, and including, ``position``
""""""
vacancies = 0
for k in range(1, position + 1):
if not self._slots[k]:
vacancies += 1
return vacancies
",[],0,[],/translate/ibm5.py_vacancies_at
3018,/home/amandapotts/git/nltk/nltk/translate/ibm5.py___len__,"def __len__(self):
return len(self._slots) - 1  # exclude dummy zeroeth element
",[],0,[],/translate/ibm5.py___len__
3019,/home/amandapotts/git/nltk/nltk/translate/ibm3.py___init__,"def __init__(self, sentence_aligned_corpus, iterations, probability_tables=None):
""""""
Train on ``sentence_aligned_corpus`` and create a lexical
translation model, a distortion model, a fertility model, and a
model for generating NULL-aligned words.
Translation direction is from ``AlignedSent.mots`` to
``AlignedSent.words``.
:param sentence_aligned_corpus: Sentence-aligned parallel corpus
:type sentence_aligned_corpus: list(AlignedSent)
:param iterations: Number of iterations to run training algorithm
:type iterations: int
:param probability_tables: Optional. Use this to pass in custom
probability values. If not specified, probabilities will be
set to a uniform distribution, or some other sensible value.
If specified, all the following entries must be present:
``translation_table``, ``alignment_table``,
``fertility_table``, ``p1``, ``distortion_table``.
See ``IBMModel`` for the type and purpose of these tables.
:type probability_tables: dict[str]: object
""""""
super().__init__(sentence_aligned_corpus)
self.reset_probabilities()
if probability_tables is None:
ibm2 = IBMModel2(sentence_aligned_corpus, iterations)
self.translation_table = ibm2.translation_table
self.alignment_table = ibm2.alignment_table
self.set_uniform_probabilities(sentence_aligned_corpus)
else:
self.translation_table = probability_tables[""translation_table""]
self.alignment_table = probability_tables[""alignment_table""]
self.fertility_table = probability_tables[""fertility_table""]
self.p1 = probability_tables[""p1""]
self.distortion_table = probability_tables[""distortion_table""]
for n in range(0, iterations):
self.train(sentence_aligned_corpus)
",[],0,[],/translate/ibm3.py___init__
3020,/home/amandapotts/git/nltk/nltk/translate/ibm3.py_train,"def train(self, parallel_corpus):
counts = Model3Counts()
for aligned_sentence in parallel_corpus:
l = len(aligned_sentence.mots)
m = len(aligned_sentence.words)
sampled_alignments, best_alignment = self.sample(aligned_sentence)
aligned_sentence.alignment = Alignment(
best_alignment.zero_indexed_alignment()
)
total_count = self.prob_of_alignments(sampled_alignments)
for alignment_info in sampled_alignments:
count = self.prob_t_a_given_s(alignment_info)
normalized_count = count / total_count
for j in range(1, m + 1):
counts.update_lexical_translation(
normalized_count, alignment_info, j
)
counts.update_distortion(normalized_count, alignment_info, j, l, m)
counts.update_null_generation(normalized_count, alignment_info)
counts.update_fertility(normalized_count, alignment_info)
existing_alignment_table = self.alignment_table
self.reset_probabilities()
self.alignment_table = existing_alignment_table  # don't retrain
self.maximize_lexical_translation_probabilities(counts)
self.maximize_distortion_probabilities(counts)
self.maximize_fertility_probabilities(counts)
self.maximize_null_generation_probabilities(counts)
",[],0,[],/translate/ibm3.py_train
3021,/home/amandapotts/git/nltk/nltk/translate/ibm3.py_maximize_distortion_probabilities,"def maximize_distortion_probabilities(self, counts):
MIN_PROB = IBMModel.MIN_PROB
for j, i_s in counts.distortion.items():
for i, src_sentence_lengths in i_s.items():
for l, trg_sentence_lengths in src_sentence_lengths.items():
for m in trg_sentence_lengths:
estimate = (
counts.distortion[j][i][l][m]
/ counts.distortion_for_any_j[i][l][m]
)
self.distortion_table[j][i][l][m] = max(estimate, MIN_PROB)
",[],0,[],/translate/ibm3.py_maximize_distortion_probabilities
3022,/home/amandapotts/git/nltk/nltk/translate/ibm3.py_prob_t_a_given_s,"def prob_t_a_given_s(self, alignment_info):
""""""
Probability of target sentence and an alignment given the
source sentence
""""""
src_sentence = alignment_info.src_sentence
trg_sentence = alignment_info.trg_sentence
l = len(src_sentence) - 1  # exclude NULL
m = len(trg_sentence) - 1
p1 = self.p1
p0 = 1 - p1
probability = 1.0
MIN_PROB = IBMModel.MIN_PROB
null_fertility = alignment_info.fertility_of_i(0)
probability *= pow(p1, null_fertility) * pow(p0, m - 2 * null_fertility)
if probability < MIN_PROB:
return MIN_PROB
for i in range(1, null_fertility + 1):
probability *= (m - null_fertility - i + 1) / i
if probability < MIN_PROB:
return MIN_PROB
for i in range(1, l + 1):
fertility = alignment_info.fertility_of_i(i)
probability *= (
factorial(fertility) * self.fertility_table[fertility][src_sentence[i]]
)
if probability < MIN_PROB:
return MIN_PROB
for j in range(1, m + 1):
t = trg_sentence[j]
i = alignment_info.alignment[j]
s = src_sentence[i]
probability *= (
self.translation_table[t][s] * self.distortion_table[j][i][l][m]
)
if probability < MIN_PROB:
return MIN_PROB
return probability
",[],0,[],/translate/ibm3.py_prob_t_a_given_s
3023,/home/amandapotts/git/nltk/nltk/translate/ibm3.py_update_distortion,"def update_distortion(self, count, alignment_info, j, l, m):
i = alignment_info.alignment[j]
self.distortion[j][i][l][m] += count
self.distortion_for_any_j[i][l][m] += count
",[],0,[],/translate/ibm3.py_update_distortion
3024,/home/amandapotts/git/nltk/nltk/translate/ibm1.py___init__,"def __init__(self, sentence_aligned_corpus, iterations, probability_tables=None):
""""""
Train on ``sentence_aligned_corpus`` and create a lexical
translation model.
Translation direction is from ``AlignedSent.mots`` to
``AlignedSent.words``.
:param sentence_aligned_corpus: Sentence-aligned parallel corpus
:type sentence_aligned_corpus: list(AlignedSent)
:param iterations: Number of iterations to run training algorithm
:type iterations: int
:param probability_tables: Optional. Use this to pass in custom
probability values. If not specified, probabilities will be
set to a uniform distribution, or some other sensible value.
If specified, the following entry must be present:
``translation_table``.
See ``IBMModel`` for the type and purpose of this table.
:type probability_tables: dict[str]: object
""""""
super().__init__(sentence_aligned_corpus)
if probability_tables is None:
self.set_uniform_probabilities(sentence_aligned_corpus)
else:
self.translation_table = probability_tables[""translation_table""]
for n in range(0, iterations):
self.train(sentence_aligned_corpus)
self.align_all(sentence_aligned_corpus)
",[],0,[],/translate/ibm1.py___init__
3025,/home/amandapotts/git/nltk/nltk/translate/ibm1.py_train,"def train(self, parallel_corpus):
counts = Counts()
for aligned_sentence in parallel_corpus:
trg_sentence = aligned_sentence.words
src_sentence = [None] + aligned_sentence.mots
total_count = self.prob_all_alignments(src_sentence, trg_sentence)
for t in trg_sentence:
for s in src_sentence:
count = self.prob_alignment_point(s, t)
normalized_count = count / total_count[t]
counts.t_given_s[t][s] += normalized_count
counts.any_t_given_s[s] += normalized_count
self.maximize_lexical_translation_probabilities(counts)
",[],0,[],/translate/ibm1.py_train
3026,/home/amandapotts/git/nltk/nltk/translate/ibm1.py_prob_all_alignments,"def prob_all_alignments(self, src_sentence, trg_sentence):
""""""
Computes the probability of all possible word alignments,
expressed as a marginal distribution over target words t
Each entry in the return value represents the contribution to
the total alignment probability by the target word t.
To obtain probability(alignment | src_sentence, trg_sentence),
simply sum the entries in the return value.
:return: Probability of t for all s in ``src_sentence``
:rtype: dict(str): float
""""""
alignment_prob_for_t = defaultdict(float)
for t in trg_sentence:
for s in src_sentence:
alignment_prob_for_t[t] += self.prob_alignment_point(s, t)
return alignment_prob_for_t
",[],0,[],/translate/ibm1.py_prob_all_alignments
3027,/home/amandapotts/git/nltk/nltk/translate/ibm1.py_prob_alignment_point,"def prob_alignment_point(self, s, t):
""""""
Probability that word ``t`` in the target sentence is aligned to
word ``s`` in the source sentence
""""""
return self.translation_table[t][s]
",[],0,[],/translate/ibm1.py_prob_alignment_point
3028,/home/amandapotts/git/nltk/nltk/translate/ibm1.py_prob_t_a_given_s,"def prob_t_a_given_s(self, alignment_info):
""""""
Probability of target sentence and an alignment given the
source sentence
""""""
prob = 1.0
for j, i in enumerate(alignment_info.alignment):
if j == 0:
continue  # skip the dummy zeroeth element
trg_word = alignment_info.trg_sentence[j]
src_word = alignment_info.src_sentence[i]
prob *= self.translation_table[trg_word][src_word]
return max(prob, IBMModel.MIN_PROB)
",[],0,[],/translate/ibm1.py_prob_t_a_given_s
3029,/home/amandapotts/git/nltk/nltk/translate/ibm1.py_align_all,"def align_all(self, parallel_corpus):
for sentence_pair in parallel_corpus:
self.align(sentence_pair)
",[],0,[],/translate/ibm1.py_align_all
3030,/home/amandapotts/git/nltk/nltk/translate/ibm1.py_align,"def align(self, sentence_pair):
""""""
Determines the best word alignment for one sentence pair from
the corpus that the model was trained on.
The best alignment will be set in ``sentence_pair`` when the
method returns. In contrast with the internal implementation of
IBM models, the word indices in the ``Alignment`` are zero-
indexed, not one-indexed.
:param sentence_pair: A sentence in the source language and its
counterpart sentence in the target language
:type sentence_pair: AlignedSent
""""""
best_alignment = []
for j, trg_word in enumerate(sentence_pair.words):
best_prob = max(self.translation_table[trg_word][None], IBMModel.MIN_PROB)
best_alignment_point = None
for i, src_word in enumerate(sentence_pair.mots):
align_prob = self.translation_table[trg_word][src_word]
if align_prob >= best_prob:  # prefer newer word in case of tie
best_prob = align_prob
best_alignment_point = i
best_alignment.append((j, best_alignment_point))
sentence_pair.alignment = Alignment(best_alignment)
",[],0,[],/translate/ibm1.py_align
3031,/home/amandapotts/git/nltk/nltk/translate/gleu_score.py_sentence_gleu,"def sentence_gleu(references, hypothesis, min_len=1, max_len=4):
""""""
Calculates the sentence level GLEU (Google-BLEU) score described in
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi,
Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey,
Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser,
Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens,
George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith,
Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,
Jeffrey Dean. (2016) Google’s Neural Machine Translation System:
Bridging the Gap between Human and Machine Translation.
eprint arXiv:1609.08144. https://arxiv.org/pdf/1609.08144v2.pdf
Retrieved on 27 Oct 2016.
From Wu et al. (2016):
""The BLEU score has some undesirable properties when used for single
sentences, as it was designed to be a corpus measure. We therefore
use a slightly different score for our RL experiments which we call
the 'GLEU score'. For the GLEU score, we record all sub-sequences of
1, 2, 3 or 4 tokens in output and target sequence (n-grams). We then
compute a recall, which is the ratio of the number of matching n-grams
to the number of total n-grams in the target (ground truth) sequence,
and a precision, which is the ratio of the number of matching n-grams
to the number of total n-grams in the generated output sequence. Then
GLEU score is simply the minimum of recall and precision. This GLEU
score's range is always between 0 (no matches) and 1 (all match) and
it is symmetrical when switching output and target. According to
our experiments, GLEU score correlates quite well with the BLEU
metric on a corpus level but does not have its drawbacks for our per
sentence reward objective.""
Note: The initial implementation only allowed a single reference, but now
a list of references is required (which is consistent with
bleu_score.sentence_bleu()).
The infamous ""the the the ... "" example
>>> ref = 'the cat is on the mat'.split()
>>> hyp = 'the the the the the the the'.split()
>>> sentence_gleu([ref], hyp)  # doctest: +ELLIPSIS
0.0909...
An example to evaluate normal machine translation outputs
>>> ref1 = str('It is a guide to action that ensures that the military '
...            'will forever heed Party commands').split()
>>> hyp1 = str('It is a guide to action which ensures that the military '
...            'always obeys the commands of the party').split()
>>> hyp2 = str('It is to insure the troops forever hearing the activity '
...            'guidebook that party direct').split()
>>> sentence_gleu([ref1], hyp1) # doctest: +ELLIPSIS
0.4393...
>>> sentence_gleu([ref1], hyp2) # doctest: +ELLIPSIS
0.1206...
:param references: a list of reference sentences
:type references: list(list(str))
:param hypothesis: a hypothesis sentence
:type hypothesis: list(str)
:param min_len: The minimum order of n-gram this function should extract.
:type min_len: int
:param max_len: The maximum order of n-gram this function should extract.
:type max_len: int
:return: the sentence level GLEU score.
:rtype: float
""""""
return corpus_gleu([references], [hypothesis], min_len=min_len, max_len=max_len)
",[],0,[],/translate/gleu_score.py_sentence_gleu
3032,/home/amandapotts/git/nltk/nltk/translate/bleu_score.py___new__,"def __new__(cls, numerator=0, denominator=None, _normalize=False):
if sys.version_info >= (3, 12):
self = super().__new__(cls, numerator, denominator)
else:
self = super().__new__(cls, numerator, denominator, _normalize=_normalize)
self._normalize = _normalize
self._original_numerator = numerator
self._original_denominator = denominator
return self
",[],0,[],/translate/bleu_score.py___new__
3033,/home/amandapotts/git/nltk/nltk/translate/bleu_score.py_numerator,"def numerator(self):
if not self._normalize:
return self._original_numerator
return super().numerator
",[],0,[],/translate/bleu_score.py_numerator
3034,/home/amandapotts/git/nltk/nltk/translate/bleu_score.py_denominator,"def denominator(self):
if not self._normalize:
return self._original_denominator
return super().denominator
",[],0,[],/translate/bleu_score.py_denominator
3035,/home/amandapotts/git/nltk/nltk/translate/bleu_score.py_sentence_bleu,"def sentence_bleu(
references,
hypothesis,
weights=(0.25, 0.25, 0.25, 0.25),
smoothing_function=None,
auto_reweigh=False,
",[],0,[],/translate/bleu_score.py_sentence_bleu
3036,/home/amandapotts/git/nltk/nltk/translate/bleu_score.py_corpus_bleu,"def corpus_bleu(
list_of_references,
hypotheses,
weights=(0.25, 0.25, 0.25, 0.25),
smoothing_function=None,
auto_reweigh=False,
",[],0,[],/translate/bleu_score.py_corpus_bleu
3037,/home/amandapotts/git/nltk/nltk/translate/bleu_score.py_modified_precision,"def modified_precision(references, hypothesis, n):
""""""
Calculate modified ngram precision.
The normal precision method may lead to some wrong translations with
high-precision, e.g., the translation, in which a word of reference
repeats several times, has very high precision.
This function only returns the Fraction object that contains the numerator
and denominator necessary to calculate the corpus-level precision.
To calculate the modified precision for a single pair of hypothesis and
references, cast the Fraction object into a float.
The famous ""the the the ... "" example shows that you can get BLEU precision
by duplicating high frequency words.
>>> reference1 = 'the cat is on the mat'.split()
>>> reference2 = 'there is a cat on the mat'.split()
>>> hypothesis1 = 'the the the the the the the'.split()
>>> references = [reference1, reference2]
>>> float(modified_precision(references, hypothesis1, n=1)) # doctest: +ELLIPSIS
0.2857...
In the modified n-gram precision, a reference word will be considered
exhausted after a matching hypothesis word is identified, e.g.
>>> reference1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'that',
...               'ensures', 'that', 'the', 'military', 'will',
...               'forever', 'heed', 'Party', 'commands']
>>> reference2 = ['It', 'is', 'the', 'guiding', 'principle', 'which',
...               'guarantees', 'the', 'military', 'forces', 'always',
...               'being', 'under', 'the', 'command', 'of', 'the',
...               'Party']
>>> reference3 = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the',
...               'army', 'always', 'to', 'heed', 'the', 'directions',
...               'of', 'the', 'party']
>>> hypothesis = 'of the'.split()
>>> references = [reference1, reference2, reference3]
>>> float(modified_precision(references, hypothesis, n=1))
1.0
>>> float(modified_precision(references, hypothesis, n=2))
1.0
An example of a normal machine translation hypothesis:
>>> hypothesis1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which',
...               'ensures', 'that', 'the', 'military', 'always',
...               'obeys', 'the', 'commands', 'of', 'the', 'party']
>>> hypothesis2 = ['It', 'is', 'to', 'insure', 'the', 'troops',
...               'forever', 'hearing', 'the', 'activity', 'guidebook',
...               'that', 'party', 'direct']
>>> reference1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'that',
...               'ensures', 'that', 'the', 'military', 'will',
...               'forever', 'heed', 'Party', 'commands']
>>> reference2 = ['It', 'is', 'the', 'guiding', 'principle', 'which',
...               'guarantees', 'the', 'military', 'forces', 'always',
...               'being', 'under', 'the', 'command', 'of', 'the',
...               'Party']
>>> reference3 = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the',
...               'army', 'always', 'to', 'heed', 'the', 'directions',
...               'of', 'the', 'party']
>>> references = [reference1, reference2, reference3]
>>> float(modified_precision(references, hypothesis1, n=1)) # doctest: +ELLIPSIS
0.9444...
>>> float(modified_precision(references, hypothesis2, n=1)) # doctest: +ELLIPSIS
0.5714...
>>> float(modified_precision(references, hypothesis1, n=2)) # doctest: +ELLIPSIS
0.5882352941176471
>>> float(modified_precision(references, hypothesis2, n=2)) # doctest: +ELLIPSIS
0.07692...
:param references: A list of reference translations.
:type references: list(list(str))
:param hypothesis: A hypothesis translation.
:type hypothesis: list(str)
:param n: The ngram order.
:type n: int
:return: BLEU's modified precision for the nth order ngram.
:rtype: Fraction
""""""
counts = Counter(ngrams(hypothesis, n)) if len(hypothesis) >= n else Counter()
max_counts = {}
for reference in references:
reference_counts = (
Counter(ngrams(reference, n)) if len(reference) >= n else Counter()
)
for ngram in counts:
max_counts[ngram] = max(max_counts.get(ngram, 0), reference_counts[ngram])
clipped_counts = {
ngram: min(count, max_counts[ngram]) for ngram, count in counts.items()
}
numerator = sum(clipped_counts.values())
denominator = max(1, sum(counts.values()))
return Fraction(numerator, denominator, _normalize=False)
",[],0,[],/translate/bleu_score.py_modified_precision
3038,/home/amandapotts/git/nltk/nltk/translate/bleu_score.py_brevity_penalty,"def brevity_penalty(closest_ref_len, hyp_len):
""""""
Calculate brevity penalty.
As the modified n-gram precision still has the problem from the short
length sentence, brevity penalty is used to modify the overall BLEU
score according to length.
An example from the paper. There are three references with length 12, 15
and 17. And a concise hypothesis of the length 12. The brevity penalty is 1.
>>> reference1 = list('aaaaaaaaaaaa')      # i.e. ['a'] * 12
>>> reference2 = list('aaaaaaaaaaaaaaa')   # i.e. ['a'] * 15
>>> reference3 = list('aaaaaaaaaaaaaaaaa') # i.e. ['a'] * 17
>>> hypothesis = list('aaaaaaaaaaaa')      # i.e. ['a'] * 12
>>> references = [reference1, reference2, reference3]
>>> hyp_len = len(hypothesis)
>>> closest_ref_len =  closest_ref_length(references, hyp_len)
>>> brevity_penalty(closest_ref_len, hyp_len)
1.0
In case a hypothesis translation is shorter than the references, penalty is
applied.
>>> references = [['a'] * 28, ['a'] * 28]
>>> hypothesis = ['a'] * 12
>>> hyp_len = len(hypothesis)
>>> closest_ref_len =  closest_ref_length(references, hyp_len)
>>> brevity_penalty(closest_ref_len, hyp_len)
0.2635971381157267
The length of the closest reference is used to compute the penalty. If the
length of a hypothesis is 12, and the reference lengths are 13 and 2, the
penalty is applied because the hypothesis length (12) is less then the
closest reference length (13).
>>> references = [['a'] * 13, ['a'] * 2]
>>> hypothesis = ['a'] * 12
>>> hyp_len = len(hypothesis)
>>> closest_ref_len =  closest_ref_length(references, hyp_len)
>>> brevity_penalty(closest_ref_len, hyp_len) # doctest: +ELLIPSIS
0.9200...
The brevity penalty doesn't depend on reference order. More importantly,
when two reference sentences are at the same distance, the shortest
reference sentence length is used.
>>> references = [['a'] * 13, ['a'] * 11]
>>> hypothesis = ['a'] * 12
>>> hyp_len = len(hypothesis)
>>> closest_ref_len =  closest_ref_length(references, hyp_len)
>>> bp1 = brevity_penalty(closest_ref_len, hyp_len)
>>> hyp_len = len(hypothesis)
>>> closest_ref_len =  closest_ref_length(reversed(references), hyp_len)
>>> bp2 = brevity_penalty(closest_ref_len, hyp_len)
>>> bp1 == bp2 == 1
True
A test example from mteval-v13a.pl (starting from the line 705):
>>> references = [['a'] * 11, ['a'] * 8]
>>> hypothesis = ['a'] * 7
>>> hyp_len = len(hypothesis)
>>> closest_ref_len =  closest_ref_length(references, hyp_len)
>>> brevity_penalty(closest_ref_len, hyp_len) # doctest: +ELLIPSIS
0.8668...
>>> references = [['a'] * 11, ['a'] * 8, ['a'] * 6, ['a'] * 7]
>>> hypothesis = ['a'] * 7
>>> hyp_len = len(hypothesis)
>>> closest_ref_len =  closest_ref_length(references, hyp_len)
>>> brevity_penalty(closest_ref_len, hyp_len)
1.0
:param hyp_len: The length of the hypothesis for a single sentence OR the
sum of all the hypotheses' lengths for a corpus
:type hyp_len: int
:param closest_ref_len: The length of the closest reference for a single
hypothesis OR the sum of all the closest references for every hypotheses.
:type closest_ref_len: int
:return: BLEU's brevity penalty.
:rtype: float
""""""
if hyp_len > closest_ref_len:
return 1
elif hyp_len == 0:
return 0
else:
return math.exp(1 - closest_ref_len / hyp_len)
",[],0,[],/translate/bleu_score.py_brevity_penalty
3039,/home/amandapotts/git/nltk/nltk/translate/bleu_score.py___init__,"def __init__(self, epsilon=0.1, alpha=5, k=5):
""""""
This will initialize the parameters required for the various smoothing
techniques, the default values are set to the numbers used in the
experiments from Chen and Cherry (2014).
>>> hypothesis1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which', 'ensures',
...                 'that', 'the', 'military', 'always', 'obeys', 'the',
...                 'commands', 'of', 'the', 'party']
>>> reference1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'that', 'ensures',
...               'that', 'the', 'military', 'will', 'forever', 'heed',
...               'Party', 'commands']
>>> chencherry = SmoothingFunction()
>>> print(sentence_bleu([reference1], hypothesis1)) # doctest: +ELLIPSIS
0.4118...
>>> print(sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method0)) # doctest: +ELLIPSIS
0.4118...
>>> print(sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method1)) # doctest: +ELLIPSIS
0.4118...
>>> print(sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method2)) # doctest: +ELLIPSIS
0.4452...
>>> print(sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method3)) # doctest: +ELLIPSIS
0.4118...
>>> print(sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method4)) # doctest: +ELLIPSIS
0.4118...
>>> print(sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method5)) # doctest: +ELLIPSIS
0.4905...
>>> print(sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method6)) # doctest: +ELLIPSIS
0.4135...
>>> print(sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method7)) # doctest: +ELLIPSIS
0.4905...
:param epsilon: the epsilon value use in method 1
:type epsilon: float
:param alpha: the alpha value use in method 6
:type alpha: int
:param k: the k value use in method 4
:type k: int
""""""
self.epsilon = epsilon
self.alpha = alpha
self.k = k
",[],0,[],/translate/bleu_score.py___init__
3040,/home/amandapotts/git/nltk/nltk/translate/bleu_score.py_method0,"def method0(self, p_n, *args, **kwargs):
""""""
No smoothing.
""""""
p_n_new = []
for i, p_i in enumerate(p_n):
if p_i.numerator != 0:
p_n_new.append(p_i)
else:
_msg = str(
""\nThe hypothesis contains 0 counts of {}-gram overlaps.\n""
""Therefore the BLEU score evaluates to 0, independently of\n""
""how many N-gram overlaps of lower order it contains.\n""
""Consider using lower n-gram order or use ""
""SmoothingFunction()""
).format(i + 1)
warnings.warn(_msg)
p_n_new.append(sys.float_info.min)
return p_n_new
",[],0,[],/translate/bleu_score.py_method0
3041,/home/amandapotts/git/nltk/nltk/translate/bleu_score.py_method1,"def method1(self, p_n, *args, **kwargs):
""""""
Smoothing method 1: Add *epsilon* counts to precision with 0 counts.
""""""
return [
(p_i.numerator + self.epsilon) / p_i.denominator
if p_i.numerator == 0
else p_i
for p_i in p_n
]
",[],0,[],/translate/bleu_score.py_method1
3042,/home/amandapotts/git/nltk/nltk/translate/bleu_score.py_method2,"def method2(self, p_n, *args, **kwargs):
""""""
Smoothing method 2: Add 1 to both numerator and denominator from
Chin-Yew Lin and Franz Josef Och (2004) ORANGE: a Method for
Evaluating Automatic Evaluation Metrics for Machine Translation.
In COLING 2004.
""""""
return [
Fraction(p_n[i].numerator + 1, p_n[i].denominator + 1, _normalize=False)
if i != 0
else p_n[0]
for i in range(len(p_n))
]
",[],0,[],/translate/bleu_score.py_method2
3043,/home/amandapotts/git/nltk/nltk/translate/bleu_score.py_method3,"def method3(self, p_n, *args, **kwargs):
""""""
Smoothing method 3: NIST geometric sequence smoothing
The smoothing is computed by taking 1 / ( 2^k ), instead of 0, for each
precision score whose matching n-gram count is null.
k is 1 for the first 'n' value for which the n-gram match count is null/
For example, if the text contains:
- one 2-gram match
- and (consequently) two 1-gram matches
the n-gram count for each individual precision score would be:
- n=1  =>  prec_count = 2     (two unigrams)
- n=2  =>  prec_count = 1     (one bigram)
- n=3  =>  prec_count = 1/2   (no trigram,  taking 'smoothed' value of 1 / ( 2^k ), with k=1)
- n=4  =>  prec_count = 1/4   (no fourgram, taking 'smoothed' value of 1 / ( 2^k ), with k=2)
""""""
incvnt = 1  # From the mteval-v13a.pl, it's referred to as k.
for i, p_i in enumerate(p_n):
if p_i.numerator == 0:
p_n[i] = 1 / (2**incvnt * p_i.denominator)
incvnt += 1
return p_n
",[],0,[],/translate/bleu_score.py_method3
3044,/home/amandapotts/git/nltk/nltk/translate/bleu_score.py_method4,"def method4(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):
""""""
Smoothing method 4:
Shorter translations may have inflated precision values due to having
smaller denominators
smaller smoothed counts. Instead of scaling to 1/(2^k), Chen and Cherry
suggests dividing by 1/ln(len(T)), where T is the length of the translation.
""""""
incvnt = 1
hyp_len = hyp_len if hyp_len else len(hypothesis)
for i, p_i in enumerate(p_n):
if p_i.numerator == 0 and hyp_len > 1:
numerator = 1 / (2**incvnt * self.k / math.log(hyp_len))
p_n[i] = numerator / p_i.denominator
incvnt += 1
return p_n
",[],0,[],/translate/bleu_score.py_method4
3045,/home/amandapotts/git/nltk/nltk/translate/bleu_score.py_method5,"def method5(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):
""""""
Smoothing method 5:
The matched counts for similar values of n should be similar. To a
calculate the n-gram matched count, it averages the n−1, n and n+1 gram
matched counts.
""""""
hyp_len = hyp_len if hyp_len else len(hypothesis)
m = {}
p_n_plus1 = p_n + [modified_precision(references, hypothesis, 5)]
m[-1] = p_n[0] + 1
for i, p_i in enumerate(p_n):
p_n[i] = (m[i - 1] + p_i + p_n_plus1[i + 1]) / 3
m[i] = p_n[i]
return p_n
",[],0,[],/translate/bleu_score.py_method5
3046,/home/amandapotts/git/nltk/nltk/translate/bleu_score.py_method6,"def method6(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):
""""""
Smoothing method 6:
Interpolates the maximum likelihood estimate of the precision *p_n* with
a prior estimate *pi0*. The prior is estimated by assuming that the ratio
between pn and pn−1 will be the same as that between pn−1 and pn−2
Gao and He (2013) Training MRF-Based Phrase Translation Models using
Gradient Ascent. In NAACL.
""""""
hyp_len = hyp_len if hyp_len else len(hypothesis)
assert p_n[2], ""This smoothing method requires non-zero precision for bigrams.""
for i, p_i in enumerate(p_n):
if i in [0, 1]:  # Skips the first 2 orders of ngrams.
continue
else:
pi0 = 0 if p_n[i - 2] == 0 else p_n[i - 1] ** 2 / p_n[i - 2]
m = p_i.numerator
l = sum(1 for _ in ngrams(hypothesis, i + 1))
p_n[i] = (m + self.alpha * pi0) / (l + self.alpha)
return p_n
",[],0,[],/translate/bleu_score.py_method6
3047,/home/amandapotts/git/nltk/nltk/translate/bleu_score.py_method7,"def method7(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):
""""""
Smoothing method 7:
Interpolates methods 4 and 5.
""""""
hyp_len = hyp_len if hyp_len else len(hypothesis)
p_n = self.method4(p_n, references, hypothesis, hyp_len)
p_n = self.method5(p_n, references, hypothesis, hyp_len)
return p_n
",[],0,[],/translate/bleu_score.py_method7
3048,/home/amandapotts/git/nltk/nltk/translate/ibm2.py___init__,"def __init__(self, sentence_aligned_corpus, iterations, probability_tables=None):
""""""
Train on ``sentence_aligned_corpus`` and create a lexical
translation model and an alignment model.
Translation direction is from ``AlignedSent.mots`` to
``AlignedSent.words``.
:param sentence_aligned_corpus: Sentence-aligned parallel corpus
:type sentence_aligned_corpus: list(AlignedSent)
:param iterations: Number of iterations to run training algorithm
:type iterations: int
:param probability_tables: Optional. Use this to pass in custom
probability values. If not specified, probabilities will be
set to a uniform distribution, or some other sensible value.
If specified, all the following entries must be present:
``translation_table``, ``alignment_table``.
See ``IBMModel`` for the type and purpose of these tables.
:type probability_tables: dict[str]: object
""""""
super().__init__(sentence_aligned_corpus)
if probability_tables is None:
ibm1 = IBMModel1(sentence_aligned_corpus, 2 * iterations)
self.translation_table = ibm1.translation_table
self.set_uniform_probabilities(sentence_aligned_corpus)
else:
self.translation_table = probability_tables[""translation_table""]
self.alignment_table = probability_tables[""alignment_table""]
for n in range(0, iterations):
self.train(sentence_aligned_corpus)
self.align_all(sentence_aligned_corpus)
",[],0,[],/translate/ibm2.py___init__
3049,/home/amandapotts/git/nltk/nltk/translate/ibm2.py_set_uniform_probabilities,"def set_uniform_probabilities(self, sentence_aligned_corpus):
l_m_combinations = set()
for aligned_sentence in sentence_aligned_corpus:
l = len(aligned_sentence.mots)
m = len(aligned_sentence.words)
if (l, m) not in l_m_combinations:
l_m_combinations.add((l, m))
initial_prob = 1 / (l + 1)
if initial_prob < IBMModel.MIN_PROB:
warnings.warn(
""A source sentence is too long (""
+ str(l)
+ "" words). Results may be less accurate.""
)
for i in range(0, l + 1):
for j in range(1, m + 1):
self.alignment_table[i][j][l][m] = initial_prob
",[],0,[],/translate/ibm2.py_set_uniform_probabilities
3050,/home/amandapotts/git/nltk/nltk/translate/ibm2.py_train,"def train(self, parallel_corpus):
counts = Model2Counts()
for aligned_sentence in parallel_corpus:
src_sentence = [None] + aligned_sentence.mots
trg_sentence = [""UNUSED""] + aligned_sentence.words  # 1-indexed
l = len(aligned_sentence.mots)
m = len(aligned_sentence.words)
total_count = self.prob_all_alignments(src_sentence, trg_sentence)
for j in range(1, m + 1):
t = trg_sentence[j]
for i in range(0, l + 1):
s = src_sentence[i]
count = self.prob_alignment_point(i, j, src_sentence, trg_sentence)
normalized_count = count / total_count[t]
counts.update_lexical_translation(normalized_count, s, t)
counts.update_alignment(normalized_count, i, j, l, m)
self.maximize_lexical_translation_probabilities(counts)
self.maximize_alignment_probabilities(counts)
",[],0,[],/translate/ibm2.py_train
3051,/home/amandapotts/git/nltk/nltk/translate/ibm2.py_maximize_alignment_probabilities,"def maximize_alignment_probabilities(self, counts):
MIN_PROB = IBMModel.MIN_PROB
for i, j_s in counts.alignment.items():
for j, src_sentence_lengths in j_s.items():
for l, trg_sentence_lengths in src_sentence_lengths.items():
for m in trg_sentence_lengths:
estimate = (
counts.alignment[i][j][l][m]
/ counts.alignment_for_any_i[j][l][m]
)
self.alignment_table[i][j][l][m] = max(estimate, MIN_PROB)
",[],0,[],/translate/ibm2.py_maximize_alignment_probabilities
3052,/home/amandapotts/git/nltk/nltk/translate/ibm2.py_prob_all_alignments,"def prob_all_alignments(self, src_sentence, trg_sentence):
""""""
Computes the probability of all possible word alignments,
expressed as a marginal distribution over target words t
Each entry in the return value represents the contribution to
the total alignment probability by the target word t.
To obtain probability(alignment | src_sentence, trg_sentence),
simply sum the entries in the return value.
:return: Probability of t for all s in ``src_sentence``
:rtype: dict(str): float
""""""
alignment_prob_for_t = defaultdict(float)
for j in range(1, len(trg_sentence)):
t = trg_sentence[j]
for i in range(0, len(src_sentence)):
alignment_prob_for_t[t] += self.prob_alignment_point(
i, j, src_sentence, trg_sentence
)
return alignment_prob_for_t
",[],0,[],/translate/ibm2.py_prob_all_alignments
3053,/home/amandapotts/git/nltk/nltk/translate/ibm2.py_prob_alignment_point,"def prob_alignment_point(self, i, j, src_sentence, trg_sentence):
""""""
Probability that position j in ``trg_sentence`` is aligned to
position i in the ``src_sentence``
""""""
l = len(src_sentence) - 1
m = len(trg_sentence) - 1
s = src_sentence[i]
t = trg_sentence[j]
return self.translation_table[t][s] * self.alignment_table[i][j][l][m]
",[],0,[],/translate/ibm2.py_prob_alignment_point
3054,/home/amandapotts/git/nltk/nltk/translate/ibm2.py_prob_t_a_given_s,"def prob_t_a_given_s(self, alignment_info):
""""""
Probability of target sentence and an alignment given the
source sentence
""""""
prob = 1.0
l = len(alignment_info.src_sentence) - 1
m = len(alignment_info.trg_sentence) - 1
for j, i in enumerate(alignment_info.alignment):
if j == 0:
continue  # skip the dummy zeroeth element
trg_word = alignment_info.trg_sentence[j]
src_word = alignment_info.src_sentence[i]
prob *= (
self.translation_table[trg_word][src_word]
)
return max(prob, IBMModel.MIN_PROB)
",[],0,[],/translate/ibm2.py_prob_t_a_given_s
3055,/home/amandapotts/git/nltk/nltk/translate/ibm2.py_align_all,"def align_all(self, parallel_corpus):
for sentence_pair in parallel_corpus:
self.align(sentence_pair)
",[],0,[],/translate/ibm2.py_align_all
3056,/home/amandapotts/git/nltk/nltk/translate/ibm2.py_align,"def align(self, sentence_pair):
""""""
Determines the best word alignment for one sentence pair from
the corpus that the model was trained on.
The best alignment will be set in ``sentence_pair`` when the
method returns. In contrast with the internal implementation of
IBM models, the word indices in the ``Alignment`` are zero-
indexed, not one-indexed.
:param sentence_pair: A sentence in the source language and its
counterpart sentence in the target language
:type sentence_pair: AlignedSent
""""""
best_alignment = []
l = len(sentence_pair.mots)
m = len(sentence_pair.words)
for j, trg_word in enumerate(sentence_pair.words):
best_prob = (
self.translation_table[trg_word][None]
)
best_prob = max(best_prob, IBMModel.MIN_PROB)
best_alignment_point = None
for i, src_word in enumerate(sentence_pair.mots):
align_prob = (
self.translation_table[trg_word][src_word]
)
if align_prob >= best_prob:
best_prob = align_prob
best_alignment_point = i
best_alignment.append((j, best_alignment_point))
sentence_pair.alignment = Alignment(best_alignment)
",[],0,[],/translate/ibm2.py_align
3057,/home/amandapotts/git/nltk/nltk/translate/ibm2.py_update_lexical_translation,"def update_lexical_translation(self, count, s, t):
self.t_given_s[t][s] += count
self.any_t_given_s[s] += count
",[],0,[],/translate/ibm2.py_update_lexical_translation
3058,/home/amandapotts/git/nltk/nltk/translate/ibm2.py_update_alignment,"def update_alignment(self, count, i, j, l, m):
self.alignment[i][j][l][m] += count
self.alignment_for_any_i[j][l][m] += count
",[],0,[],/translate/ibm2.py_update_alignment
3059,/home/amandapotts/git/nltk/nltk/translate/phrase_based.py_extract,"def extract(
f_start,
f_end,
e_start,
e_end,
alignment,
f_aligned,
srctext,
trgtext,
srclen,
trglen,
max_phrase_length,
",[],0,[],/translate/phrase_based.py_extract
3060,/home/amandapotts/git/nltk/nltk/translate/phrase_based.py_phrase_extraction,"def phrase_extraction(srctext, trgtext, alignment, max_phrase_length=0):
""""""
Phrase extraction algorithm extracts all consistent phrase pairs from
a word-aligned sentence pair.
The idea is to loop over all possible source language (e) phrases and find
the minimal foreign phrase (f) that matches each of them. Matching is done
by identifying all alignment points for the source phrase and finding the
shortest foreign phrase that includes all the foreign counterparts for the
source words.
In short, a phrase alignment has to
(a) contain all alignment points for all covered words
(b) contain at least one alignment point
>>> srctext = ""michael assumes that he will stay in the house""
>>> trgtext = ""michael geht davon aus , dass er im haus bleibt""
>>> alignment = [(0,0), (1,1), (1,2), (1,3), (2,5), (3,6), (4,9),
... (5,9), (6,7), (7,7), (8,8)]
>>> phrases = phrase_extraction(srctext, trgtext, alignment)
>>> for i in sorted(phrases):
...    print(i)
...
((0, 1), (0, 1), 'michael', 'michael')
((0, 2), (0, 4), 'michael assumes', 'michael geht davon aus')
((0, 2), (0, 5), 'michael assumes', 'michael geht davon aus ,')
((0, 3), (0, 6), 'michael assumes that', 'michael geht davon aus , dass')
((0, 4), (0, 7), 'michael assumes that he', 'michael geht davon aus , dass er')
((0, 9), (0, 10), 'michael assumes that he will stay in the house', 'michael geht davon aus , dass er im haus bleibt')
((1, 2), (1, 4), 'assumes', 'geht davon aus')
((1, 2), (1, 5), 'assumes', 'geht davon aus ,')
((1, 3), (1, 6), 'assumes that', 'geht davon aus , dass')
((1, 4), (1, 7), 'assumes that he', 'geht davon aus , dass er')
((1, 9), (1, 10), 'assumes that he will stay in the house', 'geht davon aus , dass er im haus bleibt')
((2, 3), (4, 6), 'that', ', dass')
((2, 3), (5, 6), 'that', 'dass')
((2, 4), (4, 7), 'that he', ', dass er')
((2, 4), (5, 7), 'that he', 'dass er')
((2, 9), (4, 10), 'that he will stay in the house', ', dass er im haus bleibt')
((2, 9), (5, 10), 'that he will stay in the house', 'dass er im haus bleibt')
((3, 4), (6, 7), 'he', 'er')
((3, 9), (6, 10), 'he will stay in the house', 'er im haus bleibt')
((4, 6), (9, 10), 'will stay', 'bleibt')
((4, 9), (7, 10), 'will stay in the house', 'im haus bleibt')
((6, 8), (7, 8), 'in the', 'im')
((6, 9), (7, 9), 'in the house', 'im haus')
((8, 9), (8, 9), 'house', 'haus')
:type srctext: str
:param srctext: The sentence string from the source language.
:type trgtext: str
:param trgtext: The sentence string from the target language.
:type alignment: list(tuple)
:param alignment: The word alignment outputs as list of tuples, where
the first elements of tuples are the source words' indices and
second elements are the target words' indices. This is also the output
format of nltk.translate.ibm1
:rtype: list(tuple)
:return: A list of tuples, each element in a list is a phrase and each
phrase is a tuple made up of (i) its source location, (ii) its target
location, (iii) the source phrase and (iii) the target phrase. The phrase
list of tuples represents all the possible phrases extracted from the
word alignments.
:type max_phrase_length: int
:param max_phrase_length: maximal phrase length, if 0 or not specified
it is set to a length of the longer sentence (srctext or trgtext).
""""""
srctext = srctext.split()  # e
trgtext = trgtext.split()  # f
srclen = len(srctext)  # len(e)
trglen = len(trgtext)  # len(f)
f_aligned = [j for _, j in alignment]
max_phrase_length = max_phrase_length or max(srclen, trglen)
bp = set()
for e_start in range(srclen):
max_idx = min(srclen, e_start + max_phrase_length)
for e_end in range(e_start, max_idx):
f_start, f_end = trglen - 1, -1  #  0-based indexing
for e, f in alignment:
if e_start <= e <= e_end:
f_start = min(f, f_start)
f_end = max(f, f_end)
phrases = extract(
f_start,
f_end,
e_start,
e_end,
alignment,
f_aligned,
srctext,
trgtext,
srclen,
trglen,
max_phrase_length,
)
if phrases:
bp.update(phrases)
return bp
",[],0,[],/translate/phrase_based.py_phrase_extraction
3061,/home/amandapotts/git/nltk/nltk/translate/chrf_score.py_sentence_chrf,"def sentence_chrf(
reference, hypothesis, min_len=1, max_len=6, beta=3.0, ignore_whitespace=True
",[],0,[],/translate/chrf_score.py_sentence_chrf
3062,/home/amandapotts/git/nltk/nltk/translate/chrf_score.py__preprocess,"def _preprocess(sent, ignore_whitespace):
if type(sent) != str:
sent = "" "".join(sent)
if ignore_whitespace:
sent = re.sub(r""\s+"", """", sent)
return sent
",[],0,[],/translate/chrf_score.py__preprocess
3063,/home/amandapotts/git/nltk/nltk/translate/chrf_score.py_chrf_precision_recall_fscore_support,"def chrf_precision_recall_fscore_support(
reference, hypothesis, n, beta=3.0, epsilon=1e-16
",[],0,[],/translate/chrf_score.py_chrf_precision_recall_fscore_support
3064,/home/amandapotts/git/nltk/nltk/translate/chrf_score.py_corpus_chrf,"def corpus_chrf(
references, hypotheses, min_len=1, max_len=6, beta=3.0, ignore_whitespace=True
",[],0,[],/translate/chrf_score.py_corpus_chrf
3065,/home/amandapotts/git/nltk/nltk/translate/api.py___init__,"def __init__(self, words, mots, alignment=None):
self._words = words
self._mots = mots
if alignment is None:
self.alignment = Alignment([])
else:
assert type(alignment) is Alignment
self.alignment = alignment
",[],0,[],/translate/api.py___init__
3066,/home/amandapotts/git/nltk/nltk/translate/api.py_words,"def words(self):
return self._words
",[],0,[],/translate/api.py_words
3067,/home/amandapotts/git/nltk/nltk/translate/api.py_mots,"def mots(self):
return self._mots
",[],0,[],/translate/api.py_mots
3068,/home/amandapotts/git/nltk/nltk/translate/api.py__get_alignment,"def _get_alignment(self):
return self._alignment
",[],0,[],/translate/api.py__get_alignment
3069,/home/amandapotts/git/nltk/nltk/translate/api.py__set_alignment,"def _set_alignment(self, alignment):
_check_alignment(len(self.words), len(self.mots), alignment)
self._alignment = alignment
",[],0,[],/translate/api.py__set_alignment
3070,/home/amandapotts/git/nltk/nltk/translate/api.py___repr__,"def __repr__(self):
""""""
Return a string representation for this ``AlignedSent``.
:rtype: str
""""""
words = ""[%s]"" % ("", "".join(""'%s'"" % w for w in self._words))
mots = ""[%s]"" % ("", "".join(""'%s'"" % w for w in self._mots))
return f""AlignedSent({words}, {mots}, {self._alignment!r})""
",[],0,[],/translate/api.py___repr__
3071,/home/amandapotts/git/nltk/nltk/translate/api.py__to_dot,"def _to_dot(self):
""""""
Dot representation of the aligned sentence
""""""
s = ""graph align {\n""
s += ""node[shape=plaintext]\n""
s += """".join([f'""{w}_source"" [label=""{w}""] \n' for w in self._words])
s += """".join([f'""{w}_target"" [label=""{w}""] \n' for w in self._mots])
s += """".join(
[
f'""{self._words[u]}_source"" -- ""{self._mots[v]}_target"" \n'
for u, v in self._alignment
]
)
for i in range(len(self._words) - 1):
s += '""{}_source"" -- ""{}_source"" [style=invis]\n'.format(
self._words[i],
self._words[i + 1],
)
for i in range(len(self._mots) - 1):
s += '""{}_target"" -- ""{}_target"" [style=invis]\n'.format(
self._mots[i],
self._mots[i + 1],
)
s += ""{rank = same
s += ""{rank = same
s += ""}""
return s
",[],0,[],/translate/api.py__to_dot
3072,/home/amandapotts/git/nltk/nltk/translate/api.py__repr_svg_,"def _repr_svg_(self):
""""""
Ipython magic : show SVG representation of this ``AlignedSent``.
""""""
dot_string = self._to_dot().encode(""utf8"")
output_format = ""svg""
try:
process = subprocess.Popen(
[""dot"", ""-T%s"" % output_format],
stdin=subprocess.PIPE,
stdout=subprocess.PIPE,
stderr=subprocess.PIPE,
)
except OSError as e:
raise Exception(""Cannot find the dot binary from Graphviz package"") from e
out, err = process.communicate(dot_string)
return out.decode(""utf8"")
",[],0,[],/translate/api.py__repr_svg_
3073,/home/amandapotts/git/nltk/nltk/translate/api.py___str__,"def __str__(self):
""""""
Return a human-readable string representation for this ``AlignedSent``.
:rtype: str
""""""
source = "" "".join(self._words)[:20] + ""...""
target = "" "".join(self._mots)[:20] + ""...""
return f""<AlignedSent: '{source}' -> '{target}'>""
",[],0,[],/translate/api.py___str__
3074,/home/amandapotts/git/nltk/nltk/translate/api.py_invert,"def invert(self):
""""""
Return the aligned sentence pair, reversing the directionality
:rtype: AlignedSent
""""""
return AlignedSent(self._mots, self._words, self._alignment.invert())
",[],0,[],/translate/api.py_invert
3075,/home/amandapotts/git/nltk/nltk/translate/api.py___new__,"def __new__(cls, pairs):
self = frozenset.__new__(cls, pairs)
self._len = max(p[0] for p in self) if self != frozenset([]) else 0
self._index = None
return self
",[],0,[],/translate/api.py___new__
3076,/home/amandapotts/git/nltk/nltk/translate/api.py_fromstring,"def fromstring(cls, s):
""""""
Read a giza-formatted string and return an Alignment object.
>>> Alignment.fromstring('0-0 2-1 9-2 21-3 10-4 7-5')
Alignment([(0, 0), (2, 1), (7, 5), (9, 2), (10, 4), (21, 3)])
:type s: str
:param s: the positional alignments in giza format
:rtype: Alignment
:return: An Alignment object corresponding to the string representation ``s``.
""""""
return Alignment([_giza2pair(a) for a in s.split()])
",[],0,[],/translate/api.py_fromstring
3077,/home/amandapotts/git/nltk/nltk/translate/api.py___getitem__,"def __getitem__(self, key):
""""""
Look up the alignments that map from a given index or slice.
""""""
if not self._index:
self._build_index()
return self._index.__getitem__(key)
",[],0,[],/translate/api.py___getitem__
3078,/home/amandapotts/git/nltk/nltk/translate/api.py_invert,"def invert(self):
""""""
Return an Alignment object, being the inverted mapping.
""""""
return Alignment(((p[1], p[0]) + p[2:]) for p in self)
",[],0,[],/translate/api.py_invert
3079,/home/amandapotts/git/nltk/nltk/translate/api.py_range,"def range(self, positions=None):
""""""
Work out the range of the mapping from the given positions.
If no positions are specified, compute the range of the entire mapping.
""""""
image = set()
if not self._index:
self._build_index()
if not positions:
positions = list(range(len(self._index)))
for p in positions:
image.update(f for _, f in self._index[p])
return sorted(image)
",[],0,[],/translate/api.py_range
3080,/home/amandapotts/git/nltk/nltk/translate/api.py___repr__,"def __repr__(self):
""""""
Produce a Giza-formatted string representing the alignment.
""""""
return ""Alignment(%r)"" % sorted(self)
",[],0,[],/translate/api.py___repr__
3081,/home/amandapotts/git/nltk/nltk/translate/api.py___str__,"def __str__(self):
""""""
Produce a Giza-formatted string representing the alignment.
""""""
return "" "".join(""%d-%d"" % p[:2] for p in sorted(self))
",[],0,[],/translate/api.py___str__
3082,/home/amandapotts/git/nltk/nltk/translate/api.py__build_index,"def _build_index(self):
""""""
Build a list self._index such that self._index[i] is a list
of the alignments originating from word i.
""""""
self._index = [[] for _ in range(self._len + 1)]
for p in self:
self._index[p[0]].append(p)
",[],0,[],/translate/api.py__build_index
3083,/home/amandapotts/git/nltk/nltk/translate/api.py__giza2pair,"def _giza2pair(pair_string):
i, j = pair_string.split(""-"")
return int(i), int(j)
",[],0,[],/translate/api.py__giza2pair
3084,/home/amandapotts/git/nltk/nltk/translate/api.py__naacl2pair,"def _naacl2pair(pair_string):
i, j, p = pair_string.split(""-"")
return int(i), int(j)
",[],0,[],/translate/api.py__naacl2pair
3085,/home/amandapotts/git/nltk/nltk/translate/api.py__check_alignment,"def _check_alignment(num_words, num_mots, alignment):
""""""
Check whether the alignments are legal.
:param num_words: the number of source language words
:type num_words: int
:param num_mots: the number of target language words
:type num_mots: int
:param alignment: alignment to be checked
:type alignment: Alignment
:raise IndexError: if alignment falls outside the sentence
""""""
assert type(alignment) is Alignment
if not all(0 <= pair[0] < num_words for pair in alignment):
raise IndexError(""Alignment is outside boundary of words"")
if not all(pair[1] is None or 0 <= pair[1] < num_mots for pair in alignment):
raise IndexError(""Alignment is outside boundary of mots"")
",[],0,[],/translate/api.py__check_alignment
3086,/home/amandapotts/git/nltk/nltk/translate/api.py___init__,"def __init__(self):
self.src_phrases = dict()
",[],0,[],/translate/api.py___init__
3087,/home/amandapotts/git/nltk/nltk/translate/api.py_translations_for,"def translations_for(self, src_phrase):
""""""
Get the translations for a source language phrase
:param src_phrase: Source language phrase of interest
:type src_phrase: tuple(str)
:return: A list of target language phrases that are translations
of ``src_phrase``, ordered in decreasing order of
likelihood. Each list element is a tuple of the target
phrase and its log probability.
:rtype: list(PhraseTableEntry)
""""""
return self.src_phrases[src_phrase]
",[],0,[],/translate/api.py_translations_for
3088,/home/amandapotts/git/nltk/nltk/translate/api.py___contains__,"def __contains__(self, src_phrase):
return src_phrase in self.src_phrases
",[],0,[],/translate/api.py___contains__
3089,/home/amandapotts/git/nltk/nltk/translate/meteor_score.py__generate_enums,"def _generate_enums(
hypothesis: Iterable[str],
reference: Iterable[str],
preprocess: Callable[[str], str] = str.lower,
",[],0,[],/translate/meteor_score.py__generate_enums
3090,/home/amandapotts/git/nltk/nltk/translate/meteor_score.py_exact_match,"def exact_match(
hypothesis: Iterable[str], reference: Iterable[str]
",[],0,[],/translate/meteor_score.py_exact_match
3091,/home/amandapotts/git/nltk/nltk/translate/meteor_score.py__match_enums,"def _match_enums(
enum_hypothesis_list: List[Tuple[int, str]],
enum_reference_list: List[Tuple[int, str]],
",[],0,[],/translate/meteor_score.py__match_enums
3092,/home/amandapotts/git/nltk/nltk/translate/meteor_score.py__enum_stem_match,"def _enum_stem_match(
enum_hypothesis_list: List[Tuple[int, str]],
enum_reference_list: List[Tuple[int, str]],
stemmer: StemmerI = PorterStemmer(),
",[],0,[],/translate/meteor_score.py__enum_stem_match
3093,/home/amandapotts/git/nltk/nltk/translate/meteor_score.py_stem_match,"def stem_match(
hypothesis: Iterable[str],
reference: Iterable[str],
stemmer: StemmerI = PorterStemmer(),
",[],0,[],/translate/meteor_score.py_stem_match
3094,/home/amandapotts/git/nltk/nltk/translate/meteor_score.py__enum_wordnetsyn_match,"def _enum_wordnetsyn_match(
enum_hypothesis_list: List[Tuple[int, str]],
enum_reference_list: List[Tuple[int, str]],
wordnet: WordNetCorpusReader = wordnet,
",[],0,[],/translate/meteor_score.py__enum_wordnetsyn_match
3095,/home/amandapotts/git/nltk/nltk/translate/meteor_score.py_wordnetsyn_match,"def wordnetsyn_match(
hypothesis: Iterable[str],
reference: Iterable[str],
wordnet: WordNetCorpusReader = wordnet,
",[],0,[],/translate/meteor_score.py_wordnetsyn_match
3096,/home/amandapotts/git/nltk/nltk/translate/meteor_score.py_align_words,"def align_words(
hypothesis: Iterable[str],
reference: Iterable[str],
stemmer: StemmerI = PorterStemmer(),
wordnet: WordNetCorpusReader = wordnet,
",[],0,[],/translate/meteor_score.py_align_words
3097,/home/amandapotts/git/nltk/nltk/translate/meteor_score.py__count_chunks,"def _count_chunks(matches: List[Tuple[int, int]]) -> int:
""""""
Counts the fewest possible number of chunks such that matched unigrams
of each chunk are adjacent to each other. This is used to calculate the
fragmentation part of the metric.
:param matches: list containing a mapping of matched words (output of align_words)
:return: Number of chunks a sentence is divided into post alignment
""""""
i = 0
chunks = 1
while i < len(matches) - 1:
if (matches[i + 1][0] == matches[i][0] + 1) and (
matches[i + 1][1] == matches[i][1] + 1
):
i += 1
continue
i += 1
chunks += 1
return chunks
",[],0,[],/translate/meteor_score.py__count_chunks
3098,/home/amandapotts/git/nltk/nltk/translate/meteor_score.py_single_meteor_score,"def single_meteor_score(
reference: Iterable[str],
hypothesis: Iterable[str],
preprocess: Callable[[str], str] = str.lower,
stemmer: StemmerI = PorterStemmer(),
wordnet: WordNetCorpusReader = wordnet,
alpha: float = 0.9,
beta: float = 3.0,
gamma: float = 0.5,
",[],0,[],/translate/meteor_score.py_single_meteor_score
3099,/home/amandapotts/git/nltk/nltk/translate/meteor_score.py_meteor_score,"def meteor_score(
references: Iterable[Iterable[str]],
hypothesis: Iterable[str],
preprocess: Callable[[str], str] = str.lower,
stemmer: StemmerI = PorterStemmer(),
wordnet: WordNetCorpusReader = wordnet,
alpha: float = 0.9,
beta: float = 3.0,
gamma: float = 0.5,
",[],0,[],/translate/meteor_score.py_meteor_score
3100,/home/amandapotts/git/nltk/nltk/translate/gdfa.py_grow_diag_final_and,"def grow_diag_final_and(srclen, trglen, e2f, f2e):
""""""
This module symmetrisatizes the source-to-target and target-to-source
word alignment output and produces, aka. GDFA algorithm (Koehn, 2005).
Step 1: Find the intersection of the bidirectional alignment.
Step 2: Search for additional neighbor alignment points to be added, given
these criteria: (i) neighbor alignments points are not in the
intersection and (ii) neighbor alignments are in the union.
Step 3: Add all other alignment points that are not in the intersection, not in
the neighboring alignments that met the criteria but in the original
forward/backward alignment outputs.
>>> forw = ('0-0 2-1 9-2 21-3 10-4 7-5 11-6 9-7 12-8 1-9 3-10 '
...         '4-11 17-12 17-13 25-14 13-15 24-16 11-17 28-18')
>>> back = ('0-0 1-9 2-9 3-10 4-11 5-12 6-6 7-5 8-6 9-7 10-4 '
...         '11-6 12-8 13-12 15-12 17-13 18-13 19-12 20-13 '
...         '21-3 22-12 23-14 24-17 25-15 26-17 27-18 28-18')
>>> srctext = (""この よう な ハロー 白色 わい 星 の Ｌ 関数 ""
...            ""は Ｌ と 共 に 不連続 に 増加 する こと が ""
...            ""期待 さ れる こと を 示し た 。"")
>>> trgtext = (""Therefore , we expect that the luminosity function ""
...            ""of such halo white dwarfs increases discontinuously ""
...            ""with the luminosity ."")
>>> srclen = len(srctext.split())
>>> trglen = len(trgtext.split())
>>>
>>> gdfa = grow_diag_final_and(srclen, trglen, forw, back)
>>> gdfa == sorted(set([(28, 18), (6, 6), (24, 17), (2, 1), (15, 12), (13, 12),
...         (2, 9), (3, 10), (26, 17), (25, 15), (8, 6), (9, 7), (20,
...         13), (18, 13), (0, 0), (10, 4), (13, 15), (23, 14), (7, 5),
...         (25, 14), (1, 9), (17, 13), (4, 11), (11, 17), (9, 2), (22,
...         12), (27, 18), (24, 16), (21, 3), (19, 12), (17, 12), (5,
...         12), (11, 6), (12, 8)]))
True
References:
Koehn, P., A. Axelrod, A. Birch, C. Callison, M. Osborne, and D. Talbot.
2005. Edinburgh System Description for the 2005 IWSLT Speech
Translation Evaluation. In MT Eval Workshop.
:type srclen: int
:param srclen: the number of tokens in the source language
:type trglen: int
:param trglen: the number of tokens in the target language
:type e2f: str
:param e2f: the forward word alignment outputs from source-to-target
language (in pharaoh output format)
:type f2e: str
:param f2e: the backward word alignment outputs from target-to-source
language (in pharaoh output format)
:rtype: set(tuple(int))
:return: the symmetrized alignment points from the GDFA algorithm
""""""
e2f = [tuple(map(int, a.split(""-""))) for a in e2f.split()]
f2e = [tuple(map(int, a.split(""-""))) for a in f2e.split()]
neighbors = [(-1, 0), (0, -1), (1, 0), (0, 1), (-1, -1), (-1, 1), (1, -1), (1, 1)]
alignment = set(e2f).intersection(set(f2e))  # Find the intersection.
union = set(e2f).union(set(f2e))
aligned = defaultdict(set)
for i, j in alignment:
aligned[""e""].add(i)
aligned[""f""].add(j)
",[],0,[],/translate/gdfa.py_grow_diag_final_and
3101,/home/amandapotts/git/nltk/nltk/translate/gdfa.py_grow_diag,"def grow_diag():
""""""
Search for the neighbor points and them to the intersected alignment
points if criteria are met.
""""""
prev_len = len(alignment) - 1
while prev_len < len(alignment):
no_new_points = True
for e in range(srclen):
for f in range(trglen):
if (e, f) in alignment:
for neighbor in neighbors:
neighbor = tuple(i + j for i, j in zip((e, f), neighbor))
e_new, f_new = neighbor
if (
e_new not in aligned and f_new not in aligned
) and neighbor in union:
alignment.add(neighbor)
aligned[""e""].add(e_new)
aligned[""f""].add(f_new)
prev_len += 1
no_new_points = False
if no_new_points:
break
",[],0,[],/translate/gdfa.py_grow_diag
3102,/home/amandapotts/git/nltk/nltk/translate/gdfa.py_final_and,"def final_and(a):
""""""
Adds remaining points that are not in the intersection, not in the
neighboring alignments but in the original *e2f* and *f2e* alignments
""""""
for e_new in range(srclen):
for f_new in range(trglen):
if (
e_new not in aligned
and f_new not in aligned
and (e_new, f_new) in union
):
alignment.add((e_new, f_new))
aligned[""e""].add(e_new)
aligned[""f""].add(f_new)
",[],0,[],/translate/gdfa.py_final_and
3103,/home/amandapotts/git/nltk/nltk/translate/ibm_model.py_longest_target_sentence_length,"def longest_target_sentence_length(sentence_aligned_corpus):
""""""
:param sentence_aligned_corpus: Parallel corpus under consideration
:type sentence_aligned_corpus: list(AlignedSent)
:return: Number of words in the longest target language sentence
of ``sentence_aligned_corpus``
""""""
max_m = 0
for aligned_sentence in sentence_aligned_corpus:
m = len(aligned_sentence.words)
max_m = max(m, max_m)
return max_m
",[],0,[],/translate/ibm_model.py_longest_target_sentence_length
3104,/home/amandapotts/git/nltk/nltk/translate/ibm_model.py___init__,"def __init__(self, sentence_aligned_corpus):
self.init_vocab(sentence_aligned_corpus)
self.reset_probabilities()
",[],0,[],/translate/ibm_model.py___init__
3105,/home/amandapotts/git/nltk/nltk/translate/ibm_model.py_set_uniform_probabilities,"def set_uniform_probabilities(self, sentence_aligned_corpus):
""""""
Initialize probability tables to a uniform distribution
Derived classes should implement this accordingly.
""""""
pass
",[],0,[],/translate/ibm_model.py_set_uniform_probabilities
3106,/home/amandapotts/git/nltk/nltk/translate/ibm_model.py_init_vocab,"def init_vocab(self, sentence_aligned_corpus):
src_vocab = set()
trg_vocab = set()
for aligned_sentence in sentence_aligned_corpus:
trg_vocab.update(aligned_sentence.words)
src_vocab.update(aligned_sentence.mots)
src_vocab.add(None)
self.src_vocab = src_vocab
""""""
set(str): All source language words used in training
""""""
self.trg_vocab = trg_vocab
""""""
set(str): All target language words used in training
""""""
",[],0,[],/translate/ibm_model.py_init_vocab
3107,/home/amandapotts/git/nltk/nltk/translate/ibm_model.py_sample,"def sample(self, sentence_pair):
""""""
Sample the most probable alignments from the entire alignment
space
First, determine the best alignment according to IBM Model 2.
With this initial alignment, use hill climbing to determine the
best alignment according to a higher IBM Model. Add this
alignment and its neighbors to the sample set. Repeat this
process with other initial alignments obtained by pegging an
alignment point.
Hill climbing may be stuck in a local maxima, hence the pegging
and trying out of different alignments.
:param sentence_pair: Source and target language sentence pair
to generate a sample of alignments from
:type sentence_pair: AlignedSent
:return: A set of best alignments represented by their ``AlignmentInfo``
and the best alignment of the set for convenience
:rtype: set(AlignmentInfo), AlignmentInfo
""""""
sampled_alignments = set()
l = len(sentence_pair.mots)
m = len(sentence_pair.words)
initial_alignment = self.best_model2_alignment(sentence_pair)
potential_alignment = self.hillclimb(initial_alignment)
sampled_alignments.update(self.neighboring(potential_alignment))
best_alignment = potential_alignment
for j in range(1, m + 1):
for i in range(0, l + 1):
initial_alignment = self.best_model2_alignment(sentence_pair, j, i)
potential_alignment = self.hillclimb(initial_alignment, j)
neighbors = self.neighboring(potential_alignment, j)
sampled_alignments.update(neighbors)
if potential_alignment.score > best_alignment.score:
best_alignment = potential_alignment
return sampled_alignments, best_alignment
",[],0,[],/translate/ibm_model.py_sample
3108,/home/amandapotts/git/nltk/nltk/translate/ibm_model.py_best_model2_alignment,"def best_model2_alignment(self, sentence_pair, j_pegged=None, i_pegged=0):
""""""
Finds the best alignment according to IBM Model 2
Used as a starting point for hill climbing in Models 3 and
above, because it is easier to compute than the best alignments
in higher models
:param sentence_pair: Source and target language sentence pair
to be word-aligned
:type sentence_pair: AlignedSent
:param j_pegged: If specified, the alignment point of j_pegged
will be fixed to i_pegged
:type j_pegged: int
:param i_pegged: Alignment point to j_pegged
:type i_pegged: int
""""""
src_sentence = [None] + sentence_pair.mots
trg_sentence = [""UNUSED""] + sentence_pair.words  # 1-indexed
l = len(src_sentence) - 1  # exclude NULL
m = len(trg_sentence) - 1
alignment = [0] * (m + 1)  # init all alignments to NULL
cepts = [[] for i in range(l + 1)]  # init all cepts to empty list
for j in range(1, m + 1):
if j == j_pegged:
best_i = i_pegged
else:
best_i = 0
max_alignment_prob = IBMModel.MIN_PROB
t = trg_sentence[j]
for i in range(0, l + 1):
s = src_sentence[i]
alignment_prob = (
self.translation_table[t][s] * self.alignment_table[i][j][l][m]
)
if alignment_prob >= max_alignment_prob:
max_alignment_prob = alignment_prob
best_i = i
alignment[j] = best_i
cepts[best_i].append(j)
return AlignmentInfo(
tuple(alignment), tuple(src_sentence), tuple(trg_sentence), cepts
)
",[],0,[],/translate/ibm_model.py_best_model2_alignment
3109,/home/amandapotts/git/nltk/nltk/translate/ibm_model.py_hillclimb,"def hillclimb(self, alignment_info, j_pegged=None):
""""""
Starting from the alignment in ``alignment_info``, look at
neighboring alignments iteratively for the best one
There is no guarantee that the best alignment in the alignment
space will be found, because the algorithm might be stuck in a
local maximum.
:param j_pegged: If specified, the search will be constrained to
alignments where ``j_pegged`` remains unchanged
:type j_pegged: int
:return: The best alignment found from hill climbing
:rtype: AlignmentInfo
""""""
alignment = alignment_info  # alias with shorter name
max_probability = self.prob_t_a_given_s(alignment)
while True:
old_alignment = alignment
for neighbor_alignment in self.neighboring(alignment, j_pegged):
neighbor_probability = self.prob_t_a_given_s(neighbor_alignment)
if neighbor_probability > max_probability:
alignment = neighbor_alignment
max_probability = neighbor_probability
if alignment == old_alignment:
break
alignment.score = max_probability
return alignment
",[],0,[],/translate/ibm_model.py_hillclimb
3110,/home/amandapotts/git/nltk/nltk/translate/ibm_model.py_neighboring,"def neighboring(self, alignment_info, j_pegged=None):
""""""
Determine the neighbors of ``alignment_info``, obtained by
moving or swapping one alignment point
:param j_pegged: If specified, neighbors that have a different
alignment point from j_pegged will not be considered
:type j_pegged: int
:return: A set neighboring alignments represented by their
``AlignmentInfo``
:rtype: set(AlignmentInfo)
""""""
neighbors = set()
l = len(alignment_info.src_sentence) - 1  # exclude NULL
m = len(alignment_info.trg_sentence) - 1
original_alignment = alignment_info.alignment
original_cepts = alignment_info.cepts
for j in range(1, m + 1):
if j != j_pegged:
for i in range(0, l + 1):
new_alignment = list(original_alignment)
new_cepts = deepcopy(original_cepts)
old_i = original_alignment[j]
new_alignment[j] = i
insort_left(new_cepts[i], j)
new_cepts[old_i].remove(j)
new_alignment_info = AlignmentInfo(
tuple(new_alignment),
alignment_info.src_sentence,
alignment_info.trg_sentence,
new_cepts,
)
neighbors.add(new_alignment_info)
for j in range(1, m + 1):
if j != j_pegged:
for other_j in range(1, m + 1):
if other_j != j_pegged and other_j != j:
new_alignment = list(original_alignment)
new_cepts = deepcopy(original_cepts)
other_i = original_alignment[other_j]
i = original_alignment[j]
new_alignment[j] = other_i
new_alignment[other_j] = i
new_cepts[other_i].remove(other_j)
insort_left(new_cepts[other_i], j)
new_cepts[i].remove(j)
insort_left(new_cepts[i], other_j)
new_alignment_info = AlignmentInfo(
tuple(new_alignment),
alignment_info.src_sentence,
alignment_info.trg_sentence,
new_cepts,
)
neighbors.add(new_alignment_info)
return neighbors
",[],0,[],/translate/ibm_model.py_neighboring
3111,/home/amandapotts/git/nltk/nltk/translate/ibm_model.py_maximize_lexical_translation_probabilities,"def maximize_lexical_translation_probabilities(self, counts):
for t, src_words in counts.t_given_s.items():
for s in src_words:
estimate = counts.t_given_s[t][s] / counts.any_t_given_s[s]
self.translation_table[t][s] = max(estimate, IBMModel.MIN_PROB)
",[],0,[],/translate/ibm_model.py_maximize_lexical_translation_probabilities
3112,/home/amandapotts/git/nltk/nltk/translate/ibm_model.py_maximize_fertility_probabilities,"def maximize_fertility_probabilities(self, counts):
for phi, src_words in counts.fertility.items():
for s in src_words:
estimate = counts.fertility[phi][s] / counts.fertility_for_any_phi[s]
self.fertility_table[phi][s] = max(estimate, IBMModel.MIN_PROB)
",[],0,[],/translate/ibm_model.py_maximize_fertility_probabilities
3113,/home/amandapotts/git/nltk/nltk/translate/ibm_model.py_maximize_null_generation_probabilities,"def maximize_null_generation_probabilities(self, counts):
p1_estimate = counts.p1 / (counts.p1 + counts.p0)
p1_estimate = max(p1_estimate, IBMModel.MIN_PROB)
self.p1 = min(p1_estimate, 1 - IBMModel.MIN_PROB)
",[],0,[],/translate/ibm_model.py_maximize_null_generation_probabilities
3114,/home/amandapotts/git/nltk/nltk/translate/ibm_model.py_prob_of_alignments,"def prob_of_alignments(self, alignments):
probability = 0
for alignment_info in alignments:
probability += self.prob_t_a_given_s(alignment_info)
return probability
",[],0,[],/translate/ibm_model.py_prob_of_alignments
3115,/home/amandapotts/git/nltk/nltk/translate/ibm_model.py_prob_t_a_given_s,"def prob_t_a_given_s(self, alignment_info):
""""""
Probability of target sentence and an alignment given the
source sentence
All required information is assumed to be in ``alignment_info``
and self.
Derived classes should override this method
""""""
return 0.0
",[],0,[],/translate/ibm_model.py_prob_t_a_given_s
3116,/home/amandapotts/git/nltk/nltk/translate/ibm_model.py___init__,"def __init__(self, alignment, src_sentence, trg_sentence, cepts):
if not isinstance(alignment, tuple):
raise TypeError(
""The alignment must be a tuple because it is used ""
""to uniquely identify AlignmentInfo objects.""
)
self.alignment = alignment
""""""
tuple(int): Alignment function. ``alignment[j]`` is the position
in the source sentence that is aligned to the position j in the
target sentence.
""""""
self.src_sentence = src_sentence
""""""
tuple(str): Source sentence referred to by this object.
Should include NULL token (None) in index 0.
""""""
self.trg_sentence = trg_sentence
""""""
tuple(str): Target sentence referred to by this object.
Should have a dummy element in index 0 so that the first word
starts from index 1.
""""""
self.cepts = cepts
""""""
list(list(int)): The positions of the target words, in
ascending order, aligned to a source word position. For example,
cepts[4] = (2, 3, 7) means that words in positions 2, 3 and 7
of the target sentence are aligned to the word in position 4 of
the source sentence
""""""
self.score = None
""""""
float: Optional. Probability of alignment, as defined by the
IBM model that assesses this alignment
""""""
",[],0,[],/translate/ibm_model.py___init__
3117,/home/amandapotts/git/nltk/nltk/translate/ibm_model.py_fertility_of_i,"def fertility_of_i(self, i):
""""""
Fertility of word in position ``i`` of the source sentence
""""""
return len(self.cepts[i])
",[],0,[],/translate/ibm_model.py_fertility_of_i
3118,/home/amandapotts/git/nltk/nltk/translate/ibm_model.py_is_head_word,"def is_head_word(self, j):
""""""
:return: Whether the word in position ``j`` of the target
sentence is a head word
""""""
i = self.alignment[j]
return self.cepts[i][0] == j
",[],0,[],/translate/ibm_model.py_is_head_word
3119,/home/amandapotts/git/nltk/nltk/translate/ibm_model.py_center_of_cept,"def center_of_cept(self, i):
""""""
:return: The ceiling of the average positions of the words in
the tablet of cept ``i``, or 0 if ``i`` is None
""""""
if i is None:
return 0
average_position = sum(self.cepts[i]) / len(self.cepts[i])
return int(ceil(average_position))
",[],0,[],/translate/ibm_model.py_center_of_cept
3120,/home/amandapotts/git/nltk/nltk/translate/ibm_model.py_previous_cept,"def previous_cept(self, j):
""""""
:return: The previous cept of ``j``, or None if ``j`` belongs to
the first cept
""""""
i = self.alignment[j]
if i == 0:
raise ValueError(
""Words aligned to NULL cannot have a previous ""
""cept because NULL has no position""
)
previous_cept = i - 1
while previous_cept > 0 and self.fertility_of_i(previous_cept) == 0:
previous_cept -= 1
if previous_cept <= 0:
previous_cept = None
return previous_cept
",[],0,[],/translate/ibm_model.py_previous_cept
3121,/home/amandapotts/git/nltk/nltk/translate/ibm_model.py_previous_in_tablet,"def previous_in_tablet(self, j):
""""""
:return: The position of the previous word that is in the same
tablet as ``j``, or None if ``j`` is the first word of the
tablet
""""""
i = self.alignment[j]
tablet_position = self.cepts[i].index(j)
if tablet_position == 0:
return None
return self.cepts[i][tablet_position - 1]
",[],0,[],/translate/ibm_model.py_previous_in_tablet
3122,/home/amandapotts/git/nltk/nltk/translate/ibm_model.py_zero_indexed_alignment,"def zero_indexed_alignment(self):
""""""
:return: Zero-indexed alignment, suitable for use in external
``nltk.translate`` modules like ``nltk.translate.Alignment``
:rtype: list(tuple)
""""""
zero_indexed_alignment = []
for j in range(1, len(self.trg_sentence)):
i = self.alignment[j] - 1
if i < 0:
i = None  # alignment to NULL token
zero_indexed_alignment.append((j - 1, i))
return zero_indexed_alignment
",[],0,[],/translate/ibm_model.py_zero_indexed_alignment
3123,/home/amandapotts/git/nltk/nltk/translate/ibm_model.py___eq__,"def __eq__(self, other):
return self.alignment == other.alignment
",[],0,[],/translate/ibm_model.py___eq__
3124,/home/amandapotts/git/nltk/nltk/translate/ibm_model.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/translate/ibm_model.py___ne__
3125,/home/amandapotts/git/nltk/nltk/translate/ibm_model.py___hash__,"def __hash__(self):
return hash(self.alignment)
",[],0,[],/translate/ibm_model.py___hash__
3126,/home/amandapotts/git/nltk/nltk/translate/ibm_model.py_update_lexical_translation,"def update_lexical_translation(self, count, alignment_info, j):
i = alignment_info.alignment[j]
t = alignment_info.trg_sentence[j]
s = alignment_info.src_sentence[i]
self.t_given_s[t][s] += count
self.any_t_given_s[s] += count
",[],0,[],/translate/ibm_model.py_update_lexical_translation
3127,/home/amandapotts/git/nltk/nltk/translate/ibm_model.py_update_null_generation,"def update_null_generation(self, count, alignment_info):
m = len(alignment_info.trg_sentence) - 1
fertility_of_null = alignment_info.fertility_of_i(0)
self.p1 += fertility_of_null * count
self.p0 += (m - 2 * fertility_of_null) * count
",[],0,[],/translate/ibm_model.py_update_null_generation
3128,/home/amandapotts/git/nltk/nltk/translate/ibm_model.py_update_fertility,"def update_fertility(self, count, alignment_info):
for i in range(0, len(alignment_info.src_sentence)):
s = alignment_info.src_sentence[i]
phi = alignment_info.fertility_of_i(i)
self.fertility[phi][s] += count
self.fertility_for_any_phi[s] += count
",[],0,[],/translate/ibm_model.py_update_fertility
3129,/home/amandapotts/git/nltk/nltk/translate/stack_decoder.py___init__,"def __init__(self, phrase_table, language_model):
""""""
:param phrase_table: Table of translations for source language
phrases and the log probabilities for those translations.
:type phrase_table: PhraseTable
:param language_model: Target language model. Must define a
``probability_change`` method that calculates the change in
log probability of a sentence, if a given string is appended
to it.
This interface is experimental and will likely be replaced
with nltk.model once it is implemented.
:type language_model: object
""""""
self.phrase_table = phrase_table
self.language_model = language_model
self.word_penalty = 0.0
""""""
float: Influences the translation length exponentially.
If positive, shorter translations are preferred.
If negative, longer translations are preferred.
If zero, no penalty is applied.
""""""
self.beam_threshold = 0.0
""""""
float: Hypotheses that score below this factor of the best
hypothesis in a stack are dropped from consideration.
Value between 0.0 and 1.0.
""""""
self.stack_size = 100
""""""
int: Maximum number of hypotheses to consider in a stack.
Higher values increase the likelihood of a good translation,
but increases processing time.
""""""
self.__distortion_factor = 0.5
self.__compute_log_distortion()
",[],0,[],/translate/stack_decoder.py___init__
3130,/home/amandapotts/git/nltk/nltk/translate/stack_decoder.py_distortion_factor,"def distortion_factor(self):
""""""
float: Amount of reordering of source phrases.
Lower values favour monotone translation, suitable when
word order is similar for both source and target languages.
Value between 0.0 and 1.0. Default 0.5.
""""""
return self.__distortion_factor
",[],0,[],/translate/stack_decoder.py_distortion_factor
3131,/home/amandapotts/git/nltk/nltk/translate/stack_decoder.py_distortion_factor,"def distortion_factor(self, d):
self.__distortion_factor = d
self.__compute_log_distortion()
",[],0,[],/translate/stack_decoder.py_distortion_factor
3132,/home/amandapotts/git/nltk/nltk/translate/stack_decoder.py___compute_log_distortion,"def __compute_log_distortion(self):
if self.__distortion_factor == 0.0:
self.__log_distortion_factor = log(1e-9)  # 1e-9 is almost zero
else:
self.__log_distortion_factor = log(self.__distortion_factor)
",[],0,[],/translate/stack_decoder.py___compute_log_distortion
3133,/home/amandapotts/git/nltk/nltk/translate/stack_decoder.py_translate,"def translate(self, src_sentence):
""""""
:param src_sentence: Sentence to be translated
:type src_sentence: list(str)
:return: Translated sentence
:rtype: list(str)
""""""
sentence = tuple(src_sentence)  # prevent accidental modification
sentence_length = len(sentence)
stacks = [
_Stack(self.stack_size, self.beam_threshold)
for _ in range(0, sentence_length + 1)
]
empty_hypothesis = _Hypothesis()
stacks[0].push(empty_hypothesis)
all_phrases = self.find_all_src_phrases(sentence)
future_score_table = self.compute_future_scores(sentence)
for stack in stacks:
for hypothesis in stack:
possible_expansions = StackDecoder.valid_phrases(
all_phrases, hypothesis
)
for src_phrase_span in possible_expansions:
src_phrase = sentence[src_phrase_span[0] : src_phrase_span[1]]
for translation_option in self.phrase_table.translations_for(
src_phrase
):
raw_score = self.expansion_score(
hypothesis, translation_option, src_phrase_span
)
new_hypothesis = _Hypothesis(
raw_score=raw_score,
src_phrase_span=src_phrase_span,
trg_phrase=translation_option.trg_phrase,
previous=hypothesis,
)
new_hypothesis.future_score = self.future_score(
new_hypothesis, future_score_table, sentence_length
)
total_words = new_hypothesis.total_translated_words()
stacks[total_words].push(new_hypothesis)
if not stacks[sentence_length]:
warnings.warn(
""Unable to translate all words. ""
""The source sentence contains words not in ""
""the phrase table""
)
return []
best_hypothesis = stacks[sentence_length].best()
return best_hypothesis.translation_so_far()
",[],0,[],/translate/stack_decoder.py_translate
3134,/home/amandapotts/git/nltk/nltk/translate/stack_decoder.py_find_all_src_phrases,"def find_all_src_phrases(self, src_sentence):
""""""
Finds all subsequences in src_sentence that have a phrase
translation in the translation table
:type src_sentence: tuple(str)
:return: Subsequences that have a phrase translation,
represented as a table of lists of end positions.
For example, if result[2] is [5, 6, 9], then there are
three phrases starting from position 2 in ``src_sentence``,
ending at positions 5, 6, and 9 exclusive. The list of
ending positions are in ascending order.
:rtype: list(list(int))
""""""
sentence_length = len(src_sentence)
phrase_indices = [[] for _ in src_sentence]
for start in range(0, sentence_length):
for end in range(start + 1, sentence_length + 1):
potential_phrase = src_sentence[start:end]
if potential_phrase in self.phrase_table:
phrase_indices[start].append(end)
return phrase_indices
",[],0,[],/translate/stack_decoder.py_find_all_src_phrases
3135,/home/amandapotts/git/nltk/nltk/translate/stack_decoder.py_future_score,"def future_score(self, hypothesis, future_score_table, sentence_length):
""""""
Determines the approximate score for translating the
untranslated words in ``hypothesis``
""""""
score = 0.0
for span in hypothesis.untranslated_spans(sentence_length):
score += future_score_table[span[0]][span[1]]
return score
",[],0,[],/translate/stack_decoder.py_future_score
3136,/home/amandapotts/git/nltk/nltk/translate/stack_decoder.py_expansion_score,"def expansion_score(self, hypothesis, translation_option, src_phrase_span):
""""""
Calculate the score of expanding ``hypothesis`` with
``translation_option``
:param hypothesis: Hypothesis being expanded
:type hypothesis: _Hypothesis
:param translation_option: Information about the proposed expansion
:type translation_option: PhraseTableEntry
:param src_phrase_span: Word position span of the source phrase
:type src_phrase_span: tuple(int, int)
""""""
score = hypothesis.raw_score
score += translation_option.log_prob
score += self.language_model.probability_change(
hypothesis, translation_option.trg_phrase
)
score += self.distortion_score(hypothesis, src_phrase_span)
score -= self.word_penalty * len(translation_option.trg_phrase)
return score
",[],0,[],/translate/stack_decoder.py_expansion_score
3137,/home/amandapotts/git/nltk/nltk/translate/stack_decoder.py_distortion_score,"def distortion_score(self, hypothesis, next_src_phrase_span):
if not hypothesis.src_phrase_span:
return 0.0
next_src_phrase_start = next_src_phrase_span[0]
prev_src_phrase_end = hypothesis.src_phrase_span[1]
distortion_distance = next_src_phrase_start - prev_src_phrase_end
return abs(distortion_distance) * self.__log_distortion_factor
",[],0,[],/translate/stack_decoder.py_distortion_score
3138,/home/amandapotts/git/nltk/nltk/translate/stack_decoder.py_valid_phrases,"def valid_phrases(all_phrases_from, hypothesis):
""""""
Extract phrases from ``all_phrases_from`` that contains words
that have not been translated by ``hypothesis``
:param all_phrases_from: Phrases represented by their spans, in
the same format as the return value of
``find_all_src_phrases``
:type all_phrases_from: list(list(int))
:type hypothesis: _Hypothesis
:return: A list of phrases, represented by their spans, that
cover untranslated positions.
:rtype: list(tuple(int, int))
""""""
untranslated_spans = hypothesis.untranslated_spans(len(all_phrases_from))
valid_phrases = []
for available_span in untranslated_spans:
start = available_span[0]
available_end = available_span[1]
while start < available_end:
for phrase_end in all_phrases_from[start]:
if phrase_end > available_end:
break
valid_phrases.append((start, phrase_end))
start += 1
return valid_phrases
",[],0,[],/translate/stack_decoder.py_valid_phrases
3139,/home/amandapotts/git/nltk/nltk/translate/stack_decoder.py___init__,"def __init__(
self,
raw_score=0.0,
src_phrase_span=(),
trg_phrase=(),
previous=None,
future_score=0.0,
",[],0,[],/translate/stack_decoder.py___init__
3140,/home/amandapotts/git/nltk/nltk/translate/stack_decoder.py_score,"def score(self):
""""""
Overall score of hypothesis after accounting for local and
global features
""""""
return self.raw_score + self.future_score
",[],0,[],/translate/stack_decoder.py_score
3141,/home/amandapotts/git/nltk/nltk/translate/stack_decoder.py_untranslated_spans,"def untranslated_spans(self, sentence_length):
""""""
Starting from each untranslated word, find the longest
continuous span of untranslated positions
:param sentence_length: Length of source sentence being
translated by the hypothesis
:type sentence_length: int
:rtype: list(tuple(int, int))
""""""
translated_positions = self.translated_positions()
translated_positions.sort()
translated_positions.append(sentence_length)  # add sentinel position
untranslated_spans = []
start = 0
for end in translated_positions:
if start < end:
untranslated_spans.append((start, end))
start = end + 1
return untranslated_spans
",[],0,[],/translate/stack_decoder.py_untranslated_spans
3142,/home/amandapotts/git/nltk/nltk/translate/stack_decoder.py_translated_positions,"def translated_positions(self):
""""""
List of positions in the source sentence of words already
translated. The list is not sorted.
:rtype: list(int)
""""""
translated_positions = []
current_hypothesis = self
while current_hypothesis.previous is not None:
translated_span = current_hypothesis.src_phrase_span
translated_positions.extend(range(translated_span[0], translated_span[1]))
current_hypothesis = current_hypothesis.previous
return translated_positions
",[],0,[],/translate/stack_decoder.py_translated_positions
3143,/home/amandapotts/git/nltk/nltk/translate/stack_decoder.py_total_translated_words,"def total_translated_words(self):
return len(self.translated_positions())
",[],0,[],/translate/stack_decoder.py_total_translated_words
3144,/home/amandapotts/git/nltk/nltk/translate/stack_decoder.py_translation_so_far,"def translation_so_far(self):
translation = []
self.__build_translation(self, translation)
return translation
",[],0,[],/translate/stack_decoder.py_translation_so_far
3145,/home/amandapotts/git/nltk/nltk/translate/stack_decoder.py___build_translation,"def __build_translation(self, hypothesis, output):
if hypothesis.previous is None:
return
self.__build_translation(hypothesis.previous, output)
output.extend(hypothesis.trg_phrase)
",[],0,[],/translate/stack_decoder.py___build_translation
3146,/home/amandapotts/git/nltk/nltk/translate/stack_decoder.py___init__,"def __init__(self, max_size=100, beam_threshold=0.0):
""""""
:param beam_threshold: Hypotheses that score less than this
factor of the best hypothesis are discarded from the stack.
Value must be between 0.0 and 1.0.
:type beam_threshold: float
""""""
self.max_size = max_size
self.items = []
if beam_threshold == 0.0:
self.__log_beam_threshold = float(""-inf"")
else:
self.__log_beam_threshold = log(beam_threshold)
",[],0,[],/translate/stack_decoder.py___init__
3147,/home/amandapotts/git/nltk/nltk/translate/stack_decoder.py_threshold_prune,"def threshold_prune(self):
if not self.items:
return
threshold = self.items[0].score() + self.__log_beam_threshold
for hypothesis in reversed(self.items):
if hypothesis.score() < threshold:
self.items.pop()
else:
break
",[],0,[],/translate/stack_decoder.py_threshold_prune
3148,/home/amandapotts/git/nltk/nltk/translate/stack_decoder.py_best,"def best(self):
""""""
:return: Hypothesis with the highest score in the stack
:rtype: _Hypothesis
""""""
if self.items:
return self.items[0]
return None
",[],0,[],/translate/stack_decoder.py_best
3149,/home/amandapotts/git/nltk/nltk/translate/stack_decoder.py___iter__,"def __iter__(self):
return iter(self.items)
",[],0,[],/translate/stack_decoder.py___iter__
3150,/home/amandapotts/git/nltk/nltk/translate/stack_decoder.py___contains__,"def __contains__(self, hypothesis):
return hypothesis in self.items
",[],0,[],/translate/stack_decoder.py___contains__
3151,/home/amandapotts/git/nltk/nltk/translate/stack_decoder.py___bool__,"def __bool__(self):
return len(self.items) != 0
",[],0,[],/translate/stack_decoder.py___bool__
3152,/home/amandapotts/git/nltk/nltk/translate/ibm4.py___init__,"def __init__(
self,
sentence_aligned_corpus,
iterations,
source_word_classes,
target_word_classes,
probability_tables=None,
",[],0,[],/translate/ibm4.py___init__
3153,/home/amandapotts/git/nltk/nltk/translate/ibm4.py_train,"def train(self, parallel_corpus):
counts = Model4Counts()
for aligned_sentence in parallel_corpus:
m = len(aligned_sentence.words)
sampled_alignments, best_alignment = self.sample(aligned_sentence)
aligned_sentence.alignment = Alignment(
best_alignment.zero_indexed_alignment()
)
total_count = self.prob_of_alignments(sampled_alignments)
for alignment_info in sampled_alignments:
count = self.prob_t_a_given_s(alignment_info)
normalized_count = count / total_count
for j in range(1, m + 1):
counts.update_lexical_translation(
normalized_count, alignment_info, j
)
counts.update_distortion(
normalized_count,
alignment_info,
j,
self.src_classes,
self.trg_classes,
)
counts.update_null_generation(normalized_count, alignment_info)
counts.update_fertility(normalized_count, alignment_info)
existing_alignment_table = self.alignment_table
self.reset_probabilities()
self.alignment_table = existing_alignment_table  # don't retrain
self.maximize_lexical_translation_probabilities(counts)
self.maximize_distortion_probabilities(counts)
self.maximize_fertility_probabilities(counts)
self.maximize_null_generation_probabilities(counts)
",[],0,[],/translate/ibm4.py_train
3154,/home/amandapotts/git/nltk/nltk/translate/ibm4.py_maximize_distortion_probabilities,"def maximize_distortion_probabilities(self, counts):
head_d_table = self.head_distortion_table
for dj, src_classes in counts.head_distortion.items():
for s_cls, trg_classes in src_classes.items():
for t_cls in trg_classes:
estimate = (
counts.head_distortion[dj][s_cls][t_cls]
/ counts.head_distortion_for_any_dj[s_cls][t_cls]
)
head_d_table[dj][s_cls][t_cls] = max(estimate, IBMModel.MIN_PROB)
non_head_d_table = self.non_head_distortion_table
for dj, trg_classes in counts.non_head_distortion.items():
for t_cls in trg_classes:
estimate = (
counts.non_head_distortion[dj][t_cls]
/ counts.non_head_distortion_for_any_dj[t_cls]
)
non_head_d_table[dj][t_cls] = max(estimate, IBMModel.MIN_PROB)
",[],0,[],/translate/ibm4.py_maximize_distortion_probabilities
3155,/home/amandapotts/git/nltk/nltk/translate/ibm4.py_prob_t_a_given_s,"def prob_t_a_given_s(self, alignment_info):
""""""
Probability of target sentence and an alignment given the
source sentence
""""""
return IBMModel4.model4_prob_t_a_given_s(alignment_info, self)
",[],0,[],/translate/ibm4.py_prob_t_a_given_s
3156,/home/amandapotts/git/nltk/nltk/translate/ibm4.py_model4_prob_t_a_given_s,"def model4_prob_t_a_given_s(alignment_info, ibm_model):
probability = 1.0
MIN_PROB = IBMModel.MIN_PROB
",[],0,[],/translate/ibm4.py_model4_prob_t_a_given_s
3157,/home/amandapotts/git/nltk/nltk/translate/ibm4.py_null_generation_term,"def null_generation_term():
value = 1.0
p1 = ibm_model.p1
p0 = 1 - p1
null_fertility = alignment_info.fertility_of_i(0)
m = len(alignment_info.trg_sentence) - 1
value *= pow(p1, null_fertility) * pow(p0, m - 2 * null_fertility)
if value < MIN_PROB:
return MIN_PROB
for i in range(1, null_fertility + 1):
value *= (m - null_fertility - i + 1) / i
return value
",[],0,[],/translate/ibm4.py_null_generation_term
3158,/home/amandapotts/git/nltk/nltk/translate/ibm4.py_fertility_term,"def fertility_term():
value = 1.0
src_sentence = alignment_info.src_sentence
for i in range(1, len(src_sentence)):
fertility = alignment_info.fertility_of_i(i)
value *= (
factorial(fertility)
)
if value < MIN_PROB:
return MIN_PROB
return value
",[],0,[],/translate/ibm4.py_fertility_term
3159,/home/amandapotts/git/nltk/nltk/translate/ibm4.py_lexical_translation_term,"def lexical_translation_term(j):
t = alignment_info.trg_sentence[j]
i = alignment_info.alignment[j]
s = alignment_info.src_sentence[i]
return ibm_model.translation_table[t][s]
",[],0,[],/translate/ibm4.py_lexical_translation_term
3160,/home/amandapotts/git/nltk/nltk/translate/ibm4.py_distortion_term,"def distortion_term(j):
t = alignment_info.trg_sentence[j]
i = alignment_info.alignment[j]
if i == 0:
return 1.0
if alignment_info.is_head_word(j):
previous_cept = alignment_info.previous_cept(j)
src_class = None
if previous_cept is not None:
previous_s = alignment_info.src_sentence[previous_cept]
src_class = ibm_model.src_classes[previous_s]
trg_class = ibm_model.trg_classes[t]
dj = j - alignment_info.center_of_cept(previous_cept)
return ibm_model.head_distortion_table[dj][src_class][trg_class]
previous_position = alignment_info.previous_in_tablet(j)
trg_class = ibm_model.trg_classes[t]
dj = j - previous_position
return ibm_model.non_head_distortion_table[dj][trg_class]
",[],0,[],/translate/ibm4.py_distortion_term
3161,/home/amandapotts/git/nltk/nltk/translate/ibm4.py_update_distortion,"def update_distortion(self, count, alignment_info, j, src_classes, trg_classes):
i = alignment_info.alignment[j]
t = alignment_info.trg_sentence[j]
if i == 0:
pass
elif alignment_info.is_head_word(j):
previous_cept = alignment_info.previous_cept(j)
if previous_cept is not None:
previous_src_word = alignment_info.src_sentence[previous_cept]
src_class = src_classes[previous_src_word]
else:
src_class = None
trg_class = trg_classes[t]
dj = j - alignment_info.center_of_cept(previous_cept)
self.head_distortion[dj][src_class][trg_class] += count
self.head_distortion_for_any_dj[src_class][trg_class] += count
else:
previous_j = alignment_info.previous_in_tablet(j)
trg_class = trg_classes[t]
dj = j - previous_j
self.non_head_distortion[dj][trg_class] += count
self.non_head_distortion_for_any_dj[trg_class] += count
",[],0,[],/translate/ibm4.py_update_distortion
3162,/home/amandapotts/git/nltk/nltk/translate/nist_score.py_sentence_nist,"def sentence_nist(references, hypothesis, n=5):
""""""
Calculate NIST score from
George Doddington. 2002. ""Automatic evaluation of machine translation quality
using n-gram co-occurrence statistics."" Proceedings of HLT.
Morgan Kaufmann Publishers Inc. https://dl.acm.org/citation.cfm?id=1289189.1289273
DARPA commissioned NIST to develop an MT evaluation facility based on the BLEU
score. The official script used by NIST to compute BLEU and NIST score is
mteval-14.pl. The main differences are:
- BLEU uses geometric mean of the ngram overlaps, NIST uses arithmetic mean.
- NIST has a different brevity penalty
- NIST score from mteval-14.pl has a self-contained tokenizer
Note: The mteval-14.pl includes a smoothing function for BLEU score that is NOT
used in the NIST score computation.
>>> hypothesis1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which',
...               'ensures', 'that', 'the', 'military', 'always',
...               'obeys', 'the', 'commands', 'of', 'the', 'party']
>>> hypothesis2 = ['It', 'is', 'to', 'insure', 'the', 'troops',
...               'forever', 'hearing', 'the', 'activity', 'guidebook',
...               'that', 'party', 'direct']
>>> reference1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'that',
...               'ensures', 'that', 'the', 'military', 'will', 'forever',
...               'heed', 'Party', 'commands']
>>> reference2 = ['It', 'is', 'the', 'guiding', 'principle', 'which',
...               'guarantees', 'the', 'military', 'forces', 'always',
...               'being', 'under', 'the', 'command', 'of', 'the',
...               'Party']
>>> reference3 = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the',
...               'army', 'always', 'to', 'heed', 'the', 'directions',
...               'of', 'the', 'party']
>>> sentence_nist([reference1, reference2, reference3], hypothesis1) # doctest: +ELLIPSIS
3.3709...
>>> sentence_nist([reference1, reference2, reference3], hypothesis2) # doctest: +ELLIPSIS
1.4619...
:param references: reference sentences
:type references: list(list(str))
:param hypothesis: a hypothesis sentence
:type hypothesis: list(str)
:param n: highest n-gram order
:type n: int
""""""
return corpus_nist([references], [hypothesis], n)
",[],0,[],/translate/nist_score.py_sentence_nist
3163,/home/amandapotts/git/nltk/nltk/translate/nist_score.py_corpus_nist,"def corpus_nist(list_of_references, hypotheses, n=5):
""""""
Calculate a single corpus-level NIST score (aka. system-level BLEU) for all
the hypotheses and their respective references.
:param references: a corpus of lists of reference sentences, w.r.t. hypotheses
:type references: list(list(list(str)))
:param hypotheses: a list of hypothesis sentences
:type hypotheses: list(list(str))
:param n: highest n-gram order
:type n: int
""""""
assert len(list_of_references) == len(
hypotheses
), ""The number of hypotheses and their reference(s) should be the same""
ngram_freq = Counter()
total_reference_words = 0
for (
references
) in list_of_references:  # For each source sent, there's a list of reference sents.
for reference in references:
for i in range(1, n + 1):
ngram_freq.update(ngrams(reference, i))
total_reference_words += len(reference)
information_weights = {}
for _ngram in ngram_freq:  # w_1 ... w_n
_mgram = _ngram[:-1]  #  w_1 ... w_n-1
if _mgram and _mgram in ngram_freq:
numerator = ngram_freq[_mgram]
else:
numerator = total_reference_words
information_weights[_ngram] = math.log(numerator / ngram_freq[_ngram], 2)
nist_precision_numerator_per_ngram = Counter()
nist_precision_denominator_per_ngram = Counter()
l_ref, l_sys = 0, 0
for i in range(1, n + 1):
for references, hypothesis in zip(list_of_references, hypotheses):
hyp_len = len(hypothesis)
nist_score_per_ref = []
for reference in references:
_ref_len = len(reference)
hyp_ngrams = (
Counter(ngrams(hypothesis, i))
if len(hypothesis) >= i
else Counter()
)
ref_ngrams = (
Counter(ngrams(reference, i)) if len(reference) >= i else Counter()
)
ngram_overlaps = hyp_ngrams & ref_ngrams
_numerator = sum(
information_weights[_ngram] * count
for _ngram, count in ngram_overlaps.items()
)
_denominator = sum(hyp_ngrams.values())
_precision = 0 if _denominator == 0 else _numerator / _denominator
nist_score_per_ref.append(
(_precision, _numerator, _denominator, _ref_len)
)
precision, numerator, denominator, ref_len = max(nist_score_per_ref)
nist_precision_numerator_per_ngram[i] += numerator
nist_precision_denominator_per_ngram[i] += denominator
l_ref += ref_len
l_sys += hyp_len
nist_precision = 0
for i in nist_precision_numerator_per_ngram:
precision = (
nist_precision_numerator_per_ngram[i]
/ nist_precision_denominator_per_ngram[i]
)
nist_precision += precision
return nist_precision * nist_length_penalty(l_ref, l_sys)
",[],0,[],/translate/nist_score.py_corpus_nist
3164,/home/amandapotts/git/nltk/nltk/translate/nist_score.py_nist_length_penalty,"def nist_length_penalty(ref_len, hyp_len):
""""""
Calculates the NIST length penalty, from Eq. 3 in Doddington (2002)
penalty = exp( beta * log( min( len(hyp)/len(ref) , 1.0 )))
where,
`beta` is chosen to make the brevity penalty factor = 0.5 when the
no. of words in the system output (hyp) is 2/3 of the average
no. of words in the reference translation (ref)
The NIST penalty is different from BLEU's such that it minimize the impact
of the score of small variations in the length of a translation.
See Fig. 4 in  Doddington (2002)
""""""
ratio = hyp_len / ref_len
if 0 < ratio < 1:
ratio_x, score_x = 1.5, 0.5
beta = math.log(score_x) / math.log(ratio_x) ** 2
return math.exp(beta * math.log(ratio) ** 2)
else:  # ratio <= 0 or ratio >= 1
return max(min(ratio, 1.0), 0.0)
",[],0,[],/translate/nist_score.py_nist_length_penalty
3165,/home/amandapotts/git/nltk/nltk/tag/senna.py___init__,"def __init__(self, path, encoding=""utf-8""):
super().__init__(path, [""pos""], encoding)
",[],0,[],/tag/senna.py___init__
3166,/home/amandapotts/git/nltk/nltk/tag/senna.py_tag_sents,"def tag_sents(self, sentences):
""""""
Applies the tag method over a list of sentences. This method will return
for each sentence a list of tuples of (word, tag).
""""""
tagged_sents = super().tag_sents(sentences)
for i in range(len(tagged_sents)):
for j in range(len(tagged_sents[i])):
annotations = tagged_sents[i][j]
tagged_sents[i][j] = (annotations[""word""], annotations[""pos""])
return tagged_sents
",[],0,[],/tag/senna.py_tag_sents
3167,/home/amandapotts/git/nltk/nltk/tag/senna.py___init__,"def __init__(self, path, encoding=""utf-8""):
super().__init__(path, [""chk""], encoding)
",[],0,[],/tag/senna.py___init__
3168,/home/amandapotts/git/nltk/nltk/tag/senna.py_tag_sents,"def tag_sents(self, sentences):
""""""
Applies the tag method over a list of sentences. This method will return
for each sentence a list of tuples of (word, tag).
""""""
tagged_sents = super().tag_sents(sentences)
for i in range(len(tagged_sents)):
for j in range(len(tagged_sents[i])):
annotations = tagged_sents[i][j]
tagged_sents[i][j] = (annotations[""word""], annotations[""chk""])
return tagged_sents
",[],0,[],/tag/senna.py_tag_sents
3169,/home/amandapotts/git/nltk/nltk/tag/senna.py_bio_to_chunks,"def bio_to_chunks(self, tagged_sent, chunk_type):
""""""
Extracts the chunks in a BIO chunk-tagged sentence.
>>> from nltk.tag import SennaChunkTagger
>>> chktagger = SennaChunkTagger('/usr/share/senna-v3.0')  # doctest: +SKIP
>>> sent = 'What is the airspeed of an unladen swallow ?'.split()
>>> tagged_sent = chktagger.tag(sent)  # doctest: +SKIP
>>> tagged_sent  # doctest: +SKIP
[('What', 'B-NP'), ('is', 'B-VP'), ('the', 'B-NP'), ('airspeed', 'I-NP'),
('of', 'B-PP'), ('an', 'B-NP'), ('unladen', 'I-NP'), ('swallow', 'I-NP'),
('?', 'O')]
>>> list(chktagger.bio_to_chunks(tagged_sent, chunk_type='NP'))  # doctest: +SKIP
[('What', '0'), ('the airspeed', '2-3'), ('an unladen swallow', '5-6-7')]
:param tagged_sent: A list of tuples of word and BIO chunk tag.
:type tagged_sent: list(tuple)
:param tagged_sent: The chunk tag that users want to extract, e.g. 'NP' or 'VP'
:type tagged_sent: str
:return: An iterable of tuples of chunks that users want to extract
and their corresponding indices.
:rtype: iter(tuple(str))
""""""
current_chunk = []
current_chunk_position = []
for idx, word_pos in enumerate(tagged_sent):
word, pos = word_pos
if ""-"" + chunk_type in pos:  # Append the word to the current_chunk.
current_chunk.append(word)
current_chunk_position.append(idx)
else:
if current_chunk:  # Flush the full chunk when out of an NP.
_chunk_str = "" "".join(current_chunk)
_chunk_pos_str = ""-"".join(map(str, current_chunk_position))
yield _chunk_str, _chunk_pos_str
current_chunk = []
current_chunk_position = []
if current_chunk:  # Flush the last chunk.
yield "" "".join(current_chunk), ""-"".join(map(str, current_chunk_position))
",[],0,[],/tag/senna.py_bio_to_chunks
3170,/home/amandapotts/git/nltk/nltk/tag/senna.py___init__,"def __init__(self, path, encoding=""utf-8""):
super().__init__(path, [""ner""], encoding)
",[],0,[],/tag/senna.py___init__
3171,/home/amandapotts/git/nltk/nltk/tag/senna.py_tag_sents,"def tag_sents(self, sentences):
""""""
Applies the tag method over a list of sentences. This method will return
for each sentence a list of tuples of (word, tag).
""""""
tagged_sents = super().tag_sents(sentences)
for i in range(len(tagged_sents)):
for j in range(len(tagged_sents[i])):
annotations = tagged_sents[i][j]
tagged_sents[i][j] = (annotations[""word""], annotations[""ner""])
return tagged_sents
",[],0,[],/tag/senna.py_tag_sents
3172,/home/amandapotts/git/nltk/nltk/tag/__init__.py__get_tagger,"def _get_tagger(lang=None):
if lang == ""rus"":
tagger = PerceptronTagger(False)
ap_russian_model_loc = ""file:"" + str(find(RUS_PICKLE))
tagger.load(ap_russian_model_loc)
else:
tagger = PerceptronTagger()
return tagger
",[],0,[],/tag/__init__.py__get_tagger
3173,/home/amandapotts/git/nltk/nltk/tag/__init__.py__pos_tag,"def _pos_tag(tokens, tagset=None, tagger=None, lang=None):
if lang not in [""eng"", ""rus""]:
raise NotImplementedError(
""Currently, NLTK pos_tag only supports English and Russian ""
""(i.e. lang='eng' or lang='rus')""
)
elif isinstance(tokens, str):
raise TypeError(""tokens: expected a list of strings, got a string"")
else:
tagged_tokens = tagger.tag(tokens)
if tagset:  # Maps to the specified tagset.
if lang == ""eng"":
tagged_tokens = [
(token, map_tag(""en-ptb"", tagset, tag))
for (token, tag) in tagged_tokens
]
elif lang == ""rus"":
tagged_tokens = [
(token, map_tag(""ru-rnc-new"", tagset, tag.partition(""="")[0]))
for (token, tag) in tagged_tokens
]
return tagged_tokens
",[],0,[],/tag/__init__.py__pos_tag
3174,/home/amandapotts/git/nltk/nltk/tag/__init__.py_pos_tag,"def pos_tag(tokens, tagset=None, lang=""eng""):
""""""
Use NLTK's currently recommended part of speech tagger to
tag the given list of tokens.
>>> from nltk.tag import pos_tag
>>> from nltk.tokenize import word_tokenize
>>> pos_tag(word_tokenize(""John's big idea isn't all that bad."")) # doctest: +NORMALIZE_WHITESPACE
[('John', 'NNP'), (""'s"", 'POS'), ('big', 'JJ'), ('idea', 'NN'), ('is', 'VBZ'),
(""n't"", 'RB'), ('all', 'PDT'), ('that', 'DT'), ('bad', 'JJ'), ('.', '.')]
>>> pos_tag(word_tokenize(""John's big idea isn't all that bad.""), tagset='universal') # doctest: +NORMALIZE_WHITESPACE
[('John', 'NOUN'), (""'s"", 'PRT'), ('big', 'ADJ'), ('idea', 'NOUN'), ('is', 'VERB'),
(""n't"", 'ADV'), ('all', 'DET'), ('that', 'DET'), ('bad', 'ADJ'), ('.', '.')]
NB. Use `pos_tag_sents()` for efficient tagging of more than one sentence.
:param tokens: Sequence of tokens to be tagged
:type tokens: list(str)
:param tagset: the tagset to be used, e.g. universal, wsj, brown
:type tagset: str
:param lang: the ISO 639 code of the language, e.g. 'eng' for English, 'rus' for Russian
:type lang: str
:return: The tagged tokens
:rtype: list(tuple(str, str))
""""""
tagger = _get_tagger(lang)
return _pos_tag(tokens, tagset, tagger, lang)
",[],0,[],/tag/__init__.py_pos_tag
3175,/home/amandapotts/git/nltk/nltk/tag/__init__.py_pos_tag_sents,"def pos_tag_sents(sentences, tagset=None, lang=""eng""):
""""""
Use NLTK's currently recommended part of speech tagger to tag the
given list of sentences, each consisting of a list of tokens.
:param sentences: List of sentences to be tagged
:type sentences: list(list(str))
:param tagset: the tagset to be used, e.g. universal, wsj, brown
:type tagset: str
:param lang: the ISO 639 code of the language, e.g. 'eng' for English, 'rus' for Russian
:type lang: str
:return: The list of tagged sentences
:rtype: list(list(tuple(str, str)))
""""""
tagger = _get_tagger(lang)
return [_pos_tag(sent, tagset, tagger, lang) for sent in sentences]
",[],0,[],/tag/__init__.py_pos_tag_sents
3176,/home/amandapotts/git/nltk/nltk/tag/hunpos.py___init__,"def __init__(
self, path_to_model, path_to_bin=None, encoding=_hunpos_charset, verbose=False
",[],0,[],/tag/hunpos.py___init__
3177,/home/amandapotts/git/nltk/nltk/tag/hunpos.py___del__,"def __del__(self):
self.close()
",[],0,[],/tag/hunpos.py___del__
3178,/home/amandapotts/git/nltk/nltk/tag/hunpos.py_close,"def close(self):
""""""Closes the pipe to the hunpos executable.""""""
if not self._closed:
self._hunpos.communicate()
self._closed = True
",[],0,[],/tag/hunpos.py_close
3179,/home/amandapotts/git/nltk/nltk/tag/hunpos.py___enter__,"def __enter__(self):
return self
",[],0,[],/tag/hunpos.py___enter__
3180,/home/amandapotts/git/nltk/nltk/tag/hunpos.py___exit__,"def __exit__(self, exc_type, exc_value, traceback):
self.close()
",[],0,[],/tag/hunpos.py___exit__
3181,/home/amandapotts/git/nltk/nltk/tag/hunpos.py_tag,"def tag(self, tokens):
""""""Tags a single sentence: a list of words.
The tokens should not contain any newline characters.
""""""
for token in tokens:
assert ""\n"" not in token, ""Tokens should not contain newlines""
if isinstance(token, str):
token = token.encode(self._encoding)
self._hunpos.stdin.write(token + b""\n"")
self._hunpos.stdin.write(b""\n"")
self._hunpos.stdin.flush()
tagged_tokens = []
for token in tokens:
tagged = self._hunpos.stdout.readline().strip().split(b""\t"")
tag = tagged[1] if len(tagged) > 1 else None
tagged_tokens.append((token, tag))
self._hunpos.stdout.readline()
return tagged_tokens
",[],0,[],/tag/hunpos.py_tag
3182,/home/amandapotts/git/nltk/nltk/tag/tnt.py___init__,"def __init__(self, unk=None, Trained=False, N=1000, C=False):
""""""
Construct a TnT statistical tagger. Tagger must be trained
before being used to tag input.
:param unk: instance of a POS tagger, conforms to TaggerI
:type  unk: TaggerI
:param Trained: Indication that the POS tagger is trained or not
:type  Trained: bool
:param N: Beam search degree (see above)
:type  N: int
:param C: Capitalization flag
:type  C: bool
Initializer, creates frequency distributions to be used
for tagging
_lx values represent the portion of the tri/bi/uni taggers
to be used to calculate the probability
N value is the number of possible solutions to maintain
while tagging. A good value for this is 1000
C is a boolean value which specifies to use or
not use the Capitalization of the word as additional
information for tagging.
NOTE: using capitalization may not increase the accuracy
of the tagger
""""""
self._uni = FreqDist()
self._bi = ConditionalFreqDist()
self._tri = ConditionalFreqDist()
self._wd = ConditionalFreqDist()
self._eos = ConditionalFreqDist()
self._l1 = 0.0
self._l2 = 0.0
self._l3 = 0.0
self._N = N
self._C = C
self._T = Trained
self._unk = unk
self.unknown = 0
self.known = 0
",[],0,[],/tag/tnt.py___init__
3183,/home/amandapotts/git/nltk/nltk/tag/tnt.py__safe_div,"def _safe_div(self, v1, v2):
""""""
Safe floating point division function, does not allow division by 0
returns -1 if the denominator is 0
""""""
if v2 == 0:
return -1
else:
return v1 / v2
",[],0,[],/tag/tnt.py__safe_div
3184,/home/amandapotts/git/nltk/nltk/tag/tnt.py_tagdata,"def tagdata(self, data):
""""""
Tags each sentence in a list of sentences
:param data:list of list of words
:type data: [[string,],]
:return: list of list of (word, tag) tuples
Invokes tag(sent) function for each sentence
compiles the results into a list of tagged sentences
each tagged sentence is a list of (word, tag) tuples
""""""
res = []
for sent in data:
res1 = self.tag(sent)
res.append(res1)
return res
",[],0,[],/tag/tnt.py_tagdata
3185,/home/amandapotts/git/nltk/nltk/tag/tnt.py_tag,"def tag(self, data):
""""""
Tags a single sentence
:param data: list of words
:type data: [string,]
:return: [(word, tag),]
Calls recursive function '_tagword'
to produce a list of tags
Associates the sequence of returned tags
with the correct words in the input sequence
returns a list of (word, tag) tuples
""""""
current_state = [([""BOS"", ""BOS""], 0.0)]
sent = list(data)
tags = self._tagword(sent, current_state)
res = []
for i in range(len(sent)):
(t, C) = tags[i + 2]
res.append((sent[i], t))
return res
",[],0,[],/tag/tnt.py_tag
3186,/home/amandapotts/git/nltk/nltk/tag/tnt.py__tagword,"def _tagword(self, sent, current_states):
""""""
:param sent : List of words remaining in the sentence
:type sent  : [word,]
:param current_states : List of possible tag combinations for
the sentence so far, and the log probability
associated with each tag combination
:type current_states  : [([tag, ], logprob), ]
Tags the first word in the sentence and
recursively tags the reminder of sentence
Uses formula specified above to calculate the probability
of a particular tag
""""""
if sent == []:
(h, logp) = current_states[0]
return h
word = sent[0]
sent = sent[1:]
new_states = []
C = False
if self._C and word[0].isupper():
C = True
if word in self._wd:
self.known += 1
for history, curr_sent_logprob in current_states:
logprobs = []
for t in self._wd[word].keys():
tC = (t, C)
p_uni = self._uni.freq(tC)
p_bi = self._bi[history[-1]].freq(tC)
p_tri = self._tri[tuple(history[-2:])].freq(tC)
p_wd = self._wd[word][t] / self._uni[tC]
p = self._l1 * p_uni + self._l2 * p_bi + self._l3 * p_tri
p2 = log(p, 2) + log(p_wd, 2)
new_states.append((history + [tC], curr_sent_logprob + p2))
else:
self.unknown += 1
p = 1
if self._unk is None:
tag = (""Unk"", C)
else:
[(_w, t)] = list(self._unk.tag([word]))
tag = (t, C)
for history, logprob in current_states:
history.append(tag)
new_states = current_states
new_states.sort(reverse=True, key=itemgetter(1))
if len(new_states) > self._N:
new_states = new_states[: self._N]
return self._tagword(sent, new_states)
",[],0,[],/tag/tnt.py__tagword
3187,/home/amandapotts/git/nltk/nltk/tag/tnt.py_basic_sent_chop,"def basic_sent_chop(data, raw=True):
""""""
Basic method for tokenizing input into sentences
for this tagger:
:param data: list of tokens (words or (word, tag) tuples)
:type data: str or tuple(str, str)
:param raw: boolean flag marking the input data
as a list of words or a list of tagged words
:type raw: bool
:return: list of sentences
sentences are a list of tokens
tokens are the same as the input
Function takes a list of tokens and separates the tokens into lists
where each list represents a sentence fragment
This function can separate both tagged and raw sequences into
basic sentences.
Sentence markers are the set of [,.!?]
This is a simple method which enhances the performance of the TnT
tagger. Better sentence tokenization will further enhance the results.
""""""
new_data = []
curr_sent = []
sent_mark = ["","", ""."", ""?"", ""!""]
if raw:
for word in data:
if word in sent_mark:
curr_sent.append(word)
new_data.append(curr_sent)
curr_sent = []
else:
curr_sent.append(word)
else:
for word, tag in data:
if word in sent_mark:
curr_sent.append((word, tag))
new_data.append(curr_sent)
curr_sent = []
else:
curr_sent.append((word, tag))
return new_data
",[],0,[],/tag/tnt.py_basic_sent_chop
3188,/home/amandapotts/git/nltk/nltk/tag/tnt.py_demo,"def demo():
from nltk.corpus import brown
sents = list(brown.tagged_sents())
test = list(brown.sents())
tagger = TnT()
tagger.train(sents[200:1000])
tagged_data = tagger.tagdata(test[100:120])
for j in range(len(tagged_data)):
s = tagged_data[j]
t = sents[j + 100]
for i in range(len(s)):
print(s[i], ""--"", t[i])
print()
",[],0,[],/tag/tnt.py_demo
3189,/home/amandapotts/git/nltk/nltk/tag/tnt.py_demo2,"def demo2():
from nltk.corpus import treebank
d = list(treebank.tagged_sents())
t = TnT(N=1000, C=False)
s = TnT(N=1000, C=True)
t.train(d[(11) * 100 :])
s.train(d[(11) * 100 :])
for i in range(10):
tacc = t.accuracy(d[i * 100 : ((i + 1) * 100)])
tp_un = t.unknown / (t.known + t.unknown)
tp_kn = t.known / (t.known + t.unknown)
t.unknown = 0
t.known = 0
print(""Capitalization off:"")
print(""Accuracy:"", tacc)
print(""Percentage known:"", tp_kn)
print(""Percentage unknown:"", tp_un)
print(""Accuracy over known words:"", (tacc / tp_kn))
sacc = s.accuracy(d[i * 100 : ((i + 1) * 100)])
sp_un = s.unknown / (s.known + s.unknown)
sp_kn = s.known / (s.known + s.unknown)
s.unknown = 0
s.known = 0
print(""Capitalization on:"")
print(""Accuracy:"", sacc)
print(""Percentage known:"", sp_kn)
print(""Percentage unknown:"", sp_un)
print(""Accuracy over known words:"", (sacc / sp_kn))
",[],0,[],/tag/tnt.py_demo2
3190,/home/amandapotts/git/nltk/nltk/tag/tnt.py_demo3,"def demo3():
from nltk.corpus import brown, treebank
d = list(treebank.tagged_sents())
e = list(brown.tagged_sents())
d = d[:1000]
e = e[:1000]
d10 = int(len(d) * 0.1)
e10 = int(len(e) * 0.1)
tknacc = 0
sknacc = 0
tallacc = 0
sallacc = 0
tknown = 0
sknown = 0
for i in range(10):
t = TnT(N=1000, C=False)
s = TnT(N=1000, C=False)
dtest = d[(i * d10) : ((i + 1) * d10)]
etest = e[(i * e10) : ((i + 1) * e10)]
dtrain = d[: (i * d10)] + d[((i + 1) * d10) :]
etrain = e[: (i * e10)] + e[((i + 1) * e10) :]
t.train(dtrain)
s.train(etrain)
tacc = t.accuracy(dtest)
tp_un = t.unknown / (t.known + t.unknown)
tp_kn = t.known / (t.known + t.unknown)
tknown += tp_kn
t.unknown = 0
t.known = 0
sacc = s.accuracy(etest)
sp_un = s.unknown / (s.known + s.unknown)
sp_kn = s.known / (s.known + s.unknown)
sknown += sp_kn
s.unknown = 0
s.known = 0
tknacc += tacc / tp_kn
sknacc += sacc / tp_kn
tallacc += tacc
sallacc += sacc
print(""brown: acc over words known:"", 10 * tknacc)
print(""     : overall accuracy:"", 10 * tallacc)
print(""     : words known:"", 10 * tknown)
print(""treebank: acc over words known:"", 10 * sknacc)
print(""        : overall accuracy:"", 10 * sallacc)
print(""        : words known:"", 10 * sknown)
",[],0,[],/tag/tnt.py_demo3
3191,/home/amandapotts/git/nltk/nltk/tag/util.py_str2tuple,"def str2tuple(s, sep=""/""):
""""""
Given the string representation of a tagged token, return the
corresponding tuple representation.  The rightmost occurrence of
a tag string.  If *sep* does not occur in *s*, return (s, None).
>>> from nltk.tag.util import str2tuple
>>> str2tuple('fly/NN')
('fly', 'NN')
:type s: str
:param s: The string representation of a tagged token.
:type sep: str
:param sep: The separator string used to separate word strings
from tags.
""""""
loc = s.rfind(sep)
if loc >= 0:
return (s[:loc], s[loc + len(sep) :].upper())
else:
return (s, None)
",[],0,[],/tag/util.py_str2tuple
3192,/home/amandapotts/git/nltk/nltk/tag/util.py_tuple2str,"def tuple2str(tagged_token, sep=""/""):
""""""
Given the tuple representation of a tagged token, return the
corresponding string representation.  This representation is
formed by concatenating the token's word string, followed by the
separator, followed by the token's tag.  (If the tag is None,
then just return the bare word string.)
>>> from nltk.tag.util import tuple2str
>>> tagged_token = ('fly', 'NN')
>>> tuple2str(tagged_token)
'fly/NN'
:type tagged_token: tuple(str, str)
:param tagged_token: The tuple representation of a tagged token.
:type sep: str
:param sep: The separator string used to separate word strings
from tags.
""""""
word, tag = tagged_token
if tag is None:
return word
else:
assert sep not in tag, ""tag may not contain sep!""
return f""{word}{sep}{tag}""
",[],0,[],/tag/util.py_tuple2str
3193,/home/amandapotts/git/nltk/nltk/tag/util.py_untag,"def untag(tagged_sentence):
""""""
Given a tagged sentence, return an untagged version of that
sentence.  I.e., return a list containing the first element
of each tuple in *tagged_sentence*.
>>> from nltk.tag.util import untag
>>> untag([('John', 'NNP'), ('saw', 'VBD'), ('Mary', 'NNP')])
['John', 'saw', 'Mary']
""""""
return [w for (w, t) in tagged_sentence]
",[],0,[],/tag/util.py_untag
3194,/home/amandapotts/git/nltk/nltk/tag/brill.py_extract_property,"def extract_property(tokens, index):
""""""@return: The given token's text.""""""
return tokens[index][0]
",[],0,[],/tag/brill.py_extract_property
3195,/home/amandapotts/git/nltk/nltk/tag/brill.py_extract_property,"def extract_property(tokens, index):
""""""@return: The given token's tag.""""""
return tokens[index][1]
",[],0,[],/tag/brill.py_extract_property
3196,/home/amandapotts/git/nltk/nltk/tag/brill.py_nltkdemo18,"def nltkdemo18():
""""""
Return 18 templates, from the original nltk demo, in multi-feature syntax
""""""
return [
Template(Pos([-1])),
Template(Pos([1])),
Template(Pos([-2])),
Template(Pos([2])),
Template(Pos([-2, -1])),
Template(Pos([1, 2])),
Template(Pos([-3, -2, -1])),
Template(Pos([1, 2, 3])),
Template(Pos([-1]), Pos([1])),
Template(Word([-1])),
Template(Word([1])),
Template(Word([-2])),
Template(Word([2])),
Template(Word([-2, -1])),
Template(Word([1, 2])),
Template(Word([-3, -2, -1])),
Template(Word([1, 2, 3])),
Template(Word([-1]), Word([1])),
]
",[],0,[],/tag/brill.py_nltkdemo18
3197,/home/amandapotts/git/nltk/nltk/tag/brill.py_nltkdemo18plus,"def nltkdemo18plus():
""""""
Return 18 templates, from the original nltk demo, and additionally a few
multi-feature ones (the motivation is easy comparison with nltkdemo18)
""""""
return nltkdemo18() + [
Template(Word([-1]), Pos([1])),
Template(Pos([-1]), Word([1])),
Template(Word([-1]), Word([0]), Pos([1])),
Template(Pos([-1]), Word([0]), Word([1])),
Template(Pos([-1]), Word([0]), Pos([1])),
]
",[],0,[],/tag/brill.py_nltkdemo18plus
3198,/home/amandapotts/git/nltk/nltk/tag/brill.py_fntbl37,"def fntbl37():
""""""
Return 37 templates taken from the postagging task of the
fntbl distribution https://www.cs.jhu.edu/~rflorian/fntbl/
(37 is after excluding a handful which do not condition on Pos[0]
fntbl can do that but the current nltk implementation cannot.)
""""""
return [
Template(Word([0]), Word([1]), Word([2])),
Template(Word([-1]), Word([0]), Word([1])),
Template(Word([0]), Word([-1])),
Template(Word([0]), Word([1])),
Template(Word([0]), Word([2])),
Template(Word([0]), Word([-2])),
Template(Word([1, 2])),
Template(Word([-2, -1])),
Template(Word([1, 2, 3])),
Template(Word([-3, -2, -1])),
Template(Word([0]), Pos([2])),
Template(Word([0]), Pos([-2])),
Template(Word([0]), Pos([1])),
Template(Word([0]), Pos([-1])),
Template(Word([0])),
Template(Word([-2])),
Template(Word([2])),
Template(Word([1])),
Template(Word([-1])),
Template(Pos([-1]), Pos([1])),
Template(Pos([1]), Pos([2])),
Template(Pos([-1]), Pos([-2])),
Template(Pos([1])),
Template(Pos([-1])),
Template(Pos([-2])),
Template(Pos([2])),
Template(Pos([1, 2, 3])),
Template(Pos([1, 2])),
Template(Pos([-3, -2, -1])),
Template(Pos([-2, -1])),
Template(Pos([1]), Word([0]), Word([1])),
Template(Pos([1]), Word([0]), Word([-1])),
Template(Pos([-1]), Word([-1]), Word([0])),
Template(Pos([-1]), Word([0]), Word([1])),
Template(Pos([-2]), Pos([-1])),
Template(Pos([1]), Pos([2])),
Template(Pos([1]), Pos([2]), Word([1])),
]
",[],0,[],/tag/brill.py_fntbl37
3199,/home/amandapotts/git/nltk/nltk/tag/brill.py_brill24,"def brill24():
""""""
Return 24 templates of the seminal TBL paper, Brill (1995)
""""""
return [
Template(Pos([-1])),
Template(Pos([1])),
Template(Pos([-2])),
Template(Pos([2])),
Template(Pos([-2, -1])),
Template(Pos([1, 2])),
Template(Pos([-3, -2, -1])),
Template(Pos([1, 2, 3])),
Template(Pos([-1]), Pos([1])),
Template(Pos([-2]), Pos([-1])),
Template(Pos([1]), Pos([2])),
Template(Word([-1])),
Template(Word([1])),
Template(Word([-2])),
Template(Word([2])),
Template(Word([-2, -1])),
Template(Word([1, 2])),
Template(Word([-1, 0])),
Template(Word([0, 1])),
Template(Word([0])),
Template(Word([-1]), Pos([-1])),
Template(Word([1]), Pos([1])),
Template(Word([0]), Word([-1]), Pos([-1])),
Template(Word([0]), Word([1]), Pos([1])),
]
",[],0,[],/tag/brill.py_brill24
3200,/home/amandapotts/git/nltk/nltk/tag/brill.py_describe_template_sets,"def describe_template_sets():
""""""
Print the available template sets in this demo, with a short description""
""""""
import inspect
import sys
templatesets = inspect.getmembers(sys.modules[__name__], inspect.isfunction)
for name, obj in templatesets:
if name == ""describe_template_sets"":
continue
print(name, obj.__doc__, ""\n"")
",[],0,[],/tag/brill.py_describe_template_sets
3201,/home/amandapotts/git/nltk/nltk/tag/brill.py___init__,"def __init__(self, initial_tagger, rules, training_stats=None):
""""""
:param initial_tagger: The initial tagger
:type initial_tagger: TaggerI
:param rules: An ordered list of transformation rules that
should be used to correct the initial tagging.
:type rules: list(TagRule)
:param training_stats: A dictionary of statistics collected
during training, for possible later use
:type training_stats: dict
""""""
self._initial_tagger = initial_tagger
self._rules = tuple(rules)
self._training_stats = training_stats
",[],0,[],/tag/brill.py___init__
3202,/home/amandapotts/git/nltk/nltk/tag/brill.py_encode_json_obj,"def encode_json_obj(self):
return self._initial_tagger, self._rules, self._training_stats
",[],0,[],/tag/brill.py_encode_json_obj
3203,/home/amandapotts/git/nltk/nltk/tag/brill.py_decode_json_obj,"def decode_json_obj(cls, obj):
_initial_tagger, _rules, _training_stats = obj
return cls(_initial_tagger, _rules, _training_stats)
",[],0,[],/tag/brill.py_decode_json_obj
3204,/home/amandapotts/git/nltk/nltk/tag/brill.py_rules,"def rules(self):
""""""
Return the ordered list of  transformation rules that this tagger has learnt
:return: the ordered list of transformation rules that correct the initial tagging
:rtype: list of Rules
""""""
return self._rules
",[],0,[],/tag/brill.py_rules
3205,/home/amandapotts/git/nltk/nltk/tag/brill.py_train_stats,"def train_stats(self, statistic=None):
""""""
Return a named statistic collected during training, or a dictionary of all
available statistics if no name given
:param statistic: name of statistic
:type statistic: str
:return: some statistic collected during training of this tagger
:rtype: any (but usually a number)
""""""
if statistic is None:
return self._training_stats
else:
return self._training_stats.get(statistic)
",[],0,[],/tag/brill.py_train_stats
3206,/home/amandapotts/git/nltk/nltk/tag/brill.py_tag,"def tag(self, tokens):
tagged_tokens = self._initial_tagger.tag(tokens)
tag_to_positions = defaultdict(set)
for i, (token, tag) in enumerate(tagged_tokens):
tag_to_positions[tag].add(i)
for rule in self._rules:
positions = tag_to_positions.get(rule.original_tag, [])
changed = rule.apply(tagged_tokens, positions)
for i in changed:
tag_to_positions[rule.original_tag].remove(i)
tag_to_positions[rule.replacement_tag].add(i)
return tagged_tokens
",[],0,[],/tag/brill.py_tag
3207,/home/amandapotts/git/nltk/nltk/tag/brill.py_print_template_statistics,"def print_template_statistics(self, test_stats=None, printunused=True):
""""""
Print a list of all templates, ranked according to efficiency.
If test_stats is available, the templates are ranked according to their
relative contribution (summed for all rules created from a given template,
weighted by score) to the performance on the test set. If no test_stats, then
statistics collected during training are used instead. There is also
an unweighted measure (just counting the rules). This is less informative,
though, as many low-score rules will appear towards end of training.
:param test_stats: dictionary of statistics collected during testing
:type test_stats: dict of str -> any (but usually numbers)
:param printunused: if True, print a list of all unused templates
:type printunused: bool
:return: None
:rtype: None
""""""
tids = [r.templateid for r in self._rules]
train_stats = self.train_stats()
trainscores = train_stats[""rulescores""]
assert len(trainscores) == len(
tids
), ""corrupt statistics: "" ""{} train scores for {} rules"".format(
trainscores, tids
)
template_counts = Counter(tids)
weighted_traincounts = Counter()
for tid, score in zip(tids, trainscores):
weighted_traincounts[tid] += score
tottrainscores = sum(trainscores)
",[],0,[],/tag/brill.py_print_template_statistics
3208,/home/amandapotts/git/nltk/nltk/tag/brill.py_det_tplsort,"def det_tplsort(tpl_value):
return (tpl_value[1], repr(tpl_value[0]))
",[],0,[],/tag/brill.py_det_tplsort
3209,/home/amandapotts/git/nltk/nltk/tag/brill.py_print_train_stats,"def print_train_stats():
print(
""TEMPLATE STATISTICS (TRAIN)  {} templates, {} rules)"".format(
len(template_counts), len(tids)
)
)
print(
""TRAIN ({tokencount:7d} tokens) initial {initialerrors:5d} {initialacc:.4f} ""
""final: {finalerrors:5d} {finalacc:.4f}"".format(**train_stats)
)
head = ""#ID | Score (train) |  #Rules     | Template""
print(head, ""\n"", ""-"" * len(head), sep="""")
train_tplscores = sorted(
weighted_traincounts.items(), key=det_tplsort, reverse=True
)
for tid, trainscore in train_tplscores:
s = ""{} | {:5d}   {:5.3f} |{:4d}   {:.3f} | {}"".format(
tid,
trainscore,
trainscore / tottrainscores,
template_counts[tid],
template_counts[tid] / len(tids),
Template.ALLTEMPLATES[int(tid)],
)
print(s)
",[],0,[],/tag/brill.py_print_train_stats
3210,/home/amandapotts/git/nltk/nltk/tag/brill.py_print_testtrain_stats,"def print_testtrain_stats():
testscores = test_stats[""rulescores""]
print(
""TEMPLATE STATISTICS (TEST AND TRAIN) ({} templates, {} rules)"".format(
len(template_counts), len(tids)
)
)
print(
""TEST  ({tokencount:7d} tokens) initial {initialerrors:5d} {initialacc:.4f} ""
""final: {finalerrors:5d} {finalacc:.4f} "".format(**test_stats)
)
print(
""TRAIN ({tokencount:7d} tokens) initial {initialerrors:5d} {initialacc:.4f} ""
""final: {finalerrors:5d} {finalacc:.4f} "".format(**train_stats)
)
weighted_testcounts = Counter()
for tid, score in zip(tids, testscores):
weighted_testcounts[tid] += score
tottestscores = sum(testscores)
head = ""#ID | Score (test) | Score (train) |  #Rules     | Template""
print(head, ""\n"", ""-"" * len(head), sep="""")
test_tplscores = sorted(
weighted_testcounts.items(), key=det_tplsort, reverse=True
)
for tid, testscore in test_tplscores:
s = ""{:s} |{:5d}  {:6.3f} |  {:4d}   {:.3f} |{:4d}   {:.3f} | {:s}"".format(
tid,
testscore,
testscore / tottestscores,
weighted_traincounts[tid],
weighted_traincounts[tid] / tottrainscores,
template_counts[tid],
template_counts[tid] / len(tids),
Template.ALLTEMPLATES[int(tid)],
)
print(s)
",[],0,[],/tag/brill.py_print_testtrain_stats
3211,/home/amandapotts/git/nltk/nltk/tag/brill.py_print_unused_templates,"def print_unused_templates():
usedtpls = {int(tid) for tid in tids}
unused = [
(tid, tpl)
for (tid, tpl) in enumerate(Template.ALLTEMPLATES)
if tid not in usedtpls
]
print(f""UNUSED TEMPLATES ({len(unused)})"")
for tid, tpl in unused:
print(f""{tid:03d} {str(tpl):s}"")
",[],0,[],/tag/brill.py_print_unused_templates
3212,/home/amandapotts/git/nltk/nltk/tag/brill.py_batch_tag_incremental,"def batch_tag_incremental(self, sequences, gold):
""""""
Tags by applying each rule to the entire corpus (rather than all rules to a
single sequence). The point is to collect statistics on the test set for
individual rules.
NOTE: This is inefficient (does not build any index, so will traverse the entire
corpus N times for N rules) -- usually you would not care about statistics for
individual rules and thus use batch_tag() instead
:param sequences: lists of token sequences (sentences, in some applications) to be tagged
:type sequences: list of list of strings
:param gold: the gold standard
:type gold: list of list of strings
:returns: tuple of (tagged_sequences, ordered list of rule scores (one for each rule))
""""""
",[],0,[],/tag/brill.py_batch_tag_incremental
3213,/home/amandapotts/git/nltk/nltk/tag/brill.py_counterrors,"def counterrors(xs):
return sum(t[1] != g[1] for pair in zip(xs, gold) for (t, g) in zip(*pair))
",[],0,[],/tag/brill.py_counterrors
3214,/home/amandapotts/git/nltk/nltk/tag/perceptron.py___init__,"def __init__(self, weights=None):
self.weights = weights if weights else {}
self.classes = set()
self._totals = defaultdict(int)
self._tstamps = defaultdict(int)
self.i = 0
",[],0,[],/tag/perceptron.py___init__
3215,/home/amandapotts/git/nltk/nltk/tag/perceptron.py__softmax,"def _softmax(self, scores):
s = np.fromiter(scores.values(), dtype=float)
exps = np.exp(s)
return exps / np.sum(exps)
","['fromiter', 'exp', 'sum']",3,"['fromiter(scores.values(), dtype=float)', 'exp(s)', 'sum(exps)']",/tag/perceptron.py__softmax
3216,/home/amandapotts/git/nltk/nltk/tag/perceptron.py_update,"def update(self, truth, guess, features):
""""""Update the feature weights.""""""
",[],0,[],/tag/perceptron.py_update
3217,/home/amandapotts/git/nltk/nltk/tag/perceptron.py_upd_feat,"def upd_feat(c, f, w, v):
param = (f, c)
self._totals[param] += (self.i - self._tstamps[param]) * w
self._tstamps[param] = self.i
self.weights[f][c] = w + v
",[],0,[],/tag/perceptron.py_upd_feat
3218,/home/amandapotts/git/nltk/nltk/tag/perceptron.py_average_weights,"def average_weights(self):
""""""Average weights from all iterations.""""""
for feat, weights in self.weights.items():
new_feat_weights = {}
for clas, weight in weights.items():
param = (feat, clas)
total = self._totals[param]
total += (self.i - self._tstamps[param]) * weight
averaged = round(total / self.i, 3)
if averaged:
new_feat_weights[clas] = averaged
self.weights[feat] = new_feat_weights
",[],0,[],/tag/perceptron.py_average_weights
3219,/home/amandapotts/git/nltk/nltk/tag/perceptron.py_save,"def save(self, path):
""""""Save the pickled model weights.""""""
with open(path, ""wb"") as fout:
return pickle.dump(dict(self.weights), fout)
",[],0,[],/tag/perceptron.py_save
3220,/home/amandapotts/git/nltk/nltk/tag/perceptron.py_load,"def load(self, path):
""""""Load the pickled model weights.""""""
self.weights = load(path)
",[],0,[],/tag/perceptron.py_load
3221,/home/amandapotts/git/nltk/nltk/tag/perceptron.py_encode_json_obj,"def encode_json_obj(self):
return self.weights
",[],0,[],/tag/perceptron.py_encode_json_obj
3222,/home/amandapotts/git/nltk/nltk/tag/perceptron.py_decode_json_obj,"def decode_json_obj(cls, obj):
return cls(obj)
",[],0,[],/tag/perceptron.py_decode_json_obj
3223,/home/amandapotts/git/nltk/nltk/tag/perceptron.py___init__,"def __init__(self, load=True):
""""""
:param load: Load the pickled model upon instantiation.
""""""
self.model = AveragedPerceptron()
self.tagdict = {}
self.classes = set()
if load:
AP_MODEL_LOC = ""file:"" + str(
find(""taggers/averaged_perceptron_tagger/"" + PICKLE)
)
self.load(AP_MODEL_LOC)
",[],0,[],/tag/perceptron.py___init__
3224,/home/amandapotts/git/nltk/nltk/tag/perceptron.py_tag,"def tag(self, tokens, return_conf=False, use_tagdict=True):
""""""
Tag tokenized sentences.
:params tokens: list of word
:type tokens: list(str)
""""""
prev, prev2 = self.START
output = []
context = self.START + [self.normalize(w) for w in tokens] + self.END
for i, word in enumerate(tokens):
tag, conf = (
(self.tagdict.get(word), 1.0) if use_tagdict == True else (None, None)
)
if not tag:
features = self._get_features(i, word, context, prev, prev2)
tag, conf = self.model.predict(features, return_conf)
output.append((word, tag, conf) if return_conf == True else (word, tag))
prev2 = prev
prev = tag
return output
",[],0,[],/tag/perceptron.py_tag
3225,/home/amandapotts/git/nltk/nltk/tag/perceptron.py_train,"def train(self, sentences, save_loc=None, nr_iter=5):
""""""Train a model from sentences, and save it at ``save_loc``. ``nr_iter``
controls the number of Perceptron training iterations.
:param sentences: A list or iterator of sentences, where each sentence
is a list of (words, tags) tuples.
:param save_loc: If not ``None``, saves a pickled model in this location.
:param nr_iter: Number of training iterations.
""""""
self._sentences = list()  # to be populated by self._make_tagdict...
self._make_tagdict(sentences)
self.model.classes = self.classes
for iter_ in range(nr_iter):
c = 0
n = 0
for sentence in self._sentences:
words, tags = zip(*sentence)
prev, prev2 = self.START
context = self.START + [self.normalize(w) for w in words] + self.END
for i, word in enumerate(words):
guess = self.tagdict.get(word)
if not guess:
feats = self._get_features(i, word, context, prev, prev2)
guess, _ = self.model.predict(feats)
self.model.update(tags[i], guess, feats)
prev2 = prev
prev = guess
c += guess == tags[i]
n += 1
random.shuffle(self._sentences)
logging.info(f""Iter {iter_}: {c}/{n}={_pc(c, n)}"")
self._sentences = None
self.model.average_weights()
if save_loc is not None:
with open(save_loc, ""wb"") as fout:
pickle.dump((self.model.weights, self.tagdict, self.classes), fout, 2)
",[],0,[],/tag/perceptron.py_train
3226,/home/amandapotts/git/nltk/nltk/tag/perceptron.py_load,"def load(self, loc):
""""""
:param loc: Load a pickled model at location.
:type loc: str
""""""
self.model.weights, self.tagdict, self.classes = load(loc)
self.model.classes = self.classes
",[],0,[],/tag/perceptron.py_load
3227,/home/amandapotts/git/nltk/nltk/tag/perceptron.py_encode_json_obj,"def encode_json_obj(self):
return self.model.weights, self.tagdict, list(self.classes)
",[],0,[],/tag/perceptron.py_encode_json_obj
3228,/home/amandapotts/git/nltk/nltk/tag/perceptron.py_decode_json_obj,"def decode_json_obj(cls, obj):
tagger = cls(load=False)
tagger.model.weights, tagger.tagdict, tagger.classes = obj
tagger.classes = set(tagger.classes)
tagger.model.classes = tagger.classes
return tagger
",[],0,[],/tag/perceptron.py_decode_json_obj
3229,/home/amandapotts/git/nltk/nltk/tag/perceptron.py_normalize,"def normalize(self, word):
""""""
Normalization used in pre-processing.
- All words are lower cased
- Groups of digits of length 4 are represented as !YEAR
- Other digits are represented as !DIGITS
:rtype: str
""""""
if ""-"" in word and word[0] != ""-"":
return ""!HYPHEN""
if word.isdigit() and len(word) == 4:
return ""!YEAR""
if word and word[0].isdigit():
return ""!DIGITS""
return word.lower()
",[],0,[],/tag/perceptron.py_normalize
3230,/home/amandapotts/git/nltk/nltk/tag/perceptron.py__get_features,"def _get_features(self, i, word, context, prev, prev2):
""""""Map tokens into a feature representation, implemented as a
{hashable: int} dict. If the features change, a new model must be
trained.
""""""
",[],0,[],/tag/perceptron.py__get_features
3231,/home/amandapotts/git/nltk/nltk/tag/perceptron.py_add,"def add(name, *args):
features["" "".join((name,) + tuple(args))] += 1
",[],0,[],/tag/perceptron.py_add
3232,/home/amandapotts/git/nltk/nltk/tag/perceptron.py__pc,"def _pc(n, d):
return (n / d) * 100
",[],0,[],/tag/perceptron.py__pc
3233,/home/amandapotts/git/nltk/nltk/tag/perceptron.py__load_data_conll_format,"def _load_data_conll_format(filename):
print(""Read from file: "", filename)
with open(filename, ""rb"") as fin:
sentences = []
sentence = []
for line in fin.readlines():
line = line.strip()
if len(line) == 0:
sentences.append(sentence)
sentence = []
continue
tokens = line.split(""\t"")
word = tokens[1]
tag = tokens[4]
sentence.append((word, tag))
return sentences
",[],0,[],/tag/perceptron.py__load_data_conll_format
3234,/home/amandapotts/git/nltk/nltk/tag/perceptron.py__get_pretrain_model,"def _get_pretrain_model():
tagger = PerceptronTagger()
training = _load_data_conll_format(""english_ptb_train.conll"")
testing = _load_data_conll_format(""english_ptb_test.conll"")
print(""Size of training and testing (sentence)"", len(training), len(testing))
tagger.train(training, PICKLE)
print(""Accuracy : "", tagger.accuracy(testing))
",[],0,[],/tag/perceptron.py__get_pretrain_model
3235,/home/amandapotts/git/nltk/nltk/tag/stanford.py___init__,"def __init__(
self,
model_filename,
path_to_jar=None,
encoding=""utf8"",
verbose=False,
java_options=""-mx1000m"",
",[],0,[],/tag/stanford.py___init__
3236,/home/amandapotts/git/nltk/nltk/tag/stanford.py__cmd,"def _cmd(self):
""""""
A property that returns the command that will be executed.
""""""
",[],0,[],/tag/stanford.py__cmd
3237,/home/amandapotts/git/nltk/nltk/tag/stanford.py_tag,"def tag(self, tokens):
return sum(self.tag_sents([tokens]), [])
",[],0,[],/tag/stanford.py_tag
3238,/home/amandapotts/git/nltk/nltk/tag/stanford.py_tag_sents,"def tag_sents(self, sentences):
encoding = self._encoding
default_options = "" "".join(_java_options)
config_java(options=self.java_options, verbose=False)
_input_fh, self._input_file_path = tempfile.mkstemp(text=True)
cmd = list(self._cmd)
cmd.extend([""-encoding"", encoding])
_input_fh = os.fdopen(_input_fh, ""wb"")
_input = ""\n"".join("" "".join(x) for x in sentences)
if isinstance(_input, str) and encoding:
_input = _input.encode(encoding)
_input_fh.write(_input)
_input_fh.close()
stanpos_output, _stderr = java(
cmd, classpath=self._stanford_jar, stdout=PIPE, stderr=PIPE
)
stanpos_output = stanpos_output.decode(encoding)
os.unlink(self._input_file_path)
config_java(options=default_options, verbose=False)
return self.parse_output(stanpos_output, sentences)
",[],0,[],/tag/stanford.py_tag_sents
3239,/home/amandapotts/git/nltk/nltk/tag/stanford.py_parse_output,"def parse_output(self, text, sentences=None):
tagged_sentences = []
for tagged_sentence in text.strip().split(""\n""):
sentence = []
for tagged_word in tagged_sentence.strip().split():
word_tags = tagged_word.strip().split(self._SEPARATOR)
sentence.append(
("""".join(word_tags[:-1]), word_tags[-1].replace(""0"", """").upper())
)
tagged_sentences.append(sentence)
return tagged_sentences
",[],0,[],/tag/stanford.py_parse_output
3240,/home/amandapotts/git/nltk/nltk/tag/stanford.py___init__,"def __init__(self, *args, **kwargs):
super().__init__(*args, **kwargs)
",[],0,[],/tag/stanford.py___init__
3241,/home/amandapotts/git/nltk/nltk/tag/stanford.py__cmd,"def _cmd(self):
return [
""edu.stanford.nlp.tagger.maxent.MaxentTagger"",
""-model"",
self._stanford_model,
""-textFile"",
self._input_file_path,
""-tokenize"",
""false"",
""-outputFormatOptions"",
""keepEmptySentences"",
]
",[],0,[],/tag/stanford.py__cmd
3242,/home/amandapotts/git/nltk/nltk/tag/stanford.py___init__,"def __init__(self, *args, **kwargs):
super().__init__(*args, **kwargs)
",[],0,[],/tag/stanford.py___init__
3243,/home/amandapotts/git/nltk/nltk/tag/stanford.py__cmd,"def _cmd(self):
return [
""edu.stanford.nlp.ie.crf.CRFClassifier"",
""-loadClassifier"",
self._stanford_model,
""-textFile"",
self._input_file_path,
""-outputFormat"",
self._FORMAT,
""-tokenizerFactory"",
""edu.stanford.nlp.process.WhitespaceTokenizer"",
""-tokenizerOptions"",
'""tokenizeNLs=false""',
]
",[],0,[],/tag/stanford.py__cmd
3244,/home/amandapotts/git/nltk/nltk/tag/stanford.py_parse_output,"def parse_output(self, text, sentences):
if self._FORMAT == ""slashTags"":
tagged_sentences = []
for tagged_sentence in text.strip().split(""\n""):
for tagged_word in tagged_sentence.strip().split():
word_tags = tagged_word.strip().split(self._SEPARATOR)
tagged_sentences.append(("""".join(word_tags[:-1]), word_tags[-1]))
result = []
start = 0
for sent in sentences:
result.append(tagged_sentences[start : start + len(sent)])
start += len(sent)
return result
raise NotImplementedError
",[],0,[],/tag/stanford.py_parse_output
3245,/home/amandapotts/git/nltk/nltk/tag/api.py_tag,"def tag(self, tokens):
""""""
Determine the most appropriate tag sequence for the given
token sequence, and return a corresponding list of tagged
tokens.  A tagged token is encoded as a tuple ``(token, tag)``.
:rtype: list(tuple(str, str))
""""""
if overridden(self.tag_sents):
return self.tag_sents([tokens])[0]
",[],0,[],/tag/api.py_tag
3246,/home/amandapotts/git/nltk/nltk/tag/api.py_tag_sents,"def tag_sents(self, sentences):
""""""
Apply ``self.tag()`` to each element of *sentences*.  I.e.::
return [self.tag(sent) for sent in sentences]
""""""
return [self.tag(sent) for sent in sentences]
",[],0,[],/tag/api.py_tag_sents
3247,/home/amandapotts/git/nltk/nltk/tag/api.py_evaluate,"def evaluate(self, gold):
return self.accuracy(gold)
",[],0,[],/tag/api.py_evaluate
3248,/home/amandapotts/git/nltk/nltk/tag/api.py_accuracy,"def accuracy(self, gold):
""""""
Score the accuracy of the tagger against the gold standard.
Strip the tags from the gold standard text, retag it using
the tagger, then compute the accuracy score.
:param gold: The list of tagged sentences to score the tagger on.
:type gold: list(list(tuple(str, str)))
:rtype: float
""""""
tagged_sents = self.tag_sents(untag(sent) for sent in gold)
gold_tokens = list(chain.from_iterable(gold))
test_tokens = list(chain.from_iterable(tagged_sents))
return accuracy(gold_tokens, test_tokens)
",[],0,[],/tag/api.py_accuracy
3249,/home/amandapotts/git/nltk/nltk/tag/api.py__confusion_cached,"def _confusion_cached(self, gold):
""""""
Inner function used after ``gold`` is converted to a
``tuple(tuple(tuple(str, str)))``. That way, we can use caching on
creating a ConfusionMatrix.
:param gold: The list of tagged sentences to run the tagger with,
also used as the reference values in the generated confusion matrix.
:type gold: tuple(tuple(tuple(str, str)))
:rtype: ConfusionMatrix
""""""
tagged_sents = self.tag_sents(untag(sent) for sent in gold)
gold_tokens = [token for _word, token in chain.from_iterable(gold)]
test_tokens = [token for _word, token in chain.from_iterable(tagged_sents)]
return ConfusionMatrix(gold_tokens, test_tokens)
",[],0,[],/tag/api.py__confusion_cached
3250,/home/amandapotts/git/nltk/nltk/tag/api.py_confusion,"def confusion(self, gold):
""""""
Return a ConfusionMatrix with the tags from ``gold`` as the reference
values, with the predictions from ``tag_sents`` as the predicted values.
>>> from nltk.tag import PerceptronTagger
>>> from nltk.corpus import treebank
>>> tagger = PerceptronTagger()
>>> gold_data = treebank.tagged_sents()[:10]
>>> print(tagger.confusion(gold_data))
|        -                                                                                     |
|        N                                                                                     |
|        O                                               P                                     |
|        N                       J  J        N  N  P  P  R     R           V  V  V  V  V  W    |
|  '     E     C  C  D  E  I  J  J  J  M  N  N  N  O  R  P  R  B  R  T  V  B  B  B  B  B  D  ` |
|  '  ,  -  .  C  D  T  X  N  J  R  S  D  N  P  S  S  P  $  B  R  P  O  B  D  G  N  P  Z  T  ` |
-------+----------------------------------------------------------------------------------------------+
'' | <1> .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |
, |  .<15> .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |
-NONE- |  .  . <.> .  .  2  .  .  .  2  .  .  .  5  1  .  .  .  .  2  .  .  .  .  .  .  .  .  .  .  . |
. |  .  .  .<10> .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |
CC |  .  .  .  . <1> .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |
CD |  .  .  .  .  . <5> .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |
DT |  .  .  .  .  .  .<20> .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |
EX |  .  .  .  .  .  .  . <1> .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |
IN |  .  .  .  .  .  .  .  .<22> .  .  .  .  .  .  .  .  .  .  3  .  .  .  .  .  .  .  .  .  .  . |
JJ |  .  .  .  .  .  .  .  .  .<16> .  .  .  .  1  .  .  .  .  1  .  .  .  .  .  .  .  .  .  .  . |
JJR |  .  .  .  .  .  .  .  .  .  . <.> .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |
JJS |  .  .  .  .  .  .  .  .  .  .  . <1> .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |
MD |  .  .  .  .  .  .  .  .  .  .  .  . <1> .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |
NN |  .  .  .  .  .  .  .  .  .  .  .  .  .<28> 1  1  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |
NNP |  .  .  .  .  .  .  .  .  .  .  .  .  .  .<25> .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |
NNS |  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .<19> .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |
POS |  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . <1> .  .  .  .  .  .  .  .  .  .  .  .  .  . |
PRP |  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . <4> .  .  .  .  .  .  .  .  .  .  .  .  . |
PRP$ |  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . <2> .  .  .  .  .  .  .  .  .  .  .  . |
RB |  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . <4> .  .  .  .  .  .  .  .  .  .  . |
RBR |  .  .  .  .  .  .  .  .  .  .  1  .  .  .  .  .  .  .  .  . <1> .  .  .  .  .  .  .  .  .  . |
RP |  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . <1> .  .  .  .  .  .  .  .  . |
TO |  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . <5> .  .  .  .  .  .  .  . |
VB |  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . <3> .  .  .  .  .  .  . |
VBD |  .  .  .  .  .  .  .  .  .  .  .  .  .  1  .  .  .  .  .  .  .  .  .  . <6> .  .  .  .  .  . |
VBG |  .  .  .  .  .  .  .  .  .  .  .  .  .  1  .  .  .  .  .  .  .  .  .  .  . <4> .  .  .  .  . |
VBN |  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  1  . <4> .  .  .  . |
VBP |  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . <3> .  .  . |
VBZ |  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . <7> .  . |
WDT |  .  .  .  .  .  .  .  .  2  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . <.> . |
`` |  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . <1>|
-------+----------------------------------------------------------------------------------------------+
(row = reference
<BLANKLINE>
:param gold: The list of tagged sentences to run the tagger with,
also used as the reference values in the generated confusion matrix.
:type gold: list(list(tuple(str, str)))
:rtype: ConfusionMatrix
""""""
return self._confusion_cached(tuple(tuple(sent) for sent in gold))
",[],0,[],/tag/api.py_confusion
3251,/home/amandapotts/git/nltk/nltk/tag/api.py_recall,"def recall(self, gold) -> Dict[str, float]:
""""""
Compute the recall for each tag from ``gold`` or from running ``tag``
on the tokenized sentences from ``gold``. Then, return the dictionary
with mappings from tag to recall. The recall is defined as:
- *r* = true positive / (true positive + false positive)
:param gold: The list of tagged sentences to score the tagger on.
:type gold: list(list(tuple(str, str)))
:return: A mapping from tags to recall
:rtype: Dict[str, float]
""""""
cm = self.confusion(gold)
return {tag: cm.recall(tag) for tag in cm._values}
",[],0,[],/tag/api.py_recall
3252,/home/amandapotts/git/nltk/nltk/tag/api.py_precision,"def precision(self, gold):
""""""
Compute the precision for each tag from ``gold`` or from running ``tag``
on the tokenized sentences from ``gold``. Then, return the dictionary
with mappings from tag to precision. The precision is defined as:
- *p* = true positive / (true positive + false negative)
:param gold: The list of tagged sentences to score the tagger on.
:type gold: list(list(tuple(str, str)))
:return: A mapping from tags to precision
:rtype: Dict[str, float]
""""""
cm = self.confusion(gold)
return {tag: cm.precision(tag) for tag in cm._values}
",[],0,[],/tag/api.py_precision
3253,/home/amandapotts/git/nltk/nltk/tag/api.py_f_measure,"def f_measure(self, gold, alpha=0.5):
""""""
Compute the f-measure for each tag from ``gold`` or from running ``tag``
on the tokenized sentences from ``gold``. Then, return the dictionary
with mappings from tag to f-measure. The f-measure is the harmonic mean
of the ``precision`` and ``recall``, weighted by ``alpha``.
In particular, given the precision *p* and recall *r* defined by:
- *p* = true positive / (true positive + false negative)
- *r* = true positive / (true positive + false positive)
The f-measure is:
- *1/(alpha/p + (1-alpha)/r)*
With ``alpha = 0.5``, this reduces to:
- *2pr / (p + r)*
:param gold: The list of tagged sentences to score the tagger on.
:type gold: list(list(tuple(str, str)))
:param alpha: Ratio of the cost of false negative compared to false
positives. Defaults to 0.5, where the costs are equal.
:type alpha: float
:return: A mapping from tags to precision
:rtype: Dict[str, float]
""""""
cm = self.confusion(gold)
return {tag: cm.f_measure(tag, alpha) for tag in cm._values}
",[],0,[],/tag/api.py_f_measure
3254,/home/amandapotts/git/nltk/nltk/tag/api.py_evaluate_per_tag,"def evaluate_per_tag(self, gold, alpha=0.5, truncate=None, sort_by_count=False):
""""""Tabulate the **recall**, **precision** and **f-measure**
for each tag from ``gold`` or from running ``tag`` on the tokenized
sentences from ``gold``.
>>> from nltk.tag import PerceptronTagger
>>> from nltk.corpus import treebank
>>> tagger = PerceptronTagger()
>>> gold_data = treebank.tagged_sents()[:10]
>>> print(tagger.evaluate_per_tag(gold_data))
Tag | Prec.  | Recall | F-measure
-------+--------+--------+-----------
'' | 1.0000 | 1.0000 | 1.0000
, | 1.0000 | 1.0000 | 1.0000
-NONE- | 0.0000 | 0.0000 | 0.0000
. | 1.0000 | 1.0000 | 1.0000
CC | 1.0000 | 1.0000 | 1.0000
CD | 0.7143 | 1.0000 | 0.8333
DT | 1.0000 | 1.0000 | 1.0000
EX | 1.0000 | 1.0000 | 1.0000
IN | 0.9167 | 0.8800 | 0.8980
JJ | 0.8889 | 0.8889 | 0.8889
JJR | 0.0000 | 0.0000 | 0.0000
JJS | 1.0000 | 1.0000 | 1.0000
MD | 1.0000 | 1.0000 | 1.0000
NN | 0.8000 | 0.9333 | 0.8615
NNP | 0.8929 | 1.0000 | 0.9434
NNS | 0.9500 | 1.0000 | 0.9744
POS | 1.0000 | 1.0000 | 1.0000
PRP | 1.0000 | 1.0000 | 1.0000
PRP$ | 1.0000 | 1.0000 | 1.0000
RB | 0.4000 | 1.0000 | 0.5714
RBR | 1.0000 | 0.5000 | 0.6667
RP | 1.0000 | 1.0000 | 1.0000
TO | 1.0000 | 1.0000 | 1.0000
VB | 1.0000 | 1.0000 | 1.0000
VBD | 0.8571 | 0.8571 | 0.8571
VBG | 1.0000 | 0.8000 | 0.8889
VBN | 1.0000 | 0.8000 | 0.8889
VBP | 1.0000 | 1.0000 | 1.0000
VBZ | 1.0000 | 1.0000 | 1.0000
WDT | 0.0000 | 0.0000 | 0.0000
`` | 1.0000 | 1.0000 | 1.0000
<BLANKLINE>
:param gold: The list of tagged sentences to score the tagger on.
:type gold: list(list(tuple(str, str)))
:param alpha: Ratio of the cost of false negative compared to false
positives, as used in the f-measure computation. Defaults to 0.5,
where the costs are equal.
:type alpha: float
:param truncate: If specified, then only show the specified
number of values.  Any sorting (e.g., sort_by_count)
will be performed before truncation. Defaults to None
:type truncate: int, optional
:param sort_by_count: Whether to sort the outputs on number of
occurrences of that tag in the ``gold`` data, defaults to False
:type sort_by_count: bool, optional
:return: A tabulated recall, precision and f-measure string
:rtype: str
""""""
cm = self.confusion(gold)
return cm.evaluate(alpha=alpha, truncate=truncate, sort_by_count=sort_by_count)
",[],0,[],/tag/api.py_evaluate_per_tag
3255,/home/amandapotts/git/nltk/nltk/tag/api.py__check_params,"def _check_params(self, train, model):
if (train and model) or (not train and not model):
raise ValueError(""Must specify either training data or trained model."")
",[],0,[],/tag/api.py__check_params
3256,/home/amandapotts/git/nltk/nltk/tag/hmm.py__identity,"def _identity(labeled_symbols):
return labeled_symbols
",[],0,[],/tag/hmm.py__identity
3257,/home/amandapotts/git/nltk/nltk/tag/hmm.py___init__,"def __init__(
self, symbols, states, transitions, outputs, priors, transform=_identity
",[],0,[],/tag/hmm.py___init__
3258,/home/amandapotts/git/nltk/nltk/tag/hmm.py__train,"def _train(
cls,
labeled_sequence,
test_sequence=None,
unlabeled_sequence=None,
transform=_identity,
estimator=None,
",[],0,[],/tag/hmm.py__train
3259,/home/amandapotts/git/nltk/nltk/tag/hmm.py_estimator,"def estimator(fd, bins):
return LidstoneProbDist(fd, 0.1, bins)
",[],0,[],/tag/hmm.py_estimator
3260,/home/amandapotts/git/nltk/nltk/tag/hmm.py_train,"def train(
cls, labeled_sequence, test_sequence=None, unlabeled_sequence=None, **kwargs
",[],0,[],/tag/hmm.py_train
3261,/home/amandapotts/git/nltk/nltk/tag/hmm.py_probability,"def probability(self, sequence):
""""""
Returns the probability of the given symbol sequence. If the sequence
is labelled, then returns the joint probability of the symbol, state
sequence. Otherwise, uses the forward algorithm to find the
probability over all label sequences.
:return: the probability of the sequence
:rtype: float
:param sequence: the sequence of symbols which must contain the TEXT
property, and optionally the TAG property
:type sequence:  Token
""""""
return 2 ** (self.log_probability(self._transform(sequence)))
",[],0,[],/tag/hmm.py_probability
3262,/home/amandapotts/git/nltk/nltk/tag/hmm.py_log_probability,"def log_probability(self, sequence):
""""""
Returns the log-probability of the given symbol sequence. If the
sequence is labelled, then returns the joint log-probability of the
symbol, state sequence. Otherwise, uses the forward algorithm to find
the log-probability over all label sequences.
:return: the log-probability of the sequence
:rtype: float
:param sequence: the sequence of symbols which must contain the TEXT
property, and optionally the TAG property
:type sequence:  Token
""""""
sequence = self._transform(sequence)
T = len(sequence)
if T > 0 and sequence[0][_TAG]:
last_state = sequence[0][_TAG]
p = self._priors.logprob(last_state) + self._output_logprob(
last_state, sequence[0][_TEXT]
)
for t in range(1, T):
state = sequence[t][_TAG]
p += self._transitions[last_state].logprob(
state
) + self._output_logprob(state, sequence[t][_TEXT])
last_state = state
return p
else:
alpha = self._forward_probability(sequence)
p = logsumexp2(alpha[T - 1])
return p
",[],0,[],/tag/hmm.py_log_probability
3263,/home/amandapotts/git/nltk/nltk/tag/hmm.py_tag,"def tag(self, unlabeled_sequence):
""""""
Tags the sequence with the highest probability state sequence. This
uses the best_path method to find the Viterbi path.
:return: a labelled sequence of symbols
:rtype: list
:param unlabeled_sequence: the sequence of unlabeled symbols
:type unlabeled_sequence: list
""""""
unlabeled_sequence = self._transform(unlabeled_sequence)
return self._tag(unlabeled_sequence)
",[],0,[],/tag/hmm.py_tag
3264,/home/amandapotts/git/nltk/nltk/tag/hmm.py__tag,"def _tag(self, unlabeled_sequence):
path = self._best_path(unlabeled_sequence)
return list(zip(unlabeled_sequence, path))
",[],0,[],/tag/hmm.py__tag
3265,/home/amandapotts/git/nltk/nltk/tag/hmm.py__output_logprob,"def _output_logprob(self, state, symbol):
""""""
:return: the log probability of the symbol being observed in the given
state
:rtype: float
""""""
return self._outputs[state].logprob(symbol)
",[],0,[],/tag/hmm.py__output_logprob
3266,/home/amandapotts/git/nltk/nltk/tag/hmm.py__create_cache,"def _create_cache(self):
""""""
The cache is a tuple (P, O, X, S) where:
- S maps symbols to integers.  I.e., it is the inverse
mapping from self._symbols
self._symbols, the following is true::
self._symbols[S[s]] == s
- O is the log output probabilities::
O[i,k] = log( P(token[t]=sym[k]|tag[t]=state[i]) )
- X is the log transition probabilities::
X[i,j] = log( P(tag[t]=state[j]|tag[t-1]=state[i]) )
- P is the log prior probabilities::
P[i] = log( P(tag[0]=state[i]) )
""""""
if not self._cache:
N = len(self._states)
M = len(self._symbols)
P = np.zeros(N, np.float32)
X = np.zeros((N, N), np.float32)
O = np.zeros((N, M), np.float32)
for i in range(N):
si = self._states[i]
P[i] = self._priors.logprob(si)
for j in range(N):
X[i, j] = self._transitions[si].logprob(self._states[j])
for k in range(M):
O[i, k] = self._output_logprob(si, self._symbols[k])
S = {}
for k in range(M):
S[self._symbols[k]] = k
self._cache = (P, O, X, S)
","['zeros', 'float32', 'zeros', 'float32', 'zeros', 'float32']",6,"['zeros(N, np.float32)', 'zeros((N, N), np.float32)', 'zeros((N, M), np.float32)']",/tag/hmm.py__create_cache
3267,/home/amandapotts/git/nltk/nltk/tag/hmm.py__update_cache,"def _update_cache(self, symbols):
if symbols:
self._create_cache()
P, O, X, S = self._cache
for symbol in symbols:
if symbol not in self._symbols:
self._cache = None
self._symbols.append(symbol)
if not self._cache:
N = len(self._states)
M = len(self._symbols)
Q = O.shape[1]
O = np.hstack([O, np.zeros((N, M - Q), np.float32)])
for i in range(N):
si = self._states[i]
for k in range(Q, M):
O[i, k] = self._output_logprob(si, self._symbols[k])
for k in range(Q, M):
S[self._symbols[k]] = k
self._cache = (P, O, X, S)
","['hstack', 'zeros', 'float32']",3,"['hstack([O, np.zeros((N, M - Q), np.float32)])']",/tag/hmm.py__update_cache
3268,/home/amandapotts/git/nltk/nltk/tag/hmm.py_reset_cache,"def reset_cache(self):
self._cache = None
",[],0,[],/tag/hmm.py_reset_cache
3269,/home/amandapotts/git/nltk/nltk/tag/hmm.py_best_path,"def best_path(self, unlabeled_sequence):
""""""
Returns the state sequence of the optimal (most probable) path through
the HMM. Uses the Viterbi algorithm to calculate this part by dynamic
programming.
:return: the state sequence
:rtype: sequence of any
:param unlabeled_sequence: the sequence of unlabeled symbols
:type unlabeled_sequence: list
""""""
unlabeled_sequence = self._transform(unlabeled_sequence)
return self._best_path(unlabeled_sequence)
",[],0,[],/tag/hmm.py_best_path
3270,/home/amandapotts/git/nltk/nltk/tag/hmm.py__best_path,"def _best_path(self, unlabeled_sequence):
T = len(unlabeled_sequence)
N = len(self._states)
self._create_cache()
self._update_cache(unlabeled_sequence)
P, O, X, S = self._cache
V = np.zeros((T, N), np.float32)
B = -np.ones((T, N), int)
V[0] = P + O[:, S[unlabeled_sequence[0]]]
for t in range(1, T):
for j in range(N):
vs = V[t - 1, :] + X[:, j]
best = np.argmax(vs)
V[t, j] = vs[best] + O[j, S[unlabeled_sequence[t]]]
B[t, j] = best
current = np.argmax(V[T - 1, :])
sequence = [current]
for t in range(T - 1, 0, -1):
last = B[t, current]
sequence.append(last)
current = last
sequence.reverse()
return list(map(self._states.__getitem__, sequence))
","['zeros', 'float32', 'ones', 'argmax', 'argmax']",5,"['zeros((T, N), np.float32)', 'ones((T, N), int)', 'argmax(vs)', 'argmax(V[T - 1, :])']",/tag/hmm.py__best_path
3271,/home/amandapotts/git/nltk/nltk/tag/hmm.py_best_path_simple,"def best_path_simple(self, unlabeled_sequence):
""""""
Returns the state sequence of the optimal (most probable) path through
the HMM. Uses the Viterbi algorithm to calculate this part by dynamic
programming.  This uses a simple, direct method, and is included for
teaching purposes.
:return: the state sequence
:rtype: sequence of any
:param unlabeled_sequence: the sequence of unlabeled symbols
:type unlabeled_sequence: list
""""""
unlabeled_sequence = self._transform(unlabeled_sequence)
return self._best_path_simple(unlabeled_sequence)
",[],0,[],/tag/hmm.py_best_path_simple
3272,/home/amandapotts/git/nltk/nltk/tag/hmm.py__best_path_simple,"def _best_path_simple(self, unlabeled_sequence):
T = len(unlabeled_sequence)
N = len(self._states)
V = np.zeros((T, N), np.float64)
B = {}
symbol = unlabeled_sequence[0]
for i, state in enumerate(self._states):
V[0, i] = self._priors.logprob(state) + self._output_logprob(state, symbol)
B[0, state] = None
for t in range(1, T):
symbol = unlabeled_sequence[t]
for j in range(N):
sj = self._states[j]
best = None
for i in range(N):
si = self._states[i]
va = V[t - 1, i] + self._transitions[si].logprob(sj)
if not best or va > best[0]:
best = (va, si)
V[t, j] = best[0] + self._output_logprob(sj, symbol)
B[t, sj] = best[1]
best = None
for i in range(N):
val = V[T - 1, i]
if not best or val > best[0]:
best = (val, self._states[i])
current = best[1]
sequence = [current]
for t in range(T - 1, 0, -1):
last = B[t, current]
sequence.append(last)
current = last
sequence.reverse()
return sequence
","['zeros', 'float64']",2,"['zeros((T, N), np.float64)']",/tag/hmm.py__best_path_simple
3273,/home/amandapotts/git/nltk/nltk/tag/hmm.py_random_sample,"def random_sample(self, rng, length):
""""""
Randomly sample the HMM to generate a sentence of a given length. This
samples the prior distribution then the observation distribution and
transition distribution for each subsequent observation and state.
This will mostly generate unintelligible garbage, but can provide some
amusement.
:return:        the randomly created state/observation sequence,
generated according to the HMM's probability
distributions. The SUBTOKENS have TEXT and TAG
properties containing the observation and state
respectively.
:rtype:         list
:param rng:     random number generator
:type rng:      Random (or any object with a random() method)
:param length:  desired output length
:type length:   int
""""""
tokens = []
state = self._sample_probdist(self._priors, rng.random(), self._states)
symbol = self._sample_probdist(
self._outputs[state], rng.random(), self._symbols
)
tokens.append((symbol, state))
for i in range(1, length):
state = self._sample_probdist(
self._transitions[state], rng.random(), self._states
)
symbol = self._sample_probdist(
self._outputs[state], rng.random(), self._symbols
)
tokens.append((symbol, state))
return tokens
",[],0,[],/tag/hmm.py_random_sample
3274,/home/amandapotts/git/nltk/nltk/tag/hmm.py__sample_probdist,"def _sample_probdist(self, probdist, p, samples):
cum_p = 0
for sample in samples:
add_p = probdist.prob(sample)
if cum_p <= p <= cum_p + add_p:
return sample
cum_p += add_p
raise Exception(""Invalid probability distribution - "" ""does not sum to one"")
",[],0,[],/tag/hmm.py__sample_probdist
3275,/home/amandapotts/git/nltk/nltk/tag/hmm.py_entropy,"def entropy(self, unlabeled_sequence):
""""""
Returns the entropy over labellings of the given sequence. This is
given by::
H(O) = - sum_S Pr(S | O) log Pr(S | O)
where the summation ranges over all state sequences, S. Let
sequences and O is the observation sequence. As such the entropy can
be re-expressed as::
H = - sum_S Pr(S | O) log [ Pr(S, O) / Z ]
= log Z - sum_S Pr(S | O) log Pr(S, 0)
= log Z - sum_S Pr(S | O) [ log Pr(S_0) + sum_t Pr(S_t | S_{t-1}) + sum_t Pr(O_t | S_t) ]
The order of summation for the log terms can be flipped, allowing
dynamic programming to be used to calculate the entropy. Specifically,
we use the forward and backward probabilities (alpha, beta) giving::
H = log Z - sum_s0 alpha_0(s0) beta_0(s0) / Z * log Pr(s0)
+ sum_t,si,sj alpha_t(si) Pr(sj | si) Pr(O_t+1 | sj) beta_t(sj) / Z * log Pr(sj | si)
+ sum_t,st alpha_t(st) beta_t(st) / Z * log Pr(O_t | st)
This simply uses alpha and beta to find the probabilities of partial
sequences, constrained to include the given state(s) at some point in
time.
""""""
unlabeled_sequence = self._transform(unlabeled_sequence)
T = len(unlabeled_sequence)
N = len(self._states)
alpha = self._forward_probability(unlabeled_sequence)
beta = self._backward_probability(unlabeled_sequence)
normalisation = logsumexp2(alpha[T - 1])
entropy = normalisation
for i, state in enumerate(self._states):
p = 2 ** (alpha[0, i] + beta[0, i] - normalisation)
entropy -= p * self._priors.logprob(state)
for t0 in range(T - 1):
t1 = t0 + 1
for i0, s0 in enumerate(self._states):
for i1, s1 in enumerate(self._states):
p = 2 ** (
alpha[t0, i0]
+ self._transitions[s0].logprob(s1)
+ self._outputs[s1].logprob(unlabeled_sequence[t1][_TEXT])
+ beta[t1, i1]
- normalisation
)
entropy -= p * self._transitions[s0].logprob(s1)
for t in range(T):
for i, state in enumerate(self._states):
p = 2 ** (alpha[t, i] + beta[t, i] - normalisation)
entropy -= p * self._outputs[state].logprob(
unlabeled_sequence[t][_TEXT]
)
return entropy
",[],0,[],/tag/hmm.py_entropy
3276,/home/amandapotts/git/nltk/nltk/tag/hmm.py_point_entropy,"def point_entropy(self, unlabeled_sequence):
""""""
Returns the pointwise entropy over the possible states at each
position in the chain, given the observation sequence.
""""""
unlabeled_sequence = self._transform(unlabeled_sequence)
T = len(unlabeled_sequence)
N = len(self._states)
alpha = self._forward_probability(unlabeled_sequence)
beta = self._backward_probability(unlabeled_sequence)
normalisation = logsumexp2(alpha[T - 1])
entropies = np.zeros(T, np.float64)
probs = np.zeros(N, np.float64)
for t in range(T):
for s in range(N):
probs[s] = alpha[t, s] + beta[t, s] - normalisation
for s in range(N):
entropies[t] -= 2 ** (probs[s]) * probs[s]
return entropies
","['zeros', 'float64', 'zeros', 'float64']",4,"['zeros(T, np.float64)', 'zeros(N, np.float64)']",/tag/hmm.py_point_entropy
3277,/home/amandapotts/git/nltk/nltk/tag/hmm.py__exhaustive_entropy,"def _exhaustive_entropy(self, unlabeled_sequence):
unlabeled_sequence = self._transform(unlabeled_sequence)
T = len(unlabeled_sequence)
N = len(self._states)
labellings = [[state] for state in self._states]
for t in range(T - 1):
current = labellings
labellings = []
for labelling in current:
for state in self._states:
labellings.append(labelling + [state])
log_probs = []
for labelling in labellings:
labeled_sequence = unlabeled_sequence[:]
for t, label in enumerate(labelling):
labeled_sequence[t] = (labeled_sequence[t][_TEXT], label)
lp = self.log_probability(labeled_sequence)
log_probs.append(lp)
normalisation = _log_add(*log_probs)
entropy = 0
for lp in log_probs:
lp -= normalisation
entropy -= 2 ** (lp) * lp
return entropy
",[],0,[],/tag/hmm.py__exhaustive_entropy
3278,/home/amandapotts/git/nltk/nltk/tag/hmm.py__exhaustive_point_entropy,"def _exhaustive_point_entropy(self, unlabeled_sequence):
unlabeled_sequence = self._transform(unlabeled_sequence)
T = len(unlabeled_sequence)
N = len(self._states)
labellings = [[state] for state in self._states]
for t in range(T - 1):
current = labellings
labellings = []
for labelling in current:
for state in self._states:
labellings.append(labelling + [state])
log_probs = []
for labelling in labellings:
labelled_sequence = unlabeled_sequence[:]
for t, label in enumerate(labelling):
labelled_sequence[t] = (labelled_sequence[t][_TEXT], label)
lp = self.log_probability(labelled_sequence)
log_probs.append(lp)
normalisation = _log_add(*log_probs)
probabilities = _ninf_array((T, N))
for labelling, lp in zip(labellings, log_probs):
lp -= normalisation
for t, label in enumerate(labelling):
index = self._states.index(label)
probabilities[t, index] = _log_add(probabilities[t, index], lp)
entropies = np.zeros(T, np.float64)
for t in range(T):
for s in range(N):
entropies[t] -= 2 ** (probabilities[t, s]) * probabilities[t, s]
return entropies
","['zeros', 'float64']",2,"['zeros(T, np.float64)']",/tag/hmm.py__exhaustive_point_entropy
3279,/home/amandapotts/git/nltk/nltk/tag/hmm.py__transitions_matrix,"def _transitions_matrix(self):
""""""Return a matrix of transition log probabilities.""""""
trans_iter = (
self._transitions[sj].logprob(si)
for sj in self._states
for si in self._states
)
transitions_logprob = np.fromiter(trans_iter, dtype=np.float64)
N = len(self._states)
return transitions_logprob.reshape((N, N)).T
","['fromiter', 'float64']",2,"['fromiter(trans_iter, dtype=np.float64)']",/tag/hmm.py__transitions_matrix
3280,/home/amandapotts/git/nltk/nltk/tag/hmm.py__outputs_vector,"def _outputs_vector(self, symbol):
""""""
Return a vector with log probabilities of emitting a symbol
when entering states.
""""""
out_iter = (self._output_logprob(sj, symbol) for sj in self._states)
return np.fromiter(out_iter, dtype=np.float64)
","['fromiter', 'float64']",2,"['fromiter(out_iter, dtype=np.float64)']",/tag/hmm.py__outputs_vector
3281,/home/amandapotts/git/nltk/nltk/tag/hmm.py__forward_probability,"def _forward_probability(self, unlabeled_sequence):
""""""
Return the forward probability matrix, a T by N array of
log-probabilities, where T is the length of the sequence and N is the
number of states. Each entry (t, s) gives the probability of being in
state s at time t after observing the partial symbol sequence up to
and including t.
:param unlabeled_sequence: the sequence of unlabeled symbols
:type unlabeled_sequence: list
:return: the forward log probability matrix
:rtype: array
""""""
T = len(unlabeled_sequence)
N = len(self._states)
alpha = _ninf_array((T, N))
transitions_logprob = self._transitions_matrix()
symbol = unlabeled_sequence[0][_TEXT]
for i, state in enumerate(self._states):
alpha[0, i] = self._priors.logprob(state) + self._output_logprob(
state, symbol
)
for t in range(1, T):
symbol = unlabeled_sequence[t][_TEXT]
output_logprob = self._outputs_vector(symbol)
for i in range(N):
summand = alpha[t - 1] + transitions_logprob[i]
alpha[t, i] = logsumexp2(summand) + output_logprob[i]
return alpha
",[],0,[],/tag/hmm.py__forward_probability
3282,/home/amandapotts/git/nltk/nltk/tag/hmm.py__backward_probability,"def _backward_probability(self, unlabeled_sequence):
""""""
Return the backward probability matrix, a T by N array of
log-probabilities, where T is the length of the sequence and N is the
number of states. Each entry (t, s) gives the probability of being in
state s at time t after observing the partial symbol sequence from t
.. T.
:return: the backward log probability matrix
:rtype:  array
:param unlabeled_sequence: the sequence of unlabeled symbols
:type unlabeled_sequence: list
""""""
T = len(unlabeled_sequence)
N = len(self._states)
beta = _ninf_array((T, N))
transitions_logprob = self._transitions_matrix().T
beta[T - 1, :] = np.log2(1)
for t in range(T - 2, -1, -1):
symbol = unlabeled_sequence[t + 1][_TEXT]
outputs = self._outputs_vector(symbol)
for i in range(N):
summand = transitions_logprob[i] + beta[t + 1] + outputs
beta[t, i] = logsumexp2(summand)
return beta
",['log2'],1,['log2(1)'],/tag/hmm.py__backward_probability
3283,/home/amandapotts/git/nltk/nltk/tag/hmm.py_test,"def test(self, test_sequence, verbose=False, **kwargs):
""""""
Tests the HiddenMarkovModelTagger instance.
:param test_sequence: a sequence of labeled test instances
:type test_sequence: list(list)
:param verbose: boolean flag indicating whether training should be
verbose or include printed output
:type verbose: bool
""""""
",[],0,[],/tag/hmm.py_test
3284,/home/amandapotts/git/nltk/nltk/tag/hmm.py_words,"def words(sent):
return [word for (word, tag) in sent]
",[],0,[],/tag/hmm.py_words
3285,/home/amandapotts/git/nltk/nltk/tag/hmm.py_tags,"def tags(sent):
return [tag for (word, tag) in sent]
",[],0,[],/tag/hmm.py_tags
3286,/home/amandapotts/git/nltk/nltk/tag/hmm.py_flatten,"def flatten(seq):
return list(itertools.chain(*seq))
",[],0,[],/tag/hmm.py_flatten
3287,/home/amandapotts/git/nltk/nltk/tag/hmm.py___repr__,"def __repr__(self):
return ""<HiddenMarkovModelTagger %d states and %d output symbols>"" % (
len(self._states),
len(self._symbols),
)
",[],0,[],/tag/hmm.py___repr__
3288,/home/amandapotts/git/nltk/nltk/tag/hmm.py___init__,"def __init__(self, states=None, symbols=None):
self._states = states if states else []
self._symbols = symbols if symbols else []
",[],0,[],/tag/hmm.py___init__
3289,/home/amandapotts/git/nltk/nltk/tag/hmm.py_train,"def train(self, labeled_sequences=None, unlabeled_sequences=None, **kwargs):
""""""
Trains the HMM using both (or either of) supervised and unsupervised
techniques.
:return: the trained model
:rtype: HiddenMarkovModelTagger
:param labelled_sequences: the supervised training data, a set of
labelled sequences of observations
ex: [ (word_1, tag_1),...,(word_n,tag_n) ]
:type labelled_sequences: list
:param unlabeled_sequences: the unsupervised training data, a set of
sequences of observations
ex: [ word_1, ..., word_n ]
:type unlabeled_sequences: list
:param kwargs: additional arguments to pass to the training methods
""""""
assert labeled_sequences or unlabeled_sequences
model = None
if labeled_sequences:
model = self.train_supervised(labeled_sequences, **kwargs)
if unlabeled_sequences:
if model:
kwargs[""model""] = model
model = self.train_unsupervised(unlabeled_sequences, **kwargs)
return model
",[],0,[],/tag/hmm.py_train
3290,/home/amandapotts/git/nltk/nltk/tag/hmm.py__baum_welch_step,"def _baum_welch_step(self, sequence, model, symbol_to_number):
N = len(model._states)
M = len(model._symbols)
T = len(sequence)
alpha = model._forward_probability(sequence)
beta = model._backward_probability(sequence)
lpk = logsumexp2(alpha[T - 1])
A_numer = _ninf_array((N, N))
B_numer = _ninf_array((N, M))
A_denom = _ninf_array(N)
B_denom = _ninf_array(N)
transitions_logprob = model._transitions_matrix().T
for t in range(T):
symbol = sequence[t][_TEXT]  # not found? FIXME
next_symbol = None
if t < T - 1:
next_symbol = sequence[t + 1][_TEXT]  # not found? FIXME
xi = symbol_to_number[symbol]
next_outputs_logprob = model._outputs_vector(next_symbol)
alpha_plus_beta = alpha[t] + beta[t]
if t < T - 1:
numer_add = (
transitions_logprob
+ next_outputs_logprob
+ beta[t + 1]
+ alpha[t].reshape(N, 1)
)
A_numer = np.logaddexp2(A_numer, numer_add)
A_denom = np.logaddexp2(A_denom, alpha_plus_beta)
else:
B_denom = np.logaddexp2(A_denom, alpha_plus_beta)
B_numer[:, xi] = np.logaddexp2(B_numer[:, xi], alpha_plus_beta)
return lpk, A_numer, A_denom, B_numer, B_denom
","['logaddexp2', 'logaddexp2', 'logaddexp2', 'logaddexp2']",4,"['logaddexp2(A_numer, numer_add)', 'logaddexp2(A_denom, alpha_plus_beta)', 'logaddexp2(A_denom, alpha_plus_beta)', 'logaddexp2(B_numer[:, xi], alpha_plus_beta)']",/tag/hmm.py__baum_welch_step
3291,/home/amandapotts/git/nltk/nltk/tag/hmm.py_train_unsupervised,"def train_unsupervised(self, unlabeled_sequences, update_outputs=True, **kwargs):
""""""
Trains the HMM using the Baum-Welch algorithm to maximise the
probability of the data sequence. This is a variant of the EM
algorithm, and is unsupervised in that it doesn't need the state
sequences for the symbols. The code is based on 'A Tutorial on Hidden
Markov Models and Selected Applications in Speech Recognition',
Lawrence Rabiner, IEEE, 1989.
:return: the trained model
:rtype: HiddenMarkovModelTagger
:param unlabeled_sequences: the training data, a set of
sequences of observations
:type unlabeled_sequences: list
kwargs may include following parameters:
:param model: a HiddenMarkovModelTagger instance used to begin
the Baum-Welch algorithm
:param max_iterations: the maximum number of EM iterations
:param convergence_logprob: the maximum change in log probability to
allow convergence
""""""
model = kwargs.get(""model"")
if not model:
priors = RandomProbDist(self._states)
transitions = DictionaryConditionalProbDist(
{state: RandomProbDist(self._states) for state in self._states}
)
outputs = DictionaryConditionalProbDist(
{state: RandomProbDist(self._symbols) for state in self._states}
)
model = HiddenMarkovModelTagger(
self._symbols, self._states, transitions, outputs, priors
)
self._states = model._states
self._symbols = model._symbols
N = len(self._states)
M = len(self._symbols)
symbol_numbers = {sym: i for i, sym in enumerate(self._symbols)}
model._transitions = DictionaryConditionalProbDist(
{
s: MutableProbDist(model._transitions[s], self._states)
for s in self._states
}
)
if update_outputs:
model._outputs = DictionaryConditionalProbDist(
{
s: MutableProbDist(model._outputs[s], self._symbols)
for s in self._states
}
)
model.reset_cache()
converged = False
last_logprob = None
iteration = 0
max_iterations = kwargs.get(""max_iterations"", 1000)
epsilon = kwargs.get(""convergence_logprob"", 1e-6)
while not converged and iteration < max_iterations:
A_numer = _ninf_array((N, N))
B_numer = _ninf_array((N, M))
A_denom = _ninf_array(N)
B_denom = _ninf_array(N)
logprob = 0
for sequence in unlabeled_sequences:
sequence = list(sequence)
if not sequence:
continue
(
lpk,
seq_A_numer,
seq_A_denom,
seq_B_numer,
seq_B_denom,
) = self._baum_welch_step(sequence, model, symbol_numbers)
for i in range(N):
A_numer[i] = np.logaddexp2(A_numer[i], seq_A_numer[i] - lpk)
B_numer[i] = np.logaddexp2(B_numer[i], seq_B_numer[i] - lpk)
A_denom = np.logaddexp2(A_denom, seq_A_denom - lpk)
B_denom = np.logaddexp2(B_denom, seq_B_denom - lpk)
logprob += lpk
for i in range(N):
logprob_Ai = A_numer[i] - A_denom[i]
logprob_Bi = B_numer[i] - B_denom[i]
logprob_Ai -= logsumexp2(logprob_Ai)
logprob_Bi -= logsumexp2(logprob_Bi)
si = self._states[i]
for j in range(N):
sj = self._states[j]
model._transitions[si].update(sj, logprob_Ai[j])
if update_outputs:
for k in range(M):
ok = self._symbols[k]
model._outputs[si].update(ok, logprob_Bi[k])
if iteration > 0 and abs(logprob - last_logprob) < epsilon:
converged = True
print(""iteration"", iteration, ""logprob"", logprob)
iteration += 1
last_logprob = logprob
return model
","['logaddexp2', 'logaddexp2', 'logaddexp2', 'logaddexp2']",4,"['logaddexp2(A_numer[i], seq_A_numer[i] - lpk)', 'logaddexp2(B_numer[i], seq_B_numer[i] - lpk)', 'logaddexp2(A_denom, seq_A_denom - lpk)', 'logaddexp2(B_denom, seq_B_denom - lpk)']",/tag/hmm.py_train_unsupervised
3292,/home/amandapotts/git/nltk/nltk/tag/hmm.py_train_supervised,"def train_supervised(self, labelled_sequences, estimator=None):
""""""
Supervised training maximising the joint probability of the symbol and
state sequences. This is done via collecting frequencies of
transitions between states, symbol observations while within each
state and which states start a sentence. These frequency distributions
are then normalised into probability estimates, which can be
smoothed if desired.
:return: the trained model
:rtype: HiddenMarkovModelTagger
:param labelled_sequences: the training data, a set of
labelled sequences of observations
:type labelled_sequences: list
:param estimator: a function taking
a FreqDist and a number of bins and returning a CProbDistI
otherwise a MLE estimate is used
""""""
if estimator is None:
",[],0,[],/tag/hmm.py_train_supervised
3293,/home/amandapotts/git/nltk/nltk/tag/hmm.py__ninf_array,"def _ninf_array(shape):
res = np.empty(shape, np.float64)
res.fill(-np.inf)
return res
","['empty', 'float64', 'inf']",3,"['empty(shape, np.float64)']",/tag/hmm.py__ninf_array
3294,/home/amandapotts/git/nltk/nltk/tag/hmm.py_logsumexp2,"def logsumexp2(arr):
max_ = arr.max()
return np.log2(np.sum(2 ** (arr - max_))) + max_
","['log2', 'sum']",2,['log2(np.sum(2 ** (arr - max_)))'],/tag/hmm.py_logsumexp2
3295,/home/amandapotts/git/nltk/nltk/tag/hmm.py__log_add,"def _log_add(*values):
""""""
Adds the logged values, returning the logarithm of the addition.
""""""
x = max(values)
if x > -np.inf:
sum_diffs = 0
for value in values:
sum_diffs += 2 ** (value - x)
return x + np.log2(sum_diffs)
else:
return x
","['inf', 'log2']",2,['log2(sum_diffs)'],/tag/hmm.py__log_add
3296,/home/amandapotts/git/nltk/nltk/tag/hmm.py__create_hmm_tagger,"def _create_hmm_tagger(states, symbols, A, B, pi):
",[],0,[],/tag/hmm.py__create_hmm_tagger
3297,/home/amandapotts/git/nltk/nltk/tag/hmm.py_pd,"def pd(values, samples):
d = dict(zip(samples, values))
return DictionaryProbDist(d)
",[],0,[],/tag/hmm.py_pd
3298,/home/amandapotts/git/nltk/nltk/tag/hmm.py_cpd,"def cpd(array, conditions, samples):
d = {}
for values, condition in zip(array, conditions):
d[condition] = pd(values, samples)
return DictionaryConditionalProbDist(d)
",[],0,[],/tag/hmm.py_cpd
3299,/home/amandapotts/git/nltk/nltk/tag/hmm.py__market_hmm_example,"def _market_hmm_example():
""""""
Return an example HMM (described at page 381, Huang et al)
""""""
states = [""bull"", ""bear"", ""static""]
symbols = [""up"", ""down"", ""unchanged""]
A = np.array([[0.6, 0.2, 0.2], [0.5, 0.3, 0.2], [0.4, 0.1, 0.5]], np.float64)
B = np.array([[0.7, 0.1, 0.2], [0.1, 0.6, 0.3], [0.3, 0.3, 0.4]], np.float64)
pi = np.array([0.5, 0.2, 0.3], np.float64)
model = _create_hmm_tagger(states, symbols, A, B, pi)
return model, states, symbols
","['array', 'float64', 'array', 'float64', 'array', 'float64']",6,"['array([[0.6, 0.2, 0.2], [0.5, 0.3, 0.2], [0.4, 0.1, 0.5]], np.float64)', 'array([[0.7, 0.1, 0.2], [0.1, 0.6, 0.3], [0.3, 0.3, 0.4]], np.float64)', 'array([0.5, 0.2, 0.3], np.float64)']",/tag/hmm.py__market_hmm_example
3300,/home/amandapotts/git/nltk/nltk/tag/hmm.py_demo,"def demo():
print()
print(""HMM probability calculation demo"")
print()
model, states, symbols = _market_hmm_example()
print(""Testing"", model)
for test in [
[""up"", ""up""],
[""up"", ""down"", ""up""],
[""down""] * 5,
[""unchanged""] * 5 + [""up""],
]:
sequence = [(t, None) for t in test]
print(""Testing with state sequence"", test)
print(""probability ="", model.probability(sequence))
print(""tagging =    "", model.tag([word for (word, tag) in sequence]))
print(""p(tagged) =  "", model.probability(sequence))
print(""H =          "", model.entropy(sequence))
print(""H_exh =      "", model._exhaustive_entropy(sequence))
print(""H(point) =   "", model.point_entropy(sequence))
print(""H_exh(point)="", model._exhaustive_point_entropy(sequence))
print()
",[],0,[],/tag/hmm.py_demo
3301,/home/amandapotts/git/nltk/nltk/tag/hmm.py_load_pos,"def load_pos(num_sents):
from nltk.corpus import brown
sentences = brown.tagged_sents(categories=""news"")[:num_sents]
tag_re = re.compile(r""[*]|--|[^+*-]+"")
tag_set = set()
symbols = set()
cleaned_sentences = []
for sentence in sentences:
for i in range(len(sentence)):
word, tag = sentence[i]
word = word.lower()  # normalize
symbols.add(word)  # log this word
tag = tag_re.match(tag).group()
tag_set.add(tag)
sentence[i] = (word, tag)  # store cleaned-up tagged token
cleaned_sentences += [sentence]
return cleaned_sentences, list(tag_set), list(symbols)
",[],0,[],/tag/hmm.py_load_pos
3302,/home/amandapotts/git/nltk/nltk/tag/hmm.py__untag,"def _untag(sentences):
unlabeled = []
for sentence in sentences:
unlabeled.append([(token[_TEXT], None) for token in sentence])
return unlabeled
",[],0,[],/tag/hmm.py__untag
3303,/home/amandapotts/git/nltk/nltk/tag/hmm.py_demo_bw,"def demo_bw():
print()
print(""Baum-Welch demo for market example"")
print()
model, states, symbols = _market_hmm_example()
training = []
import random
rng = random.Random()
rng.seed(0)
for i in range(10):
item = model.random_sample(rng, 5)
training.append([(i[0], None) for i in item])
trainer = HiddenMarkovModelTrainer(states, symbols)
hmm = trainer.train_unsupervised(training, model=model, max_iterations=1000)
",[],0,[],/tag/hmm.py_demo_bw
3304,/home/amandapotts/git/nltk/nltk/tag/mapping.py_tagset_mapping,"def tagset_mapping(source, target):
""""""
Retrieve the mapping dictionary between tagsets.
>>> tagset_mapping('ru-rnc', 'universal') == {'!': '.', 'A': 'ADJ', 'C': 'CONJ', 'AD': 'ADV',\
'NN': 'NOUN', 'VG': 'VERB', 'COMP': 'CONJ', 'NC': 'NUM', 'VP': 'VERB', 'P': 'ADP',\
'IJ': 'X', 'V': 'VERB', 'Z': 'X', 'VI': 'VERB', 'YES_NO_SENT': 'X', 'PTCL': 'PRT'}
True
""""""
if source not in _MAPPINGS or target not in _MAPPINGS[source]:
if target == ""universal"":
_load_universal_map(source)
_MAPPINGS[""ru-rnc-new""][""universal""] = {
""A"": ""ADJ"",
""A-PRO"": ""PRON"",
""ADV"": ""ADV"",
""ADV-PRO"": ""PRON"",
""ANUM"": ""ADJ"",
""CONJ"": ""CONJ"",
""INTJ"": ""X"",
""NONLEX"": ""."",
""NUM"": ""NUM"",
""PARENTH"": ""PRT"",
""PART"": ""PRT"",
""PR"": ""ADP"",
""PRAEDIC"": ""PRT"",
""PRAEDIC-PRO"": ""PRON"",
""S"": ""NOUN"",
""S-PRO"": ""PRON"",
""V"": ""VERB"",
}
return _MAPPINGS[source][target]
",[],0,[],/tag/mapping.py_tagset_mapping
3305,/home/amandapotts/git/nltk/nltk/tag/mapping.py_map_tag,"def map_tag(source, target, source_tag):
""""""
Maps the tag from the source tagset to the target tagset.
>>> map_tag('en-ptb', 'universal', 'VBZ')
'VERB'
>>> map_tag('en-ptb', 'universal', 'VBP')
'VERB'
>>> map_tag('en-ptb', 'universal', '``')
'.'
""""""
if target == ""universal"":
if source == ""wsj"":
source = ""en-ptb""
if source == ""brown"":
source = ""en-brown""
return tagset_mapping(source, target)[source_tag]
",[],0,[],/tag/mapping.py_map_tag
3306,/home/amandapotts/git/nltk/nltk/tag/brill_trainer.py___init__,"def __init__(
self, initial_tagger, templates, trace=0, deterministic=None, ruleformat=""str""
",[],0,[],/tag/brill_trainer.py___init__
3307,/home/amandapotts/git/nltk/nltk/tag/brill_trainer.py_train,"def train(self, train_sents, max_rules=200, min_score=2, min_acc=None):
r""""""
Trains the Brill tagger on the corpus *train_sents*,
producing at most *max_rules* transformations, each of which
reduces the net number of errors in the corpus by at least
>>> # Relevant imports
>>> from nltk.tbl.template import Template
>>> from nltk.tag.brill import Pos, Word
>>> from nltk.tag import untag, RegexpTagger, BrillTaggerTrainer
>>> # Load some data
>>> from nltk.corpus import treebank
>>> training_data = treebank.tagged_sents()[:100]
>>> baseline_data = treebank.tagged_sents()[100:200]
>>> gold_data = treebank.tagged_sents()[200:300]
>>> testing_data = [untag(s) for s in gold_data]
>>> backoff = RegexpTagger([
... (r'^-?[0-9]+(\.[0-9]+)?$', 'CD'),  # cardinal numbers
... (r'(The|the|A|a|An|an)$', 'AT'),   # articles
... (r'.*able$', 'JJ'),                # adjectives
... (r'.*ness$', 'NN'),                # nouns formed from adjectives
... (r'.*ly$', 'RB'),                  # adverbs
... (r'.*s$', 'NNS'),                  # plural nouns
... (r'.*ing$', 'VBG'),                # gerunds
... (r'.*ed$', 'VBD'),                 # past tense verbs
... (r'.*', 'NN')                      # nouns (default)
... ])
>>> baseline = backoff #see NOTE1
>>> baseline.accuracy(gold_data) #doctest: +ELLIPSIS
0.243...
>>> # Set up templates
>>> Template._cleartemplates() #clear any templates created in earlier tests
>>> templates = [Template(Pos([-1])), Template(Pos([-1]), Word([0]))]
>>> # Construct a BrillTaggerTrainer
>>> tt = BrillTaggerTrainer(baseline, templates, trace=3)
>>> tagger1 = tt.train(training_data, max_rules=10)
TBL train (fast) (seqs: 100
Finding initial useful rules...
Found 847 useful rules.
<BLANKLINE>
B      |
S   F   r   O  |        Score = Fixed - Broken
c   i   o   t  |  R     Fixed = num tags changed incorrect -> correct
o   x   k   h  |  u     Broken = num tags changed correct -> incorrect
r   e   e   e  |  l     Other = num tags changed incorrect -> incorrect
e   d   n   r  |  e
------------------+-------------------------------------------------------
132 132   0   0  | AT->DT if Pos:NN@[-1]
85  85   0   0  | NN->, if Pos:NN@[-1] & Word:,@[0]
69  69   0   0  | NN->. if Pos:NN@[-1] & Word:.@[0]
51  51   0   0  | NN->IN if Pos:NN@[-1] & Word:of@[0]
47  63  16 162  | NN->IN if Pos:NNS@[-1]
33  33   0   0  | NN->TO if Pos:NN@[-1] & Word:to@[0]
26  26   0   0  | IN->. if Pos:NNS@[-1] & Word:.@[0]
24  24   0   0  | IN->, if Pos:NNS@[-1] & Word:,@[0]
22  27   5  24  | NN->-NONE- if Pos:VBD@[-1]
17  17   0   0  | NN->CC if Pos:NN@[-1] & Word:and@[0]
>>> tagger1.rules()[1:3]
(Rule('001', 'NN', ',', [(Pos([-1]),'NN'), (Word([0]),',')]), Rule('001', 'NN', '.', [(Pos([-1]),'NN'), (Word([0]),'.')]))
>>> train_stats = tagger1.train_stats()
>>> [train_stats[stat] for stat in ['initialerrors', 'finalerrors', 'rulescores']]
[1776, 1270, [132, 85, 69, 51, 47, 33, 26, 24, 22, 17]]
>>> tagger1.print_template_statistics(printunused=False)
TEMPLATE STATISTICS (TRAIN)  2 templates, 10 rules)
TRAIN (   2417 tokens) initial  1776 0.2652 final:  1270 0.4746
--------------------------------------------
001 |   305   0.603 |   7   0.700 | Template(Pos([-1]),Word([0]))
000 |   201   0.397 |   3   0.300 | Template(Pos([-1]))
<BLANKLINE>
<BLANKLINE>
>>> round(tagger1.accuracy(gold_data),5)
0.43834
>>> tagged, test_stats = tagger1.batch_tag_incremental(testing_data, gold_data)
>>> tagged[33][12:] == [('foreign', 'IN'), ('debt', 'NN'), ('of', 'IN'), ('$', 'NN'), ('64', 'CD'),
... ('billion', 'NN'), ('*U*', 'NN'), ('--', 'NN'), ('the', 'DT'), ('third-highest', 'NN'), ('in', 'NN'),
... ('the', 'DT'), ('developing', 'VBG'), ('world', 'NN'), ('.', '.')]
True
>>> [test_stats[stat] for stat in ['initialerrors', 'finalerrors', 'rulescores']]
[1859, 1380, [100, 85, 67, 58, 27, 36, 27, 16, 31, 32]]
>>> # A high-accuracy tagger
>>> tagger2 = tt.train(training_data, max_rules=10, min_acc=0.99)
TBL train (fast) (seqs: 100
Finding initial useful rules...
Found 847 useful rules.
<BLANKLINE>
B      |
S   F   r   O  |        Score = Fixed - Broken
c   i   o   t  |  R     Fixed = num tags changed incorrect -> correct
o   x   k   h  |  u     Broken = num tags changed correct -> incorrect
r   e   e   e  |  l     Other = num tags changed incorrect -> incorrect
e   d   n   r  |  e
------------------+-------------------------------------------------------
132 132   0   0  | AT->DT if Pos:NN@[-1]
85  85   0   0  | NN->, if Pos:NN@[-1] & Word:,@[0]
69  69   0   0  | NN->. if Pos:NN@[-1] & Word:.@[0]
51  51   0   0  | NN->IN if Pos:NN@[-1] & Word:of@[0]
36  36   0   0  | NN->TO if Pos:NN@[-1] & Word:to@[0]
26  26   0   0  | NN->. if Pos:NNS@[-1] & Word:.@[0]
24  24   0   0  | NN->, if Pos:NNS@[-1] & Word:,@[0]
19  19   0   6  | NN->VB if Pos:TO@[-1]
18  18   0   0  | CD->-NONE- if Pos:NN@[-1] & Word:0@[0]
18  18   0   0  | NN->CC if Pos:NN@[-1] & Word:and@[0]
>>> round(tagger2.accuracy(gold_data), 8)
0.43996744
>>> tagger2.rules()[2:4]
(Rule('001', 'NN', '.', [(Pos([-1]),'NN'), (Word([0]),'.')]), Rule('001', 'NN', 'IN', [(Pos([-1]),'NN'), (Word([0]),'of')]))
:param train_sents: training data
:type train_sents: list(list(tuple))
:param max_rules: output at most max_rules rules
:type max_rules: int
:param min_score: stop training when no rules better than min_score can be found
:type min_score: int
:param min_acc: discard any rule with lower accuracy than min_acc
:type min_acc: float or None
:return: the learned tagger
:rtype: BrillTagger
""""""
test_sents = [
list(self._initial_tagger.tag(untag(sent))) for sent in train_sents
]
trainstats = {}
trainstats[""min_acc""] = min_acc
trainstats[""min_score""] = min_score
trainstats[""tokencount""] = sum(len(t) for t in test_sents)
trainstats[""sequencecount""] = len(test_sents)
trainstats[""templatecount""] = len(self._templates)
trainstats[""rulescores""] = []
trainstats[""initialerrors""] = sum(
tag[1] != truth[1]
for paired in zip(test_sents, train_sents)
for (tag, truth) in zip(*paired)
)
trainstats[""initialacc""] = (
1 - trainstats[""initialerrors""] / trainstats[""tokencount""]
)
if self._trace > 0:
print(
""TBL train (fast) (seqs: {sequencecount}
""tpls: {templatecount}
)
)
if self._trace:
print(""Finding initial useful rules..."")
self._init_mappings(test_sents, train_sents)
if self._trace:
print(f""    Found {len(self._rule_scores)} useful rules."")
if self._trace > 2:
self._trace_header()
elif self._trace == 1:
print(""Selecting rules..."")
rules = []
try:
while len(rules) < max_rules:
rule = self._best_rule(train_sents, test_sents, min_score, min_acc)
if rule:
rules.append(rule)
score = self._rule_scores[rule]
trainstats[""rulescores""].append(score)
else:
break  # No more good rules left!
if self._trace > 1:
self._trace_rule(rule)
self._apply_rule(rule, test_sents)
self._update_tag_positions(rule)
self._update_rules(rule, train_sents, test_sents)
except KeyboardInterrupt:
print(f""Training stopped manually -- {len(rules)} rules found"")
self._clean()
trainstats[""finalerrors""] = trainstats[""initialerrors""] - sum(
trainstats[""rulescores""]
)
trainstats[""finalacc""] = (
1 - trainstats[""finalerrors""] / trainstats[""tokencount""]
)
return BrillTagger(self._initial_tagger, rules, trainstats)
",[],0,[],/tag/brill_trainer.py_train
3308,/home/amandapotts/git/nltk/nltk/tag/brill_trainer.py__init_mappings,"def _init_mappings(self, test_sents, train_sents):
""""""
Initialize the tag position mapping & the rule related
mappings.  For each error in test_sents, find new rules that
would correct them, and add them to the rule mappings.
""""""
self._tag_positions = defaultdict(list)
self._rules_by_position = defaultdict(set)
self._positions_by_rule = defaultdict(dict)
self._rules_by_score = defaultdict(set)
self._rule_scores = defaultdict(int)
self._first_unknown_position = defaultdict(int)
for sentnum, sent in enumerate(test_sents):
for wordnum, (word, tag) in enumerate(sent):
self._tag_positions[tag].append((sentnum, wordnum))
correct_tag = train_sents[sentnum][wordnum][1]
if tag != correct_tag:
for rule in self._find_rules(sent, wordnum, correct_tag):
self._update_rule_applies(rule, sentnum, wordnum, train_sents)
",[],0,[],/tag/brill_trainer.py__init_mappings
3309,/home/amandapotts/git/nltk/nltk/tag/brill_trainer.py__clean,"def _clean(self):
self._tag_positions = None
self._rules_by_position = None
self._positions_by_rule = None
self._rules_by_score = None
self._rule_scores = None
self._first_unknown_position = None
",[],0,[],/tag/brill_trainer.py__clean
3310,/home/amandapotts/git/nltk/nltk/tag/brill_trainer.py__find_rules,"def _find_rules(self, sent, wordnum, new_tag):
""""""
Use the templates to find rules that apply at index *wordnum*
in the sentence *sent* and generate the tag *new_tag*.
""""""
for template in self._templates:
yield from template.applicable_rules(sent, wordnum, new_tag)
",[],0,[],/tag/brill_trainer.py__find_rules
3311,/home/amandapotts/git/nltk/nltk/tag/brill_trainer.py__update_rule_applies,"def _update_rule_applies(self, rule, sentnum, wordnum, train_sents):
""""""
Update the rule data tables to reflect the fact that
""""""
pos = sentnum, wordnum
if pos in self._positions_by_rule[rule]:
return
correct_tag = train_sents[sentnum][wordnum][1]
if rule.replacement_tag == correct_tag:
self._positions_by_rule[rule][pos] = 1
elif rule.original_tag == correct_tag:
self._positions_by_rule[rule][pos] = -1
else:  # was wrong, remains wrong
self._positions_by_rule[rule][pos] = 0
self._rules_by_position[pos].add(rule)
old_score = self._rule_scores[rule]
self._rule_scores[rule] += self._positions_by_rule[rule][pos]
self._rules_by_score[old_score].discard(rule)
self._rules_by_score[self._rule_scores[rule]].add(rule)
",[],0,[],/tag/brill_trainer.py__update_rule_applies
3312,/home/amandapotts/git/nltk/nltk/tag/brill_trainer.py__update_rule_not_applies,"def _update_rule_not_applies(self, rule, sentnum, wordnum):
""""""
Update the rule data tables to reflect the fact that *rule*
does not apply at the position *(sentnum, wordnum)*.
""""""
pos = sentnum, wordnum
old_score = self._rule_scores[rule]
self._rule_scores[rule] -= self._positions_by_rule[rule][pos]
self._rules_by_score[old_score].discard(rule)
self._rules_by_score[self._rule_scores[rule]].add(rule)
del self._positions_by_rule[rule][pos]
self._rules_by_position[pos].remove(rule)
",[],0,[],/tag/brill_trainer.py__update_rule_not_applies
3313,/home/amandapotts/git/nltk/nltk/tag/brill_trainer.py__best_rule,"def _best_rule(self, train_sents, test_sents, min_score, min_acc):
""""""
Find the next best rule.  This is done by repeatedly taking a
rule with the highest score and stepping through the corpus to
see where it applies.  When it makes an error (decreasing its
score) it's bumped down, and we try a new rule with the
highest score.  When we find a rule which has the highest
score *and* which has been tested against the entire corpus, we
can conclude that it's the next best rule.
""""""
for max_score in sorted(self._rules_by_score.keys(), reverse=True):
if len(self._rules_by_score) == 0:
return None
if max_score < min_score or max_score <= 0:
return None
best_rules = list(self._rules_by_score[max_score])
if self._deterministic:
best_rules.sort(key=repr)
for rule in best_rules:
positions = self._tag_positions[rule.original_tag]
unk = self._first_unknown_position.get(rule, (0, -1))
start = bisect.bisect_left(positions, unk)
for i in range(start, len(positions)):
sentnum, wordnum = positions[i]
if rule.applies(test_sents[sentnum], wordnum):
self._update_rule_applies(rule, sentnum, wordnum, train_sents)
if self._rule_scores[rule] < max_score:
self._first_unknown_position[rule] = (sentnum, wordnum + 1)
break  # The update demoted the rule.
if self._rule_scores[rule] == max_score:
self._first_unknown_position[rule] = (len(train_sents) + 1, 0)
if min_acc is None:
return rule
else:
changes = self._positions_by_rule[rule].values()
num_fixed = len([c for c in changes if c == 1])
num_broken = len([c for c in changes if c == -1])
acc = num_fixed / (num_fixed + num_broken)
if acc >= min_acc:
return rule
assert min_acc is not None or not self._rules_by_score[max_score]
if not self._rules_by_score[max_score]:
del self._rules_by_score[max_score]
",[],0,[],/tag/brill_trainer.py__best_rule
3314,/home/amandapotts/git/nltk/nltk/tag/brill_trainer.py__apply_rule,"def _apply_rule(self, rule, test_sents):
""""""
Update *test_sents* by applying *rule* everywhere where its
conditions are met.
""""""
update_positions = set(self._positions_by_rule[rule])
new_tag = rule.replacement_tag
if self._trace > 3:
self._trace_apply(len(update_positions))
for sentnum, wordnum in update_positions:
text = test_sents[sentnum][wordnum][0]
test_sents[sentnum][wordnum] = (text, new_tag)
",[],0,[],/tag/brill_trainer.py__apply_rule
3315,/home/amandapotts/git/nltk/nltk/tag/brill_trainer.py__update_tag_positions,"def _update_tag_positions(self, rule):
""""""
Update _tag_positions to reflect the changes to tags that are
made by *rule*.
""""""
for pos in self._positions_by_rule[rule]:
old_tag_positions = self._tag_positions[rule.original_tag]
old_index = bisect.bisect_left(old_tag_positions, pos)
del old_tag_positions[old_index]
new_tag_positions = self._tag_positions[rule.replacement_tag]
bisect.insort_left(new_tag_positions, pos)
",[],0,[],/tag/brill_trainer.py__update_tag_positions
3316,/home/amandapotts/git/nltk/nltk/tag/brill_trainer.py__update_rules,"def _update_rules(self, rule, train_sents, test_sents):
""""""
Check if we should add or remove any rules from consideration,
given the changes made by *rule*.
""""""
neighbors = set()
for sentnum, wordnum in self._positions_by_rule[rule]:
for template in self._templates:
n = template.get_neighborhood(test_sents[sentnum], wordnum)
neighbors.update([(sentnum, i) for i in n])
num_obsolete = num_new = num_unseen = 0
for sentnum, wordnum in neighbors:
test_sent = test_sents[sentnum]
correct_tag = train_sents[sentnum][wordnum][1]
old_rules = set(self._rules_by_position[sentnum, wordnum])
for old_rule in old_rules:
if not old_rule.applies(test_sent, wordnum):
num_obsolete += 1
self._update_rule_not_applies(old_rule, sentnum, wordnum)
for template in self._templates:
for new_rule in template.applicable_rules(
test_sent, wordnum, correct_tag
):
if new_rule not in old_rules:
num_new += 1
if new_rule not in self._rule_scores:
num_unseen += 1
old_rules.add(new_rule)
self._update_rule_applies(
new_rule, sentnum, wordnum, train_sents
)
for new_rule, pos in self._first_unknown_position.items():
if pos > (sentnum, wordnum):
if new_rule not in old_rules:
num_new += 1
if new_rule.applies(test_sent, wordnum):
self._update_rule_applies(
new_rule, sentnum, wordnum, train_sents
)
if self._trace > 3:
self._trace_update_rules(num_obsolete, num_new, num_unseen)
",[],0,[],/tag/brill_trainer.py__update_rules
3317,/home/amandapotts/git/nltk/nltk/tag/brill_trainer.py__trace_header,"def _trace_header(self):
print(
""""""
B      |
",[],0,[],/tag/brill_trainer.py__trace_header
3318,/home/amandapotts/git/nltk/nltk/tag/brill_trainer.py__trace_rule,"def _trace_rule(self, rule):
assert self._rule_scores[rule] == sum(self._positions_by_rule[rule].values())
changes = self._positions_by_rule[rule].values()
num_fixed = len([c for c in changes if c == 1])
num_broken = len([c for c in changes if c == -1])
num_other = len([c for c in changes if c == 0])
score = self._rule_scores[rule]
rulestr = rule.format(self._ruleformat)
if self._trace > 2:
print(
""{:4d}{:4d}{:4d}{:4d}  |"".format(
score, num_fixed, num_broken, num_other
),
end="" "",
)
print(
textwrap.fill(
rulestr,
initial_indent="" "" * 20,
width=79,
subsequent_indent="" "" * 18 + ""|   "",
).strip()
)
else:
print(rulestr)
",[],0,[],/tag/brill_trainer.py__trace_rule
3319,/home/amandapotts/git/nltk/nltk/tag/brill_trainer.py__trace_apply,"def _trace_apply(self, num_updates):
prefix = "" "" * 18 + ""|""
print(prefix)
print(prefix, f""Applying rule to {num_updates} positions."")
",[],0,[],/tag/brill_trainer.py__trace_apply
3320,/home/amandapotts/git/nltk/nltk/tag/brill_trainer.py__trace_update_rules,"def _trace_update_rules(self, num_obsolete, num_new, num_unseen):
prefix = "" "" * 18 + ""|""
print(prefix, ""Updated rule tables:"")
print(prefix, (f""  - {num_obsolete} rule applications removed""))
print(
prefix,
(f""  - {num_new} rule applications added ({num_unseen} novel)""),
)
print(prefix)
",[],0,[],/tag/brill_trainer.py__trace_update_rules
3321,/home/amandapotts/git/nltk/nltk/tag/crf.py___init__,"def __init__(self, feature_func=None, verbose=False, training_opt={}):
""""""
Initialize the CRFSuite tagger
:param feature_func: The function that extracts features for each token of a sentence. This function should take
2 parameters: tokens and index which extract features at index position from tokens list. See the build in
_get_features function for more detail.
:param verbose: output the debugging messages during training.
:type verbose: boolean
:param training_opt: python-crfsuite training options
:type training_opt: dictionary
Set of possible training options (using LBFGS training algorithm).
:'feature.minfreq': The minimum frequency of features.
:'feature.possible_states': Force to generate possible state features.
:'feature.possible_transitions': Force to generate possible transition features.
:'c1': Coefficient for L1 regularization.
:'c2': Coefficient for L2 regularization.
:'max_iterations': The maximum number of iterations for L-BFGS optimization.
:'num_memories': The number of limited memories for approximating the inverse hessian matrix.
:'epsilon': Epsilon for testing the convergence of the objective.
:'period': The duration of iterations to test the stopping criterion.
:'delta': The threshold for the stopping criterion
improvement of the log likelihood over the last ${period} iterations is no greater than this threshold.
:'linesearch': The line search algorithm used in L-BFGS updates:
- 'MoreThuente': More and Thuente's method,
- 'Backtracking': Backtracking method with regular Wolfe condition,
- 'StrongBacktracking': Backtracking method with strong Wolfe condition
:'max_linesearch':  The maximum number of trials for the line search algorithm.
""""""
self._model_file = """"
self._tagger = pycrfsuite.Tagger()
if feature_func is None:
self._feature_func = self._get_features
else:
self._feature_func = feature_func
self._verbose = verbose
self._training_options = training_opt
self._pattern = re.compile(r""\d"")
",[],0,[],/tag/crf.py___init__
3322,/home/amandapotts/git/nltk/nltk/tag/crf.py_set_model_file,"def set_model_file(self, model_file):
self._model_file = model_file
self._tagger.open(self._model_file)
",[],0,[],/tag/crf.py_set_model_file
3323,/home/amandapotts/git/nltk/nltk/tag/crf.py__get_features,"def _get_features(self, tokens, idx):
""""""
Extract basic features about this word including
- Current word
- is it capitalized?
- Does it have punctuation?
- Does it have a number?
- Suffixes up to length 3
Note that : we might include feature over previous word, next word etc.
:return: a list which contains the features
:rtype: list(str)
""""""
token = tokens[idx]
feature_list = []
if not token:
return feature_list
if token[0].isupper():
feature_list.append(""CAPITALIZATION"")
if re.search(self._pattern, token) is not None:
feature_list.append(""HAS_NUM"")
punc_cat = {""Pc"", ""Pd"", ""Ps"", ""Pe"", ""Pi"", ""Pf"", ""Po""}
if all(unicodedata.category(x) in punc_cat for x in token):
feature_list.append(""PUNCTUATION"")
if len(token) > 1:
feature_list.append(""SUF_"" + token[-1:])
if len(token) > 2:
feature_list.append(""SUF_"" + token[-2:])
if len(token) > 3:
feature_list.append(""SUF_"" + token[-3:])
feature_list.append(""WORD_"" + token)
return feature_list
",[],0,[],/tag/crf.py__get_features
3324,/home/amandapotts/git/nltk/nltk/tag/crf.py_tag_sents,"def tag_sents(self, sents):
""""""
Tag a list of sentences. NB before using this function, user should specify the mode_file either by
- Train a new model using ``train`` function
- Use the pre-trained model which is set via ``set_model_file`` function
:params sentences: list of sentences needed to tag.
:type sentences: list(list(str))
:return: list of tagged sentences.
:rtype: list(list(tuple(str,str)))
""""""
if self._model_file == """":
raise Exception(
"" No model file is found !! Please use train or set_model_file function""
)
result = []
for tokens in sents:
features = [self._feature_func(tokens, i) for i in range(len(tokens))]
labels = self._tagger.tag(features)
if len(labels) != len(tokens):
raise Exception("" Predicted Length Not Matched, Expect Errors !"")
tagged_sent = list(zip(tokens, labels))
result.append(tagged_sent)
return result
",[],0,[],/tag/crf.py_tag_sents
3325,/home/amandapotts/git/nltk/nltk/tag/crf.py_train,"def train(self, train_data, model_file):
""""""
Train the CRF tagger using CRFSuite
:params train_data : is the list of annotated sentences.
:type train_data : list (list(tuple(str,str)))
:params model_file : the model will be saved to this file.
""""""
trainer = pycrfsuite.Trainer(verbose=self._verbose)
trainer.set_params(self._training_options)
for sent in train_data:
tokens, labels = zip(*sent)
features = [self._feature_func(tokens, i) for i in range(len(tokens))]
trainer.append(features, labels)
trainer.train(model_file)
self.set_model_file(model_file)
",[],0,[],/tag/crf.py_train
3326,/home/amandapotts/git/nltk/nltk/tag/crf.py_tag,"def tag(self, tokens):
""""""
Tag a sentence using Python CRFSuite Tagger. NB before using this function, user should specify the mode_file either by
- Train a new model using ``train`` function
- Use the pre-trained model which is set via ``set_model_file`` function
:params tokens: list of tokens needed to tag.
:type tokens: list(str)
:return: list of tagged tokens.
:rtype: list(tuple(str,str))
""""""
return self.tag_sents([tokens])[0]
",[],0,[],/tag/crf.py_tag
3327,/home/amandapotts/git/nltk/nltk/tag/sequential.py___init__,"def __init__(self, backoff=None):
if backoff is None:
self._taggers = [self]
else:
self._taggers = [self] + backoff._taggers
",[],0,[],/tag/sequential.py___init__
3328,/home/amandapotts/git/nltk/nltk/tag/sequential.py_backoff,"def backoff(self):
""""""The backoff tagger for this tagger.""""""
return self._taggers[1] if len(self._taggers) > 1 else None
",[],0,[],/tag/sequential.py_backoff
3329,/home/amandapotts/git/nltk/nltk/tag/sequential.py_tag,"def tag(self, tokens):
tags = []
for i in range(len(tokens)):
tags.append(self.tag_one(tokens, i, tags))
return list(zip(tokens, tags))
",[],0,[],/tag/sequential.py_tag
3330,/home/amandapotts/git/nltk/nltk/tag/sequential.py_tag_one,"def tag_one(self, tokens, index, history):
""""""
Determine an appropriate tag for the specified token, and
return that tag.  If this tagger is unable to determine a tag
for the specified token, then its backoff tagger is consulted.
:rtype: str
:type tokens: list
:param tokens: The list of words that are being tagged.
:type index: int
:param index: The index of the word whose tag should be
returned.
:type history: list(str)
:param history: A list of the tags for all words before *index*.
""""""
tag = None
for tagger in self._taggers:
tag = tagger.choose_tag(tokens, index, history)
if tag is not None:
break
return tag
",[],0,[],/tag/sequential.py_tag_one
3331,/home/amandapotts/git/nltk/nltk/tag/sequential.py_choose_tag,"def choose_tag(self, tokens, index, history):
""""""
Decide which tag should be used for the specified token, and
return that tag.  If this tagger is unable to determine a tag
for the specified token, return None -- do not consult
the backoff tagger.  This method should be overridden by
subclasses of SequentialBackoffTagger.
:rtype: str
:type tokens: list
:param tokens: The list of words that are being tagged.
:type index: int
:param index: The index of the word whose tag should be
returned.
:type history: list(str)
:param history: A list of the tags for all words before *index*.
""""""
",[],0,[],/tag/sequential.py_choose_tag
3332,/home/amandapotts/git/nltk/nltk/tag/sequential.py___init__,"def __init__(self, context_to_tag, backoff=None):
""""""
:param context_to_tag: A dictionary mapping contexts to tags.
:param backoff: The backoff tagger that should be used for this tagger.
""""""
super().__init__(backoff)
self._context_to_tag = context_to_tag if context_to_tag else {}
",[],0,[],/tag/sequential.py___init__
3333,/home/amandapotts/git/nltk/nltk/tag/sequential.py_context,"def context(self, tokens, index, history):
""""""
:return: the context that should be used to look up the tag
for the specified token
should not be handled by this tagger.
:rtype: (hashable)
""""""
",[],0,[],/tag/sequential.py_context
3334,/home/amandapotts/git/nltk/nltk/tag/sequential.py_choose_tag,"def choose_tag(self, tokens, index, history):
context = self.context(tokens, index, history)
return self._context_to_tag.get(context)
",[],0,[],/tag/sequential.py_choose_tag
3335,/home/amandapotts/git/nltk/nltk/tag/sequential.py_size,"def size(self):
""""""
:return: The number of entries in the table used by this
tagger to map from contexts to tags.
""""""
return len(self._context_to_tag)
",[],0,[],/tag/sequential.py_size
3336,/home/amandapotts/git/nltk/nltk/tag/sequential.py___repr__,"def __repr__(self):
return f""<{self.__class__.__name__}: size={self.size()}>""
",[],0,[],/tag/sequential.py___repr__
3337,/home/amandapotts/git/nltk/nltk/tag/sequential.py__train,"def _train(self, tagged_corpus, cutoff=0, verbose=False):
""""""
Initialize this ContextTagger's ``_context_to_tag`` table
based on the given training data.  In particular, for each
context ``c`` in the training data, set
``_context_to_tag[c]`` to the most frequent tag for that
context.  However, exclude any contexts that are already
tagged perfectly by the backoff tagger(s).
The old value of ``self._context_to_tag`` (if any) is discarded.
:param tagged_corpus: A tagged corpus.  Each item should be
a list of (word, tag tuples.
:param cutoff: If the most likely tag for a context occurs
fewer than cutoff times, then exclude it from the
context-to-tag table for the new tagger.
""""""
token_count = hit_count = 0
useful_contexts = set()
fd = ConditionalFreqDist()
for sentence in tagged_corpus:
tokens, tags = zip(*sentence)
for index, (token, tag) in enumerate(sentence):
token_count += 1
context = self.context(tokens, index, tags[:index])
if context is None:
continue
fd[context][tag] += 1
if self.backoff is None or tag != self.backoff.tag_one(
tokens, index, tags[:index]
):
useful_contexts.add(context)
for context in useful_contexts:
best_tag = fd[context].max()
hits = fd[context][best_tag]
if hits > cutoff:
self._context_to_tag[context] = best_tag
hit_count += hits
if verbose:
size = len(self._context_to_tag)
backoff = 100 - (hit_count * 100.0) / token_count
pruning = 100 - (size * 100.0) / len(fd.conditions())
print(""[Trained Unigram tagger:"", end="" "")
print(
""size={}, backoff={:.2f}%, pruning={:.2f}%]"".format(
size, backoff, pruning
)
)
",[],0,[],/tag/sequential.py__train
3338,/home/amandapotts/git/nltk/nltk/tag/sequential.py___init__,"def __init__(self, tag):
self._tag = tag
super().__init__(None)
",[],0,[],/tag/sequential.py___init__
3339,/home/amandapotts/git/nltk/nltk/tag/sequential.py_encode_json_obj,"def encode_json_obj(self):
return self._tag
",[],0,[],/tag/sequential.py_encode_json_obj
3340,/home/amandapotts/git/nltk/nltk/tag/sequential.py_decode_json_obj,"def decode_json_obj(cls, obj):
tag = obj
return cls(tag)
",[],0,[],/tag/sequential.py_decode_json_obj
3341,/home/amandapotts/git/nltk/nltk/tag/sequential.py_choose_tag,"def choose_tag(self, tokens, index, history):
return self._tag  # ignore token and history
",[],0,[],/tag/sequential.py_choose_tag
3342,/home/amandapotts/git/nltk/nltk/tag/sequential.py___repr__,"def __repr__(self):
return f""<DefaultTagger: tag={self._tag}>""
",[],0,[],/tag/sequential.py___repr__
3343,/home/amandapotts/git/nltk/nltk/tag/sequential.py___init__,"def __init__(
self, n, train=None, model=None, backoff=None, cutoff=0, verbose=False
",[],0,[],/tag/sequential.py___init__
3344,/home/amandapotts/git/nltk/nltk/tag/sequential.py_encode_json_obj,"def encode_json_obj(self):
_context_to_tag = {repr(k): v for k, v in self._context_to_tag.items()}
if ""NgramTagger"" in self.__class__.__name__:
return self._n, _context_to_tag, self.backoff
else:
return _context_to_tag, self.backoff
",[],0,[],/tag/sequential.py_encode_json_obj
3345,/home/amandapotts/git/nltk/nltk/tag/sequential.py_decode_json_obj,"def decode_json_obj(cls, obj):
try:
_n, _context_to_tag, backoff = obj
except ValueError:
_context_to_tag, backoff = obj
if not _context_to_tag:
return backoff
_context_to_tag = {ast.literal_eval(k): v for k, v in _context_to_tag.items()}
if ""NgramTagger"" in cls.__name__:
return cls(_n, model=_context_to_tag, backoff=backoff)
else:
return cls(model=_context_to_tag, backoff=backoff)
",[],0,[],/tag/sequential.py_decode_json_obj
3346,/home/amandapotts/git/nltk/nltk/tag/sequential.py_context,"def context(self, tokens, index, history):
tag_context = tuple(history[max(0, index - self._n + 1) : index])
return tag_context, tokens[index]
",[],0,[],/tag/sequential.py_context
3347,/home/amandapotts/git/nltk/nltk/tag/sequential.py___init__,"def __init__(self, train=None, model=None, backoff=None, cutoff=0, verbose=False):
super().__init__(1, train, model, backoff, cutoff, verbose)
",[],0,[],/tag/sequential.py___init__
3348,/home/amandapotts/git/nltk/nltk/tag/sequential.py_context,"def context(self, tokens, index, history):
return tokens[index]
",[],0,[],/tag/sequential.py_context
3349,/home/amandapotts/git/nltk/nltk/tag/sequential.py___init__,"def __init__(self, train=None, model=None, backoff=None, cutoff=0, verbose=False):
super().__init__(2, train, model, backoff, cutoff, verbose)
",[],0,[],/tag/sequential.py___init__
3350,/home/amandapotts/git/nltk/nltk/tag/sequential.py___init__,"def __init__(self, train=None, model=None, backoff=None, cutoff=0, verbose=False):
super().__init__(3, train, model, backoff, cutoff, verbose)
",[],0,[],/tag/sequential.py___init__
3351,/home/amandapotts/git/nltk/nltk/tag/sequential.py___init__,"def __init__(
self,
train=None,
model=None,
affix_length=-3,
min_stem_length=2,
backoff=None,
cutoff=0,
verbose=False,
",[],0,[],/tag/sequential.py___init__
3352,/home/amandapotts/git/nltk/nltk/tag/sequential.py_encode_json_obj,"def encode_json_obj(self):
return (
self._affix_length,
self._min_word_length,
self._context_to_tag,
self.backoff,
)
",[],0,[],/tag/sequential.py_encode_json_obj
3353,/home/amandapotts/git/nltk/nltk/tag/sequential.py_decode_json_obj,"def decode_json_obj(cls, obj):
_affix_length, _min_word_length, _context_to_tag, backoff = obj
return cls(
affix_length=_affix_length,
min_stem_length=_min_word_length - abs(_affix_length),
model=_context_to_tag,
backoff=backoff,
)
",[],0,[],/tag/sequential.py_decode_json_obj
3354,/home/amandapotts/git/nltk/nltk/tag/sequential.py_context,"def context(self, tokens, index, history):
token = tokens[index]
if len(token) < self._min_word_length:
return None
elif self._affix_length > 0:
return token[: self._affix_length]
else:
return token[self._affix_length :]
",[],0,[],/tag/sequential.py_context
3355,/home/amandapotts/git/nltk/nltk/tag/sequential.py___init__,"def __init__(
self, regexps: List[Tuple[str, str]], backoff: Optional[TaggerI] = None
",[],0,[],/tag/sequential.py___init__
3356,/home/amandapotts/git/nltk/nltk/tag/sequential.py_encode_json_obj,"def encode_json_obj(self):
return [(regexp.pattern, tag) for regexp, tag in self._regexps], self.backoff
",[],0,[],/tag/sequential.py_encode_json_obj
3357,/home/amandapotts/git/nltk/nltk/tag/sequential.py_decode_json_obj,"def decode_json_obj(cls, obj):
regexps, backoff = obj
return cls(regexps, backoff)
",[],0,[],/tag/sequential.py_decode_json_obj
3358,/home/amandapotts/git/nltk/nltk/tag/sequential.py_choose_tag,"def choose_tag(self, tokens, index, history):
for regexp, tag in self._regexps:
if re.match(regexp, tokens[index]):
return tag
return None
",[],0,[],/tag/sequential.py_choose_tag
3359,/home/amandapotts/git/nltk/nltk/tag/sequential.py___repr__,"def __repr__(self):
return f""<Regexp Tagger: size={len(self._regexps)}>""
",[],0,[],/tag/sequential.py___repr__
3360,/home/amandapotts/git/nltk/nltk/tag/sequential.py___init__,"def __init__(
self,
feature_detector=None,
train=None,
classifier_builder=NaiveBayesClassifier.train,
classifier=None,
backoff=None,
cutoff_prob=None,
verbose=False,
",[],0,[],/tag/sequential.py___init__
3361,/home/amandapotts/git/nltk/nltk/tag/sequential.py_choose_tag,"def choose_tag(self, tokens, index, history):
featureset = self.feature_detector(tokens, index, history)
if self._cutoff_prob is None:
return self._classifier.classify(featureset)
pdist = self._classifier.prob_classify(featureset)
tag = pdist.max()
return tag if pdist.prob(tag) >= self._cutoff_prob else None
",[],0,[],/tag/sequential.py_choose_tag
3362,/home/amandapotts/git/nltk/nltk/tag/sequential.py__train,"def _train(self, tagged_corpus, classifier_builder, verbose):
""""""
Build a new classifier, based on the given training data
""""""
classifier_corpus = []
if verbose:
print(""Constructing training corpus for classifier."")
for sentence in tagged_corpus:
history = []
untagged_sentence, tags = zip(*sentence)
for index in range(len(sentence)):
featureset = self.feature_detector(untagged_sentence, index, history)
classifier_corpus.append((featureset, tags[index]))
history.append(tags[index])
if verbose:
print(f""Training classifier ({len(classifier_corpus)} instances)"")
self._classifier = classifier_builder(classifier_corpus)
",[],0,[],/tag/sequential.py__train
3363,/home/amandapotts/git/nltk/nltk/tag/sequential.py___repr__,"def __repr__(self):
return f""<ClassifierBasedTagger: {self._classifier}>""
",[],0,[],/tag/sequential.py___repr__
3364,/home/amandapotts/git/nltk/nltk/tag/sequential.py_feature_detector,"def feature_detector(self, tokens, index, history):
""""""
Return the feature detector that this tagger uses to generate
featuresets for its classifier.  The feature detector is a
function with the signature::
feature_detector(tokens, index, history) -> featureset
See ``classifier()``
""""""
return self._feature_detector(tokens, index, history)
",[],0,[],/tag/sequential.py_feature_detector
3365,/home/amandapotts/git/nltk/nltk/tag/sequential.py_classifier,"def classifier(self):
""""""
Return the classifier that this tagger uses to choose a tag
for each word in a sentence.  The input for this classifier is
generated using this tagger's feature detector.
See ``feature_detector()``
""""""
return self._classifier
",[],0,[],/tag/sequential.py_classifier
3366,/home/amandapotts/git/nltk/nltk/tag/sequential.py_feature_detector,"def feature_detector(self, tokens, index, history):
word = tokens[index]
if index == 0:
prevword = prevprevword = None
prevtag = prevprevtag = None
elif index == 1:
prevword = tokens[index - 1].lower()
prevprevword = None
prevtag = history[index - 1]
prevprevtag = None
else:
prevword = tokens[index - 1].lower()
prevprevword = tokens[index - 2].lower()
prevtag = history[index - 1]
prevprevtag = history[index - 2]
if re.match(r""[0-9]+(\.[0-9]*)?|[0-9]*\.[0-9]+$"", word):
shape = ""number""
elif re.match(r""\W+$"", word):
shape = ""punct""
elif re.match(""[A-Z][a-z]+$"", word):
shape = ""upcase""
elif re.match(""[a-z]+$"", word):
shape = ""downcase""
elif re.match(r""\w+$"", word):
shape = ""mixedcase""
else:
shape = ""other""
features = {
""prevtag"": prevtag,
""prevprevtag"": prevprevtag,
""word"": word,
""word.lower"": word.lower(),
""suffix3"": word.lower()[-3:],
""suffix2"": word.lower()[-2:],
""suffix1"": word.lower()[-1:],
""prevprevword"": prevprevword,
""prevword"": prevword,
""prevtag+word"": f""{prevtag}+{word.lower()}"",
""prevprevtag+word"": f""{prevprevtag}+{word.lower()}"",
""prevword+word"": f""{prevword}+{word.lower()}"",
""shape"": shape,
}
return features
",[],0,[],/tag/sequential.py_feature_detector
3367,/home/amandapotts/git/nltk/nltk/draw/util.py___init__,"def __init__(self, canvas, parent=None, **attribs):
""""""
Create a new canvas widget.  This constructor should only be
called by subclass constructors
""after"" the subclass has constructed all graphical canvas
objects and registered all child widgets.
:param canvas: This canvas widget's canvas.
:type canvas: Tkinter.Canvas
:param parent: This canvas widget's hierarchical parent.
:type parent: CanvasWidget
:param attribs: The new canvas widget's attributes.
""""""
if self.__class__ == CanvasWidget:
raise TypeError(""CanvasWidget is an abstract base class"")
if not isinstance(canvas, Canvas):
raise TypeError(""Expected a canvas!"")
self.__canvas = canvas
self.__parent = parent
if not hasattr(self, ""_CanvasWidget__children""):
self.__children = []
self.__hidden = 0
self.__updating = 0
self.__press = None
self.__drag_x = self.__drag_y = 0
self.__callbacks = {}
self.__draggable = 0
for attr, value in list(attribs.items()):
self[attr] = value
self._manage()
for tag in self._tags():
self.__canvas.tag_bind(tag, ""<ButtonPress-1>"", self.__press_cb)
self.__canvas.tag_bind(tag, ""<ButtonPress-2>"", self.__press_cb)
self.__canvas.tag_bind(tag, ""<ButtonPress-3>"", self.__press_cb)
",[],0,[],/draw/util.py___init__
3368,/home/amandapotts/git/nltk/nltk/draw/util.py_bbox,"def bbox(self):
""""""
:return: A bounding box for this ``CanvasWidget``. The bounding
box is a tuple of four coordinates, *(xmin, ymin, xmax, ymax)*,
for a rectangle which encloses all of the canvas
widget's graphical elements.  Bounding box coordinates are
specified with respect to the coordinate space of the ``Canvas``.
:rtype: tuple(int, int, int, int)
""""""
if self.__hidden:
return (0, 0, 0, 0)
if len(self.tags()) == 0:
raise ValueError(""No tags"")
return self.__canvas.bbox(*self.tags())
",[],0,[],/draw/util.py_bbox
3369,/home/amandapotts/git/nltk/nltk/draw/util.py_width,"def width(self):
""""""
:return: The width of this canvas widget's bounding box, in
its ``Canvas``'s coordinate space.
:rtype: int
""""""
if len(self.tags()) == 0:
raise ValueError(""No tags"")
bbox = self.__canvas.bbox(*self.tags())
return bbox[2] - bbox[0]
",[],0,[],/draw/util.py_width
3370,/home/amandapotts/git/nltk/nltk/draw/util.py_height,"def height(self):
""""""
:return: The height of this canvas widget's bounding box, in
its ``Canvas``'s coordinate space.
:rtype: int
""""""
if len(self.tags()) == 0:
raise ValueError(""No tags"")
bbox = self.__canvas.bbox(*self.tags())
return bbox[3] - bbox[1]
",[],0,[],/draw/util.py_height
3371,/home/amandapotts/git/nltk/nltk/draw/util.py_parent,"def parent(self):
""""""
:return: The hierarchical parent of this canvas widget.
``self`` is considered a subpart of its parent for
purposes of user interaction.
:rtype: CanvasWidget or None
""""""
return self.__parent
",[],0,[],/draw/util.py_parent
3372,/home/amandapotts/git/nltk/nltk/draw/util.py_child_widgets,"def child_widgets(self):
""""""
:return: A list of the hierarchical children of this canvas
widget.  These children are considered part of ``self``
for purposes of user interaction.
:rtype: list of CanvasWidget
""""""
return self.__children
",[],0,[],/draw/util.py_child_widgets
3373,/home/amandapotts/git/nltk/nltk/draw/util.py_canvas,"def canvas(self):
""""""
:return: The canvas that this canvas widget is bound to.
:rtype: Tkinter.Canvas
""""""
return self.__canvas
",[],0,[],/draw/util.py_canvas
3374,/home/amandapotts/git/nltk/nltk/draw/util.py_move,"def move(self, dx, dy):
""""""
Move this canvas widget by a given distance.  In particular,
shift the canvas widget right by ``dx`` pixels, and down by
``dy`` pixels.  Both ``dx`` and ``dy`` may be negative, resulting
in leftward or upward movement.
:type dx: int
:param dx: The number of pixels to move this canvas widget
rightwards.
:type dy: int
:param dy: The number of pixels to move this canvas widget
downwards.
:rtype: None
""""""
if dx == dy == 0:
return
for tag in self.tags():
self.__canvas.move(tag, dx, dy)
if self.__parent:
self.__parent.update(self)
",[],0,[],/draw/util.py_move
3375,/home/amandapotts/git/nltk/nltk/draw/util.py_moveto,"def moveto(self, x, y, anchor=""NW""):
""""""
Move this canvas widget to the given location.  In particular,
shift the canvas widget such that the corner or side of the
bounding box specified by ``anchor`` is at location (``x``,
``y``).
:param x,y: The location that the canvas widget should be moved
to.
:param anchor: The corner or side of the canvas widget that
should be moved to the specified location.  ``'N'``
specifies the top center
corner
""""""
x1, y1, x2, y2 = self.bbox()
if anchor == ""NW"":
self.move(x - x1, y - y1)
if anchor == ""N"":
self.move(x - x1 / 2 - x2 / 2, y - y1)
if anchor == ""NE"":
self.move(x - x2, y - y1)
if anchor == ""E"":
self.move(x - x2, y - y1 / 2 - y2 / 2)
if anchor == ""SE"":
self.move(x - x2, y - y2)
if anchor == ""S"":
self.move(x - x1 / 2 - x2 / 2, y - y2)
if anchor == ""SW"":
self.move(x - x1, y - y2)
if anchor == ""W"":
self.move(x - x1, y - y1 / 2 - y2 / 2)
",[],0,[],/draw/util.py_moveto
3376,/home/amandapotts/git/nltk/nltk/draw/util.py_destroy,"def destroy(self):
""""""
Remove this ``CanvasWidget`` from its ``Canvas``.  After a
``CanvasWidget`` has been destroyed, it should not be accessed.
Note that you only need to destroy a top-level
``CanvasWidget``
automatically.  If you destroy a non-top-level
``CanvasWidget``, then the entire top-level widget will be
destroyed.
:raise ValueError: if this ``CanvasWidget`` has a parent.
:rtype: None
""""""
if self.__parent is not None:
self.__parent.destroy()
return
for tag in self.tags():
self.__canvas.tag_unbind(tag, ""<ButtonPress-1>"")
self.__canvas.tag_unbind(tag, ""<ButtonPress-2>"")
self.__canvas.tag_unbind(tag, ""<ButtonPress-3>"")
self.__canvas.delete(*self.tags())
self.__canvas = None
",[],0,[],/draw/util.py_destroy
3377,/home/amandapotts/git/nltk/nltk/draw/util.py_update,"def update(self, child):
""""""
Update the graphical display of this canvas widget, and all of
its ancestors, in response to a change in one of this canvas
widget's children.
:param child: The child widget that changed.
:type child: CanvasWidget
""""""
if self.__hidden or child.__hidden:
return
if self.__updating:
return
self.__updating = 1
self._update(child)
if self.__parent:
self.__parent.update(self)
self.__updating = 0
",[],0,[],/draw/util.py_update
3378,/home/amandapotts/git/nltk/nltk/draw/util.py_manage,"def manage(self):
""""""
Arrange this canvas widget and all of its descendants.
:rtype: None
""""""
if self.__hidden:
return
for child in self.__children:
child.manage()
self._manage()
",[],0,[],/draw/util.py_manage
3379,/home/amandapotts/git/nltk/nltk/draw/util.py_tags,"def tags(self):
""""""
:return: a list of the canvas tags for all graphical
elements managed by this canvas widget, including
graphical elements managed by its child widgets.
:rtype: list of int
""""""
if self.__canvas is None:
raise ValueError(""Attempt to access a destroyed canvas widget"")
tags = []
tags += self._tags()
for child in self.__children:
tags += child.tags()
return tags
",[],0,[],/draw/util.py_tags
3380,/home/amandapotts/git/nltk/nltk/draw/util.py___setitem__,"def __setitem__(self, attr, value):
""""""
Set the value of the attribute ``attr`` to ``value``.  See the
class documentation for a list of attributes supported by this
canvas widget.
:rtype: None
""""""
if attr == ""draggable"":
self.__draggable = value
else:
raise ValueError(""Unknown attribute %r"" % attr)
",[],0,[],/draw/util.py___setitem__
3381,/home/amandapotts/git/nltk/nltk/draw/util.py___getitem__,"def __getitem__(self, attr):
""""""
:return: the value of the attribute ``attr``.  See the class
documentation for a list of attributes supported by this
canvas widget.
:rtype: (any)
""""""
if attr == ""draggable"":
return self.__draggable
else:
raise ValueError(""Unknown attribute %r"" % attr)
",[],0,[],/draw/util.py___getitem__
3382,/home/amandapotts/git/nltk/nltk/draw/util.py___repr__,"def __repr__(self):
""""""
:return: a string representation of this canvas widget.
:rtype: str
""""""
return ""<%s>"" % self.__class__.__name__
",[],0,[],/draw/util.py___repr__
3383,/home/amandapotts/git/nltk/nltk/draw/util.py_hide,"def hide(self):
""""""
Temporarily hide this canvas widget.
:rtype: None
""""""
self.__hidden = 1
for tag in self.tags():
self.__canvas.itemconfig(tag, state=""hidden"")
",[],0,[],/draw/util.py_hide
3384,/home/amandapotts/git/nltk/nltk/draw/util.py_show,"def show(self):
""""""
Show a hidden canvas widget.
:rtype: None
""""""
self.__hidden = 0
for tag in self.tags():
self.__canvas.itemconfig(tag, state=""normal"")
",[],0,[],/draw/util.py_show
3385,/home/amandapotts/git/nltk/nltk/draw/util.py_hidden,"def hidden(self):
""""""
:return: True if this canvas widget is hidden.
:rtype: bool
""""""
return self.__hidden
",[],0,[],/draw/util.py_hidden
3386,/home/amandapotts/git/nltk/nltk/draw/util.py_bind_click,"def bind_click(self, callback, button=1):
""""""
Register a new callback that will be called whenever this
``CanvasWidget`` is clicked on.
:type callback: function
:param callback: The callback function that will be called
whenever this ``CanvasWidget`` is clicked.  This function
will be called with this ``CanvasWidget`` as its argument.
:type button: int
:param button: Which button the user should use to click on
this ``CanvasWidget``.  Typically, this should be 1 (left
button), 3 (right button), or 2 (middle button).
""""""
self.__callbacks[button] = callback
",[],0,[],/draw/util.py_bind_click
3387,/home/amandapotts/git/nltk/nltk/draw/util.py_bind_drag,"def bind_drag(self, callback):
""""""
Register a new callback that will be called after this
``CanvasWidget`` is dragged.  This implicitly makes this
``CanvasWidget`` draggable.
:type callback: function
:param callback: The callback function that will be called
whenever this ``CanvasWidget`` is clicked.  This function
will be called with this ``CanvasWidget`` as its argument.
""""""
self.__draggable = 1
self.__callbacks[""drag""] = callback
",[],0,[],/draw/util.py_bind_drag
3388,/home/amandapotts/git/nltk/nltk/draw/util.py_unbind_click,"def unbind_click(self, button=1):
""""""
Remove a callback that was registered with ``bind_click``.
:type button: int
:param button: Which button the user should use to click on
this ``CanvasWidget``.  Typically, this should be 1 (left
button), 3 (right button), or 2 (middle button).
""""""
try:
del self.__callbacks[button]
except:
pass
",[],0,[],/draw/util.py_unbind_click
3389,/home/amandapotts/git/nltk/nltk/draw/util.py_unbind_drag,"def unbind_drag(self):
""""""
Remove a callback that was registered with ``bind_drag``.
""""""
try:
del self.__callbacks[""drag""]
except:
pass
",[],0,[],/draw/util.py_unbind_drag
3390,/home/amandapotts/git/nltk/nltk/draw/util.py___press_cb,"def __press_cb(self, event):
""""""
Handle a button-press event:
- record the button press event in ``self.__press``
- register a button-release callback.
- if this CanvasWidget or any of its ancestors are
draggable, then register the appropriate motion callback.
""""""
if (
self.__canvas.bind(""<ButtonRelease-1>"")
or self.__canvas.bind(""<ButtonRelease-2>"")
or self.__canvas.bind(""<ButtonRelease-3>"")
):
return
self.__canvas.unbind(""<Motion>"")
self.__press = event
if event.num == 1:
widget = self
while widget is not None:
if widget[""draggable""]:
widget.__start_drag(event)
break
widget = widget.parent()
self.__canvas.bind(""<ButtonRelease-%d>"" % event.num, self.__release_cb)
",[],0,[],/draw/util.py___press_cb
3391,/home/amandapotts/git/nltk/nltk/draw/util.py___start_drag,"def __start_drag(self, event):
""""""
Begin dragging this object:
- register a motion callback
- record the drag coordinates
""""""
self.__canvas.bind(""<Motion>"", self.__motion_cb)
self.__drag_x = event.x
self.__drag_y = event.y
",[],0,[],/draw/util.py___start_drag
3392,/home/amandapotts/git/nltk/nltk/draw/util.py___motion_cb,"def __motion_cb(self, event):
""""""
Handle a motion event:
- move this object to the new location
- record the new drag coordinates
""""""
self.move(event.x - self.__drag_x, event.y - self.__drag_y)
self.__drag_x = event.x
self.__drag_y = event.y
",[],0,[],/draw/util.py___motion_cb
3393,/home/amandapotts/git/nltk/nltk/draw/util.py___release_cb,"def __release_cb(self, event):
""""""
Handle a release callback:
- unregister motion & button release callbacks.
- decide whether they clicked, dragged, or cancelled
- call the appropriate handler.
""""""
self.__canvas.unbind(""<ButtonRelease-%d>"" % event.num)
self.__canvas.unbind(""<Motion>"")
if (
event.time - self.__press.time < 100
and abs(event.x - self.__press.x) + abs(event.y - self.__press.y) < 5
):
if self.__draggable and event.num == 1:
self.move(
self.__press.x - self.__drag_x, self.__press.y - self.__drag_y
)
self.__click(event.num)
elif event.num == 1:
self.__drag()
self.__press = None
",[],0,[],/draw/util.py___release_cb
3394,/home/amandapotts/git/nltk/nltk/draw/util.py___drag,"def __drag(self):
""""""
If this ``CanvasWidget`` has a drag callback, then call it
otherwise, find the closest ancestor with a drag callback, and
call it.  If no ancestors have a drag callback, do nothing.
""""""
if self.__draggable:
if ""drag"" in self.__callbacks:
cb = self.__callbacks[""drag""]
try:
cb(self)
except:
print(""Error in drag callback for %r"" % self)
elif self.__parent is not None:
self.__parent.__drag()
",[],0,[],/draw/util.py___drag
3395,/home/amandapotts/git/nltk/nltk/draw/util.py___click,"def __click(self, button):
""""""
If this ``CanvasWidget`` has a drag callback, then call it
otherwise, find the closest ancestor with a click callback, and
call it.  If no ancestors have a click callback, do nothing.
""""""
if button in self.__callbacks:
cb = self.__callbacks[button]
cb(self)
elif self.__parent is not None:
self.__parent.__click(button)
",[],0,[],/draw/util.py___click
3396,/home/amandapotts/git/nltk/nltk/draw/util.py__add_child_widget,"def _add_child_widget(self, child):
""""""
Register a hierarchical child widget.  The child will be
considered part of this canvas widget for purposes of user
interaction.  ``_add_child_widget`` has two direct effects:
- It sets ``child``'s parent to this canvas widget.
- It adds ``child`` to the list of canvas widgets returned by
the ``child_widgets`` member function.
:param child: The new child widget.  ``child`` must not already
have a parent.
:type child: CanvasWidget
""""""
if not hasattr(self, ""_CanvasWidget__children""):
self.__children = []
if child.__parent is not None:
raise ValueError(f""{child} already has a parent"")
child.__parent = self
self.__children.append(child)
",[],0,[],/draw/util.py__add_child_widget
3397,/home/amandapotts/git/nltk/nltk/draw/util.py__remove_child_widget,"def _remove_child_widget(self, child):
""""""
Remove a hierarchical child widget.  This child will no longer
be considered part of this canvas widget for purposes of user
interaction.  ``_add_child_widget`` has two direct effects:
- It sets ``child``'s parent to None.
- It removes ``child`` from the list of canvas widgets
returned by the ``child_widgets`` member function.
:param child: The child widget to remove.  ``child`` must be a
child of this canvas widget.
:type child: CanvasWidget
""""""
self.__children.remove(child)
child.__parent = None
",[],0,[],/draw/util.py__remove_child_widget
3398,/home/amandapotts/git/nltk/nltk/draw/util.py__tags,"def _tags(self):
""""""
:return: a list of canvas tags for all graphical elements
managed by this canvas widget, not including graphical
elements managed by its child widgets.
:rtype: list of int
""""""
",[],0,[],/draw/util.py__tags
3399,/home/amandapotts/git/nltk/nltk/draw/util.py__manage,"def _manage(self):
""""""
Arrange the child widgets of this canvas widget.  This method
is called when the canvas widget is initially created.  It is
also called if the user calls the ``manage`` method on this
canvas widget or any of its ancestors.
:rtype: None
""""""
",[],0,[],/draw/util.py__manage
3400,/home/amandapotts/git/nltk/nltk/draw/util.py__update,"def _update(self, child):
""""""
Update this canvas widget in response to a change in one of
its children.
:param child: The child that changed.
:type child: CanvasWidget
:rtype: None
""""""
",[],0,[],/draw/util.py__update
3401,/home/amandapotts/git/nltk/nltk/draw/util.py___init__,"def __init__(self, canvas, text, **attribs):
""""""
Create a new text widget.
:type canvas: Tkinter.Canvas
:param canvas: This canvas widget's canvas.
:type text: str
:param text: The string of text to display.
:param attribs: The new canvas widget's attributes.
""""""
self._text = text
self._tag = canvas.create_text(1, 1, text=text)
CanvasWidget.__init__(self, canvas, **attribs)
",[],0,[],/draw/util.py___init__
3402,/home/amandapotts/git/nltk/nltk/draw/util.py___setitem__,"def __setitem__(self, attr, value):
if attr in (""color"", ""font"", ""justify"", ""width""):
if attr == ""color"":
attr = ""fill""
self.canvas().itemconfig(self._tag, {attr: value})
else:
CanvasWidget.__setitem__(self, attr, value)
",[],0,[],/draw/util.py___setitem__
3403,/home/amandapotts/git/nltk/nltk/draw/util.py___getitem__,"def __getitem__(self, attr):
if attr == ""width"":
return int(self.canvas().itemcget(self._tag, attr))
elif attr in (""color"", ""font"", ""justify""):
if attr == ""color"":
attr = ""fill""
return self.canvas().itemcget(self._tag, attr)
else:
return CanvasWidget.__getitem__(self, attr)
",[],0,[],/draw/util.py___getitem__
3404,/home/amandapotts/git/nltk/nltk/draw/util.py__tags,"def _tags(self):
return [self._tag]
",[],0,[],/draw/util.py__tags
3405,/home/amandapotts/git/nltk/nltk/draw/util.py_text,"def text(self):
""""""
:return: The text displayed by this text widget.
:rtype: str
""""""
return self.canvas().itemcget(self._tag, ""TEXT"")
",[],0,[],/draw/util.py_text
3406,/home/amandapotts/git/nltk/nltk/draw/util.py_set_text,"def set_text(self, text):
""""""
Change the text that is displayed by this text widget.
:type text: str
:param text: The string of text to display.
:rtype: None
""""""
self.canvas().itemconfig(self._tag, text=text)
if self.parent() is not None:
self.parent().update(self)
",[],0,[],/draw/util.py_set_text
3407,/home/amandapotts/git/nltk/nltk/draw/util.py___repr__,"def __repr__(self):
return ""[Text: %r]"" % self._text
",[],0,[],/draw/util.py___repr__
3408,/home/amandapotts/git/nltk/nltk/draw/util.py___init__,"def __init__(self, canvas, symbol, **attribs):
""""""
Create a new symbol widget.
:type canvas: Tkinter.Canvas
:param canvas: This canvas widget's canvas.
:type symbol: str
:param symbol: The name of the symbol to display.
:param attribs: The new canvas widget's attributes.
""""""
attribs[""font""] = ""symbol""
TextWidget.__init__(self, canvas, """", **attribs)
self.set_symbol(symbol)
",[],0,[],/draw/util.py___init__
3409,/home/amandapotts/git/nltk/nltk/draw/util.py_symbol,"def symbol(self):
""""""
:return: the name of the symbol that is displayed by this
symbol widget.
:rtype: str
""""""
return self._symbol
",[],0,[],/draw/util.py_symbol
3410,/home/amandapotts/git/nltk/nltk/draw/util.py_set_symbol,"def set_symbol(self, symbol):
""""""
Change the symbol that is displayed by this symbol widget.
:type symbol: str
:param symbol: The name of the symbol to display.
""""""
if symbol not in SymbolWidget.SYMBOLS:
raise ValueError(""Unknown symbol: %s"" % symbol)
self._symbol = symbol
self.set_text(SymbolWidget.SYMBOLS[symbol])
",[],0,[],/draw/util.py_set_symbol
3411,/home/amandapotts/git/nltk/nltk/draw/util.py___repr__,"def __repr__(self):
return ""[Symbol: %r]"" % self._symbol
",[],0,[],/draw/util.py___repr__
3412,/home/amandapotts/git/nltk/nltk/draw/util.py_symbolsheet,"def symbolsheet(size=20):
""""""
Open a new Tkinter window that displays the entire alphabet
for the symbol font.  This is useful for constructing the
``SymbolWidget.SYMBOLS`` dictionary.
""""""
top = Tk()
",[],0,[],/draw/util.py_symbolsheet
3413,/home/amandapotts/git/nltk/nltk/draw/util.py_destroy,"def destroy(e, top=top):
top.destroy()
",[],0,[],/draw/util.py_destroy
3414,/home/amandapotts/git/nltk/nltk/draw/util.py___init__,"def __init__(self, canvas, child, **attribs):
""""""
Create a new container widget.  This constructor should only
be called by subclass constructors.
:type canvas: Tkinter.Canvas
:param canvas: This canvas widget's canvas.
:param child: The container's child widget.  ``child`` must not
have a parent.
:type child: CanvasWidget
:param attribs: The new canvas widget's attributes.
""""""
self._child = child
self._add_child_widget(child)
CanvasWidget.__init__(self, canvas, **attribs)
",[],0,[],/draw/util.py___init__
3415,/home/amandapotts/git/nltk/nltk/draw/util.py__manage,"def _manage(self):
self._update(self._child)
",[],0,[],/draw/util.py__manage
3416,/home/amandapotts/git/nltk/nltk/draw/util.py_child,"def child(self):
""""""
:return: The child widget contained by this container widget.
:rtype: CanvasWidget
""""""
return self._child
",[],0,[],/draw/util.py_child
3417,/home/amandapotts/git/nltk/nltk/draw/util.py_set_child,"def set_child(self, child):
""""""
Change the child widget contained by this container widget.
:param child: The new child widget.  ``child`` must not have a
parent.
:type child: CanvasWidget
:rtype: None
""""""
self._remove_child_widget(self._child)
self._add_child_widget(child)
self._child = child
self.update(child)
",[],0,[],/draw/util.py_set_child
3418,/home/amandapotts/git/nltk/nltk/draw/util.py___repr__,"def __repr__(self):
name = self.__class__.__name__
if name[-6:] == ""Widget"":
name = name[:-6]
return f""[{name}: {self._child!r}]""
",[],0,[],/draw/util.py___repr__
3419,/home/amandapotts/git/nltk/nltk/draw/util.py___init__,"def __init__(self, canvas, child, **attribs):
""""""
Create a new box widget.
:type canvas: Tkinter.Canvas
:param canvas: This canvas widget's canvas.
:param child: The child widget.  ``child`` must not have a
parent.
:type child: CanvasWidget
:param attribs: The new canvas widget's attributes.
""""""
self._child = child
self._margin = 1
self._box = canvas.create_rectangle(1, 1, 1, 1)
canvas.tag_lower(self._box)
AbstractContainerWidget.__init__(self, canvas, child, **attribs)
",[],0,[],/draw/util.py___init__
3420,/home/amandapotts/git/nltk/nltk/draw/util.py___setitem__,"def __setitem__(self, attr, value):
if attr == ""margin"":
self._margin = value
elif attr in (""outline"", ""fill"", ""width""):
self.canvas().itemconfig(self._box, {attr: value})
else:
CanvasWidget.__setitem__(self, attr, value)
",[],0,[],/draw/util.py___setitem__
3421,/home/amandapotts/git/nltk/nltk/draw/util.py___getitem__,"def __getitem__(self, attr):
if attr == ""margin"":
return self._margin
elif attr == ""width"":
return float(self.canvas().itemcget(self._box, attr))
elif attr in (""outline"", ""fill"", ""width""):
return self.canvas().itemcget(self._box, attr)
else:
return CanvasWidget.__getitem__(self, attr)
",[],0,[],/draw/util.py___getitem__
3422,/home/amandapotts/git/nltk/nltk/draw/util.py__update,"def _update(self, child):
(x1, y1, x2, y2) = child.bbox()
margin = self._margin + self[""width""] / 2
self.canvas().coords(
self._box, x1 - margin, y1 - margin, x2 + margin, y2 + margin
)
",[],0,[],/draw/util.py__update
3423,/home/amandapotts/git/nltk/nltk/draw/util.py__tags,"def _tags(self):
return [self._box]
",[],0,[],/draw/util.py__tags
3424,/home/amandapotts/git/nltk/nltk/draw/util.py___init__,"def __init__(self, canvas, child, **attribs):
""""""
Create a new oval widget.
:type canvas: Tkinter.Canvas
:param canvas: This canvas widget's canvas.
:param child: The child widget.  ``child`` must not have a
parent.
:type child: CanvasWidget
:param attribs: The new canvas widget's attributes.
""""""
self._child = child
self._margin = 1
self._oval = canvas.create_oval(1, 1, 1, 1)
self._circle = attribs.pop(""circle"", False)
self._double = attribs.pop(""double"", False)
if self._double:
self._oval2 = canvas.create_oval(1, 1, 1, 1)
else:
self._oval2 = None
canvas.tag_lower(self._oval)
AbstractContainerWidget.__init__(self, canvas, child, **attribs)
",[],0,[],/draw/util.py___init__
3425,/home/amandapotts/git/nltk/nltk/draw/util.py___setitem__,"def __setitem__(self, attr, value):
c = self.canvas()
if attr == ""margin"":
self._margin = value
elif attr == ""double"":
if value == True and self._oval2 is None:
x1, y1, x2, y2 = c.bbox(self._oval)
w = self[""width""] * 2
self._oval2 = c.create_oval(
x1 - w,
y1 - w,
x2 + w,
y2 + w,
outline=c.itemcget(self._oval, ""outline""),
width=c.itemcget(self._oval, ""width""),
)
c.tag_lower(self._oval2)
if value == False and self._oval2 is not None:
c.delete(self._oval2)
self._oval2 = None
elif attr in (""outline"", ""fill"", ""width""):
c.itemconfig(self._oval, {attr: value})
if self._oval2 is not None and attr != ""fill"":
c.itemconfig(self._oval2, {attr: value})
if self._oval2 is not None and attr != ""fill"":
self.canvas().itemconfig(self._oval2, {attr: value})
else:
CanvasWidget.__setitem__(self, attr, value)
",[],0,[],/draw/util.py___setitem__
3426,/home/amandapotts/git/nltk/nltk/draw/util.py___getitem__,"def __getitem__(self, attr):
if attr == ""margin"":
return self._margin
elif attr == ""double"":
return self._double is not None
elif attr == ""width"":
return float(self.canvas().itemcget(self._oval, attr))
elif attr in (""outline"", ""fill"", ""width""):
return self.canvas().itemcget(self._oval, attr)
else:
return CanvasWidget.__getitem__(self, attr)
",[],0,[],/draw/util.py___getitem__
3427,/home/amandapotts/git/nltk/nltk/draw/util.py__update,"def _update(self, child):
R = OvalWidget.RATIO
(x1, y1, x2, y2) = child.bbox()
margin = self._margin
if self._circle:
dx, dy = abs(x1 - x2), abs(y1 - y2)
if dx > dy:
y = (y1 + y2) / 2
y1, y2 = y - dx / 2, y + dx / 2
elif dy > dx:
x = (x1 + x2) / 2
x1, x2 = x - dy / 2, x + dy / 2
left = int((x1 * (1 + R) + x2 * (1 - R)) / 2)
right = left + int((x2 - x1) * R)
top = int((y1 * (1 + R) + y2 * (1 - R)) / 2)
bot = top + int((y2 - y1) * R)
self.canvas().coords(
self._oval, left - margin, top - margin, right + margin, bot + margin
)
if self._oval2 is not None:
self.canvas().coords(
self._oval2,
left - margin + 2,
top - margin + 2,
right + margin - 2,
bot + margin - 2,
)
",[],0,[],/draw/util.py__update
3428,/home/amandapotts/git/nltk/nltk/draw/util.py__tags,"def _tags(self):
if self._oval2 is None:
return [self._oval]
else:
return [self._oval, self._oval2]
",[],0,[],/draw/util.py__tags
3429,/home/amandapotts/git/nltk/nltk/draw/util.py___init__,"def __init__(self, canvas, child, **attribs):
""""""
Create a new parenthasis widget.
:type canvas: Tkinter.Canvas
:param canvas: This canvas widget's canvas.
:param child: The child widget.  ``child`` must not have a
parent.
:type child: CanvasWidget
:param attribs: The new canvas widget's attributes.
""""""
self._child = child
self._oparen = canvas.create_arc(1, 1, 1, 1, style=""arc"", start=90, extent=180)
self._cparen = canvas.create_arc(1, 1, 1, 1, style=""arc"", start=-90, extent=180)
AbstractContainerWidget.__init__(self, canvas, child, **attribs)
",[],0,[],/draw/util.py___init__
3430,/home/amandapotts/git/nltk/nltk/draw/util.py___setitem__,"def __setitem__(self, attr, value):
if attr == ""color"":
self.canvas().itemconfig(self._oparen, outline=value)
self.canvas().itemconfig(self._cparen, outline=value)
elif attr == ""width"":
self.canvas().itemconfig(self._oparen, width=value)
self.canvas().itemconfig(self._cparen, width=value)
else:
CanvasWidget.__setitem__(self, attr, value)
",[],0,[],/draw/util.py___setitem__
3431,/home/amandapotts/git/nltk/nltk/draw/util.py___getitem__,"def __getitem__(self, attr):
if attr == ""color"":
return self.canvas().itemcget(self._oparen, ""outline"")
elif attr == ""width"":
return self.canvas().itemcget(self._oparen, ""width"")
else:
return CanvasWidget.__getitem__(self, attr)
",[],0,[],/draw/util.py___getitem__
3432,/home/amandapotts/git/nltk/nltk/draw/util.py__update,"def _update(self, child):
(x1, y1, x2, y2) = child.bbox()
width = max((y2 - y1) / 6, 4)
self.canvas().coords(self._oparen, x1 - width, y1, x1 + width, y2)
self.canvas().coords(self._cparen, x2 - width, y1, x2 + width, y2)
",[],0,[],/draw/util.py__update
3433,/home/amandapotts/git/nltk/nltk/draw/util.py__tags,"def _tags(self):
return [self._oparen, self._cparen]
",[],0,[],/draw/util.py__tags
3434,/home/amandapotts/git/nltk/nltk/draw/util.py___init__,"def __init__(self, canvas, child, **attribs):
""""""
Create a new bracket widget.
:type canvas: Tkinter.Canvas
:param canvas: This canvas widget's canvas.
:param child: The child widget.  ``child`` must not have a
parent.
:type child: CanvasWidget
:param attribs: The new canvas widget's attributes.
""""""
self._child = child
self._obrack = canvas.create_line(1, 1, 1, 1, 1, 1, 1, 1)
self._cbrack = canvas.create_line(1, 1, 1, 1, 1, 1, 1, 1)
AbstractContainerWidget.__init__(self, canvas, child, **attribs)
",[],0,[],/draw/util.py___init__
3435,/home/amandapotts/git/nltk/nltk/draw/util.py___setitem__,"def __setitem__(self, attr, value):
if attr == ""color"":
self.canvas().itemconfig(self._obrack, fill=value)
self.canvas().itemconfig(self._cbrack, fill=value)
elif attr == ""width"":
self.canvas().itemconfig(self._obrack, width=value)
self.canvas().itemconfig(self._cbrack, width=value)
else:
CanvasWidget.__setitem__(self, attr, value)
",[],0,[],/draw/util.py___setitem__
3436,/home/amandapotts/git/nltk/nltk/draw/util.py___getitem__,"def __getitem__(self, attr):
if attr == ""color"":
return self.canvas().itemcget(self._obrack, ""outline"")
elif attr == ""width"":
return self.canvas().itemcget(self._obrack, ""width"")
else:
return CanvasWidget.__getitem__(self, attr)
",[],0,[],/draw/util.py___getitem__
3437,/home/amandapotts/git/nltk/nltk/draw/util.py__update,"def _update(self, child):
(x1, y1, x2, y2) = child.bbox()
width = max((y2 - y1) / 8, 2)
self.canvas().coords(
self._obrack, x1, y1, x1 - width, y1, x1 - width, y2, x1, y2
)
self.canvas().coords(
self._cbrack, x2, y1, x2 + width, y1, x2 + width, y2, x2, y2
)
",[],0,[],/draw/util.py__update
3438,/home/amandapotts/git/nltk/nltk/draw/util.py__tags,"def _tags(self):
return [self._obrack, self._cbrack]
",[],0,[],/draw/util.py__tags
3439,/home/amandapotts/git/nltk/nltk/draw/util.py___init__,"def __init__(self, canvas, *children, **attribs):
""""""
Create a new sequence widget.
:type canvas: Tkinter.Canvas
:param canvas: This canvas widget's canvas.
:param children: The widgets that should be aligned
horizontally.  Each child must not have a parent.
:type children: list(CanvasWidget)
:param attribs: The new canvas widget's attributes.
""""""
self._align = ""center""
self._space = 1
self._ordered = False
self._children = list(children)
for child in children:
self._add_child_widget(child)
CanvasWidget.__init__(self, canvas, **attribs)
",[],0,[],/draw/util.py___init__
3440,/home/amandapotts/git/nltk/nltk/draw/util.py___setitem__,"def __setitem__(self, attr, value):
if attr == ""align"":
if value not in (""top"", ""bottom"", ""center""):
raise ValueError(""Bad alignment: %r"" % value)
self._align = value
elif attr == ""space"":
self._space = value
elif attr == ""ordered"":
self._ordered = value
else:
CanvasWidget.__setitem__(self, attr, value)
",[],0,[],/draw/util.py___setitem__
3441,/home/amandapotts/git/nltk/nltk/draw/util.py___getitem__,"def __getitem__(self, attr):
if attr == ""align"":
return self._align
elif attr == ""space"":
return self._space
elif attr == ""ordered"":
return self._ordered
else:
return CanvasWidget.__getitem__(self, attr)
",[],0,[],/draw/util.py___getitem__
3442,/home/amandapotts/git/nltk/nltk/draw/util.py__tags,"def _tags(self):
return []
",[],0,[],/draw/util.py__tags
3443,/home/amandapotts/git/nltk/nltk/draw/util.py__yalign,"def _yalign(self, top, bot):
if self._align == ""top"":
return top
if self._align == ""bottom"":
return bot
if self._align == ""center"":
return (top + bot) / 2
",[],0,[],/draw/util.py__yalign
3444,/home/amandapotts/git/nltk/nltk/draw/util.py__update,"def _update(self, child):
(left, top, right, bot) = child.bbox()
y = self._yalign(top, bot)
for c in self._children:
(x1, y1, x2, y2) = c.bbox()
c.move(0, y - self._yalign(y1, y2))
if self._ordered and len(self._children) > 1:
index = self._children.index(child)
x = right + self._space
for i in range(index + 1, len(self._children)):
(x1, y1, x2, y2) = self._children[i].bbox()
if x > x1:
self._children[i].move(x - x1, 0)
x += x2 - x1 + self._space
x = left - self._space
for i in range(index - 1, -1, -1):
(x1, y1, x2, y2) = self._children[i].bbox()
if x < x2:
self._children[i].move(x - x2, 0)
x -= x2 - x1 + self._space
",[],0,[],/draw/util.py__update
3445,/home/amandapotts/git/nltk/nltk/draw/util.py__manage,"def _manage(self):
if len(self._children) == 0:
return
child = self._children[0]
(left, top, right, bot) = child.bbox()
y = self._yalign(top, bot)
index = self._children.index(child)
x = right + self._space
for i in range(index + 1, len(self._children)):
(x1, y1, x2, y2) = self._children[i].bbox()
self._children[i].move(x - x1, y - self._yalign(y1, y2))
x += x2 - x1 + self._space
x = left - self._space
for i in range(index - 1, -1, -1):
(x1, y1, x2, y2) = self._children[i].bbox()
self._children[i].move(x - x2, y - self._yalign(y1, y2))
x -= x2 - x1 + self._space
",[],0,[],/draw/util.py__manage
3446,/home/amandapotts/git/nltk/nltk/draw/util.py___repr__,"def __repr__(self):
return ""[Sequence: "" + repr(self._children)[1:-1] + ""]""
",[],0,[],/draw/util.py___repr__
3447,/home/amandapotts/git/nltk/nltk/draw/util.py_replace_child,"def replace_child(self, oldchild, newchild):
""""""
Replace the child canvas widget ``oldchild`` with ``newchild``.
``newchild`` must not have a parent.  ``oldchild``'s parent will
be set to None.
:type oldchild: CanvasWidget
:param oldchild: The child canvas widget to remove.
:type newchild: CanvasWidget
:param newchild: The canvas widget that should replace
``oldchild``.
""""""
index = self._children.index(oldchild)
self._children[index] = newchild
self._remove_child_widget(oldchild)
self._add_child_widget(newchild)
self.update(newchild)
",[],0,[],/draw/util.py_replace_child
3448,/home/amandapotts/git/nltk/nltk/draw/util.py_remove_child,"def remove_child(self, child):
""""""
Remove the given child canvas widget.  ``child``'s parent will
be set to None.
:type child: CanvasWidget
:param child: The child canvas widget to remove.
""""""
index = self._children.index(child)
del self._children[index]
self._remove_child_widget(child)
if len(self._children) > 0:
self.update(self._children[0])
",[],0,[],/draw/util.py_remove_child
3449,/home/amandapotts/git/nltk/nltk/draw/util.py_insert_child,"def insert_child(self, index, child):
""""""
Insert a child canvas widget before a given index.
:type child: CanvasWidget
:param child: The canvas widget that should be inserted.
:type index: int
:param index: The index where the child widget should be
inserted.  In particular, the index of ``child`` will be
``index``
greater than equal to ``index`` before ``child`` was
inserted will be incremented by one.
""""""
self._children.insert(index, child)
self._add_child_widget(child)
",[],0,[],/draw/util.py_insert_child
3450,/home/amandapotts/git/nltk/nltk/draw/util.py___init__,"def __init__(self, canvas, *children, **attribs):
""""""
Create a new stack widget.
:type canvas: Tkinter.Canvas
:param canvas: This canvas widget's canvas.
:param children: The widgets that should be aligned
vertically.  Each child must not have a parent.
:type children: list(CanvasWidget)
:param attribs: The new canvas widget's attributes.
""""""
self._align = ""center""
self._space = 1
self._ordered = False
self._children = list(children)
for child in children:
self._add_child_widget(child)
CanvasWidget.__init__(self, canvas, **attribs)
",[],0,[],/draw/util.py___init__
3451,/home/amandapotts/git/nltk/nltk/draw/util.py___setitem__,"def __setitem__(self, attr, value):
if attr == ""align"":
if value not in (""left"", ""right"", ""center""):
raise ValueError(""Bad alignment: %r"" % value)
self._align = value
elif attr == ""space"":
self._space = value
elif attr == ""ordered"":
self._ordered = value
else:
CanvasWidget.__setitem__(self, attr, value)
",[],0,[],/draw/util.py___setitem__
3452,/home/amandapotts/git/nltk/nltk/draw/util.py___getitem__,"def __getitem__(self, attr):
if attr == ""align"":
return self._align
elif attr == ""space"":
return self._space
elif attr == ""ordered"":
return self._ordered
else:
return CanvasWidget.__getitem__(self, attr)
",[],0,[],/draw/util.py___getitem__
3453,/home/amandapotts/git/nltk/nltk/draw/util.py__tags,"def _tags(self):
return []
",[],0,[],/draw/util.py__tags
3454,/home/amandapotts/git/nltk/nltk/draw/util.py__xalign,"def _xalign(self, left, right):
if self._align == ""left"":
return left
if self._align == ""right"":
return right
if self._align == ""center"":
return (left + right) / 2
",[],0,[],/draw/util.py__xalign
3455,/home/amandapotts/git/nltk/nltk/draw/util.py__update,"def _update(self, child):
(left, top, right, bot) = child.bbox()
x = self._xalign(left, right)
for c in self._children:
(x1, y1, x2, y2) = c.bbox()
c.move(x - self._xalign(x1, x2), 0)
if self._ordered and len(self._children) > 1:
index = self._children.index(child)
y = bot + self._space
for i in range(index + 1, len(self._children)):
(x1, y1, x2, y2) = self._children[i].bbox()
if y > y1:
self._children[i].move(0, y - y1)
y += y2 - y1 + self._space
y = top - self._space
for i in range(index - 1, -1, -1):
(x1, y1, x2, y2) = self._children[i].bbox()
if y < y2:
self._children[i].move(0, y - y2)
y -= y2 - y1 + self._space
",[],0,[],/draw/util.py__update
3456,/home/amandapotts/git/nltk/nltk/draw/util.py__manage,"def _manage(self):
if len(self._children) == 0:
return
child = self._children[0]
(left, top, right, bot) = child.bbox()
x = self._xalign(left, right)
index = self._children.index(child)
y = bot + self._space
for i in range(index + 1, len(self._children)):
(x1, y1, x2, y2) = self._children[i].bbox()
self._children[i].move(x - self._xalign(x1, x2), y - y1)
y += y2 - y1 + self._space
y = top - self._space
for i in range(index - 1, -1, -1):
(x1, y1, x2, y2) = self._children[i].bbox()
self._children[i].move(x - self._xalign(x1, x2), y - y2)
y -= y2 - y1 + self._space
",[],0,[],/draw/util.py__manage
3457,/home/amandapotts/git/nltk/nltk/draw/util.py___repr__,"def __repr__(self):
return ""[Stack: "" + repr(self._children)[1:-1] + ""]""
",[],0,[],/draw/util.py___repr__
3458,/home/amandapotts/git/nltk/nltk/draw/util.py_replace_child,"def replace_child(self, oldchild, newchild):
""""""
Replace the child canvas widget ``oldchild`` with ``newchild``.
``newchild`` must not have a parent.  ``oldchild``'s parent will
be set to None.
:type oldchild: CanvasWidget
:param oldchild: The child canvas widget to remove.
:type newchild: CanvasWidget
:param newchild: The canvas widget that should replace
``oldchild``.
""""""
index = self._children.index(oldchild)
self._children[index] = newchild
self._remove_child_widget(oldchild)
self._add_child_widget(newchild)
self.update(newchild)
",[],0,[],/draw/util.py_replace_child
3459,/home/amandapotts/git/nltk/nltk/draw/util.py_remove_child,"def remove_child(self, child):
""""""
Remove the given child canvas widget.  ``child``'s parent will
be set to None.
:type child: CanvasWidget
:param child: The child canvas widget to remove.
""""""
index = self._children.index(child)
del self._children[index]
self._remove_child_widget(child)
if len(self._children) > 0:
self.update(self._children[0])
",[],0,[],/draw/util.py_remove_child
3460,/home/amandapotts/git/nltk/nltk/draw/util.py_insert_child,"def insert_child(self, index, child):
""""""
Insert a child canvas widget before a given index.
:type child: CanvasWidget
:param child: The canvas widget that should be inserted.
:type index: int
:param index: The index where the child widget should be
inserted.  In particular, the index of ``child`` will be
``index``
greater than equal to ``index`` before ``child`` was
inserted will be incremented by one.
""""""
self._children.insert(index, child)
self._add_child_widget(child)
",[],0,[],/draw/util.py_insert_child
3461,/home/amandapotts/git/nltk/nltk/draw/util.py___init__,"def __init__(self, canvas, width, height, **attribs):
""""""
Create a new space widget.
:type canvas: Tkinter.Canvas
:param canvas: This canvas widget's canvas.
:type width: int
:param width: The width of the new space widget.
:type height: int
:param height: The height of the new space widget.
:param attribs: The new canvas widget's attributes.
""""""
if width > 4:
width -= 4
if height > 4:
height -= 4
self._tag = canvas.create_line(1, 1, width, height, fill="""")
CanvasWidget.__init__(self, canvas, **attribs)
",[],0,[],/draw/util.py___init__
3462,/home/amandapotts/git/nltk/nltk/draw/util.py_set_width,"def set_width(self, width):
""""""
Change the width of this space widget.
:param width: The new width.
:type width: int
:rtype: None
""""""
[x1, y1, x2, y2] = self.bbox()
self.canvas().coords(self._tag, x1, y1, x1 + width, y2)
",[],0,[],/draw/util.py_set_width
3463,/home/amandapotts/git/nltk/nltk/draw/util.py_set_height,"def set_height(self, height):
""""""
Change the height of this space widget.
:param height: The new height.
:type height: int
:rtype: None
""""""
[x1, y1, x2, y2] = self.bbox()
self.canvas().coords(self._tag, x1, y1, x2, y1 + height)
",[],0,[],/draw/util.py_set_height
3464,/home/amandapotts/git/nltk/nltk/draw/util.py__tags,"def _tags(self):
return [self._tag]
",[],0,[],/draw/util.py__tags
3465,/home/amandapotts/git/nltk/nltk/draw/util.py___repr__,"def __repr__(self):
return ""[Space]""
",[],0,[],/draw/util.py___repr__
3466,/home/amandapotts/git/nltk/nltk/draw/util.py___init__,"def __init__(self, canvas, *children, **attribs):
""""""
Create a new scroll-watcher widget.
:type canvas: Tkinter.Canvas
:param canvas: This canvas widget's canvas.
:type children: list(CanvasWidget)
:param children: The canvas widgets watched by the
scroll-watcher.  The scroll-watcher will ensure that these
canvas widgets are always contained in their canvas's
scrollregion.
:param attribs: The new canvas widget's attributes.
""""""
for child in children:
self._add_child_widget(child)
CanvasWidget.__init__(self, canvas, **attribs)
",[],0,[],/draw/util.py___init__
3467,/home/amandapotts/git/nltk/nltk/draw/util.py_add_child,"def add_child(self, canvaswidget):
""""""
Add a new canvas widget to the scroll-watcher.  The
scroll-watcher will ensure that the new canvas widget is
always contained in its canvas's scrollregion.
:param canvaswidget: The new canvas widget.
:type canvaswidget: CanvasWidget
:rtype: None
""""""
self._add_child_widget(canvaswidget)
self.update(canvaswidget)
",[],0,[],/draw/util.py_add_child
3468,/home/amandapotts/git/nltk/nltk/draw/util.py_remove_child,"def remove_child(self, canvaswidget):
""""""
Remove a canvas widget from the scroll-watcher.  The
scroll-watcher will no longer ensure that the new canvas
widget is always contained in its canvas's scrollregion.
:param canvaswidget: The canvas widget to remove.
:type canvaswidget: CanvasWidget
:rtype: None
""""""
self._remove_child_widget(canvaswidget)
",[],0,[],/draw/util.py_remove_child
3469,/home/amandapotts/git/nltk/nltk/draw/util.py__tags,"def _tags(self):
return []
",[],0,[],/draw/util.py__tags
3470,/home/amandapotts/git/nltk/nltk/draw/util.py__update,"def _update(self, child):
self._adjust_scrollregion()
",[],0,[],/draw/util.py__update
3471,/home/amandapotts/git/nltk/nltk/draw/util.py__adjust_scrollregion,"def _adjust_scrollregion(self):
""""""
Adjust the scrollregion of this scroll-watcher's ``Canvas`` to
include the bounding boxes of all of its children.
""""""
bbox = self.bbox()
canvas = self.canvas()
scrollregion = [int(n) for n in canvas[""scrollregion""].split()]
if len(scrollregion) != 4:
return
if (
bbox[0] < scrollregion[0]
or bbox[1] < scrollregion[1]
or bbox[2] > scrollregion[2]
or bbox[3] > scrollregion[3]
):
scrollregion = ""%d %d %d %d"" % (
min(bbox[0], scrollregion[0]),
min(bbox[1], scrollregion[1]),
max(bbox[2], scrollregion[2]),
max(bbox[3], scrollregion[3]),
)
canvas[""scrollregion""] = scrollregion
",[],0,[],/draw/util.py__adjust_scrollregion
3472,/home/amandapotts/git/nltk/nltk/draw/util.py__init_menubar,"def _init_menubar(self):
menubar = Menu(self._parent)
filemenu = Menu(menubar, tearoff=0)
filemenu.add_command(
label=""Print to Postscript"",
underline=0,
command=self.print_to_file,
accelerator=""Ctrl-p"",
)
filemenu.add_command(
label=""Exit"", underline=1, command=self.destroy, accelerator=""Ctrl-x""
)
menubar.add_cascade(label=""File"", underline=0, menu=filemenu)
self._parent.config(menu=menubar)
",[],0,[],/draw/util.py__init_menubar
3473,/home/amandapotts/git/nltk/nltk/draw/util.py_print_to_file,"def print_to_file(self, filename=None):
""""""
Print the contents of this ``CanvasFrame`` to a postscript
file.  If no filename is given, then prompt the user for one.
:param filename: The name of the file to print the tree to.
:type filename: str
:rtype: None
""""""
if filename is None:
ftypes = [(""Postscript files"", "".ps""), (""All files"", ""*"")]
filename = asksaveasfilename(filetypes=ftypes, defaultextension="".ps"")
if not filename:
return
(x0, y0, w, h) = self.scrollregion()
postscript = self._canvas.postscript(
x=x0,
y=y0,
width=w + 2,
height=h + 2,
pagewidth=w + 2,  # points = 1/72 inch
pageheight=h + 2,  # points = 1/72 inch
pagex=0,
pagey=0,
)
postscript = postscript.replace("" 0 scalefont "", "" 9 scalefont "")
with open(filename, ""wb"") as f:
f.write(postscript.encode(""utf8""))
",[],0,[],/draw/util.py_print_to_file
3474,/home/amandapotts/git/nltk/nltk/draw/util.py_scrollregion,"def scrollregion(self):
""""""
:return: The current scroll region for the canvas managed by
this ``CanvasFrame``.
:rtype: 4-tuple of int
""""""
(x1, y1, x2, y2) = self._canvas[""scrollregion""].split()
return (int(x1), int(y1), int(x2), int(y2))
",[],0,[],/draw/util.py_scrollregion
3475,/home/amandapotts/git/nltk/nltk/draw/util.py_canvas,"def canvas(self):
""""""
:return: The canvas managed by this ``CanvasFrame``.
:rtype: Tkinter.Canvas
""""""
return self._canvas
",[],0,[],/draw/util.py_canvas
3476,/home/amandapotts/git/nltk/nltk/draw/util.py_add_widget,"def add_widget(self, canvaswidget, x=None, y=None):
""""""
Register a canvas widget with this ``CanvasFrame``.  The
``CanvasFrame`` will ensure that this canvas widget is always
within the ``Canvas``'s scrollregion.  If no coordinates are
given for the canvas widget, then the ``CanvasFrame`` will
attempt to find a clear area of the canvas for it.
:type canvaswidget: CanvasWidget
:param canvaswidget: The new canvas widget.  ``canvaswidget``
must have been created on this ``CanvasFrame``'s canvas.
:type x: int
:param x: The initial x coordinate for the upper left hand
corner of ``canvaswidget``, in the canvas's coordinate
space.
:type y: int
:param y: The initial y coordinate for the upper left hand
corner of ``canvaswidget``, in the canvas's coordinate
space.
""""""
if x is None or y is None:
(x, y) = self._find_room(canvaswidget, x, y)
(x1, y1, x2, y2) = canvaswidget.bbox()
canvaswidget.move(x - x1, y - y1)
self._scrollwatcher.add_child(canvaswidget)
",[],0,[],/draw/util.py_add_widget
3477,/home/amandapotts/git/nltk/nltk/draw/util.py__find_room,"def _find_room(self, widget, desired_x, desired_y):
""""""
Try to find a space for a given widget.
""""""
(left, top, right, bot) = self.scrollregion()
w = widget.width()
h = widget.height()
if w >= (right - left):
return (0, 0)
if h >= (bot - top):
return (0, 0)
(x1, y1, x2, y2) = widget.bbox()
widget.move(left - x2 - 50, top - y2 - 50)
if desired_x is not None:
x = desired_x
for y in range(top, bot - h, int((bot - top - h) / 10)):
if not self._canvas.find_overlapping(
x - 5, y - 5, x + w + 5, y + h + 5
):
return (x, y)
if desired_y is not None:
y = desired_y
for x in range(left, right - w, int((right - left - w) / 10)):
if not self._canvas.find_overlapping(
x - 5, y - 5, x + w + 5, y + h + 5
):
return (x, y)
for y in range(top, bot - h, int((bot - top - h) / 10)):
for x in range(left, right - w, int((right - left - w) / 10)):
if not self._canvas.find_overlapping(
x - 5, y - 5, x + w + 5, y + h + 5
):
return (x, y)
return (0, 0)
",[],0,[],/draw/util.py__find_room
3478,/home/amandapotts/git/nltk/nltk/draw/util.py_destroy_widget,"def destroy_widget(self, canvaswidget):
""""""
Remove a canvas widget from this ``CanvasFrame``.  This
deregisters the canvas widget, and destroys it.
""""""
self.remove_widget(canvaswidget)
canvaswidget.destroy()
",[],0,[],/draw/util.py_destroy_widget
3479,/home/amandapotts/git/nltk/nltk/draw/util.py_remove_widget,"def remove_widget(self, canvaswidget):
self._scrollwatcher.remove_child(canvaswidget)
",[],0,[],/draw/util.py_remove_widget
3480,/home/amandapotts/git/nltk/nltk/draw/util.py_pack,"def pack(self, cnf={}, **kw):
""""""
Pack this ``CanvasFrame``.  See the documentation for
``Tkinter.Pack`` for more information.
""""""
self._frame.pack(cnf, **kw)
",[],0,[],/draw/util.py_pack
3481,/home/amandapotts/git/nltk/nltk/draw/util.py_destroy,"def destroy(self, *e):
""""""
Destroy this ``CanvasFrame``.  If this ``CanvasFrame`` created a
top-level window, then this will close that window.
""""""
if self._parent is None:
return
self._parent.destroy()
self._parent = None
",[],0,[],/draw/util.py_destroy
3482,/home/amandapotts/git/nltk/nltk/draw/util.py_mainloop,"def mainloop(self, *args, **kwargs):
""""""
Enter the Tkinter mainloop.  This function must be called if
this frame is created from a non-interactive program (e.g.
from a secript)
the script completes.
""""""
if in_idle():
return
self._parent.mainloop(*args, **kwargs)
",[],0,[],/draw/util.py_mainloop
3483,/home/amandapotts/git/nltk/nltk/draw/util.py___init__,"def __init__(self, root, title, text, width=None, height=None, **textbox_options):
if width is None or height is None:
(width, height) = self.find_dimentions(text, width, height)
if root is None:
self._top = top = Tk()
else:
self._top = top = Toplevel(root)
top.title(title)
b = Button(top, text=""Ok"", command=self.destroy)
b.pack(side=""bottom"")
tbf = Frame(top)
tbf.pack(expand=1, fill=""both"")
scrollbar = Scrollbar(tbf, orient=""vertical"")
scrollbar.pack(side=""right"", fill=""y"")
textbox = Text(tbf, wrap=""word"", width=width, height=height, **textbox_options)
textbox.insert(""end"", text)
textbox[""state""] = ""disabled""
textbox.pack(side=""left"", expand=1, fill=""both"")
scrollbar[""command""] = textbox.yview
textbox[""yscrollcommand""] = scrollbar.set
top.bind(""q"", self.destroy)
top.bind(""x"", self.destroy)
top.bind(""c"", self.destroy)
top.bind(""<Return>"", self.destroy)
top.bind(""<Escape>"", self.destroy)
scrollbar.focus()
",[],0,[],/draw/util.py___init__
3484,/home/amandapotts/git/nltk/nltk/draw/util.py_find_dimentions,"def find_dimentions(self, text, width, height):
lines = text.split(""\n"")
if width is None:
maxwidth = max(len(line) for line in lines)
width = min(maxwidth, 80)
height = 0
for line in lines:
while len(line) > width:
brk = line[:width].rfind("" "")
line = line[brk:]
height += 1
height += 1
height = min(height, 25)
return (width, height)
",[],0,[],/draw/util.py_find_dimentions
3485,/home/amandapotts/git/nltk/nltk/draw/util.py_destroy,"def destroy(self, *e):
if self._top is None:
return
self._top.destroy()
self._top = None
",[],0,[],/draw/util.py_destroy
3486,/home/amandapotts/git/nltk/nltk/draw/util.py_mainloop,"def mainloop(self, *args, **kwargs):
""""""
Enter the Tkinter mainloop.  This function must be called if
this window is created from a non-interactive program (e.g.
from a secript)
the script completes.
""""""
if in_idle():
return
self._top.mainloop(*args, **kwargs)
",[],0,[],/draw/util.py_mainloop
3487,/home/amandapotts/git/nltk/nltk/draw/util.py___init__,"def __init__(
self, parent, original_text="""", instructions="""", set_callback=None, title=None
",[],0,[],/draw/util.py___init__
3488,/home/amandapotts/git/nltk/nltk/draw/util.py__reset,"def _reset(self, *e):
self._entry.delete(0, ""end"")
self._entry.insert(0, self._original_text)
if self._set_callback:
self._set_callback(self._original_text)
",[],0,[],/draw/util.py__reset
3489,/home/amandapotts/git/nltk/nltk/draw/util.py__cancel,"def _cancel(self, *e):
try:
self._reset()
except:
pass
self._destroy()
",[],0,[],/draw/util.py__cancel
3490,/home/amandapotts/git/nltk/nltk/draw/util.py__ok,"def _ok(self, *e):
self._apply()
self._destroy()
",[],0,[],/draw/util.py__ok
3491,/home/amandapotts/git/nltk/nltk/draw/util.py__apply,"def _apply(self, *e):
if self._set_callback:
self._set_callback(self._entry.get())
",[],0,[],/draw/util.py__apply
3492,/home/amandapotts/git/nltk/nltk/draw/util.py__destroy,"def _destroy(self, *e):
if self._top is None:
return
self._top.destroy()
self._top = None
",[],0,[],/draw/util.py__destroy
3493,/home/amandapotts/git/nltk/nltk/draw/util.py___init__,"def __init__(self, parent, items=[], **options):
""""""
Construct a new list.
:param parent: The Tk widget that contains the colorized list
:param items: The initial contents of the colorized list.
:param options:
""""""
self._parent = parent
self._callbacks = {}
self._marks = {}
self._init_itemframe(options.copy())
self._textwidget.bind(""<KeyPress>"", self._keypress)
self._textwidget.bind(""<ButtonPress>"", self._buttonpress)
self._items = None
self.set(items)
",[],0,[],/draw/util.py___init__
3494,/home/amandapotts/git/nltk/nltk/draw/util.py__init_colortags,"def _init_colortags(self, textwidget, options):
""""""
Set up any colortags that will be used by this colorized list.
E.g.:
textwidget.tag_config('terminal', foreground='black')
""""""
",[],0,[],/draw/util.py__init_colortags
3495,/home/amandapotts/git/nltk/nltk/draw/util.py__item_repr,"def _item_repr(self, item):
""""""
Return a list of (text, colortag) tuples that make up the
colorized representation of the item.  Colorized
representations may not span multiple lines.  I.e., the text
strings returned may not contain newline characters.
""""""
",[],0,[],/draw/util.py__item_repr
3496,/home/amandapotts/git/nltk/nltk/draw/util.py_get,"def get(self, index=None):
""""""
:return: A list of the items contained by this list.
""""""
if index is None:
return self._items[:]
else:
return self._items[index]
",[],0,[],/draw/util.py_get
3497,/home/amandapotts/git/nltk/nltk/draw/util.py_set,"def set(self, items):
""""""
Modify the list of items contained by this list.
""""""
items = list(items)
if self._items == items:
return
self._items = list(items)
self._textwidget[""state""] = ""normal""
self._textwidget.delete(""1.0"", ""end"")
for item in items:
for text, colortag in self._item_repr(item):
assert ""\n"" not in text, ""item repr may not contain newline""
self._textwidget.insert(""end"", text, colortag)
self._textwidget.insert(""end"", ""\n"")
self._textwidget.delete(""end-1char"", ""end"")
self._textwidget.mark_set(""insert"", ""1.0"")
self._textwidget[""state""] = ""disabled""
self._marks.clear()
",[],0,[],/draw/util.py_set
3498,/home/amandapotts/git/nltk/nltk/draw/util.py_unmark,"def unmark(self, item=None):
""""""
Remove highlighting from the given item
if no item is given.
:raise ValueError: If ``item`` is not contained in the list.
:raise KeyError: If ``item`` is not marked.
""""""
if item is None:
self._marks.clear()
self._textwidget.tag_remove(""highlight"", ""1.0"", ""end+1char"")
else:
index = self._items.index(item)
del self._marks[item]
(start, end) = (""%d.0"" % (index + 1), ""%d.0"" % (index + 2))
self._textwidget.tag_remove(""highlight"", start, end)
",[],0,[],/draw/util.py_unmark
3499,/home/amandapotts/git/nltk/nltk/draw/util.py_mark,"def mark(self, item):
""""""
Highlight the given item.
:raise ValueError: If ``item`` is not contained in the list.
""""""
self._marks[item] = 1
index = self._items.index(item)
(start, end) = (""%d.0"" % (index + 1), ""%d.0"" % (index + 2))
self._textwidget.tag_add(""highlight"", start, end)
",[],0,[],/draw/util.py_mark
3500,/home/amandapotts/git/nltk/nltk/draw/util.py_markonly,"def markonly(self, item):
""""""
Remove any current highlighting, and mark the given item.
:raise ValueError: If ``item`` is not contained in the list.
""""""
self.unmark()
self.mark(item)
",[],0,[],/draw/util.py_markonly
3501,/home/amandapotts/git/nltk/nltk/draw/util.py_view,"def view(self, item):
""""""
Adjust the view such that the given item is visible.  If
the item is already visible, then do nothing.
""""""
index = self._items.index(item)
self._textwidget.see(""%d.0"" % (index + 1))
",[],0,[],/draw/util.py_view
3502,/home/amandapotts/git/nltk/nltk/draw/util.py_add_callback,"def add_callback(self, event, func):
""""""
Register a callback function with the list.  This function
will be called whenever the given event occurs.
:param event: The event that will trigger the callback
function.  Valid events are: click1, click2, click3,
space, return, select, up, down, next, prior, move
:param func: The function that should be called when
the event occurs.  ``func`` will be called with a
single item as its argument.  (The item selected
or the item moved to).
""""""
if event == ""select"":
events = [""click1"", ""space"", ""return""]
elif event == ""move"":
events = [""up"", ""down"", ""next"", ""prior""]
else:
events = [event]
for e in events:
self._callbacks.setdefault(e, {})[func] = 1
",[],0,[],/draw/util.py_add_callback
3503,/home/amandapotts/git/nltk/nltk/draw/util.py_remove_callback,"def remove_callback(self, event, func=None):
""""""
Deregister a callback function.  If ``func`` is none, then
all callbacks are removed for the given event.
""""""
if event is None:
events = list(self._callbacks.keys())
elif event == ""select"":
events = [""click1"", ""space"", ""return""]
elif event == ""move"":
events = [""up"", ""down"", ""next"", ""prior""]
else:
events = [event]
for e in events:
if func is None:
del self._callbacks[e]
else:
try:
del self._callbacks[e][func]
except:
pass
",[],0,[],/draw/util.py_remove_callback
3504,/home/amandapotts/git/nltk/nltk/draw/util.py_pack,"def pack(self, cnf={}, **kw):
self._itemframe.pack(cnf, **kw)
",[],0,[],/draw/util.py_pack
3505,/home/amandapotts/git/nltk/nltk/draw/util.py_grid,"def grid(self, cnf={}, **kw):
self._itemframe.grid(cnf, *kw)
",[],0,[],/draw/util.py_grid
3506,/home/amandapotts/git/nltk/nltk/draw/util.py_focus,"def focus(self):
self._textwidget.focus()
",[],0,[],/draw/util.py_focus
3507,/home/amandapotts/git/nltk/nltk/draw/util.py__init_itemframe,"def _init_itemframe(self, options):
self._itemframe = Frame(self._parent)
options.setdefault(""background"", ""#e0e0e0"")
self._textwidget = Text(self._itemframe, **options)
self._textscroll = Scrollbar(self._itemframe, takefocus=0, orient=""vertical"")
self._textwidget.config(yscrollcommand=self._textscroll.set)
self._textscroll.config(command=self._textwidget.yview)
self._textscroll.pack(side=""right"", fill=""y"")
self._textwidget.pack(expand=1, fill=""both"", side=""left"")
self._textwidget.tag_config(
""highlight"", background=""#e0ffff"", border=""1"", relief=""raised""
)
self._init_colortags(self._textwidget, options)
self._textwidget.tag_config(""sel"", foreground="""")
self._textwidget.tag_config(
""sel"", foreground="""", background="""", border="""", underline=1
)
self._textwidget.tag_lower(""highlight"", ""sel"")
",[],0,[],/draw/util.py__init_itemframe
3508,/home/amandapotts/git/nltk/nltk/draw/util.py__fire_callback,"def _fire_callback(self, event, itemnum):
if event not in self._callbacks:
return
if 0 <= itemnum < len(self._items):
item = self._items[itemnum]
else:
item = None
for cb_func in list(self._callbacks[event].keys()):
cb_func(item)
",[],0,[],/draw/util.py__fire_callback
3509,/home/amandapotts/git/nltk/nltk/draw/util.py__buttonpress,"def _buttonpress(self, event):
clickloc = ""@%d,%d"" % (event.x, event.y)
insert_point = self._textwidget.index(clickloc)
itemnum = int(insert_point.split(""."")[0]) - 1
self._fire_callback(""click%d"" % event.num, itemnum)
",[],0,[],/draw/util.py__buttonpress
3510,/home/amandapotts/git/nltk/nltk/draw/util.py__keypress,"def _keypress(self, event):
if event.keysym == ""Return"" or event.keysym == ""space"":
insert_point = self._textwidget.index(""insert"")
itemnum = int(insert_point.split(""."")[0]) - 1
self._fire_callback(event.keysym.lower(), itemnum)
return
elif event.keysym == ""Down"":
delta = ""+1line""
elif event.keysym == ""Up"":
delta = ""-1line""
elif event.keysym == ""Next"":
delta = ""+10lines""
elif event.keysym == ""Prior"":
delta = ""-10lines""
else:
return ""continue""
self._textwidget.mark_set(""insert"", ""insert"" + delta)
self._textwidget.see(""insert"")
self._textwidget.tag_remove(""sel"", ""1.0"", ""end+1char"")
self._textwidget.tag_add(""sel"", ""insert linestart"", ""insert lineend"")
insert_point = self._textwidget.index(""insert"")
itemnum = int(insert_point.split(""."")[0]) - 1
self._fire_callback(event.keysym.lower(), itemnum)
return ""break""
",[],0,[],/draw/util.py__keypress
3511,/home/amandapotts/git/nltk/nltk/draw/util.py___init__,"def __init__(self, master, values, **options):
self._callback = options.get(""command"")
if ""command"" in options:
del options[""command""]
self._variable = variable = StringVar()
if len(values) > 0:
variable.set(values[0])
kw = {
""borderwidth"": 2,
""textvariable"": variable,
""indicatoron"": 1,
""relief"": RAISED,
""anchor"": ""c"",
""highlightthickness"": 2,
}
kw.update(options)
Widget.__init__(self, master, ""menubutton"", kw)
self.widgetName = ""tk_optionMenu""
self._menu = Menu(self, name=""menu"", tearoff=0)
self.menuname = self._menu._w
self._values = []
for value in values:
self.add(value)
self[""menu""] = self._menu
",[],0,[],/draw/util.py___init__
3512,/home/amandapotts/git/nltk/nltk/draw/util.py_add,"def add(self, value):
if value in self._values:
return
",[],0,[],/draw/util.py_add
3513,/home/amandapotts/git/nltk/nltk/draw/util.py_set,"def set(value=value):
self.set(value)
",[],0,[],/draw/util.py_set
3514,/home/amandapotts/git/nltk/nltk/draw/util.py_set,"def set(self, value):
self._variable.set(value)
if self._callback:
self._callback(value)
",[],0,[],/draw/util.py_set
3515,/home/amandapotts/git/nltk/nltk/draw/util.py_remove,"def remove(self, value):
i = self._values.index(value)
del self._values[i]
self._menu.delete(i, i)
",[],0,[],/draw/util.py_remove
3516,/home/amandapotts/git/nltk/nltk/draw/util.py___getitem__,"def __getitem__(self, name):
if name == ""menu"":
return self.__menu
return Widget.__getitem__(self, name)
",[],0,[],/draw/util.py___getitem__
3517,/home/amandapotts/git/nltk/nltk/draw/util.py_destroy,"def destroy(self):
""""""Destroy this widget and the associated menu.""""""
Menubutton.destroy(self)
self._menu = None
",[],0,[],/draw/util.py_destroy
3518,/home/amandapotts/git/nltk/nltk/draw/util.py_demo,"def demo():
""""""
A simple demonstration showing how to use canvas widgets.
""""""
",[],0,[],/draw/util.py_demo
3519,/home/amandapotts/git/nltk/nltk/draw/util.py_fill,"def fill(cw):
from random import randint
cw[""fill""] = ""#00%04d"" % randint(0, 9999)
",[],0,[],/draw/util.py_fill
3520,/home/amandapotts/git/nltk/nltk/draw/util.py_color,"def color(cw):
from random import randint
cw[""color""] = ""#ff%04d"" % randint(0, 9999)
",[],0,[],/draw/util.py_color
3521,/home/amandapotts/git/nltk/nltk/draw/tree.py___init__,"def __init__(self, canvas, label, subtrees, **attribs):
""""""
:type node:
:type subtrees: list(CanvasWidgetI)
""""""
self._label = label
self._subtrees = subtrees
self._horizontal = 0
self._roof = 0
self._xspace = 10
self._yspace = 15
self._ordered = False
self._lines = [canvas.create_line(0, 0, 0, 0, fill=""#006060"") for c in subtrees]
self._polygon = canvas.create_polygon(
0, 0, fill="""", state=""hidden"", outline=""#006060""
)
self._add_child_widget(label)
for subtree in subtrees:
self._add_child_widget(subtree)
self._managing = False
CanvasWidget.__init__(self, canvas, **attribs)
",[],0,[],/draw/tree.py___init__
3522,/home/amandapotts/git/nltk/nltk/draw/tree.py___setitem__,"def __setitem__(self, attr, value):
canvas = self.canvas()
if attr == ""roof"":
self._roof = value
if self._roof:
for l in self._lines:
canvas.itemconfig(l, state=""hidden"")
canvas.itemconfig(self._polygon, state=""normal"")
else:
for l in self._lines:
canvas.itemconfig(l, state=""normal"")
canvas.itemconfig(self._polygon, state=""hidden"")
elif attr == ""orientation"":
if value == ""horizontal"":
self._horizontal = 1
elif value == ""vertical"":
self._horizontal = 0
else:
raise ValueError(""orientation must be horizontal or vertical"")
elif attr == ""color"":
for l in self._lines:
canvas.itemconfig(l, fill=value)
canvas.itemconfig(self._polygon, outline=value)
elif isinstance(attr, tuple) and attr[0] == ""color"":
l = self._lines[int(attr[1])]
canvas.itemconfig(l, fill=value)
elif attr == ""fill"":
canvas.itemconfig(self._polygon, fill=value)
elif attr == ""width"":
canvas.itemconfig(self._polygon, {attr: value})
for l in self._lines:
canvas.itemconfig(l, {attr: value})
elif attr in (""xspace"", ""yspace""):
if attr == ""xspace"":
self._xspace = value
elif attr == ""yspace"":
self._yspace = value
self.update(self._label)
elif attr == ""ordered"":
self._ordered = value
else:
CanvasWidget.__setitem__(self, attr, value)
",[],0,[],/draw/tree.py___setitem__
3523,/home/amandapotts/git/nltk/nltk/draw/tree.py___getitem__,"def __getitem__(self, attr):
if attr == ""roof"":
return self._roof
elif attr == ""width"":
return self.canvas().itemcget(self._polygon, attr)
elif attr == ""color"":
return self.canvas().itemcget(self._polygon, ""outline"")
elif isinstance(attr, tuple) and attr[0] == ""color"":
l = self._lines[int(attr[1])]
return self.canvas().itemcget(l, ""fill"")
elif attr == ""xspace"":
return self._xspace
elif attr == ""yspace"":
return self._yspace
elif attr == ""orientation"":
if self._horizontal:
return ""horizontal""
else:
return ""vertical""
elif attr == ""ordered"":
return self._ordered
else:
return CanvasWidget.__getitem__(self, attr)
",[],0,[],/draw/tree.py___getitem__
3524,/home/amandapotts/git/nltk/nltk/draw/tree.py_label,"def label(self):
return self._label
",[],0,[],/draw/tree.py_label
3525,/home/amandapotts/git/nltk/nltk/draw/tree.py_subtrees,"def subtrees(self):
return self._subtrees[:]
",[],0,[],/draw/tree.py_subtrees
3526,/home/amandapotts/git/nltk/nltk/draw/tree.py_set_label,"def set_label(self, label):
""""""
Set the node label to ``label``.
""""""
self._remove_child_widget(self._label)
self._add_child_widget(label)
self._label = label
self.update(self._label)
",[],0,[],/draw/tree.py_set_label
3527,/home/amandapotts/git/nltk/nltk/draw/tree.py_replace_child,"def replace_child(self, oldchild, newchild):
""""""
Replace the child ``oldchild`` with ``newchild``.
""""""
index = self._subtrees.index(oldchild)
self._subtrees[index] = newchild
self._remove_child_widget(oldchild)
self._add_child_widget(newchild)
self.update(newchild)
",[],0,[],/draw/tree.py_replace_child
3528,/home/amandapotts/git/nltk/nltk/draw/tree.py_remove_child,"def remove_child(self, child):
index = self._subtrees.index(child)
del self._subtrees[index]
self._remove_child_widget(child)
self.canvas().delete(self._lines.pop())
self.update(self._label)
",[],0,[],/draw/tree.py_remove_child
3529,/home/amandapotts/git/nltk/nltk/draw/tree.py_insert_child,"def insert_child(self, index, child):
canvas = self.canvas()
self._subtrees.insert(index, child)
self._add_child_widget(child)
self._lines.append(canvas.create_line(0, 0, 0, 0, fill=""#006060""))
self.update(self._label)
",[],0,[],/draw/tree.py_insert_child
3530,/home/amandapotts/git/nltk/nltk/draw/tree.py__tags,"def _tags(self):
if self._roof:
return [self._polygon]
else:
return self._lines
",[],0,[],/draw/tree.py__tags
3531,/home/amandapotts/git/nltk/nltk/draw/tree.py__subtree_top,"def _subtree_top(self, child):
if isinstance(child, TreeSegmentWidget):
bbox = child.label().bbox()
else:
bbox = child.bbox()
if self._horizontal:
return (bbox[0], (bbox[1] + bbox[3]) / 2.0)
else:
return ((bbox[0] + bbox[2]) / 2.0, bbox[1])
",[],0,[],/draw/tree.py__subtree_top
3532,/home/amandapotts/git/nltk/nltk/draw/tree.py__node_bottom,"def _node_bottom(self):
bbox = self._label.bbox()
if self._horizontal:
return (bbox[2], (bbox[1] + bbox[3]) / 2.0)
else:
return ((bbox[0] + bbox[2]) / 2.0, bbox[3])
",[],0,[],/draw/tree.py__node_bottom
3533,/home/amandapotts/git/nltk/nltk/draw/tree.py__update,"def _update(self, child):
if len(self._subtrees) == 0:
return
if self._label.bbox() is None:
return  # [XX] ???
if child is self._label:
need_update = self._subtrees
else:
need_update = [child]
if self._ordered and not self._managing:
need_update = self._maintain_order(child)
(nodex, nodey) = self._node_bottom()
(xmin, ymin, xmax, ymax) = self._subtrees[0].bbox()
for subtree in self._subtrees[1:]:
bbox = subtree.bbox()
xmin = min(xmin, bbox[0])
ymin = min(ymin, bbox[1])
xmax = max(xmax, bbox[2])
ymax = max(ymax, bbox[3])
if self._horizontal:
self.canvas().coords(
self._polygon, nodex, nodey, xmin, ymin, xmin, ymax, nodex, nodey
)
else:
self.canvas().coords(
self._polygon, nodex, nodey, xmin, ymin, xmax, ymin, nodex, nodey
)
for subtree in need_update:
(nodex, nodey) = self._node_bottom()
line = self._lines[self._subtrees.index(subtree)]
(subtreex, subtreey) = self._subtree_top(subtree)
self.canvas().coords(line, nodex, nodey, subtreex, subtreey)
",[],0,[],/draw/tree.py__update
3534,/home/amandapotts/git/nltk/nltk/draw/tree.py__maintain_order,"def _maintain_order(self, child):
if self._horizontal:
return self._maintain_order_horizontal(child)
else:
return self._maintain_order_vertical(child)
",[],0,[],/draw/tree.py__maintain_order
3535,/home/amandapotts/git/nltk/nltk/draw/tree.py__maintain_order_vertical,"def _maintain_order_vertical(self, child):
(left, top, right, bot) = child.bbox()
if child is self._label:
for subtree in self._subtrees:
(x1, y1, x2, y2) = subtree.bbox()
if bot + self._yspace > y1:
subtree.move(0, bot + self._yspace - y1)
return self._subtrees
else:
moved = [child]
index = self._subtrees.index(child)
x = right + self._xspace
for i in range(index + 1, len(self._subtrees)):
(x1, y1, x2, y2) = self._subtrees[i].bbox()
if x > x1:
self._subtrees[i].move(x - x1, 0)
x += x2 - x1 + self._xspace
moved.append(self._subtrees[i])
x = left - self._xspace
for i in range(index - 1, -1, -1):
(x1, y1, x2, y2) = self._subtrees[i].bbox()
if x < x2:
self._subtrees[i].move(x - x2, 0)
x -= x2 - x1 + self._xspace
moved.append(self._subtrees[i])
(x1, y1, x2, y2) = self._label.bbox()
if y2 > top - self._yspace:
self._label.move(0, top - self._yspace - y2)
moved = self._subtrees
return moved
",[],0,[],/draw/tree.py__maintain_order_vertical
3536,/home/amandapotts/git/nltk/nltk/draw/tree.py__maintain_order_horizontal,"def _maintain_order_horizontal(self, child):
(left, top, right, bot) = child.bbox()
if child is self._label:
for subtree in self._subtrees:
(x1, y1, x2, y2) = subtree.bbox()
if right + self._xspace > x1:
subtree.move(right + self._xspace - x1)
return self._subtrees
else:
moved = [child]
index = self._subtrees.index(child)
y = bot + self._yspace
for i in range(index + 1, len(self._subtrees)):
(x1, y1, x2, y2) = self._subtrees[i].bbox()
if y > y1:
self._subtrees[i].move(0, y - y1)
y += y2 - y1 + self._yspace
moved.append(self._subtrees[i])
y = top - self._yspace
for i in range(index - 1, -1, -1):
(x1, y1, x2, y2) = self._subtrees[i].bbox()
if y < y2:
self._subtrees[i].move(0, y - y2)
y -= y2 - y1 + self._yspace
moved.append(self._subtrees[i])
(x1, y1, x2, y2) = self._label.bbox()
if x2 > left - self._xspace:
self._label.move(left - self._xspace - x2, 0)
moved = self._subtrees
return moved
",[],0,[],/draw/tree.py__maintain_order_horizontal
3537,/home/amandapotts/git/nltk/nltk/draw/tree.py__manage_horizontal,"def _manage_horizontal(self):
(nodex, nodey) = self._node_bottom()
y = 20
for subtree in self._subtrees:
subtree_bbox = subtree.bbox()
dx = nodex - subtree_bbox[0] + self._xspace
dy = y - subtree_bbox[1]
subtree.move(dx, dy)
y += subtree_bbox[3] - subtree_bbox[1] + self._yspace
center = 0.0
for subtree in self._subtrees:
center += self._subtree_top(subtree)[1]
center /= len(self._subtrees)
for subtree in self._subtrees:
subtree.move(0, nodey - center)
",[],0,[],/draw/tree.py__manage_horizontal
3538,/home/amandapotts/git/nltk/nltk/draw/tree.py__manage_vertical,"def _manage_vertical(self):
(nodex, nodey) = self._node_bottom()
x = 0
for subtree in self._subtrees:
subtree_bbox = subtree.bbox()
dy = nodey - subtree_bbox[1] + self._yspace
dx = x - subtree_bbox[0]
subtree.move(dx, dy)
x += subtree_bbox[2] - subtree_bbox[0] + self._xspace
center = 0.0
for subtree in self._subtrees:
center += self._subtree_top(subtree)[0] / len(self._subtrees)
for subtree in self._subtrees:
subtree.move(nodex - center, 0)
",[],0,[],/draw/tree.py__manage_vertical
3539,/home/amandapotts/git/nltk/nltk/draw/tree.py__manage,"def _manage(self):
self._managing = True
(nodex, nodey) = self._node_bottom()
if len(self._subtrees) == 0:
return
if self._horizontal:
self._manage_horizontal()
else:
self._manage_vertical()
for subtree in self._subtrees:
self._update(subtree)
self._managing = False
",[],0,[],/draw/tree.py__manage
3540,/home/amandapotts/git/nltk/nltk/draw/tree.py___repr__,"def __repr__(self):
return f""[TreeSeg {self._label}: {self._subtrees}]""
",[],0,[],/draw/tree.py___repr__
3541,/home/amandapotts/git/nltk/nltk/draw/tree.py__tree_to_treeseg,"def _tree_to_treeseg(
canvas,
t,
make_node,
make_leaf,
tree_attribs,
node_attribs,
leaf_attribs,
loc_attribs,
",[],0,[],/draw/tree.py__tree_to_treeseg
3542,/home/amandapotts/git/nltk/nltk/draw/tree.py_tree_to_treesegment,"def tree_to_treesegment(
canvas, t, make_node=TextWidget, make_leaf=TextWidget, **attribs
",[],0,[],/draw/tree.py_tree_to_treesegment
3543,/home/amandapotts/git/nltk/nltk/draw/tree.py___init__,"def __init__(
self, canvas, t, make_node=TextWidget, make_leaf=TextWidget, **attribs
",[],0,[],/draw/tree.py___init__
3544,/home/amandapotts/git/nltk/nltk/draw/tree.py_expanded_tree,"def expanded_tree(self, *path_to_tree):
""""""
Return the ``TreeSegmentWidget`` for the specified subtree.
:param path_to_tree: A list of indices i1, i2, ..., in, where
the desired widget is the widget corresponding to
``tree.children()[i1].children()[i2]....children()[in]``.
For the root, the path is ``()``.
""""""
return self._expanded_trees[path_to_tree]
",[],0,[],/draw/tree.py_expanded_tree
3545,/home/amandapotts/git/nltk/nltk/draw/tree.py_collapsed_tree,"def collapsed_tree(self, *path_to_tree):
""""""
Return the ``TreeSegmentWidget`` for the specified subtree.
:param path_to_tree: A list of indices i1, i2, ..., in, where
the desired widget is the widget corresponding to
``tree.children()[i1].children()[i2]....children()[in]``.
For the root, the path is ``()``.
""""""
return self._collapsed_trees[path_to_tree]
",[],0,[],/draw/tree.py_collapsed_tree
3546,/home/amandapotts/git/nltk/nltk/draw/tree.py_bind_click_trees,"def bind_click_trees(self, callback, button=1):
""""""
Add a binding to all tree segments.
""""""
for tseg in list(self._expanded_trees.values()):
tseg.bind_click(callback, button)
for tseg in list(self._collapsed_trees.values()):
tseg.bind_click(callback, button)
",[],0,[],/draw/tree.py_bind_click_trees
3547,/home/amandapotts/git/nltk/nltk/draw/tree.py_bind_drag_trees,"def bind_drag_trees(self, callback, button=1):
""""""
Add a binding to all tree segments.
""""""
for tseg in list(self._expanded_trees.values()):
tseg.bind_drag(callback, button)
for tseg in list(self._collapsed_trees.values()):
tseg.bind_drag(callback, button)
",[],0,[],/draw/tree.py_bind_drag_trees
3548,/home/amandapotts/git/nltk/nltk/draw/tree.py_bind_click_leaves,"def bind_click_leaves(self, callback, button=1):
""""""
Add a binding to all leaves.
""""""
for leaf in self._leaves:
leaf.bind_click(callback, button)
for leaf in self._leaves:
leaf.bind_click(callback, button)
",[],0,[],/draw/tree.py_bind_click_leaves
3549,/home/amandapotts/git/nltk/nltk/draw/tree.py_bind_drag_leaves,"def bind_drag_leaves(self, callback, button=1):
""""""
Add a binding to all leaves.
""""""
for leaf in self._leaves:
leaf.bind_drag(callback, button)
for leaf in self._leaves:
leaf.bind_drag(callback, button)
",[],0,[],/draw/tree.py_bind_drag_leaves
3550,/home/amandapotts/git/nltk/nltk/draw/tree.py_bind_click_nodes,"def bind_click_nodes(self, callback, button=1):
""""""
Add a binding to all nodes.
""""""
for node in self._nodes:
node.bind_click(callback, button)
for node in self._nodes:
node.bind_click(callback, button)
",[],0,[],/draw/tree.py_bind_click_nodes
3551,/home/amandapotts/git/nltk/nltk/draw/tree.py_bind_drag_nodes,"def bind_drag_nodes(self, callback, button=1):
""""""
Add a binding to all nodes.
""""""
for node in self._nodes:
node.bind_drag(callback, button)
for node in self._nodes:
node.bind_drag(callback, button)
",[],0,[],/draw/tree.py_bind_drag_nodes
3552,/home/amandapotts/git/nltk/nltk/draw/tree.py__make_collapsed_trees,"def _make_collapsed_trees(self, canvas, t, key):
if not isinstance(t, Tree):
return
make_node = self._make_node
make_leaf = self._make_leaf
node = make_node(canvas, t.label(), **self._nodeattribs)
self._nodes.append(node)
leaves = [make_leaf(canvas, l, **self._leafattribs) for l in t.leaves()]
self._leaves += leaves
treeseg = TreeSegmentWidget(
canvas,
node,
leaves,
roof=1,
color=self._roof_color,
fill=self._roof_fill,
width=self._line_width,
)
self._collapsed_trees[key] = treeseg
self._keys[treeseg] = key
treeseg.hide()
for i in range(len(t)):
child = t[i]
self._make_collapsed_trees(canvas, child, key + (i,))
",[],0,[],/draw/tree.py__make_collapsed_trees
3553,/home/amandapotts/git/nltk/nltk/draw/tree.py__make_expanded_tree,"def _make_expanded_tree(self, canvas, t, key):
make_node = self._make_node
make_leaf = self._make_leaf
if isinstance(t, Tree):
node = make_node(canvas, t.label(), **self._nodeattribs)
self._nodes.append(node)
children = t
subtrees = [
self._make_expanded_tree(canvas, children[i], key + (i,))
for i in range(len(children))
]
treeseg = TreeSegmentWidget(
canvas, node, subtrees, color=self._line_color, width=self._line_width
)
self._expanded_trees[key] = treeseg
self._keys[treeseg] = key
return treeseg
else:
leaf = make_leaf(canvas, t, **self._leafattribs)
self._leaves.append(leaf)
return leaf
",[],0,[],/draw/tree.py__make_expanded_tree
3554,/home/amandapotts/git/nltk/nltk/draw/tree.py___setitem__,"def __setitem__(self, attr, value):
if attr[:5] == ""node_"":
for node in self._nodes:
node[attr[5:]] = value
elif attr[:5] == ""leaf_"":
for leaf in self._leaves:
leaf[attr[5:]] = value
elif attr == ""line_color"":
self._line_color = value
for tseg in list(self._expanded_trees.values()):
tseg[""color""] = value
elif attr == ""line_width"":
self._line_width = value
for tseg in list(self._expanded_trees.values()):
tseg[""width""] = value
for tseg in list(self._collapsed_trees.values()):
tseg[""width""] = value
elif attr == ""roof_color"":
self._roof_color = value
for tseg in list(self._collapsed_trees.values()):
tseg[""color""] = value
elif attr == ""roof_fill"":
self._roof_fill = value
for tseg in list(self._collapsed_trees.values()):
tseg[""fill""] = value
elif attr == ""shapeable"":
self._shapeable = value
for tseg in list(self._expanded_trees.values()):
tseg[""draggable""] = value
for tseg in list(self._collapsed_trees.values()):
tseg[""draggable""] = value
for leaf in self._leaves:
leaf[""draggable""] = value
elif attr == ""xspace"":
self._xspace = value
for tseg in list(self._expanded_trees.values()):
tseg[""xspace""] = value
for tseg in list(self._collapsed_trees.values()):
tseg[""xspace""] = value
self.manage()
elif attr == ""yspace"":
self._yspace = value
for tseg in list(self._expanded_trees.values()):
tseg[""yspace""] = value
for tseg in list(self._collapsed_trees.values()):
tseg[""yspace""] = value
self.manage()
elif attr == ""orientation"":
self._orientation = value
for tseg in list(self._expanded_trees.values()):
tseg[""orientation""] = value
for tseg in list(self._collapsed_trees.values()):
tseg[""orientation""] = value
self.manage()
elif attr == ""ordered"":
self._ordered = value
for tseg in list(self._expanded_trees.values()):
tseg[""ordered""] = value
for tseg in list(self._collapsed_trees.values()):
tseg[""ordered""] = value
else:
CanvasWidget.__setitem__(self, attr, value)
",[],0,[],/draw/tree.py___setitem__
3555,/home/amandapotts/git/nltk/nltk/draw/tree.py___getitem__,"def __getitem__(self, attr):
if attr[:5] == ""node_"":
return self._nodeattribs.get(attr[5:], None)
elif attr[:5] == ""leaf_"":
return self._leafattribs.get(attr[5:], None)
elif attr[:4] == ""loc_"":
return self._locattribs.get(attr[4:], None)
elif attr == ""line_color"":
return self._line_color
elif attr == ""line_width"":
return self._line_width
elif attr == ""roof_color"":
return self._roof_color
elif attr == ""roof_fill"":
return self._roof_fill
elif attr == ""shapeable"":
return self._shapeable
elif attr == ""xspace"":
return self._xspace
elif attr == ""yspace"":
return self._yspace
elif attr == ""orientation"":
return self._orientation
else:
return CanvasWidget.__getitem__(self, attr)
",[],0,[],/draw/tree.py___getitem__
3556,/home/amandapotts/git/nltk/nltk/draw/tree.py__tags,"def _tags(self):
return []
",[],0,[],/draw/tree.py__tags
3557,/home/amandapotts/git/nltk/nltk/draw/tree.py__manage,"def _manage(self):
segs = list(self._expanded_trees.values()) + list(
self._collapsed_trees.values()
)
for tseg in segs:
if tseg.hidden():
tseg.show()
tseg.manage()
tseg.hide()
",[],0,[],/draw/tree.py__manage
3558,/home/amandapotts/git/nltk/nltk/draw/tree.py_toggle_collapsed,"def toggle_collapsed(self, treeseg):
""""""
Collapse/expand a tree.
""""""
old_treeseg = treeseg
if old_treeseg[""roof""]:
new_treeseg = self._expanded_trees[self._keys[old_treeseg]]
else:
new_treeseg = self._collapsed_trees[self._keys[old_treeseg]]
if old_treeseg.parent() is self:
self._remove_child_widget(old_treeseg)
self._add_child_widget(new_treeseg)
self._treeseg = new_treeseg
else:
old_treeseg.parent().replace_child(old_treeseg, new_treeseg)
new_treeseg.show()
(newx, newy) = new_treeseg.label().bbox()[:2]
(oldx, oldy) = old_treeseg.label().bbox()[:2]
new_treeseg.move(oldx - newx, oldy - newy)
old_treeseg.hide()
new_treeseg.parent().update(new_treeseg)
",[],0,[],/draw/tree.py_toggle_collapsed
3559,/home/amandapotts/git/nltk/nltk/draw/tree.py___init__,"def __init__(self, *trees):
from math import ceil, sqrt
self._trees = trees
self._top = Tk()
self._top.title(""NLTK"")
self._top.bind(""<Control-x>"", self.destroy)
self._top.bind(""<Control-q>"", self.destroy)
cf = self._cframe = CanvasFrame(self._top)
self._top.bind(""<Control-p>"", self._cframe.print_to_file)
self._size = IntVar(self._top)
self._size.set(12)
bold = (""helvetica"", -self._size.get(), ""bold"")
helv = (""helvetica"", -self._size.get())
self._width = int(ceil(sqrt(len(trees))))
self._widgets = []
for i in range(len(trees)):
widget = TreeWidget(
cf.canvas(),
trees[i],
node_font=bold,
leaf_color=""#008040"",
node_color=""#004080"",
roof_color=""#004040"",
roof_fill=""white"",
line_color=""#004040"",
draggable=1,
leaf_font=helv,
)
widget.bind_click_trees(widget.toggle_collapsed)
self._widgets.append(widget)
cf.add_widget(widget, 0, 0)
self._layout()
self._cframe.pack(expand=1, fill=""both"")
self._init_menubar()
",[],0,[],/draw/tree.py___init__
3560,/home/amandapotts/git/nltk/nltk/draw/tree.py__layout,"def _layout(self):
i = x = y = ymax = 0
width = self._width
for i in range(len(self._widgets)):
widget = self._widgets[i]
(oldx, oldy) = widget.bbox()[:2]
if i % width == 0:
y = ymax
x = 0
widget.move(x - oldx, y - oldy)
x = widget.bbox()[2] + 10
ymax = max(ymax, widget.bbox()[3] + 10)
",[],0,[],/draw/tree.py__layout
3561,/home/amandapotts/git/nltk/nltk/draw/tree.py__init_menubar,"def _init_menubar(self):
menubar = Menu(self._top)
filemenu = Menu(menubar, tearoff=0)
filemenu.add_command(
label=""Print to Postscript"",
underline=0,
command=self._cframe.print_to_file,
accelerator=""Ctrl-p"",
)
filemenu.add_command(
label=""Exit"", underline=1, command=self.destroy, accelerator=""Ctrl-x""
)
menubar.add_cascade(label=""File"", underline=0, menu=filemenu)
zoommenu = Menu(menubar, tearoff=0)
zoommenu.add_radiobutton(
label=""Tiny"",
variable=self._size,
underline=0,
value=10,
command=self.resize,
)
zoommenu.add_radiobutton(
label=""Small"",
variable=self._size,
underline=0,
value=12,
command=self.resize,
)
zoommenu.add_radiobutton(
label=""Medium"",
variable=self._size,
underline=0,
value=14,
command=self.resize,
)
zoommenu.add_radiobutton(
label=""Large"",
variable=self._size,
underline=0,
value=28,
command=self.resize,
)
zoommenu.add_radiobutton(
label=""Huge"",
variable=self._size,
underline=0,
value=50,
command=self.resize,
)
menubar.add_cascade(label=""Zoom"", underline=0, menu=zoommenu)
self._top.config(menu=menubar)
",[],0,[],/draw/tree.py__init_menubar
3562,/home/amandapotts/git/nltk/nltk/draw/tree.py_resize,"def resize(self, *e):
bold = (""helvetica"", -self._size.get(), ""bold"")
helv = (""helvetica"", -self._size.get())
xspace = self._size.get()
yspace = self._size.get()
for widget in self._widgets:
widget[""node_font""] = bold
widget[""leaf_font""] = helv
widget[""xspace""] = xspace
widget[""yspace""] = yspace
if self._size.get() < 20:
widget[""line_width""] = 1
elif self._size.get() < 30:
widget[""line_width""] = 2
else:
widget[""line_width""] = 3
self._layout()
",[],0,[],/draw/tree.py_resize
3563,/home/amandapotts/git/nltk/nltk/draw/tree.py_destroy,"def destroy(self, *e):
if self._top is None:
return
self._top.destroy()
self._top = None
",[],0,[],/draw/tree.py_destroy
3564,/home/amandapotts/git/nltk/nltk/draw/tree.py_mainloop,"def mainloop(self, *args, **kwargs):
""""""
Enter the Tkinter mainloop.  This function must be called if
this demo is created from a non-interactive program (e.g.
from a secript)
the script completes.
""""""
if in_idle():
return
self._top.mainloop(*args, **kwargs)
",[],0,[],/draw/tree.py_mainloop
3565,/home/amandapotts/git/nltk/nltk/draw/tree.py_draw_trees,"def draw_trees(*trees):
""""""
Open a new window containing a graphical diagram of the given
trees.
:rtype: None
""""""
TreeView(*trees).mainloop()
return
",[],0,[],/draw/tree.py_draw_trees
3566,/home/amandapotts/git/nltk/nltk/draw/tree.py_demo,"def demo():
import random
",[],0,[],/draw/tree.py_demo
3567,/home/amandapotts/git/nltk/nltk/draw/tree.py_fill,"def fill(cw):
cw[""fill""] = ""#%06d"" % random.randint(0, 999999)
",[],0,[],/draw/tree.py_fill
3568,/home/amandapotts/git/nltk/nltk/draw/tree.py_boxit,"def boxit(canvas, text):
big = (""helvetica"", -16, ""bold"")
return BoxWidget(canvas, TextWidget(canvas, text, font=big), fill=""green"")
",[],0,[],/draw/tree.py_boxit
3569,/home/amandapotts/git/nltk/nltk/draw/tree.py_ovalit,"def ovalit(canvas, text):
return OvalWidget(canvas, TextWidget(canvas, text), fill=""cyan"")
",[],0,[],/draw/tree.py_ovalit
3570,/home/amandapotts/git/nltk/nltk/draw/tree.py_color,"def color(node):
node[""color""] = ""#%04d00"" % random.randint(0, 9999)
",[],0,[],/draw/tree.py_color
3571,/home/amandapotts/git/nltk/nltk/draw/tree.py_color2,"def color2(treeseg):
treeseg.label()[""fill""] = ""#%06d"" % random.randint(0, 9999)
treeseg.label().child()[""color""] = ""white""
",[],0,[],/draw/tree.py_color2
3572,/home/amandapotts/git/nltk/nltk/draw/tree.py_orientswitch,"def orientswitch(treewidget):
if treewidget[""orientation""] == ""horizontal"":
treewidget.expanded_tree(1, 1).subtrees()[0].set_text(""vertical"")
treewidget.collapsed_tree(1, 1).subtrees()[0].set_text(""vertical"")
treewidget.collapsed_tree(1).subtrees()[1].set_text(""vertical"")
treewidget.collapsed_tree().subtrees()[3].set_text(""vertical"")
treewidget[""orientation""] = ""vertical""
else:
treewidget.expanded_tree(1, 1).subtrees()[0].set_text(""horizontal"")
treewidget.collapsed_tree(1, 1).subtrees()[0].set_text(""horizontal"")
treewidget.collapsed_tree(1).subtrees()[1].set_text(""horizontal"")
treewidget.collapsed_tree().subtrees()[3].set_text(""horizontal"")
treewidget[""orientation""] = ""horizontal""
",[],0,[],/draw/tree.py_orientswitch
3573,/home/amandapotts/git/nltk/nltk/draw/dispersion.py_dispersion_plot,"def dispersion_plot(text, words, ignore_case=False, title=""Lexical Dispersion Plot""):
""""""
Generate a lexical dispersion plot.
:param text: The source text
:type text: list(str) or iter(str)
:param words: The target words
:type words: list of str
:param ignore_case: flag to set if case should be ignored when searching text
:type ignore_case: bool
:return: a matplotlib Axes object that may still be modified before plotting
:rtype: Axes
""""""
try:
import matplotlib.pyplot as plt
except ImportError as e:
raise ImportError(
""The plot function requires matplotlib to be installed. ""
""See https://matplotlib.org/""
) from e
word2y = {
word.casefold() if ignore_case else word: y
for y, word in enumerate(reversed(words))
}
xs, ys = [], []
for x, token in enumerate(text):
token = token.casefold() if ignore_case else token
y = word2y.get(token)
if y is not None:
xs.append(x)
ys.append(y)
words = words[::-1]
_, ax = plt.subplots()
ax.plot(xs, ys, ""|"")
ax.dataLim.x0, ax.dataLim.x1 = 0, len(text) - 1
ax.autoscale(axis=""x"")
ax.set_yticks(list(range(len(words))), words, color=""C0"")
ax.set_ylim(-1, len(words))
ax.set_title(title)
ax.set_xlabel(""Word Offset"")
return ax
",[],0,[],/draw/dispersion.py_dispersion_plot
3574,/home/amandapotts/git/nltk/nltk/draw/table.py__resize_column,"def _resize_column(self, event):
""""""
Callback used to resize a column of the table.  Return ``True``
if the column is actually getting resized (if the user clicked
on the far left or far right 5 pixels of a label)
``False`` otherwies.
""""""
if event.widget.bind(""<ButtonRelease>""):
return False
self._resize_column_index = None
if event.widget is self:
for i, lb in enumerate(self._listboxes):
if abs(event.x - (lb.winfo_x() + lb.winfo_width())) < 10:
self._resize_column_index = i
elif event.x > (event.widget.winfo_width() - 5):
self._resize_column_index = event.widget.column_index
elif event.x < 5 and event.widget.column_index != 0:
self._resize_column_index = event.widget.column_index - 1
if self._resize_column_index is not None:
event.widget.bind(""<Motion>"", self._resize_column_motion_cb)
event.widget.bind(
""<ButtonRelease-%d>"" % event.num, self._resize_column_buttonrelease_cb
)
return True
else:
return False
",[],0,[],/draw/table.py__resize_column
3575,/home/amandapotts/git/nltk/nltk/draw/table.py__resize_column_motion_cb,"def _resize_column_motion_cb(self, event):
lb = self._listboxes[self._resize_column_index]
charwidth = lb.winfo_width() / lb[""width""]
x1 = event.x + event.widget.winfo_x()
x2 = lb.winfo_x() + lb.winfo_width()
lb[""width""] = max(3, int(lb[""width""] + (x1 - x2) // charwidth))
",[],0,[],/draw/table.py__resize_column_motion_cb
3576,/home/amandapotts/git/nltk/nltk/draw/table.py__resize_column_buttonrelease_cb,"def _resize_column_buttonrelease_cb(self, event):
event.widget.unbind(""<ButtonRelease-%d>"" % event.num)
event.widget.unbind(""<Motion>"")
",[],0,[],/draw/table.py__resize_column_buttonrelease_cb
3577,/home/amandapotts/git/nltk/nltk/draw/table.py_column_names,"def column_names(self):
""""""
A tuple containing the names of the columns used by this
multi-column listbox.
""""""
return self._column_names
",[],0,[],/draw/table.py_column_names
3578,/home/amandapotts/git/nltk/nltk/draw/table.py_column_labels,"def column_labels(self):
""""""
A tuple containing the ``Tkinter.Label`` widgets used to
display the label of each column.  If this multi-column
listbox was created without labels, then this will be an empty
tuple.  These widgets will all be augmented with a
``column_index`` attribute, which can be used to determine
which column they correspond to.  This can be convenient,
e.g., when defining callbacks for bound events.
""""""
return tuple(self._labels)
",[],0,[],/draw/table.py_column_labels
3579,/home/amandapotts/git/nltk/nltk/draw/table.py_listboxes,"def listboxes(self):
""""""
A tuple containing the ``Tkinter.Listbox`` widgets used to
display individual columns.  These widgets will all be
augmented with a ``column_index`` attribute, which can be used
to determine which column they correspond to.  This can be
convenient, e.g., when defining callbacks for bound events.
""""""
return tuple(self._listboxes)
",[],0,[],/draw/table.py_listboxes
3580,/home/amandapotts/git/nltk/nltk/draw/table.py__select,"def _select(self, e):
i = e.widget.nearest(e.y)
self.selection_clear(0, ""end"")
self.selection_set(i)
self.activate(i)
self.focus()
",[],0,[],/draw/table.py__select
3581,/home/amandapotts/git/nltk/nltk/draw/table.py__scroll,"def _scroll(self, delta):
for lb in self._listboxes:
lb.yview_scroll(delta, ""unit"")
return ""break""
",[],0,[],/draw/table.py__scroll
3582,/home/amandapotts/git/nltk/nltk/draw/table.py__pagesize,"def _pagesize(self):
"""""":return: The number of rows that makes up one page""""""
return int(self.index(""@0,1000000"")) - int(self.index(""@0,0""))
",[],0,[],/draw/table.py__pagesize
3583,/home/amandapotts/git/nltk/nltk/draw/table.py_select,"def select(self, index=None, delta=None, see=True):
""""""
Set the selected row.  If ``index`` is specified, then select
row ``index``.  Otherwise, if ``delta`` is specified, then move
the current selection by ``delta`` (negative numbers for up,
positive numbers for down).  This will not move the selection
past the top or the bottom of the list.
:param see: If true, then call ``self.see()`` with the newly
selected index, to ensure that it is visible.
""""""
if (index is not None) and (delta is not None):
raise ValueError(""specify index or delta, but not both"")
if delta is not None:
if len(self.curselection()) == 0:
index = -1 + delta
else:
index = int(self.curselection()[0]) + delta
self.selection_clear(0, ""end"")
if index is not None:
index = min(max(index, 0), self.size() - 1)
self.selection_set(index)
if see:
self.see(index)
",[],0,[],/draw/table.py_select
3584,/home/amandapotts/git/nltk/nltk/draw/table.py_configure,"def configure(self, cnf={}, **kw):
""""""
Configure this widget.  Use ``label_*`` to configure all
labels
>>> master = Tk()  # doctest: +SKIP
>>> mlb = MultiListbox(master, 5)  # doctest: +SKIP
>>> mlb.configure(label_foreground='red')  # doctest: +SKIP
>>> mlb.configure(listbox_foreground='red')  # doctest: +SKIP
""""""
cnf = dict(list(cnf.items()) + list(kw.items()))
for key, val in list(cnf.items()):
if key.startswith(""label_"") or key.startswith(""label-""):
for label in self._labels:
label.configure({key[6:]: val})
elif key.startswith(""listbox_"") or key.startswith(""listbox-""):
for listbox in self._listboxes:
listbox.configure({key[8:]: val})
else:
Frame.configure(self, {key: val})
",[],0,[],/draw/table.py_configure
3585,/home/amandapotts/git/nltk/nltk/draw/table.py___setitem__,"def __setitem__(self, key, val):
""""""
Configure this widget.  This is equivalent to
``self.configure({key,val``)}.  See ``configure()``.
""""""
self.configure({key: val})
",[],0,[],/draw/table.py___setitem__
3586,/home/amandapotts/git/nltk/nltk/draw/table.py_rowconfigure,"def rowconfigure(self, row_index, cnf={}, **kw):
""""""
Configure all table cells in the given row.  Valid keyword
arguments are: ``background``, ``bg``, ``foreground``, ``fg``,
``selectbackground``, ``selectforeground``.
""""""
for lb in self._listboxes:
lb.itemconfigure(row_index, cnf, **kw)
",[],0,[],/draw/table.py_rowconfigure
3587,/home/amandapotts/git/nltk/nltk/draw/table.py_columnconfigure,"def columnconfigure(self, col_index, cnf={}, **kw):
""""""
Configure all table cells in the given column.  Valid keyword
arguments are: ``background``, ``bg``, ``foreground``, ``fg``,
``selectbackground``, ``selectforeground``.
""""""
lb = self._listboxes[col_index]
cnf = dict(list(cnf.items()) + list(kw.items()))
for key, val in list(cnf.items()):
if key in (
""background"",
""bg"",
""foreground"",
""fg"",
""selectbackground"",
""selectforeground"",
):
for i in range(lb.size()):
lb.itemconfigure(i, {key: val})
else:
lb.configure({key: val})
",[],0,[],/draw/table.py_columnconfigure
3588,/home/amandapotts/git/nltk/nltk/draw/table.py_itemconfigure,"def itemconfigure(self, row_index, col_index, cnf=None, **kw):
""""""
Configure the table cell at the given row and column.  Valid
keyword arguments are: ``background``, ``bg``, ``foreground``,
``fg``, ``selectbackground``, ``selectforeground``.
""""""
lb = self._listboxes[col_index]
return lb.itemconfigure(row_index, cnf, **kw)
",[],0,[],/draw/table.py_itemconfigure
3589,/home/amandapotts/git/nltk/nltk/draw/table.py_insert,"def insert(self, index, *rows):
""""""
Insert the given row or rows into the table, at the given
index.  Each row value should be a tuple of cell values, one
for each column in the row.  Index may be an integer or any of
the special strings (such as ``'end'``) accepted by
``Tkinter.Listbox``.
""""""
for elt in rows:
if len(elt) != len(self._column_names):
raise ValueError(
""rows should be tuples whose length ""
""is equal to the number of columns""
)
for lb, elts in zip(self._listboxes, list(zip(*rows))):
lb.insert(index, *elts)
",[],0,[],/draw/table.py_insert
3590,/home/amandapotts/git/nltk/nltk/draw/table.py_get,"def get(self, first, last=None):
""""""
Return the value(s) of the specified row(s).  If ``last`` is
not specified, then return a single row value
return a list of row values.  Each row value is a tuple of
cell values, one for each column in the row.
""""""
values = [lb.get(first, last) for lb in self._listboxes]
if last:
return [tuple(row) for row in zip(*values)]
else:
return tuple(values)
",[],0,[],/draw/table.py_get
3591,/home/amandapotts/git/nltk/nltk/draw/table.py_bbox,"def bbox(self, row, col):
""""""
Return the bounding box for the given table cell, relative to
this widget's top-left corner.  The bounding box is a tuple
of integers ``(left, top, width, height)``.
""""""
dx, dy, _, _ = self.grid_bbox(row=0, column=col)
x, y, w, h = self._listboxes[col].bbox(row)
return int(x) + int(dx), int(y) + int(dy), int(w), int(h)
",[],0,[],/draw/table.py_bbox
3592,/home/amandapotts/git/nltk/nltk/draw/table.py_hide_column,"def hide_column(self, col_index):
""""""
Hide the given column.  The column's state is still
maintained: its values will still be returned by ``get()``, and
you must supply its values when calling ``insert()``.  It is
safe to call this on a column that is already hidden.
:see: ``show_column()``
""""""
if self._labels:
self._labels[col_index].grid_forget()
self.listboxes[col_index].grid_forget()
self.grid_columnconfigure(col_index, weight=0)
",[],0,[],/draw/table.py_hide_column
3593,/home/amandapotts/git/nltk/nltk/draw/table.py_show_column,"def show_column(self, col_index):
""""""
Display a column that has been hidden using ``hide_column()``.
It is safe to call this on a column that is not hidden.
""""""
weight = self._column_weights[col_index]
if self._labels:
self._labels[col_index].grid(
column=col_index, row=0, sticky=""news"", padx=0, pady=0
)
self._listboxes[col_index].grid(
column=col_index, row=1, sticky=""news"", padx=0, pady=0
)
self.grid_columnconfigure(col_index, weight=weight)
",[],0,[],/draw/table.py_show_column
3594,/home/amandapotts/git/nltk/nltk/draw/table.py_bind_to_labels,"def bind_to_labels(self, sequence=None, func=None, add=None):
""""""
Add a binding to each ``Tkinter.Label`` widget in this
mult-column listbox that will call ``func`` in response to the
event sequence.
:return: A list of the identifiers of replaced binding
functions (if any), allowing for their deletion (to
prevent a memory leak).
""""""
return [label.bind(sequence, func, add) for label in self.column_labels]
",[],0,[],/draw/table.py_bind_to_labels
3595,/home/amandapotts/git/nltk/nltk/draw/table.py_bind_to_listboxes,"def bind_to_listboxes(self, sequence=None, func=None, add=None):
""""""
Add a binding to each ``Tkinter.Listbox`` widget in this
mult-column listbox that will call ``func`` in response to the
event sequence.
:return: A list of the identifiers of replaced binding
functions (if any), allowing for their deletion (to
prevent a memory leak).
""""""
for listbox in self.listboxes:
listbox.bind(sequence, func, add)
",[],0,[],/draw/table.py_bind_to_listboxes
3596,/home/amandapotts/git/nltk/nltk/draw/table.py_bind_to_columns,"def bind_to_columns(self, sequence=None, func=None, add=None):
""""""
Add a binding to each ``Tkinter.Label`` and ``Tkinter.Listbox``
widget in this mult-column listbox that will call ``func`` in
response to the event sequence.
:return: A list of the identifiers of replaced binding
functions (if any), allowing for their deletion (to
prevent a memory leak).
""""""
return self.bind_to_labels(sequence, func, add) + self.bind_to_listboxes(
sequence, func, add
)
",[],0,[],/draw/table.py_bind_to_columns
3597,/home/amandapotts/git/nltk/nltk/draw/table.py_curselection,"def curselection(self, *args, **kwargs):
return self._listboxes[0].curselection(*args, **kwargs)
",[],0,[],/draw/table.py_curselection
3598,/home/amandapotts/git/nltk/nltk/draw/table.py_selection_includes,"def selection_includes(self, *args, **kwargs):
return self._listboxes[0].selection_includes(*args, **kwargs)
",[],0,[],/draw/table.py_selection_includes
3599,/home/amandapotts/git/nltk/nltk/draw/table.py_itemcget,"def itemcget(self, *args, **kwargs):
return self._listboxes[0].itemcget(*args, **kwargs)
",[],0,[],/draw/table.py_itemcget
3600,/home/amandapotts/git/nltk/nltk/draw/table.py_size,"def size(self, *args, **kwargs):
return self._listboxes[0].size(*args, **kwargs)
",[],0,[],/draw/table.py_size
3601,/home/amandapotts/git/nltk/nltk/draw/table.py_index,"def index(self, *args, **kwargs):
return self._listboxes[0].index(*args, **kwargs)
",[],0,[],/draw/table.py_index
3602,/home/amandapotts/git/nltk/nltk/draw/table.py_nearest,"def nearest(self, *args, **kwargs):
return self._listboxes[0].nearest(*args, **kwargs)
",[],0,[],/draw/table.py_nearest
3603,/home/amandapotts/git/nltk/nltk/draw/table.py_activate,"def activate(self, *args, **kwargs):
for lb in self._listboxes:
lb.activate(*args, **kwargs)
",[],0,[],/draw/table.py_activate
3604,/home/amandapotts/git/nltk/nltk/draw/table.py_delete,"def delete(self, *args, **kwargs):
for lb in self._listboxes:
lb.delete(*args, **kwargs)
",[],0,[],/draw/table.py_delete
3605,/home/amandapotts/git/nltk/nltk/draw/table.py_scan_mark,"def scan_mark(self, *args, **kwargs):
for lb in self._listboxes:
lb.scan_mark(*args, **kwargs)
",[],0,[],/draw/table.py_scan_mark
3606,/home/amandapotts/git/nltk/nltk/draw/table.py_scan_dragto,"def scan_dragto(self, *args, **kwargs):
for lb in self._listboxes:
lb.scan_dragto(*args, **kwargs)
",[],0,[],/draw/table.py_scan_dragto
3607,/home/amandapotts/git/nltk/nltk/draw/table.py_see,"def see(self, *args, **kwargs):
for lb in self._listboxes:
lb.see(*args, **kwargs)
",[],0,[],/draw/table.py_see
3608,/home/amandapotts/git/nltk/nltk/draw/table.py_selection_anchor,"def selection_anchor(self, *args, **kwargs):
for lb in self._listboxes:
lb.selection_anchor(*args, **kwargs)
",[],0,[],/draw/table.py_selection_anchor
3609,/home/amandapotts/git/nltk/nltk/draw/table.py_selection_clear,"def selection_clear(self, *args, **kwargs):
for lb in self._listboxes:
lb.selection_clear(*args, **kwargs)
",[],0,[],/draw/table.py_selection_clear
3610,/home/amandapotts/git/nltk/nltk/draw/table.py_selection_set,"def selection_set(self, *args, **kwargs):
for lb in self._listboxes:
lb.selection_set(*args, **kwargs)
",[],0,[],/draw/table.py_selection_set
3611,/home/amandapotts/git/nltk/nltk/draw/table.py_yview,"def yview(self, *args, **kwargs):
for lb in self._listboxes:
v = lb.yview(*args, **kwargs)
return v  # if called with no arguments
",[],0,[],/draw/table.py_yview
3612,/home/amandapotts/git/nltk/nltk/draw/table.py_yview_moveto,"def yview_moveto(self, *args, **kwargs):
for lb in self._listboxes:
lb.yview_moveto(*args, **kwargs)
",[],0,[],/draw/table.py_yview_moveto
3613,/home/amandapotts/git/nltk/nltk/draw/table.py_yview_scroll,"def yview_scroll(self, *args, **kwargs):
for lb in self._listboxes:
lb.yview_scroll(*args, **kwargs)
",[],0,[],/draw/table.py_yview_scroll
3614,/home/amandapotts/git/nltk/nltk/draw/table.py___init__,"def __init__(
self,
master,
column_names,
rows=None,
column_weights=None,
scrollbar=True,
click_to_sort=True,
reprfunc=None,
cnf={},
",[],0,[],/draw/table.py___init__
3615,/home/amandapotts/git/nltk/nltk/draw/table.py_pack,"def pack(self, *args, **kwargs):
""""""Position this table's main frame widget in its parent
widget.  See ``Tkinter.Frame.pack()`` for more info.""""""
self._frame.pack(*args, **kwargs)
",[],0,[],/draw/table.py_pack
3616,/home/amandapotts/git/nltk/nltk/draw/table.py_grid,"def grid(self, *args, **kwargs):
""""""Position this table's main frame widget in its parent
widget.  See ``Tkinter.Frame.grid()`` for more info.""""""
self._frame.grid(*args, **kwargs)
",[],0,[],/draw/table.py_grid
3617,/home/amandapotts/git/nltk/nltk/draw/table.py_focus,"def focus(self):
""""""Direct (keyboard) input foxus to this widget.""""""
self._mlb.focus()
",[],0,[],/draw/table.py_focus
3618,/home/amandapotts/git/nltk/nltk/draw/table.py_bind,"def bind(self, sequence=None, func=None, add=None):
""""""Add a binding to this table's main frame that will call
``func`` in response to the event sequence.""""""
self._mlb.bind(sequence, func, add)
",[],0,[],/draw/table.py_bind
3619,/home/amandapotts/git/nltk/nltk/draw/table.py_rowconfigure,"def rowconfigure(self, row_index, cnf={}, **kw):
"""""":see: ``MultiListbox.rowconfigure()``""""""
self._mlb.rowconfigure(row_index, cnf, **kw)
",[],0,[],/draw/table.py_rowconfigure
3620,/home/amandapotts/git/nltk/nltk/draw/table.py_columnconfigure,"def columnconfigure(self, col_index, cnf={}, **kw):
"""""":see: ``MultiListbox.columnconfigure()``""""""
col_index = self.column_index(col_index)
self._mlb.columnconfigure(col_index, cnf, **kw)
",[],0,[],/draw/table.py_columnconfigure
3621,/home/amandapotts/git/nltk/nltk/draw/table.py_itemconfigure,"def itemconfigure(self, row_index, col_index, cnf=None, **kw):
"""""":see: ``MultiListbox.itemconfigure()``""""""
col_index = self.column_index(col_index)
return self._mlb.itemconfigure(row_index, col_index, cnf, **kw)
",[],0,[],/draw/table.py_itemconfigure
3622,/home/amandapotts/git/nltk/nltk/draw/table.py_bind_to_labels,"def bind_to_labels(self, sequence=None, func=None, add=None):
"""""":see: ``MultiListbox.bind_to_labels()``""""""
return self._mlb.bind_to_labels(sequence, func, add)
",[],0,[],/draw/table.py_bind_to_labels
3623,/home/amandapotts/git/nltk/nltk/draw/table.py_bind_to_listboxes,"def bind_to_listboxes(self, sequence=None, func=None, add=None):
"""""":see: ``MultiListbox.bind_to_listboxes()``""""""
return self._mlb.bind_to_listboxes(sequence, func, add)
",[],0,[],/draw/table.py_bind_to_listboxes
3624,/home/amandapotts/git/nltk/nltk/draw/table.py_bind_to_columns,"def bind_to_columns(self, sequence=None, func=None, add=None):
"""""":see: ``MultiListbox.bind_to_columns()``""""""
return self._mlb.bind_to_columns(sequence, func, add)
",[],0,[],/draw/table.py_bind_to_columns
3625,/home/amandapotts/git/nltk/nltk/draw/table.py_insert,"def insert(self, row_index, rowvalue):
""""""
Insert a new row into the table, so that its row index will be
``row_index``.  If the table contains any rows whose row index
is greater than or equal to ``row_index``, then they will be
shifted down.
:param rowvalue: A tuple of cell values, one for each column
in the new row.
""""""
self._checkrow(rowvalue)
self._rows.insert(row_index, rowvalue)
if self._reprfunc is not None:
rowvalue = [
self._reprfunc(row_index, j, v) for (j, v) in enumerate(rowvalue)
]
self._mlb.insert(row_index, rowvalue)
if self._DEBUG:
self._check_table_vs_mlb()
",[],0,[],/draw/table.py_insert
3626,/home/amandapotts/git/nltk/nltk/draw/table.py_extend,"def extend(self, rowvalues):
""""""
Add new rows at the end of the table.
:param rowvalues: A list of row values used to initialize the
table.  Each row value should be a tuple of cell values,
one for each column in the row.
""""""
for rowvalue in rowvalues:
self.append(rowvalue)
if self._DEBUG:
self._check_table_vs_mlb()
",[],0,[],/draw/table.py_extend
3627,/home/amandapotts/git/nltk/nltk/draw/table.py_append,"def append(self, rowvalue):
""""""
Add a new row to the end of the table.
:param rowvalue: A tuple of cell values, one for each column
in the new row.
""""""
self.insert(len(self._rows), rowvalue)
if self._DEBUG:
self._check_table_vs_mlb()
",[],0,[],/draw/table.py_append
3628,/home/amandapotts/git/nltk/nltk/draw/table.py_clear,"def clear(self):
""""""
Delete all rows in this table.
""""""
self._rows = []
self._mlb.delete(0, ""end"")
if self._DEBUG:
self._check_table_vs_mlb()
",[],0,[],/draw/table.py_clear
3629,/home/amandapotts/git/nltk/nltk/draw/table.py___getitem__,"def __getitem__(self, index):
""""""
Return the value of a row or a cell in this table.  If
``index`` is an integer, then the row value for the ``index``th
row.  This row value consists of a tuple of cell values, one
for each column in the row.  If ``index`` is a tuple of two
integers, ``(i,j)``, then return the value of the cell in the
``i``th row and the ``j``th column.
""""""
if isinstance(index, slice):
raise ValueError(""Slicing not supported"")
elif isinstance(index, tuple) and len(index) == 2:
return self._rows[index[0]][self.column_index(index[1])]
else:
return tuple(self._rows[index])
",[],0,[],/draw/table.py___getitem__
3630,/home/amandapotts/git/nltk/nltk/draw/table.py___setitem__,"def __setitem__(self, index, val):
""""""
Replace the value of a row or a cell in this table with
``val``.
If ``index`` is an integer, then ``val`` should be a row value
(i.e., a tuple of cell values, one for each column).  In this
case, the values of the ``index``th row of the table will be
replaced with the values in ``val``.
If ``index`` is a tuple of integers, ``(i,j)``, then replace the
value of the cell in the ``i``th row and ``j``th column with
``val``.
""""""
if isinstance(index, slice):
raise ValueError(""Slicing not supported"")
elif isinstance(index, tuple) and len(index) == 2:
i, j = index[0], self.column_index(index[1])
config_cookie = self._save_config_info([i])
self._rows[i][j] = val
if self._reprfunc is not None:
val = self._reprfunc(i, j, val)
self._mlb.listboxes[j].insert(i, val)
self._mlb.listboxes[j].delete(i + 1)
self._restore_config_info(config_cookie)
else:
config_cookie = self._save_config_info([index])
self._checkrow(val)
self._rows[index] = list(val)
if self._reprfunc is not None:
val = [self._reprfunc(index, j, v) for (j, v) in enumerate(val)]
self._mlb.insert(index, val)
self._mlb.delete(index + 1)
self._restore_config_info(config_cookie)
",[],0,[],/draw/table.py___setitem__
3631,/home/amandapotts/git/nltk/nltk/draw/table.py___delitem__,"def __delitem__(self, row_index):
""""""
Delete the ``row_index``th row from this table.
""""""
if isinstance(row_index, slice):
raise ValueError(""Slicing not supported"")
if isinstance(row_index, tuple) and len(row_index) == 2:
raise ValueError(""Cannot delete a single cell!"")
del self._rows[row_index]
self._mlb.delete(row_index)
if self._DEBUG:
self._check_table_vs_mlb()
",[],0,[],/draw/table.py___delitem__
3632,/home/amandapotts/git/nltk/nltk/draw/table.py___len__,"def __len__(self):
""""""
:return: the number of rows in this table.
""""""
return len(self._rows)
",[],0,[],/draw/table.py___len__
3633,/home/amandapotts/git/nltk/nltk/draw/table.py__checkrow,"def _checkrow(self, rowvalue):
""""""
Helper function: check that a given row value has the correct
number of elements
""""""
if len(rowvalue) != self._num_columns:
raise ValueError(
""Row %r has %d columns
% (rowvalue, len(rowvalue), self._num_columns)
)
",[],0,[],/draw/table.py__checkrow
3634,/home/amandapotts/git/nltk/nltk/draw/table.py_column_names,"def column_names(self):
""""""A list of the names of the columns in this table.""""""
return self._mlb.column_names
",[],0,[],/draw/table.py_column_names
3635,/home/amandapotts/git/nltk/nltk/draw/table.py_column_index,"def column_index(self, i):
""""""
If ``i`` is a valid column index integer, then return it as is.
Otherwise, check if ``i`` is used as the name for any column
if so, return that column's index.  Otherwise, raise a
``KeyError`` exception.
""""""
if isinstance(i, int) and 0 <= i < self._num_columns:
return i
else:
return self._column_name_to_index[i]
",[],0,[],/draw/table.py_column_index
3636,/home/amandapotts/git/nltk/nltk/draw/table.py_hide_column,"def hide_column(self, column_index):
"""""":see: ``MultiListbox.hide_column()``""""""
self._mlb.hide_column(self.column_index(column_index))
",[],0,[],/draw/table.py_hide_column
3637,/home/amandapotts/git/nltk/nltk/draw/table.py_show_column,"def show_column(self, column_index):
"""""":see: ``MultiListbox.show_column()``""""""
self._mlb.show_column(self.column_index(column_index))
",[],0,[],/draw/table.py_show_column
3638,/home/amandapotts/git/nltk/nltk/draw/table.py_selected_row,"def selected_row(self):
""""""
Return the index of the currently selected row, or None if
no row is selected.  To get the row value itself, use
``table[table.selected_row()]``.
""""""
sel = self._mlb.curselection()
if sel:
return int(sel[0])
else:
return None
",[],0,[],/draw/table.py_selected_row
3639,/home/amandapotts/git/nltk/nltk/draw/table.py_select,"def select(self, index=None, delta=None, see=True):
"""""":see: ``MultiListbox.select()``""""""
self._mlb.select(index, delta, see)
",[],0,[],/draw/table.py_select
3640,/home/amandapotts/git/nltk/nltk/draw/table.py_sort_by,"def sort_by(self, column_index, order=""toggle""):
""""""
Sort the rows in this table, using the specified column's
values as a sort key.
:param column_index: Specifies which column to sort, using
either a column index (int) or a column's label name
(str).
:param order: Specifies whether to sort the values in
ascending or descending order:
- ``'ascending'``: Sort from least to greatest.
- ``'descending'``: Sort from greatest to least.
- ``'toggle'``: If the most recent call to ``sort_by()``
sorted the table by the same column (``column_index``),
then reverse the rows
order.
""""""
if order not in (""ascending"", ""descending"", ""toggle""):
raise ValueError(
'sort_by(): order should be ""ascending"", ' '""descending"", or ""toggle"".'
)
column_index = self.column_index(column_index)
config_cookie = self._save_config_info(index_by_id=True)
if order == ""toggle"" and column_index == self._sortkey:
self._rows.reverse()
else:
self._rows.sort(
key=operator.itemgetter(column_index), reverse=(order == ""descending"")
)
self._sortkey = column_index
self._fill_table()
self._restore_config_info(config_cookie, index_by_id=True, see=True)
if self._DEBUG:
self._check_table_vs_mlb()
",[],0,[],/draw/table.py_sort_by
3641,/home/amandapotts/git/nltk/nltk/draw/table.py__sort,"def _sort(self, event):
""""""Event handler for clicking on a column label -- sort by
that column.""""""
column_index = event.widget.column_index
if self._mlb._resize_column(event):
return ""continue""
else:
self.sort_by(column_index)
return ""continue""
",[],0,[],/draw/table.py__sort
3642,/home/amandapotts/git/nltk/nltk/draw/table.py__fill_table,"def _fill_table(self, save_config=True):
""""""
Re-draw the table from scratch, by clearing out the table's
multi-column listbox
``self._rows``.  Note that any cell-, row-, or column-specific
color configuration that has been done will be lost.  The
selection will also be lost -- i.e., no row will be selected
after this call completes.
""""""
self._mlb.delete(0, ""end"")
for i, row in enumerate(self._rows):
if self._reprfunc is not None:
row = [self._reprfunc(i, j, v) for (j, v) in enumerate(row)]
self._mlb.insert(""end"", row)
",[],0,[],/draw/table.py__fill_table
3643,/home/amandapotts/git/nltk/nltk/draw/table.py__get_itemconfig,"def _get_itemconfig(self, r, c):
return {
k: self._mlb.itemconfig(r, c, k)[-1]
for k in (
""foreground"",
""selectforeground"",
""background"",
""selectbackground"",
)
}
",[],0,[],/draw/table.py__get_itemconfig
3644,/home/amandapotts/git/nltk/nltk/draw/table.py__save_config_info,"def _save_config_info(self, row_indices=None, index_by_id=False):
""""""
Return a 'cookie' containing information about which row is
selected, and what color configurations have been applied.
this information can the be re-applied to the table (after
making modifications) using ``_restore_config_info()``.  Color
configuration information will be saved for any rows in
``row_indices``, or in the entire table, if
``row_indices=None``.  If ``index_by_id=True``, the the cookie
will associate rows with their configuration information based
on the rows' python id.  This is useful when performing
operations that re-arrange the rows (e.g. ``sort``).  If
``index_by_id=False``, then it is assumed that all rows will be
in the same order when ``_restore_config_info()`` is called.
""""""
if row_indices is None:
row_indices = list(range(len(self._rows)))
selection = self.selected_row()
if index_by_id and selection is not None:
selection = id(self._rows[selection])
if index_by_id:
config = {
id(self._rows[r]): [
self._get_itemconfig(r, c) for c in range(self._num_columns)
]
for r in row_indices
}
else:
config = {
r: [self._get_itemconfig(r, c) for c in range(self._num_columns)]
for r in row_indices
}
return selection, config
",[],0,[],/draw/table.py__save_config_info
3645,/home/amandapotts/git/nltk/nltk/draw/table.py__restore_config_info,"def _restore_config_info(self, cookie, index_by_id=False, see=False):
""""""
Restore selection & color configuration information that was
saved using ``_save_config_info``.
""""""
selection, config = cookie
if selection is None:
self._mlb.selection_clear(0, ""end"")
if index_by_id:
for r, row in enumerate(self._rows):
if id(row) in config:
for c in range(self._num_columns):
self._mlb.itemconfigure(r, c, config[id(row)][c])
if id(row) == selection:
self._mlb.select(r, see=see)
else:
if selection is not None:
self._mlb.select(selection, see=see)
for r in config:
for c in range(self._num_columns):
self._mlb.itemconfigure(r, c, config[r][c])
",[],0,[],/draw/table.py__restore_config_info
3646,/home/amandapotts/git/nltk/nltk/draw/table.py__check_table_vs_mlb,"def _check_table_vs_mlb(self):
""""""
Verify that the contents of the table's ``_rows`` variable match
the contents of its multi-listbox (``_mlb``).  This is just
included for debugging purposes, to make sure that the
list-modifying operations are working correctly.
""""""
for col in self._mlb.listboxes:
assert len(self) == col.size()
for row in self:
assert len(row) == self._num_columns
assert self._num_columns == len(self._mlb.column_names)
for i, row in enumerate(self):
for j, cell in enumerate(row):
if self._reprfunc is not None:
cell = self._reprfunc(i, j, cell)
assert self._mlb.get(i)[j] == cell
",[],0,[],/draw/table.py__check_table_vs_mlb
3647,/home/amandapotts/git/nltk/nltk/draw/cfg.py__init_colortags,"def _init_colortags(self, textwidget, options):
textwidget.tag_config(""terminal"", foreground=""#006000"")
textwidget.tag_config(""arrow"", font=""symbol"", underline=""0"")
textwidget.tag_config(
""nonterminal"", foreground=""blue"", font=(""helvetica"", -12, ""bold"")
)
",[],0,[],/draw/cfg.py__init_colortags
3648,/home/amandapotts/git/nltk/nltk/draw/cfg.py__item_repr,"def _item_repr(self, item):
contents = []
contents.append((""%s\t"" % item.lhs(), ""nonterminal""))
contents.append((self.ARROW, ""arrow""))
for elt in item.rhs():
if isinstance(elt, Nonterminal):
contents.append(("" %s"" % elt.symbol(), ""nonterminal""))
else:
contents.append(("" %r"" % elt, ""terminal""))
return contents
",[],0,[],/draw/cfg.py__item_repr
3649,/home/amandapotts/git/nltk/nltk/draw/cfg.py___init__,"def __init__(self, parent, cfg=None, set_cfg_callback=None):
self._parent = parent
if cfg is not None:
self._cfg = cfg
else:
self._cfg = CFG(Nonterminal(""S""), [])
self._set_cfg_callback = set_cfg_callback
self._highlight_matching_nonterminals = 1
self._top = Toplevel(parent)
self._init_bindings()
self._init_startframe()
self._startframe.pack(side=""top"", fill=""x"", expand=0)
self._init_prodframe()
self._prodframe.pack(side=""top"", fill=""both"", expand=1)
self._init_buttons()
self._buttonframe.pack(side=""bottom"", fill=""x"", expand=0)
self._textwidget.focus()
",[],0,[],/draw/cfg.py___init__
3650,/home/amandapotts/git/nltk/nltk/draw/cfg.py__init_startframe,"def _init_startframe(self):
frame = self._startframe = Frame(self._top)
self._start = Entry(frame)
self._start.pack(side=""right"")
Label(frame, text=""Start Symbol:"").pack(side=""right"")
Label(frame, text=""Productions:"").pack(side=""left"")
self._start.insert(0, self._cfg.start().symbol())
",[],0,[],/draw/cfg.py__init_startframe
3651,/home/amandapotts/git/nltk/nltk/draw/cfg.py__init_buttons,"def _init_buttons(self):
frame = self._buttonframe = Frame(self._top)
Button(frame, text=""Ok"", command=self._ok, underline=0, takefocus=0).pack(
side=""left""
)
Button(frame, text=""Apply"", command=self._apply, underline=0, takefocus=0).pack(
side=""left""
)
Button(frame, text=""Reset"", command=self._reset, underline=0, takefocus=0).pack(
side=""left""
)
Button(
frame, text=""Cancel"", command=self._cancel, underline=0, takefocus=0
).pack(side=""left"")
Button(frame, text=""Help"", command=self._help, underline=0, takefocus=0).pack(
side=""right""
)
",[],0,[],/draw/cfg.py__init_buttons
3652,/home/amandapotts/git/nltk/nltk/draw/cfg.py__init_bindings,"def _init_bindings(self):
self._top.title(""CFG Editor"")
self._top.bind(""<Control-q>"", self._cancel)
self._top.bind(""<Alt-q>"", self._cancel)
self._top.bind(""<Control-d>"", self._cancel)
self._top.bind(""<Alt-x>"", self._cancel)
self._top.bind(""<Escape>"", self._cancel)
self._top.bind(""<Alt-c>"", self._cancel)
self._top.bind(""<Control-o>"", self._ok)
self._top.bind(""<Alt-o>"", self._ok)
self._top.bind(""<Control-a>"", self._apply)
self._top.bind(""<Alt-a>"", self._apply)
self._top.bind(""<Control-r>"", self._reset)
self._top.bind(""<Alt-r>"", self._reset)
self._top.bind(""<Control-h>"", self._help)
self._top.bind(""<Alt-h>"", self._help)
self._top.bind(""<F1>"", self._help)
",[],0,[],/draw/cfg.py__init_bindings
3653,/home/amandapotts/git/nltk/nltk/draw/cfg.py__init_prodframe,"def _init_prodframe(self):
self._prodframe = Frame(self._top)
self._textwidget = Text(
self._prodframe, background=""#e0e0e0"", exportselection=1
)
self._textscroll = Scrollbar(self._prodframe, takefocus=0, orient=""vertical"")
self._textwidget.config(yscrollcommand=self._textscroll.set)
self._textscroll.config(command=self._textwidget.yview)
self._textscroll.pack(side=""right"", fill=""y"")
self._textwidget.pack(expand=1, fill=""both"", side=""left"")
self._textwidget.tag_config(""terminal"", foreground=""#006000"")
self._textwidget.tag_config(""arrow"", font=""symbol"")
self._textwidget.tag_config(""error"", background=""red"")
self._linenum = 0
self._top.bind("">"", self._replace_arrows)
self._top.bind(""<<Paste>>"", self._analyze)
self._top.bind(""<KeyPress>"", self._check_analyze)
self._top.bind(""<ButtonPress>"", self._check_analyze)
",[],0,[],/draw/cfg.py__init_prodframe
3654,/home/amandapotts/git/nltk/nltk/draw/cfg.py_cycle,"def cycle(e, textwidget=self._textwidget):
textwidget.tk_focusNext().focus()
",[],0,[],/draw/cfg.py_cycle
3655,/home/amandapotts/git/nltk/nltk/draw/cfg.py__clear_tags,"def _clear_tags(self, linenum):
""""""
Remove all tags (except ``arrow`` and ``sel``) from the given
line of the text widget used for editing the productions.
""""""
start = ""%d.0"" % linenum
end = ""%d.end"" % linenum
for tag in self._textwidget.tag_names():
if tag not in (""arrow"", ""sel""):
self._textwidget.tag_remove(tag, start, end)
",[],0,[],/draw/cfg.py__clear_tags
3656,/home/amandapotts/git/nltk/nltk/draw/cfg.py__check_analyze,"def _check_analyze(self, *e):
""""""
Check if we've moved to a new line.  If we have, then remove
all colorization from the line we moved to, and re-colorize
the line that we moved from.
""""""
linenum = int(self._textwidget.index(""insert"").split(""."")[0])
if linenum != self._linenum:
self._clear_tags(linenum)
self._analyze_line(self._linenum)
self._linenum = linenum
",[],0,[],/draw/cfg.py__check_analyze
3657,/home/amandapotts/git/nltk/nltk/draw/cfg.py__replace_arrows,"def _replace_arrows(self, *e):
""""""
Replace any ``'->'`` text strings with arrows (char \\256, in
symbol font).  This searches the whole buffer, but is fast
enough to be done anytime they press '>'.
""""""
arrow = ""1.0""
while True:
arrow = self._textwidget.search(""->"", arrow, ""end+1char"")
if arrow == """":
break
self._textwidget.delete(arrow, arrow + ""+2char"")
self._textwidget.insert(arrow, self.ARROW, ""arrow"")
self._textwidget.insert(arrow, ""\t"")
arrow = ""1.0""
while True:
arrow = self._textwidget.search(self.ARROW, arrow + ""+1char"", ""end+1char"")
if arrow == """":
break
self._textwidget.tag_add(""arrow"", arrow, arrow + ""+1char"")
",[],0,[],/draw/cfg.py__replace_arrows
3658,/home/amandapotts/git/nltk/nltk/draw/cfg.py__analyze_token,"def _analyze_token(self, match, linenum):
""""""
Given a line number and a regexp match for a token on that
line, colorize the token.  Note that the regexp match gives us
the token's text, start index (on the line), and end index (on
the line).
""""""
if match.group()[0] in ""'\"""":
tag = ""terminal""
elif match.group() in (""->"", self.ARROW):
tag = ""arrow""
else:
tag = ""nonterminal_"" + match.group()
if tag not in self._textwidget.tag_names():
self._init_nonterminal_tag(tag)
start = ""%d.%d"" % (linenum, match.start())
end = ""%d.%d"" % (linenum, match.end())
self._textwidget.tag_add(tag, start, end)
",[],0,[],/draw/cfg.py__analyze_token
3659,/home/amandapotts/git/nltk/nltk/draw/cfg.py__init_nonterminal_tag,"def _init_nonterminal_tag(self, tag, foreground=""blue""):
self._textwidget.tag_config(tag, foreground=foreground, font=CFGEditor._BOLD)
if not self._highlight_matching_nonterminals:
return
",[],0,[],/draw/cfg.py__init_nonterminal_tag
3660,/home/amandapotts/git/nltk/nltk/draw/cfg.py_enter,"def enter(e, textwidget=self._textwidget, tag=tag):
textwidget.tag_config(tag, background=""#80ff80"")
",[],0,[],/draw/cfg.py_enter
3661,/home/amandapotts/git/nltk/nltk/draw/cfg.py_leave,"def leave(e, textwidget=self._textwidget, tag=tag):
textwidget.tag_config(tag, background="""")
",[],0,[],/draw/cfg.py_leave
3662,/home/amandapotts/git/nltk/nltk/draw/cfg.py__analyze_line,"def _analyze_line(self, linenum):
""""""
Colorize a given line.
""""""
self._clear_tags(linenum)
line = self._textwidget.get(repr(linenum) + "".0"", repr(linenum) + "".end"")
if CFGEditor._PRODUCTION_RE.match(line):
",[],0,[],/draw/cfg.py__analyze_line
3663,/home/amandapotts/git/nltk/nltk/draw/cfg.py_analyze_token,"def analyze_token(match, self=self, linenum=linenum):
self._analyze_token(match, linenum)
return """"
",[],0,[],/draw/cfg.py_analyze_token
3664,/home/amandapotts/git/nltk/nltk/draw/cfg.py__mark_error,"def _mark_error(self, linenum, line):
""""""
Mark the location of an error in a line.
""""""
arrowmatch = CFGEditor._ARROW_RE.search(line)
if not arrowmatch:
start = ""%d.0"" % linenum
end = ""%d.end"" % linenum
elif not CFGEditor._LHS_RE.match(line):
start = ""%d.0"" % linenum
end = ""%d.%d"" % (linenum, arrowmatch.start())
else:
start = ""%d.%d"" % (linenum, arrowmatch.end())
end = ""%d.end"" % linenum
if self._textwidget.compare(start, ""=="", end):
start = ""%d.0"" % linenum
end = ""%d.end"" % linenum
self._textwidget.tag_add(""error"", start, end)
",[],0,[],/draw/cfg.py__mark_error
3665,/home/amandapotts/git/nltk/nltk/draw/cfg.py__analyze,"def _analyze(self, *e):
""""""
Replace ``->`` with arrows, and colorize the entire buffer.
""""""
self._replace_arrows()
numlines = int(self._textwidget.index(""end"").split(""."")[0])
for linenum in range(1, numlines + 1):  # line numbers start at 1.
self._analyze_line(linenum)
",[],0,[],/draw/cfg.py__analyze
3666,/home/amandapotts/git/nltk/nltk/draw/cfg.py__parse_productions,"def _parse_productions(self):
""""""
Parse the current contents of the textwidget buffer, to create
a list of productions.
""""""
productions = []
text = self._textwidget.get(""1.0"", ""end"")
text = re.sub(self.ARROW, ""->"", text)
text = re.sub(""\t"", "" "", text)
lines = text.split(""\n"")
for line in lines:
line = line.strip()
if line == """":
continue
productions += _read_cfg_production(line)
return productions
",[],0,[],/draw/cfg.py__parse_productions
3667,/home/amandapotts/git/nltk/nltk/draw/cfg.py__destroy,"def _destroy(self, *e):
if self._top is None:
return
self._top.destroy()
self._top = None
",[],0,[],/draw/cfg.py__destroy
3668,/home/amandapotts/git/nltk/nltk/draw/cfg.py__ok,"def _ok(self, *e):
self._apply()
self._destroy()
",[],0,[],/draw/cfg.py__ok
3669,/home/amandapotts/git/nltk/nltk/draw/cfg.py__apply,"def _apply(self, *e):
productions = self._parse_productions()
start = Nonterminal(self._start.get())
cfg = CFG(start, productions)
if self._set_cfg_callback is not None:
self._set_cfg_callback(cfg)
",[],0,[],/draw/cfg.py__apply
3670,/home/amandapotts/git/nltk/nltk/draw/cfg.py__reset,"def _reset(self, *e):
self._textwidget.delete(""1.0"", ""end"")
for production in self._cfg.productions():
self._textwidget.insert(""end"", ""%s\n"" % production)
self._analyze()
if self._set_cfg_callback is not None:
self._set_cfg_callback(self._cfg)
",[],0,[],/draw/cfg.py__reset
3671,/home/amandapotts/git/nltk/nltk/draw/cfg.py__cancel,"def _cancel(self, *e):
try:
self._reset()
except:
pass
self._destroy()
",[],0,[],/draw/cfg.py__cancel
3672,/home/amandapotts/git/nltk/nltk/draw/cfg.py__help,"def _help(self, *e):
try:
ShowText(
self._parent,
""Help: Chart Parser Demo"",
(_CFGEditor_HELP).strip(),
width=75,
font=""fixed"",
)
except:
ShowText(
self._parent,
""Help: Chart Parser Demo"",
(_CFGEditor_HELP).strip(),
width=75,
)
",[],0,[],/draw/cfg.py__help
3673,/home/amandapotts/git/nltk/nltk/draw/cfg.py___init__,"def __init__(self, grammar, text):
self._grammar = grammar
self._text = text
self._top = Tk()
self._top.title(""Context Free Grammar Demo"")
self._size = IntVar(self._top)
self._size.set(12)  # = medium
self._init_bindings(self._top)
frame1 = Frame(self._top)
frame1.pack(side=""left"", fill=""y"", expand=0)
self._init_menubar(self._top)
self._init_buttons(self._top)
self._init_grammar(frame1)
self._init_treelet(frame1)
self._init_workspace(self._top)
",[],0,[],/draw/cfg.py___init__
3674,/home/amandapotts/git/nltk/nltk/draw/cfg.py__init_bindings,"def _init_bindings(self, top):
top.bind(""<Control-q>"", self.destroy)
",[],0,[],/draw/cfg.py__init_bindings
3675,/home/amandapotts/git/nltk/nltk/draw/cfg.py__init_menubar,"def _init_menubar(self, parent):
pass
",[],0,[],/draw/cfg.py__init_menubar
3676,/home/amandapotts/git/nltk/nltk/draw/cfg.py__init_buttons,"def _init_buttons(self, parent):
pass
",[],0,[],/draw/cfg.py__init_buttons
3677,/home/amandapotts/git/nltk/nltk/draw/cfg.py__init_grammar,"def _init_grammar(self, parent):
self._prodlist = ProductionList(parent, self._grammar, width=20)
self._prodlist.pack(side=""top"", fill=""both"", expand=1)
self._prodlist.focus()
self._prodlist.add_callback(""select"", self._selectprod_cb)
self._prodlist.add_callback(""move"", self._selectprod_cb)
",[],0,[],/draw/cfg.py__init_grammar
3678,/home/amandapotts/git/nltk/nltk/draw/cfg.py__init_treelet,"def _init_treelet(self, parent):
self._treelet_canvas = Canvas(parent, background=""white"")
self._treelet_canvas.pack(side=""bottom"", fill=""x"")
self._treelet = None
",[],0,[],/draw/cfg.py__init_treelet
3679,/home/amandapotts/git/nltk/nltk/draw/cfg.py__init_workspace,"def _init_workspace(self, parent):
self._workspace = CanvasFrame(parent, background=""white"")
self._workspace.pack(side=""right"", fill=""both"", expand=1)
self._tree = None
self.reset_workspace()
",[],0,[],/draw/cfg.py__init_workspace
3680,/home/amandapotts/git/nltk/nltk/draw/cfg.py_reset_workspace,"def reset_workspace(self):
c = self._workspace.canvas()
fontsize = int(self._size.get())
node_font = (""helvetica"", -(fontsize + 4), ""bold"")
leaf_font = (""helvetica"", -(fontsize + 2))
if self._tree is not None:
self._workspace.remove_widget(self._tree)
start = self._grammar.start().symbol()
rootnode = TextWidget(c, start, font=node_font, draggable=1)
leaves = []
for word in self._text:
leaves.append(TextWidget(c, word, font=leaf_font, draggable=1))
self._tree = TreeSegmentWidget(c, rootnode, leaves, color=""white"")
self._workspace.add_widget(self._tree)
for leaf in leaves:
leaf.move(0, 100)
",[],0,[],/draw/cfg.py_reset_workspace
3681,/home/amandapotts/git/nltk/nltk/draw/cfg.py_workspace_markprod,"def workspace_markprod(self, production):
pass
",[],0,[],/draw/cfg.py_workspace_markprod
3682,/home/amandapotts/git/nltk/nltk/draw/cfg.py__markproduction,"def _markproduction(self, prod, tree=None):
if tree is None:
tree = self._tree
for i in range(len(tree.subtrees()) - len(prod.rhs())):
if tree[""color"", i] == ""white"":
self._markproduction  # FIXME: Is this necessary at all?
for j, node in enumerate(prod.rhs()):
widget = tree.subtrees()[i + j]
if (
isinstance(node, Nonterminal)
and isinstance(widget, TreeSegmentWidget)
and node.symbol == widget.label().text()
):
pass  # matching nonterminal
elif (
isinstance(node, str)
and isinstance(widget, TextWidget)
and node == widget.text()
):
pass  # matching nonterminal
else:
break
else:
print(""MATCH AT"", i)
",[],0,[],/draw/cfg.py__markproduction
3683,/home/amandapotts/git/nltk/nltk/draw/cfg.py__selectprod_cb,"def _selectprod_cb(self, production):
canvas = self._treelet_canvas
self._prodlist.highlight(production)
if self._treelet is not None:
self._treelet.destroy()
rhs = production.rhs()
for i, elt in enumerate(rhs):
if isinstance(elt, Nonterminal):
elt = Tree(elt)
tree = Tree(production.lhs().symbol(), *rhs)
fontsize = int(self._size.get())
node_font = (""helvetica"", -(fontsize + 4), ""bold"")
leaf_font = (""helvetica"", -(fontsize + 2))
self._treelet = tree_to_treesegment(
canvas, tree, node_font=node_font, leaf_font=leaf_font
)
self._treelet[""draggable""] = 1
(x1, y1, x2, y2) = self._treelet.bbox()
w, h = int(canvas[""width""]), int(canvas[""height""])
self._treelet.move((w - x1 - x2) / 2, (h - y1 - y2) / 2)
self._markproduction(production)
",[],0,[],/draw/cfg.py__selectprod_cb
3684,/home/amandapotts/git/nltk/nltk/draw/cfg.py_destroy,"def destroy(self, *args):
self._top.destroy()
",[],0,[],/draw/cfg.py_destroy
3685,/home/amandapotts/git/nltk/nltk/draw/cfg.py_mainloop,"def mainloop(self, *args, **kwargs):
self._top.mainloop(*args, **kwargs)
",[],0,[],/draw/cfg.py_mainloop
3686,/home/amandapotts/git/nltk/nltk/draw/cfg.py_demo2,"def demo2():
from nltk import CFG, Nonterminal, Production
nonterminals = ""S VP NP PP P N Name V Det""
(S, VP, NP, PP, P, N, Name, V, Det) = (Nonterminal(s) for s in nonterminals.split())
productions = (
Production(S, [NP, VP]),
Production(NP, [Det, N]),
Production(NP, [NP, PP]),
Production(VP, [VP, PP]),
Production(VP, [V, NP, PP]),
Production(VP, [V, NP]),
Production(PP, [P, NP]),
Production(PP, []),
Production(PP, [""up"", ""over"", NP]),
Production(NP, [""I""]),
Production(Det, [""the""]),
Production(Det, [""a""]),
Production(N, [""man""]),
Production(V, [""saw""]),
Production(P, [""in""]),
Production(P, [""with""]),
Production(N, [""park""]),
Production(N, [""dog""]),
Production(N, [""statue""]),
Production(Det, [""my""]),
)
grammar = CFG(S, productions)
text = ""I saw a man in the park"".split()
d = CFGDemo(grammar, text)
d.mainloop()
",[],0,[],/draw/cfg.py_demo2
3687,/home/amandapotts/git/nltk/nltk/draw/cfg.py_demo,"def demo():
from nltk import CFG, Nonterminal
nonterminals = ""S VP NP PP P N Name V Det""
(S, VP, NP, PP, P, N, Name, V, Det) = (Nonterminal(s) for s in nonterminals.split())
grammar = CFG.fromstring(
""""""
S -> NP VP
PP -> P NP
NP -> Det N
NP -> NP PP
VP -> V NP
VP -> VP PP
Det -> 'a'
Det -> 'the'
Det -> 'my'
NP -> 'I'
N -> 'dog'
N -> 'man'
N -> 'park'
N -> 'statue'
V -> 'saw'
P -> 'in'
P -> 'up'
P -> 'over'
P -> 'with'
""""""
)
",[],0,[],/draw/cfg.py_demo
3688,/home/amandapotts/git/nltk/nltk/draw/cfg.py_cb,"def cb(grammar):
print(grammar)
",[],0,[],/draw/cfg.py_cb
3689,/home/amandapotts/git/nltk/nltk/draw/cfg.py_demo3,"def demo3():
from nltk import Production
(S, VP, NP, PP, P, N, Name, V, Det) = nonterminals(
""S, VP, NP, PP, P, N, Name, V, Det""
)
productions = (
Production(S, [NP, VP]),
Production(NP, [Det, N]),
Production(NP, [NP, PP]),
Production(VP, [VP, PP]),
Production(VP, [V, NP, PP]),
Production(VP, [V, NP]),
Production(PP, [P, NP]),
Production(PP, []),
Production(PP, [""up"", ""over"", NP]),
Production(NP, [""I""]),
Production(Det, [""the""]),
Production(Det, [""a""]),
Production(N, [""man""]),
Production(V, [""saw""]),
Production(P, [""in""]),
Production(P, [""with""]),
Production(N, [""park""]),
Production(N, [""dog""]),
Production(N, [""statue""]),
Production(Det, [""my""]),
)
t = Tk()
",[],0,[],/draw/cfg.py_demo3
3690,/home/amandapotts/git/nltk/nltk/draw/cfg.py_destroy,"def destroy(e, t=t):
t.destroy()
",[],0,[],/draw/cfg.py_destroy
3691,/home/amandapotts/git/nltk/nltk/stem/isri.py___init__,"def __init__(self):
self.p3 = [
""\u0643\u0627\u0644"",
""\u0628\u0627\u0644"",
""\u0648\u0644\u0644"",
""\u0648\u0627\u0644"",
]
self.p2 = [""\u0627\u0644"", ""\u0644\u0644""]
self.p1 = [
""\u0644"",
""\u0628"",
""\u0641"",
""\u0633"",
""\u0648"",
""\u064a"",
""\u062a"",
""\u0646"",
""\u0627"",
]
self.s3 = [
""\u062a\u0645\u0644"",
""\u0647\u0645\u0644"",
""\u062a\u0627\u0646"",
""\u062a\u064a\u0646"",
""\u0643\u0645\u0644"",
]
self.s2 = [
""\u0648\u0646"",
""\u0627\u062a"",
""\u0627\u0646"",
""\u064a\u0646"",
""\u062a\u0646"",
""\u0643\u0645"",
""\u0647\u0646"",
""\u0646\u0627"",
""\u064a\u0627"",
""\u0647\u0627"",
""\u062a\u0645"",
""\u0643\u0646"",
""\u0646\u064a"",
""\u0648\u0627"",
""\u0645\u0627"",
""\u0647\u0645"",
]
self.s1 = [""\u0629"", ""\u0647"", ""\u064a"", ""\u0643"", ""\u062a"", ""\u0627"", ""\u0646""]
self.pr4 = {
0: [""\u0645""],
1: [""\u0627""],
2: [""\u0627"", ""\u0648"", ""\u064A""],
3: [""\u0629""],
}
self.pr53 = {
0: [""\u0627"", ""\u062a""],
1: [""\u0627"", ""\u064a"", ""\u0648""],
2: [""\u0627"", ""\u062a"", ""\u0645""],
3: [""\u0645"", ""\u064a"", ""\u062a""],
4: [""\u0645"", ""\u062a""],
5: [""\u0627"", ""\u0648""],
6: [""\u0627"", ""\u0645""],
}
self.re_short_vowels = re.compile(r""[\u064B-\u0652]"")
self.re_hamza = re.compile(r""[\u0621\u0624\u0626]"")
self.re_initial_hamza = re.compile(r""^[\u0622\u0623\u0625]"")
self.stop_words = [
""\u064a\u0643\u0648\u0646"",
""\u0648\u0644\u064a\u0633"",
""\u0648\u0643\u0627\u0646"",
""\u0643\u0630\u0644\u0643"",
""\u0627\u0644\u062a\u064a"",
""\u0648\u0628\u064a\u0646"",
""\u0639\u0644\u064a\u0647\u0627"",
""\u0645\u0633\u0627\u0621"",
""\u0627\u0644\u0630\u064a"",
""\u0648\u0643\u0627\u0646\u062a"",
""\u0648\u0644\u0643\u0646"",
""\u0648\u0627\u0644\u062a\u064a"",
""\u062a\u0643\u0648\u0646"",
""\u0627\u0644\u064a\u0648\u0645"",
""\u0627\u0644\u0644\u0630\u064a\u0646"",
""\u0639\u0644\u064a\u0647"",
""\u0643\u0627\u0646\u062a"",
""\u0644\u0630\u0644\u0643"",
""\u0623\u0645\u0627\u0645"",
""\u0647\u0646\u0627\u0643"",
""\u0645\u0646\u0647\u0627"",
""\u0645\u0627\u0632\u0627\u0644"",
""\u0644\u0627\u0632\u0627\u0644"",
""\u0644\u0627\u064a\u0632\u0627\u0644"",
""\u0645\u0627\u064a\u0632\u0627\u0644"",
""\u0627\u0635\u0628\u062d"",
""\u0623\u0635\u0628\u062d"",
""\u0623\u0645\u0633\u0649"",
""\u0627\u0645\u0633\u0649"",
""\u0623\u0636\u062d\u0649"",
""\u0627\u0636\u062d\u0649"",
""\u0645\u0627\u0628\u0631\u062d"",
""\u0645\u0627\u0641\u062a\u0626"",
""\u0645\u0627\u0627\u0646\u0641\u0643"",
""\u0644\u0627\u0633\u064a\u0645\u0627"",
""\u0648\u0644\u0627\u064a\u0632\u0627\u0644"",
""\u0627\u0644\u062d\u0627\u0644\u064a"",
""\u0627\u0644\u064a\u0647\u0627"",
""\u0627\u0644\u0630\u064a\u0646"",
""\u0641\u0627\u0646\u0647"",
""\u0648\u0627\u0644\u0630\u064a"",
""\u0648\u0647\u0630\u0627"",
""\u0644\u0647\u0630\u0627"",
""\u0641\u0643\u0627\u0646"",
""\u0633\u062a\u0643\u0648\u0646"",
""\u0627\u0644\u064a\u0647"",
""\u064a\u0645\u0643\u0646"",
""\u0628\u0647\u0630\u0627"",
""\u0627\u0644\u0630\u0649"",
]
",[],0,[],/stem/isri.py___init__
3692,/home/amandapotts/git/nltk/nltk/stem/isri.py_stem,"def stem(self, token):
""""""
Stemming a word token using the ISRI stemmer.
""""""
token = self.norm(
token, 1
)  # remove diacritics which representing Arabic short vowels
if token in self.stop_words:
return token  # exclude stop words from being processed
token = self.pre32(
token
)  # remove length three and length two prefixes in this order
token = self.suf32(
token
)  # remove length three and length two suffixes in this order
token = self.waw(
token
)  # remove connective ‘و’ if it precedes a word beginning with ‘و’
token = self.norm(token, 2)  # normalize initial hamza to bare alif
if len(token) == 4:  # length 4 word
token = self.pro_w4(token)
elif len(token) == 5:  # length 5 word
token = self.pro_w53(token)
token = self.end_w5(token)
elif len(token) == 6:  # length 6 word
token = self.pro_w6(token)
token = self.end_w6(token)
elif len(token) == 7:  # length 7 word
token = self.suf1(token)
if len(token) == 7:
token = self.pre1(token)
if len(token) == 6:
token = self.pro_w6(token)
token = self.end_w6(token)
return token
",[],0,[],/stem/isri.py_stem
3693,/home/amandapotts/git/nltk/nltk/stem/isri.py_norm,"def norm(self, word, num=3):
""""""
normalization:
num=1  normalize diacritics
num=2  normalize initial hamza
num=3  both 1&2
""""""
if num == 1:
word = self.re_short_vowels.sub("""", word)
elif num == 2:
word = self.re_initial_hamza.sub(""\u0627"", word)
elif num == 3:
word = self.re_short_vowels.sub("""", word)
word = self.re_initial_hamza.sub(""\u0627"", word)
return word
",[],0,[],/stem/isri.py_norm
3694,/home/amandapotts/git/nltk/nltk/stem/isri.py_pre32,"def pre32(self, word):
""""""remove length three and length two prefixes in this order""""""
if len(word) >= 6:
for pre3 in self.p3:
if word.startswith(pre3):
return word[3:]
if len(word) >= 5:
for pre2 in self.p2:
if word.startswith(pre2):
return word[2:]
return word
",[],0,[],/stem/isri.py_pre32
3695,/home/amandapotts/git/nltk/nltk/stem/isri.py_suf32,"def suf32(self, word):
""""""remove length three and length two suffixes in this order""""""
if len(word) >= 6:
for suf3 in self.s3:
if word.endswith(suf3):
return word[:-3]
if len(word) >= 5:
for suf2 in self.s2:
if word.endswith(suf2):
return word[:-2]
return word
",[],0,[],/stem/isri.py_suf32
3696,/home/amandapotts/git/nltk/nltk/stem/isri.py_waw,"def waw(self, word):
""""""remove connective ‘و’ if it precedes a word beginning with ‘و’""""""
if len(word) >= 4 and word[:2] == ""\u0648\u0648"":
word = word[1:]
return word
",[],0,[],/stem/isri.py_waw
3697,/home/amandapotts/git/nltk/nltk/stem/isri.py_pro_w4,"def pro_w4(self, word):
""""""process length four patterns and extract length three roots""""""
if word[0] in self.pr4[0]:  # مفعل
word = word[1:]
elif word[1] in self.pr4[1]:  # فاعل
word = word[:1] + word[2:]
elif word[2] in self.pr4[2]:  # فعال - فعول - فعيل
word = word[:2] + word[3]
elif word[3] in self.pr4[3]:  # فعلة
word = word[:-1]
else:
word = self.suf1(word)  # do - normalize short sufix
if len(word) == 4:
word = self.pre1(word)  # do - normalize short prefix
return word
",[],0,[],/stem/isri.py_pro_w4
3698,/home/amandapotts/git/nltk/nltk/stem/isri.py_pro_w53,"def pro_w53(self, word):
""""""process length five patterns and extract length three roots""""""
if word[2] in self.pr53[0] and word[0] == ""\u0627"":  # افتعل - افاعل
word = word[1] + word[3:]
elif word[3] in self.pr53[1] and word[0] == ""\u0645"":  # مفعول - مفعال - مفعيل
word = word[1:3] + word[4]
elif word[0] in self.pr53[2] and word[4] == ""\u0629"":  # مفعلة - تفعلة - افعلة
word = word[1:4]
elif word[0] in self.pr53[3] and word[2] == ""\u062a"":  # مفتعل - يفتعل - تفتعل
word = word[1] + word[3:]
elif word[0] in self.pr53[4] and word[2] == ""\u0627"":  # مفاعل - تفاعل
word = word[1] + word[3:]
elif word[2] in self.pr53[5] and word[4] == ""\u0629"":  # فعولة - فعالة
word = word[:2] + word[3]
elif word[0] in self.pr53[6] and word[1] == ""\u0646"":  # انفعل - منفعل
word = word[2:]
elif word[3] == ""\u0627"" and word[0] == ""\u0627"":  # افعال
word = word[1:3] + word[4]
elif word[4] == ""\u0646"" and word[3] == ""\u0627"":  # فعلان
word = word[:3]
elif word[3] == ""\u064a"" and word[0] == ""\u062a"":  # تفعيل
word = word[1:3] + word[4]
elif word[3] == ""\u0648"" and word[1] == ""\u0627"":  # فاعول
word = word[0] + word[2] + word[4]
elif word[2] == ""\u0627"" and word[1] == ""\u0648"":  # فواعل
word = word[0] + word[3:]
elif word[3] == ""\u0626"" and word[2] == ""\u0627"":  # فعائل
word = word[:2] + word[4]
elif word[4] == ""\u0629"" and word[1] == ""\u0627"":  # فاعلة
word = word[0] + word[2:4]
elif word[4] == ""\u064a"" and word[2] == ""\u0627"":  # فعالي
word = word[:2] + word[3]
else:
word = self.suf1(word)  # do - normalize short sufix
if len(word) == 5:
word = self.pre1(word)  # do - normalize short prefix
return word
",[],0,[],/stem/isri.py_pro_w53
3699,/home/amandapotts/git/nltk/nltk/stem/isri.py_pro_w54,"def pro_w54(self, word):
""""""process length five patterns and extract length four roots""""""
if word[0] in self.pr53[2]:  # تفعلل - افعلل - مفعلل
word = word[1:]
elif word[4] == ""\u0629"":  # فعللة
word = word[:4]
elif word[2] == ""\u0627"":  # فعالل
word = word[:2] + word[3:]
return word
",[],0,[],/stem/isri.py_pro_w54
3700,/home/amandapotts/git/nltk/nltk/stem/isri.py_end_w5,"def end_w5(self, word):
""""""ending step (word of length five)""""""
if len(word) == 4:
word = self.pro_w4(word)
elif len(word) == 5:
word = self.pro_w54(word)
return word
",[],0,[],/stem/isri.py_end_w5
3701,/home/amandapotts/git/nltk/nltk/stem/isri.py_pro_w6,"def pro_w6(self, word):
""""""process length six patterns and extract length three roots""""""
if word.startswith(""\u0627\u0633\u062a"") or word.startswith(
""\u0645\u0633\u062a""
):  # مستفعل - استفعل
word = word[3:]
elif (
word[0] == ""\u0645"" and word[3] == ""\u0627"" and word[5] == ""\u0629""
):  # مفعالة
word = word[1:3] + word[4]
elif (
word[0] == ""\u0627"" and word[2] == ""\u062a"" and word[4] == ""\u0627""
):  # افتعال
word = word[1] + word[3] + word[5]
elif (
word[0] == ""\u0627"" and word[3] == ""\u0648"" and word[2] == word[4]
):  # افعوعل
word = word[1] + word[4:]
elif (
word[0] == ""\u062a"" and word[2] == ""\u0627"" and word[4] == ""\u064a""
):  # تفاعيل   new pattern
word = word[1] + word[3] + word[5]
else:
word = self.suf1(word)  # do - normalize short sufix
if len(word) == 6:
word = self.pre1(word)  # do - normalize short prefix
return word
",[],0,[],/stem/isri.py_pro_w6
3702,/home/amandapotts/git/nltk/nltk/stem/isri.py_pro_w64,"def pro_w64(self, word):
""""""process length six patterns and extract length four roots""""""
if word[0] == ""\u0627"" and word[4] == ""\u0627"":  # افعلال
word = word[1:4] + word[5]
elif word.startswith(""\u0645\u062a""):  # متفعلل
word = word[2:]
return word
",[],0,[],/stem/isri.py_pro_w64
3703,/home/amandapotts/git/nltk/nltk/stem/isri.py_end_w6,"def end_w6(self, word):
""""""ending step (word of length six)""""""
if len(word) == 5:
word = self.pro_w53(word)
word = self.end_w5(word)
elif len(word) == 6:
word = self.pro_w64(word)
return word
",[],0,[],/stem/isri.py_end_w6
3704,/home/amandapotts/git/nltk/nltk/stem/isri.py_suf1,"def suf1(self, word):
""""""normalize short sufix""""""
for sf1 in self.s1:
if word.endswith(sf1):
return word[:-1]
return word
",[],0,[],/stem/isri.py_suf1
3705,/home/amandapotts/git/nltk/nltk/stem/isri.py_pre1,"def pre1(self, word):
""""""normalize short prefix""""""
for sp1 in self.p1:
if word.startswith(sp1):
return word[1:]
return word
",[],0,[],/stem/isri.py_pre1
3706,/home/amandapotts/git/nltk/nltk/stem/arlstem.py___init__,"def __init__(self):
self.re_hamzated_alif = re.compile(r""[\u0622\u0623\u0625]"")
self.re_alifMaqsura = re.compile(r""[\u0649]"")
self.re_diacritics = re.compile(r""[\u064B-\u065F]"")
self.pr2 = [""\u0627\u0644"", ""\u0644\u0644"", ""\u0641\u0644"", ""\u0641\u0628""]
self.pr3 = [""\u0628\u0627\u0644"", ""\u0643\u0627\u0644"", ""\u0648\u0627\u0644""]
self.pr32 = [""\u0641\u0644\u0644"", ""\u0648\u0644\u0644""]
self.pr4 = [
""\u0641\u0628\u0627\u0644"",
""\u0648\u0628\u0627\u0644"",
""\u0641\u0643\u0627\u0644"",
]
self.su2 = [""\u0643\u064A"", ""\u0643\u0645""]
self.su22 = [""\u0647\u0627"", ""\u0647\u0645""]
self.su3 = [""\u0643\u0645\u0627"", ""\u0643\u0646\u0651""]
self.su32 = [""\u0647\u0645\u0627"", ""\u0647\u0646\u0651""]
self.pl_si2 = [""\u0627\u0646"", ""\u064A\u0646"", ""\u0648\u0646""]
self.pl_si3 = [""\u062A\u0627\u0646"", ""\u062A\u064A\u0646""]
self.verb_su2 = [""\u0627\u0646"", ""\u0648\u0646""]
self.verb_pr2 = [""\u0633\u062A"", ""\u0633\u064A""]
self.verb_pr22 = [""\u0633\u0627"", ""\u0633\u0646""]
self.verb_pr33 = [
""\u0644\u0646"",
""\u0644\u062A"",
""\u0644\u064A"",
""\u0644\u0623"",
]
self.verb_suf3 = [""\u062A\u0645\u0627"", ""\u062A\u0646\u0651""]
self.verb_suf2 = [
""\u0646\u0627"",
""\u062A\u0645"",
""\u062A\u0627"",
""\u0648\u0627"",
]
self.verb_suf1 = [""\u062A"", ""\u0627"", ""\u0646""]
",[],0,[],/stem/arlstem.py___init__
3707,/home/amandapotts/git/nltk/nltk/stem/arlstem.py_stem,"def stem(self, token):
""""""
call this function to get the word's stem based on ARLSTem .
""""""
try:
if token is None:
raise ValueError(
""The word could not be stemmed, because \
it is empty !""
)
token = self.norm(token)
pre = self.pref(token)
if pre is not None:
token = pre
token = self.suff(token)
ps = self.plur2sing(token)
if ps is None:
fm = self.fem2masc(token)
if fm is not None:
return fm
else:
if pre is None:  # if the prefixes are not stripped
return self.verb(token)
else:
return ps
return token
except ValueError as e:
print(e)
",[],0,[],/stem/arlstem.py_stem
3708,/home/amandapotts/git/nltk/nltk/stem/arlstem.py_norm,"def norm(self, token):
""""""
normalize the word by removing diacritics, replacing hamzated Alif
with Alif replacing AlifMaqsura with Yaa and removing Waaw at the
beginning.
""""""
token = self.re_diacritics.sub("""", token)
token = self.re_hamzated_alif.sub(""\u0627"", token)
token = self.re_alifMaqsura.sub(""\u064A"", token)
if token.startswith(""\u0648"") and len(token) > 3:
token = token[1:]
return token
",[],0,[],/stem/arlstem.py_norm
3709,/home/amandapotts/git/nltk/nltk/stem/arlstem.py_pref,"def pref(self, token):
""""""
remove prefixes from the words' beginning.
""""""
if len(token) > 5:
for p3 in self.pr3:
if token.startswith(p3):
return token[3:]
if len(token) > 6:
for p4 in self.pr4:
if token.startswith(p4):
return token[4:]
if len(token) > 5:
for p3 in self.pr32:
if token.startswith(p3):
return token[3:]
if len(token) > 4:
for p2 in self.pr2:
if token.startswith(p2):
return token[2:]
",[],0,[],/stem/arlstem.py_pref
3710,/home/amandapotts/git/nltk/nltk/stem/arlstem.py_suff,"def suff(self, token):
""""""
remove suffixes from the word's end.
""""""
if token.endswith(""\u0643"") and len(token) > 3:
return token[:-1]
if len(token) > 4:
for s2 in self.su2:
if token.endswith(s2):
return token[:-2]
if len(token) > 5:
for s3 in self.su3:
if token.endswith(s3):
return token[:-3]
if token.endswith(""\u0647"") and len(token) > 3:
token = token[:-1]
return token
if len(token) > 4:
for s2 in self.su22:
if token.endswith(s2):
return token[:-2]
if len(token) > 5:
for s3 in self.su32:
if token.endswith(s3):
return token[:-3]
if token.endswith(""\u0646\u0627"") and len(token) > 4:
return token[:-2]
return token
",[],0,[],/stem/arlstem.py_suff
3711,/home/amandapotts/git/nltk/nltk/stem/arlstem.py_fem2masc,"def fem2masc(self, token):
""""""
transform the word from the feminine form to the masculine form.
""""""
if token.endswith(""\u0629"") and len(token) > 3:
return token[:-1]
",[],0,[],/stem/arlstem.py_fem2masc
3712,/home/amandapotts/git/nltk/nltk/stem/arlstem.py_plur2sing,"def plur2sing(self, token):
""""""
transform the word from the plural form to the singular form.
""""""
if len(token) > 4:
for ps2 in self.pl_si2:
if token.endswith(ps2):
return token[:-2]
if len(token) > 5:
for ps3 in self.pl_si3:
if token.endswith(ps3):
return token[:-3]
if len(token) > 3 and token.endswith(""\u0627\u062A""):
return token[:-2]
if len(token) > 3 and token.startswith(""\u0627"") and token[2] == ""\u0627"":
return token[:2] + token[3:]
if len(token) > 4 and token.startswith(""\u0627"") and token[-2] == ""\u0627"":
return token[1:-2] + token[-1]
",[],0,[],/stem/arlstem.py_plur2sing
3713,/home/amandapotts/git/nltk/nltk/stem/arlstem.py_verb,"def verb(self, token):
""""""
stem the verb prefixes and suffixes or both
""""""
vb = self.verb_t1(token)
if vb is not None:
return vb
vb = self.verb_t2(token)
if vb is not None:
return vb
vb = self.verb_t3(token)
if vb is not None:
return vb
vb = self.verb_t4(token)
if vb is not None:
return vb
vb = self.verb_t5(token)
if vb is not None:
return vb
return self.verb_t6(token)
",[],0,[],/stem/arlstem.py_verb
3714,/home/amandapotts/git/nltk/nltk/stem/arlstem.py_verb_t1,"def verb_t1(self, token):
""""""
stem the present prefixes and suffixes
""""""
if len(token) > 5 and token.startswith(""\u062A""):  # Taa
for s2 in self.pl_si2:
if token.endswith(s2):
return token[1:-2]
if len(token) > 5 and token.startswith(""\u064A""):  # Yaa
for s2 in self.verb_su2:
if token.endswith(s2):
return token[1:-2]
if len(token) > 4 and token.startswith(""\u0627""):  # Alif
if len(token) > 5 and token.endswith(""\u0648\u0627""):
return token[1:-2]
if token.endswith(""\u064A""):
return token[1:-1]
if token.endswith(""\u0627""):
return token[1:-1]
if token.endswith(""\u0646""):
return token[1:-1]
if len(token) > 4 and token.startswith(""\u064A"") and token.endswith(""\u0646""):
return token[1:-1]
if len(token) > 4 and token.startswith(""\u062A"") and token.endswith(""\u0646""):
return token[1:-1]
",[],0,[],/stem/arlstem.py_verb_t1
3715,/home/amandapotts/git/nltk/nltk/stem/arlstem.py_verb_t2,"def verb_t2(self, token):
""""""
stem the future prefixes and suffixes
""""""
if len(token) > 6:
for s2 in self.pl_si2:
if token.startswith(self.verb_pr2[0]) and token.endswith(s2):
return token[2:-2]
if token.startswith(self.verb_pr2[1]) and token.endswith(self.pl_si2[0]):
return token[2:-2]
if token.startswith(self.verb_pr2[1]) and token.endswith(self.pl_si2[2]):
return token[2:-2]
if (
len(token) > 5
and token.startswith(self.verb_pr2[0])
and token.endswith(""\u0646"")
):
return token[2:-1]
if (
len(token) > 5
and token.startswith(self.verb_pr2[1])
and token.endswith(""\u0646"")
):
return token[2:-1]
",[],0,[],/stem/arlstem.py_verb_t2
3716,/home/amandapotts/git/nltk/nltk/stem/arlstem.py_verb_t3,"def verb_t3(self, token):
""""""
stem the present suffixes
""""""
if len(token) > 5:
for su3 in self.verb_suf3:
if token.endswith(su3):
return token[:-3]
if len(token) > 4:
for su2 in self.verb_suf2:
if token.endswith(su2):
return token[:-2]
if len(token) > 3:
for su1 in self.verb_suf1:
if token.endswith(su1):
return token[:-1]
",[],0,[],/stem/arlstem.py_verb_t3
3717,/home/amandapotts/git/nltk/nltk/stem/arlstem.py_verb_t4,"def verb_t4(self, token):
""""""
stem the present prefixes
""""""
if len(token) > 3:
for pr1 in self.verb_suf1:
if token.startswith(pr1):
return token[1:]
if token.startswith(""\u064A""):
return token[1:]
",[],0,[],/stem/arlstem.py_verb_t4
3718,/home/amandapotts/git/nltk/nltk/stem/arlstem.py_verb_t5,"def verb_t5(self, token):
""""""
stem the future prefixes
""""""
if len(token) > 4:
for pr2 in self.verb_pr22:
if token.startswith(pr2):
return token[2:]
for pr2 in self.verb_pr2:
if token.startswith(pr2):
return token[2:]
return token
",[],0,[],/stem/arlstem.py_verb_t5
3719,/home/amandapotts/git/nltk/nltk/stem/arlstem.py_verb_t6,"def verb_t6(self, token):
""""""
stem the order prefixes
""""""
if len(token) > 4:
for pr3 in self.verb_pr33:
if token.startswith(pr3):
return token[2:]
return token
",[],0,[],/stem/arlstem.py_verb_t6
3720,/home/amandapotts/git/nltk/nltk/stem/cistem.py___init__,"def __init__(self, case_insensitive: bool = False):
self._case_insensitive = case_insensitive
",[],0,[],/stem/cistem.py___init__
3721,/home/amandapotts/git/nltk/nltk/stem/cistem.py_replace_to,"def replace_to(word: str) -> str:
word = word.replace(""sch"", ""$"")
word = word.replace(""ei"", ""%"")
word = word.replace(""ie"", ""&"")
word = Cistem.repl_xx.sub(r""\1*"", word)
return word
",[],0,[],/stem/cistem.py_replace_to
3722,/home/amandapotts/git/nltk/nltk/stem/cistem.py_replace_back,"def replace_back(word: str) -> str:
word = Cistem.repl_xx_back.sub(r""\1\1"", word)
word = word.replace(""%"", ""ei"")
word = word.replace(""&"", ""ie"")
word = word.replace(""$"", ""sch"")
return word
",[],0,[],/stem/cistem.py_replace_back
3723,/home/amandapotts/git/nltk/nltk/stem/cistem.py_stem,"def stem(self, word: str) -> str:
""""""Stems the input word.
:param word: The word that is to be stemmed.
:type word: str
:return: The stemmed word.
:rtype: str
>>> from nltk.stem.cistem import Cistem
>>> stemmer = Cistem()
>>> s1 = ""Speicherbehältern""
>>> stemmer.stem(s1)
'speicherbehalt'
>>> s2 = ""Grenzpostens""
>>> stemmer.stem(s2)
'grenzpost'
>>> s3 = ""Ausgefeiltere""
>>> stemmer.stem(s3)
'ausgefeilt'
>>> stemmer = Cistem(True)
>>> stemmer.stem(s1)
'speicherbehal'
>>> stemmer.stem(s2)
'grenzpo'
>>> stemmer.stem(s3)
'ausgefeil'
""""""
if len(word) == 0:
return word
upper = word[0].isupper()
word = word.lower()
word = word.replace(""ü"", ""u"")
word = word.replace(""ö"", ""o"")
word = word.replace(""ä"", ""a"")
word = word.replace(""ß"", ""ss"")
word = Cistem.strip_ge.sub(r""\1"", word)
return self._segment_inner(word, upper)[0]
",[],0,[],/stem/cistem.py_stem
3724,/home/amandapotts/git/nltk/nltk/stem/cistem.py_segment,"def segment(self, word: str) -> Tuple[str, str]:
""""""
This method works very similarly to stem (:func:'cistem.stem'). The difference is that in
addition to returning the stem, it also returns the rest that was removed at
the end. To be able to return the stem unchanged so the stem and the rest
can be concatenated to form the original word, all subsitutions that altered
the stem in any other way than by removing letters at the end were left out.
:param word: The word that is to be stemmed.
:type word: str
:return: A tuple of the stemmed word and the removed suffix.
:rtype: Tuple[str, str]
>>> from nltk.stem.cistem import Cistem
>>> stemmer = Cistem()
>>> s1 = ""Speicherbehältern""
>>> stemmer.segment(s1)
('speicherbehält', 'ern')
>>> s2 = ""Grenzpostens""
>>> stemmer.segment(s2)
('grenzpost', 'ens')
>>> s3 = ""Ausgefeiltere""
>>> stemmer.segment(s3)
('ausgefeilt', 'ere')
>>> stemmer = Cistem(True)
>>> stemmer.segment(s1)
('speicherbehäl', 'tern')
>>> stemmer.segment(s2)
('grenzpo', 'stens')
>>> stemmer.segment(s3)
('ausgefeil', 'tere')
""""""
if len(word) == 0:
return ("""", """")
upper = word[0].isupper()
word = word.lower()
return self._segment_inner(word, upper)
",[],0,[],/stem/cistem.py_segment
3725,/home/amandapotts/git/nltk/nltk/stem/cistem.py__segment_inner,"def _segment_inner(self, word: str, upper: bool):
""""""Inner method for iteratively applying the code stemming regexes.
This method receives a pre-processed variant of the word to be stemmed,
or the word to be segmented, and returns a tuple of the word and the
removed suffix.
:param word: A pre-processed variant of the word that is to be stemmed.
:type word: str
:param upper: Whether the original word started with a capital letter.
:type upper: bool
:return: A tuple of the stemmed word and the removed suffix.
:rtype: Tuple[str, str]
""""""
rest_length = 0
word_copy = word[:]
word = Cistem.replace_to(word)
rest = """"
while len(word) > 3:
if len(word) > 5:
word, n = Cistem.strip_emr.subn("""", word)
if n != 0:
rest_length += 2
continue
word, n = Cistem.strip_nd.subn("""", word)
if n != 0:
rest_length += 2
continue
if not upper or self._case_insensitive:
word, n = Cistem.strip_t.subn("""", word)
if n != 0:
rest_length += 1
continue
word, n = Cistem.strip_esn.subn("""", word)
if n != 0:
rest_length += 1
continue
else:
break
word = Cistem.replace_back(word)
if rest_length:
rest = word_copy[-rest_length:]
return (word, rest)
",[],0,[],/stem/cistem.py__segment_inner
3726,/home/amandapotts/git/nltk/nltk/stem/regexp.py___init__,"def __init__(self, regexp, min=0):
if not hasattr(regexp, ""pattern""):
regexp = re.compile(regexp)
self._regexp = regexp
self._min = min
",[],0,[],/stem/regexp.py___init__
3727,/home/amandapotts/git/nltk/nltk/stem/regexp.py_stem,"def stem(self, word):
if len(word) < self._min:
return word
else:
return self._regexp.sub("""", word)
",[],0,[],/stem/regexp.py_stem
3728,/home/amandapotts/git/nltk/nltk/stem/regexp.py___repr__,"def __repr__(self):
return f""<RegexpStemmer: {self._regexp.pattern!r}>""
",[],0,[],/stem/regexp.py___repr__
3729,/home/amandapotts/git/nltk/nltk/stem/lancaster.py___init__,"def __init__(self, rule_tuple=None, strip_prefix_flag=False):
""""""Create an instance of the Lancaster stemmer.""""""
self.rule_dictionary = {}
self._strip_prefix = strip_prefix_flag
self._rule_tuple = rule_tuple if rule_tuple else self.default_rule_tuple
",[],0,[],/stem/lancaster.py___init__
3730,/home/amandapotts/git/nltk/nltk/stem/lancaster.py_parseRules,"def parseRules(self, rule_tuple=None):
""""""Validate the set of rules used in this stemmer.
If this function is called as an individual method, without using stem
method, rule_tuple argument will be compiled into self.rule_dictionary.
If this function is called within stem, self._rule_tuple will be used.
""""""
rule_tuple = rule_tuple if rule_tuple else self._rule_tuple
valid_rule = re.compile(r""^[a-z]+\*?\d[a-z]*[>\.]?$"")
self.rule_dictionary = {}
for rule in rule_tuple:
if not valid_rule.match(rule):
raise ValueError(f""The rule {rule} is invalid"")
first_letter = rule[0:1]
if first_letter in self.rule_dictionary:
self.rule_dictionary[first_letter].append(rule)
else:
self.rule_dictionary[first_letter] = [rule]
",[],0,[],/stem/lancaster.py_parseRules
3731,/home/amandapotts/git/nltk/nltk/stem/lancaster.py_stem,"def stem(self, word):
""""""Stem a word using the Lancaster stemmer.""""""
word = word.lower()
word = self.__stripPrefix(word) if self._strip_prefix else word
intact_word = word
if not self.rule_dictionary:
self.parseRules()
return self.__doStemming(word, intact_word)
",[],0,[],/stem/lancaster.py_stem
3732,/home/amandapotts/git/nltk/nltk/stem/lancaster.py___doStemming,"def __doStemming(self, word, intact_word):
""""""Perform the actual word stemming""""""
valid_rule = re.compile(r""^([a-z]+)(\*?)(\d)([a-z]*)([>\.]?)$"")
proceed = True
while proceed:
last_letter_position = self.__getLastLetter(word)
if (
last_letter_position < 0
or word[last_letter_position] not in self.rule_dictionary
):
proceed = False
else:
rule_was_applied = False
for rule in self.rule_dictionary[word[last_letter_position]]:
rule_match = valid_rule.match(rule)
if rule_match:
(
ending_string,
intact_flag,
remove_total,
append_string,
cont_flag,
) = rule_match.groups()
remove_total = int(remove_total)
if word.endswith(ending_string[::-1]):
if intact_flag:
if word == intact_word and self.__isAcceptable(
word, remove_total
):
word = self.__applyRule(
word, remove_total, append_string
)
rule_was_applied = True
if cont_flag == ""."":
proceed = False
break
elif self.__isAcceptable(word, remove_total):
word = self.__applyRule(
word, remove_total, append_string
)
rule_was_applied = True
if cont_flag == ""."":
proceed = False
break
if rule_was_applied == False:
proceed = False
return word
",[],0,[],/stem/lancaster.py___doStemming
3733,/home/amandapotts/git/nltk/nltk/stem/lancaster.py___getLastLetter,"def __getLastLetter(self, word):
""""""Get the zero-based index of the last alphabetic character in this string""""""
last_letter = -1
for position in range(len(word)):
if word[position].isalpha():
last_letter = position
else:
break
return last_letter
",[],0,[],/stem/lancaster.py___getLastLetter
3734,/home/amandapotts/git/nltk/nltk/stem/lancaster.py___isAcceptable,"def __isAcceptable(self, word, remove_total):
""""""Determine if the word is acceptable for stemming.""""""
word_is_acceptable = False
if word[0] in ""aeiouy"":
if len(word) - remove_total >= 2:
word_is_acceptable = True
elif len(word) - remove_total >= 3:
if word[1] in ""aeiouy"":
word_is_acceptable = True
elif word[2] in ""aeiouy"":
word_is_acceptable = True
return word_is_acceptable
",[],0,[],/stem/lancaster.py___isAcceptable
3735,/home/amandapotts/git/nltk/nltk/stem/lancaster.py___applyRule,"def __applyRule(self, word, remove_total, append_string):
""""""Apply the stemming rule to the word""""""
new_word_length = len(word) - remove_total
word = word[0:new_word_length]
if append_string:
word += append_string
return word
",[],0,[],/stem/lancaster.py___applyRule
3736,/home/amandapotts/git/nltk/nltk/stem/lancaster.py___stripPrefix,"def __stripPrefix(self, word):
""""""Remove prefix from a word.
This function originally taken from Whoosh.
""""""
for prefix in (
""kilo"",
""micro"",
""milli"",
""intra"",
""ultra"",
""mega"",
""nano"",
""pico"",
""pseudo"",
):
if word.startswith(prefix):
return word[len(prefix) :]
return word
",[],0,[],/stem/lancaster.py___stripPrefix
3737,/home/amandapotts/git/nltk/nltk/stem/lancaster.py___repr__,"def __repr__(self):
return ""<LancasterStemmer>""
",[],0,[],/stem/lancaster.py___repr__
3738,/home/amandapotts/git/nltk/nltk/stem/porter.py___init__,"def __init__(self, mode=NLTK_EXTENSIONS):
if mode not in (
self.NLTK_EXTENSIONS,
self.MARTIN_EXTENSIONS,
self.ORIGINAL_ALGORITHM,
):
raise ValueError(
""Mode must be one of PorterStemmer.NLTK_EXTENSIONS, ""
""PorterStemmer.MARTIN_EXTENSIONS, or ""
""PorterStemmer.ORIGINAL_ALGORITHM""
)
self.mode = mode
if self.mode == self.NLTK_EXTENSIONS:
irregular_forms = {
""sky"": [""sky"", ""skies""],
""die"": [""dying""],
""lie"": [""lying""],
""tie"": [""tying""],
""news"": [""news""],
""inning"": [""innings"", ""inning""],
""outing"": [""outings"", ""outing""],
""canning"": [""cannings"", ""canning""],
""howe"": [""howe""],
""proceed"": [""proceed""],
""exceed"": [""exceed""],
""succeed"": [""succeed""],
}
self.pool = {}
for key in irregular_forms:
for val in irregular_forms[key]:
self.pool[val] = key
self.vowels = frozenset([""a"", ""e"", ""i"", ""o"", ""u""])
",[],0,[],/stem/porter.py___init__
3739,/home/amandapotts/git/nltk/nltk/stem/porter.py__is_consonant,"def _is_consonant(self, word, i):
""""""Returns True if word[i] is a consonant, False otherwise
A consonant is defined in the paper as follows:
A consonant in a word is a letter other than A, E, I, O or
U, and other than Y preceded by a consonant. (The fact that
the term `consonant' is defined to some extent in terms of
itself does not make it ambiguous.) So in TOY the consonants
are T and Y, and in SYZYGY they are S, Z and G. If a letter
is not a consonant it is a vowel.
""""""
if word[i] in self.vowels:
return False
if word[i] == ""y"":
if i == 0:
return True
else:
return not self._is_consonant(word, i - 1)
return True
",[],0,[],/stem/porter.py__is_consonant
3740,/home/amandapotts/git/nltk/nltk/stem/porter.py__measure,"def _measure(self, stem):
r""""""Returns the 'measure' of stem, per definition in the paper
From the paper:
A consonant will be denoted by c, a vowel by v. A list
ccc... of length greater than 0 will be denoted by C, and a
list vvv... of length greater than 0 will be denoted by V.
Any word, or part of a word, therefore has one of the four
forms:
CVCV ... C
CVCV ... V
VCVC ... C
VCVC ... V
These may all be represented by the single form
[C]VCVC ... [V]
where the square brackets denote arbitrary presence of their
contents. Using (VC){m} to denote VC repeated m times, this
may again be written as
[C](VC){m}[V].
m will be called the \measure\ of any word or word part when
represented in this form. The case m = 0 covers the null
word. Here are some examples:
m=0    TR,  EE,  TREE,  Y,  BY.
m=1    TROUBLE,  OATS,  TREES,  IVY.
m=2    TROUBLES,  PRIVATE,  OATEN,  ORRERY.
""""""
cv_sequence = """"
for i in range(len(stem)):
if self._is_consonant(stem, i):
cv_sequence += ""c""
else:
cv_sequence += ""v""
return cv_sequence.count(""vc"")
",[],0,[],/stem/porter.py__measure
3741,/home/amandapotts/git/nltk/nltk/stem/porter.py__has_positive_measure,"def _has_positive_measure(self, stem):
return self._measure(stem) > 0
",[],0,[],/stem/porter.py__has_positive_measure
3742,/home/amandapotts/git/nltk/nltk/stem/porter.py__contains_vowel,"def _contains_vowel(self, stem):
""""""Returns True if stem contains a vowel, else False""""""
for i in range(len(stem)):
if not self._is_consonant(stem, i):
return True
return False
",[],0,[],/stem/porter.py__contains_vowel
3743,/home/amandapotts/git/nltk/nltk/stem/porter.py__ends_double_consonant,"def _ends_double_consonant(self, word):
""""""Implements condition *d from the paper
Returns True if word ends with a double consonant
""""""
return (
len(word) >= 2
and word[-1] == word[-2]
and self._is_consonant(word, len(word) - 1)
)
",[],0,[],/stem/porter.py__ends_double_consonant
3744,/home/amandapotts/git/nltk/nltk/stem/porter.py__ends_cvc,"def _ends_cvc(self, word):
""""""Implements condition *o from the paper
From the paper:
(e.g. -WIL, -HOP).
""""""
return (
len(word) >= 3
and self._is_consonant(word, len(word) - 3)
and not self._is_consonant(word, len(word) - 2)
and self._is_consonant(word, len(word) - 1)
and word[-1] not in (""w"", ""x"", ""y"")
) or (
self.mode == self.NLTK_EXTENSIONS
and len(word) == 2
and not self._is_consonant(word, 0)
and self._is_consonant(word, 1)
)
",[],0,[],/stem/porter.py__ends_cvc
3745,/home/amandapotts/git/nltk/nltk/stem/porter.py__replace_suffix,"def _replace_suffix(self, word, suffix, replacement):
""""""Replaces `suffix` of `word` with `replacement""""""
assert word.endswith(suffix), ""Given word doesn't end with given suffix""
if suffix == """":
return word + replacement
else:
return word[: -len(suffix)] + replacement
",[],0,[],/stem/porter.py__replace_suffix
3746,/home/amandapotts/git/nltk/nltk/stem/porter.py__apply_rule_list,"def _apply_rule_list(self, word, rules):
""""""Applies the first applicable suffix-removal rule to the word
Takes a word and a list of suffix-removal rules represented as
3-tuples, with the first element being the suffix to remove,
the second element being the string to replace it with, and the
final element being the condition for the rule to be applicable,
or None if the rule is unconditional.
""""""
for rule in rules:
suffix, replacement, condition = rule
if suffix == ""*d"" and self._ends_double_consonant(word):
stem = word[:-2]
if condition is None or condition(stem):
return stem + replacement
else:
return word
if word.endswith(suffix):
stem = self._replace_suffix(word, suffix, """")
if condition is None or condition(stem):
return stem + replacement
else:
return word
return word
",[],0,[],/stem/porter.py__apply_rule_list
3747,/home/amandapotts/git/nltk/nltk/stem/porter.py__step1a,"def _step1a(self, word):
""""""Implements Step 1a from ""An algorithm for suffix stripping""
From the paper:
SSES -> SS                         caresses  ->  caress
IES  -> I                          ponies    ->  poni
ties      ->  ti
SS   -> SS                         caress    ->  caress
S    ->                            cats      ->  cat
""""""
if self.mode == self.NLTK_EXTENSIONS:
if word.endswith(""ies"") and len(word) == 4:
return self._replace_suffix(word, ""ies"", ""ie"")
return self._apply_rule_list(
word,
[
(""sses"", ""ss"", None),  # SSES -> SS
(""ies"", ""i"", None),  # IES  -> I
(""ss"", ""ss"", None),  # SS   -> SS
(""s"", """", None),  # S    ->
],
)
",[],0,[],/stem/porter.py__step1a
3748,/home/amandapotts/git/nltk/nltk/stem/porter.py__step1c,"def _step1c(self, word):
""""""Implements Step 1c from ""An algorithm for suffix stripping""
From the paper:
Step 1c
(*v*) Y -> I                    happy        ->  happi
sky          ->  sky
""""""
",[],0,[],/stem/porter.py__step1c
3749,/home/amandapotts/git/nltk/nltk/stem/porter.py_nltk_condition,"def nltk_condition(stem):
""""""
This has been modified from the original Porter algorithm so
that y->i is only done when y is preceded by a consonant,
but not if the stem is only a single consonant, i.e.
(*c and not c) Y -> I
So 'happy' -> 'happi', but
'enjoy' -> 'enjoy'  etc
This is a much better rule. Formerly 'enjoy'->'enjoi' and
'enjoyment'->'enjoy'. Step 1c is perhaps done too soon
with this modification that no longer really matters.
Also, the removal of the contains_vowel(z) condition means
that 'spy', 'fly', 'try' ... stem to 'spi', 'fli', 'tri' and
conflate with 'spied', 'tried', 'flies' ...
""""""
return len(stem) > 1 and self._is_consonant(stem, len(stem) - 1)
",[],0,[],/stem/porter.py_nltk_condition
3750,/home/amandapotts/git/nltk/nltk/stem/porter.py_original_condition,"def original_condition(stem):
return self._contains_vowel(stem)
",[],0,[],/stem/porter.py_original_condition
3751,/home/amandapotts/git/nltk/nltk/stem/porter.py__step3,"def _step3(self, word):
""""""Implements Step 3 from ""An algorithm for suffix stripping""
From the paper:
Step 3
(m>0) ICATE ->  IC              triplicate     ->  triplic
(m>0) ATIVE ->                  formative      ->  form
(m>0) ALIZE ->  AL              formalize      ->  formal
(m>0) ICITI ->  IC              electriciti    ->  electric
(m>0) ICAL  ->  IC              electrical     ->  electric
(m>0) FUL   ->                  hopeful        ->  hope
(m>0) NESS  ->                  goodness       ->  good
""""""
return self._apply_rule_list(
word,
[
(""icate"", ""ic"", self._has_positive_measure),
(""ative"", """", self._has_positive_measure),
(""alize"", ""al"", self._has_positive_measure),
(""iciti"", ""ic"", self._has_positive_measure),
(""ical"", ""ic"", self._has_positive_measure),
(""ful"", """", self._has_positive_measure),
(""ness"", """", self._has_positive_measure),
],
)
",[],0,[],/stem/porter.py__step3
3752,/home/amandapotts/git/nltk/nltk/stem/porter.py__step4,"def _step4(self, word):
""""""Implements Step 4 from ""An algorithm for suffix stripping""
Step 4
(m>1) AL    ->                  revival        ->  reviv
(m>1) ANCE  ->                  allowance      ->  allow
(m>1) ENCE  ->                  inference      ->  infer
(m>1) ER    ->                  airliner       ->  airlin
(m>1) IC    ->                  gyroscopic     ->  gyroscop
(m>1) ABLE  ->                  adjustable     ->  adjust
(m>1) IBLE  ->                  defensible     ->  defens
(m>1) ANT   ->                  irritant       ->  irrit
(m>1) EMENT ->                  replacement    ->  replac
(m>1) MENT  ->                  adjustment     ->  adjust
(m>1) ENT   ->                  dependent      ->  depend
(m>1 and (*S or *T)) ION ->     adoption       ->  adopt
(m>1) OU    ->                  homologou      ->  homolog
(m>1) ISM   ->                  communism      ->  commun
(m>1) ATE   ->                  activate       ->  activ
(m>1) ITI   ->                  angulariti     ->  angular
(m>1) OUS   ->                  homologous     ->  homolog
(m>1) IVE   ->                  effective      ->  effect
(m>1) IZE   ->                  bowdlerize     ->  bowdler
The suffixes are now removed. All that remains is a little
tidying up.
""""""
",[],0,[],/stem/porter.py__step4
3753,/home/amandapotts/git/nltk/nltk/stem/porter.py__step5a,"def _step5a(self, word):
""""""Implements Step 5a from ""An algorithm for suffix stripping""
From the paper:
Step 5a
(m>1) E     ->                  probate        ->  probat
rate           ->  rate
(m=1 and not *o) E ->           cease          ->  ceas
""""""
if word.endswith(""e""):
stem = self._replace_suffix(word, ""e"", """")
if self._measure(stem) > 1:
return stem
if self._measure(stem) == 1 and not self._ends_cvc(stem):
return stem
return word
",[],0,[],/stem/porter.py__step5a
3754,/home/amandapotts/git/nltk/nltk/stem/porter.py_stem,"def stem(self, word, to_lowercase=True):
""""""
:param to_lowercase: if `to_lowercase=True` the word always lowercase
""""""
stem = word.lower() if to_lowercase else word
if self.mode == self.NLTK_EXTENSIONS and word in self.pool:
return self.pool[stem]
if self.mode != self.ORIGINAL_ALGORITHM and len(word) <= 2:
return stem
stem = self._step1a(stem)
stem = self._step1b(stem)
stem = self._step1c(stem)
stem = self._step2(stem)
stem = self._step3(stem)
stem = self._step4(stem)
stem = self._step5a(stem)
stem = self._step5b(stem)
return stem
",[],0,[],/stem/porter.py_stem
3755,/home/amandapotts/git/nltk/nltk/stem/porter.py___repr__,"def __repr__(self):
return ""<PorterStemmer>""
",[],0,[],/stem/porter.py___repr__
3756,/home/amandapotts/git/nltk/nltk/stem/porter.py_demo,"def demo():
""""""
A demonstration of the porter stemmer on a sample from
the Penn Treebank corpus.
""""""
from nltk import stem
from nltk.corpus import treebank
stemmer = stem.PorterStemmer()
orig = []
stemmed = []
for item in treebank.fileids()[:3]:
for word, tag in treebank.tagged_words(item):
orig.append(word)
stemmed.append(stemmer.stem(word))
results = "" "".join(stemmed)
results = re.sub(r""(.{,70})\s"", r""\1\n"", results + "" "").rstrip()
original = "" "".join(orig)
original = re.sub(r""(.{,70})\s"", r""\1\n"", original + "" "").rstrip()
print(""-Original-"".center(70).replace("" "", ""*"").replace(""-"", "" ""))
print(original)
print(""-Results-"".center(70).replace("" "", ""*"").replace(""-"", "" ""))
print(results)
print(""*"" * 70)
",[],0,[],/stem/porter.py_demo
3757,/home/amandapotts/git/nltk/nltk/stem/util.py_suffix_replace,"def suffix_replace(original, old, new):
""""""
Replaces the old suffix of the original string by a new suffix
""""""
return original[: -len(old)] + new
",[],0,[],/stem/util.py_suffix_replace
3758,/home/amandapotts/git/nltk/nltk/stem/util.py_prefix_replace,"def prefix_replace(original, old, new):
""""""
Replaces the old prefix of the original string by a new suffix
:param original: string
:param old: string
:param new: string
:return: string
""""""
return new + original[len(old) :]
",[],0,[],/stem/util.py_prefix_replace
3759,/home/amandapotts/git/nltk/nltk/stem/snowball.py___init__,"def __init__(self, language, ignore_stopwords=False):
if language not in self.languages:
raise ValueError(f""The language '{language}' is not supported."")
stemmerclass = globals()[language.capitalize() + ""Stemmer""]
self.stemmer = stemmerclass(ignore_stopwords)
self.stem = self.stemmer.stem
self.stopwords = self.stemmer.stopwords
",[],0,[],/stem/snowball.py___init__
3760,/home/amandapotts/git/nltk/nltk/stem/snowball.py_stem,"def stem(self, token):
return self.stemmer.stem(self, token)
",[],0,[],/stem/snowball.py_stem
3761,/home/amandapotts/git/nltk/nltk/stem/snowball.py___init__,"def __init__(self, ignore_stopwords=False):
language = type(self).__name__.lower()
if language.endswith(""stemmer""):
language = language[:-7]
self.stopwords = set()
if ignore_stopwords:
try:
for word in stopwords.words(language):
self.stopwords.add(word)
except OSError as e:
raise ValueError(
""{!r} has no list of stopwords. Please set""
"" 'ignore_stopwords' to 'False'."".format(self)
) from e
",[],0,[],/stem/snowball.py___init__
3762,/home/amandapotts/git/nltk/nltk/stem/snowball.py___repr__,"def __repr__(self):
""""""
Print out the string representation of the respective class.
""""""
return f""<{type(self).__name__}>""
",[],0,[],/stem/snowball.py___repr__
3763,/home/amandapotts/git/nltk/nltk/stem/snowball.py___init__,"def __init__(self, ignore_stopwords=False):
_LanguageSpecificStemmer.__init__(self, ignore_stopwords)
porter.PorterStemmer.__init__(self)
",[],0,[],/stem/snowball.py___init__
3764,/home/amandapotts/git/nltk/nltk/stem/snowball.py__r1_scandinavian,"def _r1_scandinavian(self, word, vowels):
""""""
Return the region R1 that is used by the Scandinavian stemmers.
R1 is the region after the first non-vowel following a vowel,
or is the null region at the end of the word if there is no
such non-vowel. But then R1 is adjusted so that the region
before it contains at least three letters.
:param word: The word whose region R1 is determined.
:type word: str or unicode
:param vowels: The vowels of the respective language that are
used to determine the region R1.
:type vowels: unicode
:return: the region R1 for the respective word.
:rtype: unicode
:note: This helper method is invoked by the respective stem method of
the subclasses DanishStemmer, NorwegianStemmer, and
SwedishStemmer. It is not to be invoked directly!
""""""
r1 = """"
for i in range(1, len(word)):
if word[i] not in vowels and word[i - 1] in vowels:
if 3 > len(word[: i + 1]) > 0:
r1 = word[3:]
elif len(word[: i + 1]) >= 3:
r1 = word[i + 1 :]
else:
return word
break
return r1
",[],0,[],/stem/snowball.py__r1_scandinavian
3765,/home/amandapotts/git/nltk/nltk/stem/snowball.py__r1r2_standard,"def _r1r2_standard(self, word, vowels):
""""""
Return the standard interpretations of the string regions R1 and R2.
R1 is the region after the first non-vowel following a vowel,
or is the null region at the end of the word if there is no
such non-vowel.
R2 is the region after the first non-vowel following a vowel
in R1, or is the null region at the end of the word if there
is no such non-vowel.
:param word: The word whose regions R1 and R2 are determined.
:type word: str or unicode
:param vowels: The vowels of the respective language that are
used to determine the regions R1 and R2.
:type vowels: unicode
:return: (r1,r2), the regions R1 and R2 for the respective word.
:rtype: tuple
:note: This helper method is invoked by the respective stem method of
the subclasses DutchStemmer, FinnishStemmer,
FrenchStemmer, GermanStemmer, ItalianStemmer,
PortugueseStemmer, RomanianStemmer, and SpanishStemmer.
It is not to be invoked directly!
:note: A detailed description of how to define R1 and R2
can be found at http://snowball.tartarus.org/texts/r1r2.html
""""""
r1 = """"
r2 = """"
for i in range(1, len(word)):
if word[i] not in vowels and word[i - 1] in vowels:
r1 = word[i + 1 :]
break
for i in range(1, len(r1)):
if r1[i] not in vowels and r1[i - 1] in vowels:
r2 = r1[i + 1 :]
break
return (r1, r2)
",[],0,[],/stem/snowball.py__r1r2_standard
3766,/home/amandapotts/git/nltk/nltk/stem/snowball.py__rv_standard,"def _rv_standard(self, word, vowels):
""""""
Return the standard interpretation of the string region RV.
If the second letter is a consonant, RV is the region after the
next following vowel. If the first two letters are vowels, RV is
the region after the next following consonant. Otherwise, RV is
the region after the third letter.
:param word: The word whose region RV is determined.
:type word: str or unicode
:param vowels: The vowels of the respective language that are
used to determine the region RV.
:type vowels: unicode
:return: the region RV for the respective word.
:rtype: unicode
:note: This helper method is invoked by the respective stem method of
the subclasses ItalianStemmer, PortugueseStemmer,
RomanianStemmer, and SpanishStemmer. It is not to be
invoked directly!
""""""
rv = """"
if len(word) >= 2:
if word[1] not in vowels:
for i in range(2, len(word)):
if word[i] in vowels:
rv = word[i + 1 :]
break
elif word[0] in vowels and word[1] in vowels:
for i in range(2, len(word)):
if word[i] not in vowels:
rv = word[i + 1 :]
break
else:
rv = word[3:]
return rv
",[],0,[],/stem/snowball.py__rv_standard
3767,/home/amandapotts/git/nltk/nltk/stem/snowball.py___normalize_pre,"def __normalize_pre(self, token):
""""""
:param token: string
:return: normalized token type string
""""""
token = self.__vocalization.sub("""", token)
token = self.__kasheeda.sub("""", token)
token = self.__arabic_punctuation_marks.sub("""", token)
return token
",[],0,[],/stem/snowball.py___normalize_pre
3768,/home/amandapotts/git/nltk/nltk/stem/snowball.py___normalize_post,"def __normalize_post(self, token):
for hamza in self.__last_hamzat:
if token.endswith(hamza):
token = suffix_replace(token, hamza, ""\u0621"")
break
token = self.__initial_hamzat.sub(""\u0627"", token)
token = self.__waw_hamza.sub(""\u0648"", token)
token = self.__yeh_hamza.sub(""\u064a"", token)
token = self.__alefat.sub(""\u0627"", token)
return token
",[],0,[],/stem/snowball.py___normalize_post
3769,/home/amandapotts/git/nltk/nltk/stem/snowball.py___checks_1,"def __checks_1(self, token):
for prefix in self.__checks1:
if token.startswith(prefix):
if prefix in self.__articles_3len and len(token) > 4:
self.is_noun = True
self.is_verb = False
self.is_defined = True
break
if prefix in self.__articles_2len and len(token) > 3:
self.is_noun = True
self.is_verb = False
self.is_defined = True
break
",[],0,[],/stem/snowball.py___checks_1
3770,/home/amandapotts/git/nltk/nltk/stem/snowball.py___checks_2,"def __checks_2(self, token):
for suffix in self.__checks2:
if token.endswith(suffix):
if suffix == ""\u0629"" and len(token) > 2:
self.is_noun = True
self.is_verb = False
break
if suffix == ""\u0627\u062a"" and len(token) > 3:
self.is_noun = True
self.is_verb = False
break
",[],0,[],/stem/snowball.py___checks_2
3771,/home/amandapotts/git/nltk/nltk/stem/snowball.py___Suffix_Verb_Step1,"def __Suffix_Verb_Step1(self, token):
for suffix in self.__suffix_verb_step1:
if token.endswith(suffix):
if suffix in self.__conjugation_suffix_verb_1 and len(token) >= 4:
token = token[:-1]
self.suffixes_verb_step1_success = True
break
if suffix in self.__conjugation_suffix_verb_2 and len(token) >= 5:
token = token[:-2]
self.suffixes_verb_step1_success = True
break
if suffix in self.__conjugation_suffix_verb_3 and len(token) >= 6:
token = token[:-3]
self.suffixes_verb_step1_success = True
break
return token
",[],0,[],/stem/snowball.py___Suffix_Verb_Step1
3772,/home/amandapotts/git/nltk/nltk/stem/snowball.py___Suffix_Verb_Step2a,"def __Suffix_Verb_Step2a(self, token):
for suffix in self.__suffix_verb_step2a:
if token.endswith(suffix) and len(token) > 3:
if suffix == ""\u062a"" and len(token) >= 4:
token = token[:-1]
self.suffix_verb_step2a_success = True
break
if suffix in self.__conjugation_suffix_verb_4 and len(token) >= 4:
token = token[:-1]
self.suffix_verb_step2a_success = True
break
if suffix in self.__conjugation_suffix_verb_past and len(token) >= 5:
token = token[:-2]  # past
self.suffix_verb_step2a_success = True
break
if suffix in self.__conjugation_suffix_verb_present and len(token) > 5:
token = token[:-2]  # present
self.suffix_verb_step2a_success = True
break
if suffix == ""\u062a\u0645\u0627"" and len(token) >= 6:
token = token[:-3]
self.suffix_verb_step2a_success = True
break
return token
",[],0,[],/stem/snowball.py___Suffix_Verb_Step2a
3773,/home/amandapotts/git/nltk/nltk/stem/snowball.py___Suffix_Verb_Step2c,"def __Suffix_Verb_Step2c(self, token):
for suffix in self.__suffix_verb_step2c:
if token.endswith(suffix):
if suffix == ""\u062a\u0645\u0648"" and len(token) >= 6:
token = token[:-3]
break
if suffix == ""\u0648"" and len(token) >= 4:
token = token[:-1]
break
return token
",[],0,[],/stem/snowball.py___Suffix_Verb_Step2c
3774,/home/amandapotts/git/nltk/nltk/stem/snowball.py___Suffix_Verb_Step2b,"def __Suffix_Verb_Step2b(self, token):
for suffix in self.__suffix_verb_step2b:
if token.endswith(suffix) and len(token) >= 5:
token = token[:-2]
self.suffix_verb_step2b_success = True
break
return token
",[],0,[],/stem/snowball.py___Suffix_Verb_Step2b
3775,/home/amandapotts/git/nltk/nltk/stem/snowball.py___Suffix_Noun_Step2c2,"def __Suffix_Noun_Step2c2(self, token):
for suffix in self.__suffix_noun_step2c2:
if token.endswith(suffix) and len(token) >= 3:
token = token[:-1]
self.suffix_noun_step2c2_success = True
break
return token
",[],0,[],/stem/snowball.py___Suffix_Noun_Step2c2
3776,/home/amandapotts/git/nltk/nltk/stem/snowball.py___Suffix_Noun_Step1a,"def __Suffix_Noun_Step1a(self, token):
for suffix in self.__suffix_noun_step1a:
if token.endswith(suffix):
if suffix in self.__conjugation_suffix_noun_1 and len(token) >= 4:
token = token[:-1]
self.suffix_noun_step1a_success = True
break
if suffix in self.__conjugation_suffix_noun_2 and len(token) >= 5:
token = token[:-2]
self.suffix_noun_step1a_success = True
break
if suffix in self.__conjugation_suffix_noun_3 and len(token) >= 6:
token = token[:-3]
self.suffix_noun_step1a_success = True
break
return token
",[],0,[],/stem/snowball.py___Suffix_Noun_Step1a
3777,/home/amandapotts/git/nltk/nltk/stem/snowball.py___Suffix_Noun_Step2a,"def __Suffix_Noun_Step2a(self, token):
for suffix in self.__suffix_noun_step2a:
if token.endswith(suffix) and len(token) > 4:
token = token[:-1]
self.suffix_noun_step2a_success = True
break
return token
",[],0,[],/stem/snowball.py___Suffix_Noun_Step2a
3778,/home/amandapotts/git/nltk/nltk/stem/snowball.py___Suffix_Noun_Step2b,"def __Suffix_Noun_Step2b(self, token):
for suffix in self.__suffix_noun_step2b:
if token.endswith(suffix) and len(token) >= 5:
token = token[:-2]
self.suffix_noun_step2b_success = True
break
return token
",[],0,[],/stem/snowball.py___Suffix_Noun_Step2b
3779,/home/amandapotts/git/nltk/nltk/stem/snowball.py___Suffix_Noun_Step2c1,"def __Suffix_Noun_Step2c1(self, token):
for suffix in self.__suffix_noun_step2c1:
if token.endswith(suffix) and len(token) >= 4:
token = token[:-1]
break
return token
",[],0,[],/stem/snowball.py___Suffix_Noun_Step2c1
3780,/home/amandapotts/git/nltk/nltk/stem/snowball.py___Suffix_Noun_Step1b,"def __Suffix_Noun_Step1b(self, token):
for suffix in self.__suffix_noun_step1b:
if token.endswith(suffix) and len(token) > 5:
token = token[:-1]
self.suffixe_noun_step1b_success = True
break
return token
",[],0,[],/stem/snowball.py___Suffix_Noun_Step1b
3781,/home/amandapotts/git/nltk/nltk/stem/snowball.py___Suffix_Noun_Step3,"def __Suffix_Noun_Step3(self, token):
for suffix in self.__suffix_noun_step3:
if token.endswith(suffix) and len(token) >= 3:
token = token[:-1]  # ya' nisbiya
break
return token
",[],0,[],/stem/snowball.py___Suffix_Noun_Step3
3782,/home/amandapotts/git/nltk/nltk/stem/snowball.py___Suffix_All_alef_maqsura,"def __Suffix_All_alef_maqsura(self, token):
for suffix in self.__suffix_all_alef_maqsura:
if token.endswith(suffix):
token = suffix_replace(token, suffix, ""\u064a"")
return token
",[],0,[],/stem/snowball.py___Suffix_All_alef_maqsura
3783,/home/amandapotts/git/nltk/nltk/stem/snowball.py___Prefix_Step1,"def __Prefix_Step1(self, token):
for prefix in self.__prefix_step1:
if token.startswith(prefix) and len(token) > 3:
if prefix == ""\u0623\u0623"":
token = prefix_replace(token, prefix, ""\u0623"")
break
elif prefix == ""\u0623\u0622"":
token = prefix_replace(token, prefix, ""\u0622"")
break
elif prefix == ""\u0623\u0624"":
token = prefix_replace(token, prefix, ""\u0624"")
break
elif prefix == ""\u0623\u0627"":
token = prefix_replace(token, prefix, ""\u0627"")
break
elif prefix == ""\u0623\u0625"":
token = prefix_replace(token, prefix, ""\u0625"")
break
return token
",[],0,[],/stem/snowball.py___Prefix_Step1
3784,/home/amandapotts/git/nltk/nltk/stem/snowball.py___Prefix_Step2a,"def __Prefix_Step2a(self, token):
for prefix in self.__prefix_step2a:
if token.startswith(prefix) and len(token) > 5:
token = token[len(prefix) :]
self.prefix_step2a_success = True
break
return token
",[],0,[],/stem/snowball.py___Prefix_Step2a
3785,/home/amandapotts/git/nltk/nltk/stem/snowball.py___Prefix_Step2b,"def __Prefix_Step2b(self, token):
for prefix in self.__prefix_step2b:
if token.startswith(prefix) and len(token) > 3:
if token[:2] not in self.__prefixes1:
token = token[len(prefix) :]
break
return token
",[],0,[],/stem/snowball.py___Prefix_Step2b
3786,/home/amandapotts/git/nltk/nltk/stem/snowball.py___Prefix_Step3a_Noun,"def __Prefix_Step3a_Noun(self, token):
for prefix in self.__prefix_step3a_noun:
if token.startswith(prefix):
if prefix in self.__articles_2len and len(token) > 4:
token = token[len(prefix) :]
self.prefix_step3a_noun_success = True
break
if prefix in self.__articles_3len and len(token) > 5:
token = token[len(prefix) :]
break
return token
",[],0,[],/stem/snowball.py___Prefix_Step3a_Noun
3787,/home/amandapotts/git/nltk/nltk/stem/snowball.py___Prefix_Step3b_Noun,"def __Prefix_Step3b_Noun(self, token):
for prefix in self.__prefix_step3b_noun:
if token.startswith(prefix):
if len(token) > 3:
if prefix == ""\u0628"":
token = token[len(prefix) :]
self.prefix_step3b_noun_success = True
break
if prefix in self.__prepositions2:
token = prefix_replace(token, prefix, prefix[1])
self.prefix_step3b_noun_success = True
break
if prefix in self.__prepositions1 and len(token) > 4:
token = token[len(prefix) :]  # BUG: cause confusion
self.prefix_step3b_noun_success = True
break
return token
",[],0,[],/stem/snowball.py___Prefix_Step3b_Noun
3788,/home/amandapotts/git/nltk/nltk/stem/snowball.py___Prefix_Step3_Verb,"def __Prefix_Step3_Verb(self, token):
for prefix in self.__prefix_step3_verb:
if token.startswith(prefix) and len(token) > 4:
token = prefix_replace(token, prefix, prefix[1])
break
return token
",[],0,[],/stem/snowball.py___Prefix_Step3_Verb
3789,/home/amandapotts/git/nltk/nltk/stem/snowball.py___Prefix_Step4_Verb,"def __Prefix_Step4_Verb(self, token):
for prefix in self.__prefix_step4_verb:
if token.startswith(prefix) and len(token) > 4:
token = prefix_replace(token, prefix, ""\u0627\u0633\u062a"")
self.is_verb = True
self.is_noun = False
break
return token
",[],0,[],/stem/snowball.py___Prefix_Step4_Verb
3790,/home/amandapotts/git/nltk/nltk/stem/snowball.py_stem,"def stem(self, word):
""""""
Stem an Arabic word and return the stemmed form.
:param word: string
:return: string
""""""
self.is_verb = True
self.is_noun = True
self.is_defined = False
self.suffix_verb_step2a_success = False
self.suffix_verb_step2b_success = False
self.suffix_noun_step2c2_success = False
self.suffix_noun_step1a_success = False
self.suffix_noun_step2a_success = False
self.suffix_noun_step2b_success = False
self.suffixe_noun_step1b_success = False
self.prefix_step2a_success = False
self.prefix_step3a_noun_success = False
self.prefix_step3b_noun_success = False
modified_word = word
self.__checks_1(modified_word)
self.__checks_2(modified_word)
modified_word = self.__normalize_pre(modified_word)
if modified_word in self.stopwords or len(modified_word) <= 2:
return modified_word
if self.is_verb:
modified_word = self.__Suffix_Verb_Step1(modified_word)
if self.suffixes_verb_step1_success:
modified_word = self.__Suffix_Verb_Step2a(modified_word)
if not self.suffix_verb_step2a_success:
modified_word = self.__Suffix_Verb_Step2c(modified_word)
else:
modified_word = self.__Suffix_Verb_Step2b(modified_word)
if not self.suffix_verb_step2b_success:
modified_word = self.__Suffix_Verb_Step2a(modified_word)
if self.is_noun:
modified_word = self.__Suffix_Noun_Step2c2(modified_word)
if not self.suffix_noun_step2c2_success:
if not self.is_defined:
modified_word = self.__Suffix_Noun_Step1a(modified_word)
modified_word = self.__Suffix_Noun_Step2a(modified_word)
if not self.suffix_noun_step2a_success:
modified_word = self.__Suffix_Noun_Step2b(modified_word)
if (
not self.suffix_noun_step2b_success
and not self.suffix_noun_step2a_success
):
modified_word = self.__Suffix_Noun_Step2c1(modified_word)
else:
modified_word = self.__Suffix_Noun_Step1b(modified_word)
if self.suffixe_noun_step1b_success:
modified_word = self.__Suffix_Noun_Step2a(modified_word)
if not self.suffix_noun_step2a_success:
modified_word = self.__Suffix_Noun_Step2b(modified_word)
if (
not self.suffix_noun_step2b_success
and not self.suffix_noun_step2a_success
):
modified_word = self.__Suffix_Noun_Step2c1(modified_word)
else:
if not self.is_defined:
modified_word = self.__Suffix_Noun_Step2a(modified_word)
modified_word = self.__Suffix_Noun_Step2b(modified_word)
modified_word = self.__Suffix_Noun_Step3(modified_word)
if not self.is_noun and self.is_verb:
modified_word = self.__Suffix_All_alef_maqsura(modified_word)
modified_word = self.__Prefix_Step1(modified_word)
modified_word = self.__Prefix_Step2a(modified_word)
if not self.prefix_step2a_success:
modified_word = self.__Prefix_Step2b(modified_word)
modified_word = self.__Prefix_Step3a_Noun(modified_word)
if not self.prefix_step3a_noun_success and self.is_noun:
modified_word = self.__Prefix_Step3b_Noun(modified_word)
else:
if not self.prefix_step3b_noun_success and self.is_verb:
modified_word = self.__Prefix_Step3_Verb(modified_word)
modified_word = self.__Prefix_Step4_Verb(modified_word)
modified_word = self.__normalize_post(modified_word)
stemmed_word = modified_word
return stemmed_word
",[],0,[],/stem/snowball.py_stem
3791,/home/amandapotts/git/nltk/nltk/stem/snowball.py_stem,"def stem(self, word):
""""""
Stem a Danish word and return the stemmed form.
:param word: The word that is stemmed.
:type word: str or unicode
:return: The stemmed form.
:rtype: unicode
""""""
word = word.lower()
if word in self.stopwords:
return word
r1 = self._r1_scandinavian(word, self.__vowels)
for suffix in self.__step1_suffixes:
if r1.endswith(suffix):
if suffix == ""s"":
if word[-2] in self.__s_ending:
word = word[:-1]
r1 = r1[:-1]
else:
word = word[: -len(suffix)]
r1 = r1[: -len(suffix)]
break
for suffix in self.__step2_suffixes:
if r1.endswith(suffix):
word = word[:-1]
r1 = r1[:-1]
break
if r1.endswith(""igst""):
word = word[:-2]
r1 = r1[:-2]
for suffix in self.__step3_suffixes:
if r1.endswith(suffix):
if suffix == ""l\xF8st"":
word = word[:-1]
r1 = r1[:-1]
else:
word = word[: -len(suffix)]
r1 = r1[: -len(suffix)]
if r1.endswith(self.__step2_suffixes):
word = word[:-1]
r1 = r1[:-1]
break
for double_cons in self.__double_consonants:
if word.endswith(double_cons) and len(word) > 3:
word = word[:-1]
break
return word
",[],0,[],/stem/snowball.py_stem
3792,/home/amandapotts/git/nltk/nltk/stem/snowball.py_stem,"def stem(self, word):
""""""
Stem a Dutch word and return the stemmed form.
:param word: The word that is stemmed.
:type word: str or unicode
:return: The stemmed form.
:rtype: unicode
""""""
word = word.lower()
if word in self.stopwords:
return word
step2_success = False
word = (
word.replace(""\xE4"", ""a"")
.replace(""\xE1"", ""a"")
.replace(""\xEB"", ""e"")
.replace(""\xE9"", ""e"")
.replace(""\xED"", ""i"")
.replace(""\xEF"", ""i"")
.replace(""\xF6"", ""o"")
.replace(""\xF3"", ""o"")
.replace(""\xFC"", ""u"")
.replace(""\xFA"", ""u"")
)
if word.startswith(""y""):
word = """".join((""Y"", word[1:]))
for i in range(1, len(word)):
if word[i - 1] in self.__vowels and word[i] == ""y"":
word = """".join((word[:i], ""Y"", word[i + 1 :]))
for i in range(1, len(word) - 1):
if (
word[i - 1] in self.__vowels
and word[i] == ""i""
and word[i + 1] in self.__vowels
):
word = """".join((word[:i], ""I"", word[i + 1 :]))
r1, r2 = self._r1r2_standard(word, self.__vowels)
for i in range(1, len(word)):
if word[i] not in self.__vowels and word[i - 1] in self.__vowels:
if 3 > len(word[: i + 1]) > 0:
r1 = word[3:]
elif len(word[: i + 1]) == 0:
return word
break
for suffix in self.__step1_suffixes:
if r1.endswith(suffix):
if suffix == ""heden"":
word = suffix_replace(word, suffix, ""heid"")
r1 = suffix_replace(r1, suffix, ""heid"")
if r2.endswith(""heden""):
r2 = suffix_replace(r2, suffix, ""heid"")
elif (
suffix in (""ene"", ""en"")
and not word.endswith(""heden"")
and word[-len(suffix) - 1] not in self.__vowels
and word[-len(suffix) - 3 : -len(suffix)] != ""gem""
):
word = word[: -len(suffix)]
r1 = r1[: -len(suffix)]
r2 = r2[: -len(suffix)]
if word.endswith((""kk"", ""dd"", ""tt"")):
word = word[:-1]
r1 = r1[:-1]
r2 = r2[:-1]
elif (
suffix in (""se"", ""s"")
and word[-len(suffix) - 1] not in self.__vowels
and word[-len(suffix) - 1] != ""j""
):
word = word[: -len(suffix)]
r1 = r1[: -len(suffix)]
r2 = r2[: -len(suffix)]
break
if r1.endswith(""e"") and word[-2] not in self.__vowels:
step2_success = True
word = word[:-1]
r1 = r1[:-1]
r2 = r2[:-1]
if word.endswith((""kk"", ""dd"", ""tt"")):
word = word[:-1]
r1 = r1[:-1]
r2 = r2[:-1]
if r2.endswith(""heid"") and word[-5] != ""c"":
word = word[:-4]
r1 = r1[:-4]
r2 = r2[:-4]
if (
r1.endswith(""en"")
and word[-3] not in self.__vowels
and word[-5:-2] != ""gem""
):
word = word[:-2]
r1 = r1[:-2]
r2 = r2[:-2]
if word.endswith((""kk"", ""dd"", ""tt"")):
word = word[:-1]
r1 = r1[:-1]
r2 = r2[:-1]
for suffix in self.__step3b_suffixes:
if r2.endswith(suffix):
if suffix in (""end"", ""ing""):
word = word[:-3]
r2 = r2[:-3]
if r2.endswith(""ig"") and word[-3] != ""e"":
word = word[:-2]
else:
if word.endswith((""kk"", ""dd"", ""tt"")):
word = word[:-1]
elif suffix == ""ig"" and word[-3] != ""e"":
word = word[:-2]
elif suffix == ""lijk"":
word = word[:-4]
r1 = r1[:-4]
if r1.endswith(""e"") and word[-2] not in self.__vowels:
word = word[:-1]
if word.endswith((""kk"", ""dd"", ""tt"")):
word = word[:-1]
elif suffix == ""baar"":
word = word[:-4]
elif suffix == ""bar"" and step2_success:
word = word[:-3]
break
if len(word) >= 4:
if word[-1] not in self.__vowels and word[-1] != ""I"":
if word[-3:-1] in (""aa"", ""ee"", ""oo"", ""uu""):
if word[-4] not in self.__vowels:
word = """".join((word[:-3], word[-3], word[-1]))
word = word.replace(""I"", ""i"").replace(""Y"", ""y"")
return word
",[],0,[],/stem/snowball.py_stem
3793,/home/amandapotts/git/nltk/nltk/stem/snowball.py_stem,"def stem(self, word):
""""""
Stem an English word and return the stemmed form.
:param word: The word that is stemmed.
:type word: str or unicode
:return: The stemmed form.
:rtype: unicode
""""""
word = word.lower()
if word in self.stopwords or len(word) <= 2:
return word
elif word in self.__special_words:
return self.__special_words[word]
word = (
word.replace(""\u2019"", ""\x27"")
.replace(""\u2018"", ""\x27"")
.replace(""\u201B"", ""\x27"")
)
if word.startswith(""\x27""):
word = word[1:]
if word.startswith(""y""):
word = """".join((""Y"", word[1:]))
for i in range(1, len(word)):
if word[i - 1] in self.__vowels and word[i] == ""y"":
word = """".join((word[:i], ""Y"", word[i + 1 :]))
step1a_vowel_found = False
step1b_vowel_found = False
r1 = """"
r2 = """"
if word.startswith((""gener"", ""commun"", ""arsen"")):
if word.startswith((""gener"", ""arsen"")):
r1 = word[5:]
else:
r1 = word[6:]
for i in range(1, len(r1)):
if r1[i] not in self.__vowels and r1[i - 1] in self.__vowels:
r2 = r1[i + 1 :]
break
else:
r1, r2 = self._r1r2_standard(word, self.__vowels)
for suffix in self.__step0_suffixes:
if word.endswith(suffix):
word = word[: -len(suffix)]
r1 = r1[: -len(suffix)]
r2 = r2[: -len(suffix)]
break
for suffix in self.__step1a_suffixes:
if word.endswith(suffix):
if suffix == ""sses"":
word = word[:-2]
r1 = r1[:-2]
r2 = r2[:-2]
elif suffix in (""ied"", ""ies""):
if len(word[: -len(suffix)]) > 1:
word = word[:-2]
r1 = r1[:-2]
r2 = r2[:-2]
else:
word = word[:-1]
r1 = r1[:-1]
r2 = r2[:-1]
elif suffix == ""s"":
for letter in word[:-2]:
if letter in self.__vowels:
step1a_vowel_found = True
break
if step1a_vowel_found:
word = word[:-1]
r1 = r1[:-1]
r2 = r2[:-1]
break
for suffix in self.__step1b_suffixes:
if word.endswith(suffix):
if suffix in (""eed"", ""eedly""):
if r1.endswith(suffix):
word = suffix_replace(word, suffix, ""ee"")
if len(r1) >= len(suffix):
r1 = suffix_replace(r1, suffix, ""ee"")
else:
r1 = """"
if len(r2) >= len(suffix):
r2 = suffix_replace(r2, suffix, ""ee"")
else:
r2 = """"
else:
for letter in word[: -len(suffix)]:
if letter in self.__vowels:
step1b_vowel_found = True
break
if step1b_vowel_found:
word = word[: -len(suffix)]
r1 = r1[: -len(suffix)]
r2 = r2[: -len(suffix)]
if word.endswith((""at"", ""bl"", ""iz"")):
word = """".join((word, ""e""))
r1 = """".join((r1, ""e""))
if len(word) > 5 or len(r1) >= 3:
r2 = """".join((r2, ""e""))
elif word.endswith(self.__double_consonants):
word = word[:-1]
r1 = r1[:-1]
r2 = r2[:-1]
elif (
r1 == """"
and len(word) >= 3
and word[-1] not in self.__vowels
and word[-1] not in ""wxY""
and word[-2] in self.__vowels
and word[-3] not in self.__vowels
) or (
r1 == """"
and len(word) == 2
and word[0] in self.__vowels
and word[1] not in self.__vowels
):
word = """".join((word, ""e""))
if len(r1) > 0:
r1 = """".join((r1, ""e""))
if len(r2) > 0:
r2 = """".join((r2, ""e""))
break
if len(word) > 2 and word[-1] in ""yY"" and word[-2] not in self.__vowels:
word = """".join((word[:-1], ""i""))
if len(r1) >= 1:
r1 = """".join((r1[:-1], ""i""))
else:
r1 = """"
if len(r2) >= 1:
r2 = """".join((r2[:-1], ""i""))
else:
r2 = """"
for suffix in self.__step2_suffixes:
if word.endswith(suffix):
if r1.endswith(suffix):
if suffix == ""tional"":
word = word[:-2]
r1 = r1[:-2]
r2 = r2[:-2]
elif suffix in (""enci"", ""anci"", ""abli""):
word = """".join((word[:-1], ""e""))
if len(r1) >= 1:
r1 = """".join((r1[:-1], ""e""))
else:
r1 = """"
if len(r2) >= 1:
r2 = """".join((r2[:-1], ""e""))
else:
r2 = """"
elif suffix == ""entli"":
word = word[:-2]
r1 = r1[:-2]
r2 = r2[:-2]
elif suffix in (""izer"", ""ization""):
word = suffix_replace(word, suffix, ""ize"")
if len(r1) >= len(suffix):
r1 = suffix_replace(r1, suffix, ""ize"")
else:
r1 = """"
if len(r2) >= len(suffix):
r2 = suffix_replace(r2, suffix, ""ize"")
else:
r2 = """"
elif suffix in (""ational"", ""ation"", ""ator""):
word = suffix_replace(word, suffix, ""ate"")
if len(r1) >= len(suffix):
r1 = suffix_replace(r1, suffix, ""ate"")
else:
r1 = """"
if len(r2) >= len(suffix):
r2 = suffix_replace(r2, suffix, ""ate"")
else:
r2 = ""e""
elif suffix in (""alism"", ""aliti"", ""alli""):
word = suffix_replace(word, suffix, ""al"")
if len(r1) >= len(suffix):
r1 = suffix_replace(r1, suffix, ""al"")
else:
r1 = """"
if len(r2) >= len(suffix):
r2 = suffix_replace(r2, suffix, ""al"")
else:
r2 = """"
elif suffix == ""fulness"":
word = word[:-4]
r1 = r1[:-4]
r2 = r2[:-4]
elif suffix in (""ousli"", ""ousness""):
word = suffix_replace(word, suffix, ""ous"")
if len(r1) >= len(suffix):
r1 = suffix_replace(r1, suffix, ""ous"")
else:
r1 = """"
if len(r2) >= len(suffix):
r2 = suffix_replace(r2, suffix, ""ous"")
else:
r2 = """"
elif suffix in (""iveness"", ""iviti""):
word = suffix_replace(word, suffix, ""ive"")
if len(r1) >= len(suffix):
r1 = suffix_replace(r1, suffix, ""ive"")
else:
r1 = """"
if len(r2) >= len(suffix):
r2 = suffix_replace(r2, suffix, ""ive"")
else:
r2 = ""e""
elif suffix in (""biliti"", ""bli""):
word = suffix_replace(word, suffix, ""ble"")
if len(r1) >= len(suffix):
r1 = suffix_replace(r1, suffix, ""ble"")
else:
r1 = """"
if len(r2) >= len(suffix):
r2 = suffix_replace(r2, suffix, ""ble"")
else:
r2 = """"
elif suffix == ""ogi"" and word[-4] == ""l"":
word = word[:-1]
r1 = r1[:-1]
r2 = r2[:-1]
elif suffix in (""fulli"", ""lessli""):
word = word[:-2]
r1 = r1[:-2]
r2 = r2[:-2]
elif suffix == ""li"" and word[-3] in self.__li_ending:
word = word[:-2]
r1 = r1[:-2]
r2 = r2[:-2]
break
for suffix in self.__step3_suffixes:
if word.endswith(suffix):
if r1.endswith(suffix):
if suffix == ""tional"":
word = word[:-2]
r1 = r1[:-2]
r2 = r2[:-2]
elif suffix == ""ational"":
word = suffix_replace(word, suffix, ""ate"")
if len(r1) >= len(suffix):
r1 = suffix_replace(r1, suffix, ""ate"")
else:
r1 = """"
if len(r2) >= len(suffix):
r2 = suffix_replace(r2, suffix, ""ate"")
else:
r2 = """"
elif suffix == ""alize"":
word = word[:-3]
r1 = r1[:-3]
r2 = r2[:-3]
elif suffix in (""icate"", ""iciti"", ""ical""):
word = suffix_replace(word, suffix, ""ic"")
if len(r1) >= len(suffix):
r1 = suffix_replace(r1, suffix, ""ic"")
else:
r1 = """"
if len(r2) >= len(suffix):
r2 = suffix_replace(r2, suffix, ""ic"")
else:
r2 = """"
elif suffix in (""ful"", ""ness""):
word = word[: -len(suffix)]
r1 = r1[: -len(suffix)]
r2 = r2[: -len(suffix)]
elif suffix == ""ative"" and r2.endswith(suffix):
word = word[:-5]
r1 = r1[:-5]
r2 = r2[:-5]
break
for suffix in self.__step4_suffixes:
if word.endswith(suffix):
if r2.endswith(suffix):
if suffix == ""ion"":
if word[-4] in ""st"":
word = word[:-3]
r1 = r1[:-3]
r2 = r2[:-3]
else:
word = word[: -len(suffix)]
r1 = r1[: -len(suffix)]
r2 = r2[: -len(suffix)]
break
if r2.endswith(""l"") and word[-2] == ""l"":
word = word[:-1]
elif r2.endswith(""e""):
word = word[:-1]
elif r1.endswith(""e""):
if len(word) >= 4 and (
word[-2] in self.__vowels
or word[-2] in ""wxY""
or word[-3] not in self.__vowels
or word[-4] in self.__vowels
):
word = word[:-1]
word = word.replace(""Y"", ""y"")
return word
",[],0,[],/stem/snowball.py_stem
3794,/home/amandapotts/git/nltk/nltk/stem/snowball.py_stem,"def stem(self, word):
""""""
Stem a Finnish word and return the stemmed form.
:param word: The word that is stemmed.
:type word: str or unicode
:return: The stemmed form.
:rtype: unicode
""""""
word = word.lower()
if word in self.stopwords:
return word
step3_success = False
r1, r2 = self._r1r2_standard(word, self.__vowels)
for suffix in self.__step1_suffixes:
if r1.endswith(suffix):
if suffix == ""sti"":
if suffix in r2:
word = word[:-3]
r1 = r1[:-3]
r2 = r2[:-3]
else:
if word[-len(suffix) - 1] in ""ntaeiouy\xE4\xF6"":
word = word[: -len(suffix)]
r1 = r1[: -len(suffix)]
r2 = r2[: -len(suffix)]
break
for suffix in self.__step2_suffixes:
if r1.endswith(suffix):
if suffix == ""si"":
if word[-3] != ""k"":
word = word[:-2]
r1 = r1[:-2]
r2 = r2[:-2]
elif suffix == ""ni"":
word = word[:-2]
r1 = r1[:-2]
r2 = r2[:-2]
if word.endswith(""kse""):
word = suffix_replace(word, ""kse"", ""ksi"")
if r1.endswith(""kse""):
r1 = suffix_replace(r1, ""kse"", ""ksi"")
if r2.endswith(""kse""):
r2 = suffix_replace(r2, ""kse"", ""ksi"")
elif suffix == ""an"":
if word[-4:-2] in (""ta"", ""na"") or word[-5:-2] in (
""ssa"",
""sta"",
""lla"",
""lta"",
):
word = word[:-2]
r1 = r1[:-2]
r2 = r2[:-2]
elif suffix == ""\xE4n"":
if word[-4:-2] in (""t\xE4"", ""n\xE4"") or word[-5:-2] in (
""ss\xE4"",
""st\xE4"",
""ll\xE4"",
""lt\xE4"",
):
word = word[:-2]
r1 = r1[:-2]
r2 = r2[:-2]
elif suffix == ""en"":
if word[-5:-2] in (""lle"", ""ine""):
word = word[:-2]
r1 = r1[:-2]
r2 = r2[:-2]
else:
word = word[:-3]
r1 = r1[:-3]
r2 = r2[:-3]
break
for suffix in self.__step3_suffixes:
if r1.endswith(suffix):
if suffix in (""han"", ""hen"", ""hin"", ""hon"", ""h\xE4n"", ""h\xF6n""):
if (
(suffix == ""han"" and word[-4] == ""a"")
or (suffix == ""hen"" and word[-4] == ""e"")
or (suffix == ""hin"" and word[-4] == ""i"")
or (suffix == ""hon"" and word[-4] == ""o"")
or (suffix == ""h\xE4n"" and word[-4] == ""\xE4"")
or (suffix == ""h\xF6n"" and word[-4] == ""\xF6"")
):
word = word[:-3]
r1 = r1[:-3]
r2 = r2[:-3]
step3_success = True
elif suffix in (""siin"", ""den"", ""tten""):
if (
word[-len(suffix) - 1] == ""i""
and word[-len(suffix) - 2] in self.__restricted_vowels
):
word = word[: -len(suffix)]
r1 = r1[: -len(suffix)]
r2 = r2[: -len(suffix)]
step3_success = True
else:
continue
elif suffix == ""seen"":
if word[-6:-4] in self.__long_vowels:
word = word[:-4]
r1 = r1[:-4]
r2 = r2[:-4]
step3_success = True
else:
continue
elif suffix in (""a"", ""\xE4""):
if word[-2] in self.__vowels and word[-3] in self.__consonants:
word = word[:-1]
r1 = r1[:-1]
r2 = r2[:-1]
step3_success = True
elif suffix in (""tta"", ""tt\xE4""):
if word[-4] == ""e"":
word = word[:-3]
r1 = r1[:-3]
r2 = r2[:-3]
step3_success = True
elif suffix == ""n"":
word = word[:-1]
r1 = r1[:-1]
r2 = r2[:-1]
step3_success = True
if word[-2:] == ""ie"" or word[-2:] in self.__long_vowels:
word = word[:-1]
r1 = r1[:-1]
r2 = r2[:-1]
else:
word = word[: -len(suffix)]
r1 = r1[: -len(suffix)]
r2 = r2[: -len(suffix)]
step3_success = True
break
for suffix in self.__step4_suffixes:
if r2.endswith(suffix):
if suffix in (""mpi"", ""mpa"", ""mp\xE4"", ""mmi"", ""mma"", ""mm\xE4""):
if word[-5:-3] != ""po"":
word = word[:-3]
r1 = r1[:-3]
r2 = r2[:-3]
else:
word = word[: -len(suffix)]
r1 = r1[: -len(suffix)]
r2 = r2[: -len(suffix)]
break
if step3_success and len(r1) >= 1 and r1[-1] in ""ij"":
word = word[:-1]
r1 = r1[:-1]
elif (
not step3_success
and len(r1) >= 2
and r1[-1] == ""t""
and r1[-2] in self.__vowels
):
word = word[:-1]
r1 = r1[:-1]
r2 = r2[:-1]
if r2.endswith(""imma""):
word = word[:-4]
r1 = r1[:-4]
elif r2.endswith(""mma"") and r2[-5:-3] != ""po"":
word = word[:-3]
r1 = r1[:-3]
if r1[-2:] in self.__long_vowels:
word = word[:-1]
r1 = r1[:-1]
if len(r1) >= 2 and r1[-2] in self.__consonants and r1[-1] in ""a\xE4ei"":
word = word[:-1]
r1 = r1[:-1]
if r1.endswith((""oj"", ""uj"")):
word = word[:-1]
r1 = r1[:-1]
if r1.endswith(""jo""):
word = word[:-1]
r1 = r1[:-1]
for i in range(1, len(word)):
if word[-i] in self.__vowels:
continue
else:
if i == 1:
if word[-i - 1 :] in self.__double_consonants:
word = word[:-1]
else:
if word[-i - 1 : -i + 1] in self.__double_consonants:
word = """".join((word[:-i], word[-i + 1 :]))
break
return word
",[],0,[],/stem/snowball.py_stem
3795,/home/amandapotts/git/nltk/nltk/stem/snowball.py_stem,"def stem(self, word):
""""""
Stem a French word and return the stemmed form.
:param word: The word that is stemmed.
:type word: str or unicode
:return: The stemmed form.
:rtype: unicode
""""""
word = word.lower()
if word in self.stopwords:
return word
step1_success = False
rv_ending_found = False
step2a_success = False
step2b_success = False
for i in range(1, len(word)):
if word[i - 1] == ""q"" and word[i] == ""u"":
word = """".join((word[:i], ""U"", word[i + 1 :]))
for i in range(1, len(word) - 1):
if word[i - 1] in self.__vowels and word[i + 1] in self.__vowels:
if word[i] == ""u"":
word = """".join((word[:i], ""U"", word[i + 1 :]))
elif word[i] == ""i"":
word = """".join((word[:i], ""I"", word[i + 1 :]))
if word[i - 1] in self.__vowels or word[i + 1] in self.__vowels:
if word[i] == ""y"":
word = """".join((word[:i], ""Y"", word[i + 1 :]))
r1, r2 = self._r1r2_standard(word, self.__vowels)
rv = self.__rv_french(word, self.__vowels)
for suffix in self.__step1_suffixes:
if word.endswith(suffix):
if suffix == ""eaux"":
word = word[:-1]
step1_success = True
elif suffix in (""euse"", ""euses""):
if suffix in r2:
word = word[: -len(suffix)]
step1_success = True
elif suffix in r1:
word = suffix_replace(word, suffix, ""eux"")
step1_success = True
elif suffix in (""ement"", ""ements"") and suffix in rv:
word = word[: -len(suffix)]
step1_success = True
if word[-2:] == ""iv"" and ""iv"" in r2:
word = word[:-2]
if word[-2:] == ""at"" and ""at"" in r2:
word = word[:-2]
elif word[-3:] == ""eus"":
if ""eus"" in r2:
word = word[:-3]
elif ""eus"" in r1:
word = """".join((word[:-1], ""x""))
elif word[-3:] in (""abl"", ""iqU""):
if ""abl"" in r2 or ""iqU"" in r2:
word = word[:-3]
elif word[-3:] in (""i\xE8r"", ""I\xE8r""):
if ""i\xE8r"" in rv or ""I\xE8r"" in rv:
word = """".join((word[:-3], ""i""))
elif suffix == ""amment"" and suffix in rv:
word = suffix_replace(word, ""amment"", ""ant"")
rv = suffix_replace(rv, ""amment"", ""ant"")
rv_ending_found = True
elif suffix == ""emment"" and suffix in rv:
word = suffix_replace(word, ""emment"", ""ent"")
rv_ending_found = True
elif (
suffix in (""ment"", ""ments"")
and suffix in rv
and not rv.startswith(suffix)
and rv[rv.rindex(suffix) - 1] in self.__vowels
):
word = word[: -len(suffix)]
rv = rv[: -len(suffix)]
rv_ending_found = True
elif suffix == ""aux"" and suffix in r1:
word = """".join((word[:-2], ""l""))
step1_success = True
elif (
suffix in (""issement"", ""issements"")
and suffix in r1
and word[-len(suffix) - 1] not in self.__vowels
):
word = word[: -len(suffix)]
step1_success = True
elif (
suffix
in (
""ance"",
""iqUe"",
""isme"",
""able"",
""iste"",
""eux"",
""ances"",
""iqUes"",
""ismes"",
""ables"",
""istes"",
)
and suffix in r2
):
word = word[: -len(suffix)]
step1_success = True
elif (
suffix
in (""atrice"", ""ateur"", ""ation"", ""atrices"", ""ateurs"", ""ations"")
and suffix in r2
):
word = word[: -len(suffix)]
step1_success = True
if word[-2:] == ""ic"":
if ""ic"" in r2:
word = word[:-2]
else:
word = """".join((word[:-2], ""iqU""))
elif suffix in (""logie"", ""logies"") and suffix in r2:
word = suffix_replace(word, suffix, ""log"")
step1_success = True
elif suffix in (""usion"", ""ution"", ""usions"", ""utions"") and suffix in r2:
word = suffix_replace(word, suffix, ""u"")
step1_success = True
elif suffix in (""ence"", ""ences"") and suffix in r2:
word = suffix_replace(word, suffix, ""ent"")
step1_success = True
elif suffix in (""it\xE9"", ""it\xE9s"") and suffix in r2:
word = word[: -len(suffix)]
step1_success = True
if word[-4:] == ""abil"":
if ""abil"" in r2:
word = word[:-4]
else:
word = """".join((word[:-2], ""l""))
elif word[-2:] == ""ic"":
if ""ic"" in r2:
word = word[:-2]
else:
word = """".join((word[:-2], ""iqU""))
elif word[-2:] == ""iv"":
if ""iv"" in r2:
word = word[:-2]
elif suffix in (""if"", ""ive"", ""ifs"", ""ives"") and suffix in r2:
word = word[: -len(suffix)]
step1_success = True
if word[-2:] == ""at"" and ""at"" in r2:
word = word[:-2]
if word[-2:] == ""ic"":
if ""ic"" in r2:
word = word[:-2]
else:
word = """".join((word[:-2], ""iqU""))
break
if not step1_success or rv_ending_found:
for suffix in self.__step2a_suffixes:
if word.endswith(suffix):
if (
suffix in rv
and len(rv) > len(suffix)
and rv[rv.rindex(suffix) - 1] not in self.__vowels
):
word = word[: -len(suffix)]
step2a_success = True
break
if not step2a_success:
for suffix in self.__step2b_suffixes:
if rv.endswith(suffix):
if suffix == ""ions"" and ""ions"" in r2:
word = word[:-4]
step2b_success = True
elif suffix in (
""eraIent"",
""erions"",
""\xE8rent"",
""erais"",
""erait"",
""eriez"",
""erons"",
""eront"",
""erai"",
""eras"",
""erez"",
""\xE9es"",
""era"",
""iez"",
""\xE9e"",
""\xE9s"",
""er"",
""ez"",
""\xE9"",
):
word = word[: -len(suffix)]
step2b_success = True
elif suffix in (
""assions"",
""assent"",
""assiez"",
""aIent"",
""antes"",
""asses"",
""\xE2mes"",
""\xE2tes"",
""ante"",
""ants"",
""asse"",
""ais"",
""ait"",
""ant"",
""\xE2t"",
""ai"",
""as"",
""a"",
):
word = word[: -len(suffix)]
rv = rv[: -len(suffix)]
step2b_success = True
if rv.endswith(""e""):
word = word[:-1]
break
if step1_success or step2a_success or step2b_success:
if word[-1] == ""Y"":
word = """".join((word[:-1], ""i""))
elif word[-1] == ""\xE7"":
word = """".join((word[:-1], ""c""))
else:
if len(word) >= 2 and word[-1] == ""s"" and word[-2] not in ""aiou\xE8s"":
word = word[:-1]
for suffix in self.__step4_suffixes:
if word.endswith(suffix):
if suffix in rv:
if suffix == ""ion"" and suffix in r2 and rv[-4] in ""st"":
word = word[:-3]
elif suffix in (""ier"", ""i\xE8re"", ""Ier"", ""I\xE8re""):
word = suffix_replace(word, suffix, ""i"")
elif suffix == ""e"":
word = word[:-1]
elif suffix == ""\xEB"" and word[-3:-1] == ""gu"":
word = word[:-1]
break
if word.endswith((""enn"", ""onn"", ""ett"", ""ell"", ""eill"")):
word = word[:-1]
for i in range(1, len(word)):
if word[-i] not in self.__vowels:
i += 1
else:
if i != 1 and word[-i] in (""\xE9"", ""\xE8""):
word = """".join((word[:-i], ""e"", word[-i + 1 :]))
break
word = word.replace(""I"", ""i"").replace(""U"", ""u"").replace(""Y"", ""y"")
return word
",[],0,[],/stem/snowball.py_stem
3796,/home/amandapotts/git/nltk/nltk/stem/snowball.py___rv_french,"def __rv_french(self, word, vowels):
""""""
Return the region RV that is used by the French stemmer.
If the word begins with two vowels, RV is the region after
the third letter. Otherwise, it is the region after the first
vowel not at the beginning of the word, or the end of the word
if these positions cannot be found. (Exceptionally, u'par',
u'col' or u'tap' at the beginning of a word is also taken to
define RV as the region to their right.)
:param word: The French word whose region RV is determined.
:type word: str or unicode
:param vowels: The French vowels that are used to determine
the region RV.
:type vowels: unicode
:return: the region RV for the respective French word.
:rtype: unicode
:note: This helper method is invoked by the stem method of
the subclass FrenchStemmer. It is not to be invoked directly!
""""""
rv = """"
if len(word) >= 2:
if word.startswith((""par"", ""col"", ""tap"")) or (
word[0] in vowels and word[1] in vowels
):
rv = word[3:]
else:
for i in range(1, len(word)):
if word[i] in vowels:
rv = word[i + 1 :]
break
return rv
",[],0,[],/stem/snowball.py___rv_french
3797,/home/amandapotts/git/nltk/nltk/stem/snowball.py_stem,"def stem(self, word):
""""""
Stem a German word and return the stemmed form.
:param word: The word that is stemmed.
:type word: str or unicode
:return: The stemmed form.
:rtype: unicode
""""""
word = word.lower()
if word in self.stopwords:
return word
word = word.replace(""\xDF"", ""ss"")
for i in range(1, len(word) - 1):
if word[i - 1] in self.__vowels and word[i + 1] in self.__vowels:
if word[i] == ""u"":
word = """".join((word[:i], ""U"", word[i + 1 :]))
elif word[i] == ""y"":
word = """".join((word[:i], ""Y"", word[i + 1 :]))
r1, r2 = self._r1r2_standard(word, self.__vowels)
for i in range(1, len(word)):
if word[i] not in self.__vowels and word[i - 1] in self.__vowels:
if 3 > len(word[: i + 1]) > 0:
r1 = word[3:]
elif len(word[: i + 1]) == 0:
return word
break
for suffix in self.__step1_suffixes:
if r1.endswith(suffix):
if (
suffix in (""en"", ""es"", ""e"")
and word[-len(suffix) - 4 : -len(suffix)] == ""niss""
):
word = word[: -len(suffix) - 1]
r1 = r1[: -len(suffix) - 1]
r2 = r2[: -len(suffix) - 1]
elif suffix == ""s"":
if word[-2] in self.__s_ending:
word = word[:-1]
r1 = r1[:-1]
r2 = r2[:-1]
else:
word = word[: -len(suffix)]
r1 = r1[: -len(suffix)]
r2 = r2[: -len(suffix)]
break
for suffix in self.__step2_suffixes:
if r1.endswith(suffix):
if suffix == ""st"":
if word[-3] in self.__st_ending and len(word[:-3]) >= 3:
word = word[:-2]
r1 = r1[:-2]
r2 = r2[:-2]
else:
word = word[: -len(suffix)]
r1 = r1[: -len(suffix)]
r2 = r2[: -len(suffix)]
break
for suffix in self.__step3_suffixes:
if r2.endswith(suffix):
if suffix in (""end"", ""ung""):
if (
""ig"" in r2[-len(suffix) - 2 : -len(suffix)]
and ""e"" not in r2[-len(suffix) - 3 : -len(suffix) - 2]
):
word = word[: -len(suffix) - 2]
else:
word = word[: -len(suffix)]
elif (
suffix in (""ig"", ""ik"", ""isch"")
and ""e"" not in r2[-len(suffix) - 1 : -len(suffix)]
):
word = word[: -len(suffix)]
elif suffix in (""lich"", ""heit""):
if (
""er"" in r1[-len(suffix) - 2 : -len(suffix)]
or ""en"" in r1[-len(suffix) - 2 : -len(suffix)]
):
word = word[: -len(suffix) - 2]
else:
word = word[: -len(suffix)]
elif suffix == ""keit"":
if ""lich"" in r2[-len(suffix) - 4 : -len(suffix)]:
word = word[: -len(suffix) - 4]
elif ""ig"" in r2[-len(suffix) - 2 : -len(suffix)]:
word = word[: -len(suffix) - 2]
else:
word = word[: -len(suffix)]
break
word = (
word.replace(""\xE4"", ""a"")
.replace(""\xF6"", ""o"")
.replace(""\xFC"", ""u"")
.replace(""U"", ""u"")
.replace(""Y"", ""y"")
)
return word
",[],0,[],/stem/snowball.py_stem
3798,/home/amandapotts/git/nltk/nltk/stem/snowball.py_stem,"def stem(self, word):
""""""
Stem an Hungarian word and return the stemmed form.
:param word: The word that is stemmed.
:type word: str or unicode
:return: The stemmed form.
:rtype: unicode
""""""
word = word.lower()
if word in self.stopwords:
return word
r1 = self.__r1_hungarian(word, self.__vowels, self.__digraphs)
if r1.endswith(self.__step1_suffixes):
for double_cons in self.__double_consonants:
if word[-2 - len(double_cons) : -2] == double_cons:
word = """".join((word[:-4], word[-3]))
if r1[-2 - len(double_cons) : -2] == double_cons:
r1 = """".join((r1[:-4], r1[-3]))
break
for suffix in self.__step2_suffixes:
if word.endswith(suffix):
if r1.endswith(suffix):
word = word[: -len(suffix)]
r1 = r1[: -len(suffix)]
if r1.endswith(""\xE1""):
word = """".join((word[:-1], ""a""))
r1 = suffix_replace(r1, ""\xE1"", ""a"")
elif r1.endswith(""\xE9""):
word = """".join((word[:-1], ""e""))
r1 = suffix_replace(r1, ""\xE9"", ""e"")
break
for suffix in self.__step3_suffixes:
if r1.endswith(suffix):
if suffix == ""\xE9n"":
word = suffix_replace(word, suffix, ""e"")
r1 = suffix_replace(r1, suffix, ""e"")
else:
word = suffix_replace(word, suffix, ""a"")
r1 = suffix_replace(r1, suffix, ""a"")
break
for suffix in self.__step4_suffixes:
if r1.endswith(suffix):
if suffix == ""\xE1stul"":
word = suffix_replace(word, suffix, ""a"")
r1 = suffix_replace(r1, suffix, ""a"")
elif suffix == ""\xE9st\xFCl"":
word = suffix_replace(word, suffix, ""e"")
r1 = suffix_replace(r1, suffix, ""e"")
else:
word = word[: -len(suffix)]
r1 = r1[: -len(suffix)]
break
for suffix in self.__step5_suffixes:
if r1.endswith(suffix):
for double_cons in self.__double_consonants:
if word[-1 - len(double_cons) : -1] == double_cons:
word = """".join((word[:-3], word[-2]))
if r1[-1 - len(double_cons) : -1] == double_cons:
r1 = """".join((r1[:-3], r1[-2]))
break
for suffix in self.__step6_suffixes:
if r1.endswith(suffix):
if suffix in (""\xE1k\xE9"", ""\xE1\xE9i""):
word = suffix_replace(word, suffix, ""a"")
r1 = suffix_replace(r1, suffix, ""a"")
elif suffix in (""\xE9k\xE9"", ""\xE9\xE9i"", ""\xE9\xE9""):
word = suffix_replace(word, suffix, ""e"")
r1 = suffix_replace(r1, suffix, ""e"")
else:
word = word[: -len(suffix)]
r1 = r1[: -len(suffix)]
break
for suffix in self.__step7_suffixes:
if word.endswith(suffix):
if r1.endswith(suffix):
if suffix in (""\xE1nk"", ""\xE1juk"", ""\xE1m"", ""\xE1d"", ""\xE1""):
word = suffix_replace(word, suffix, ""a"")
r1 = suffix_replace(r1, suffix, ""a"")
elif suffix in (""\xE9nk"", ""\xE9j\xFCk"", ""\xE9m"", ""\xE9d"", ""\xE9""):
word = suffix_replace(word, suffix, ""e"")
r1 = suffix_replace(r1, suffix, ""e"")
else:
word = word[: -len(suffix)]
r1 = r1[: -len(suffix)]
break
for suffix in self.__step8_suffixes:
if word.endswith(suffix):
if r1.endswith(suffix):
if suffix in (
""\xE1im"",
""\xE1id"",
""\xE1i"",
""\xE1ink"",
""\xE1itok"",
""\xE1ik"",
):
word = suffix_replace(word, suffix, ""a"")
r1 = suffix_replace(r1, suffix, ""a"")
elif suffix in (
""\xE9im"",
""\xE9id"",
""\xE9i"",
""\xE9ink"",
""\xE9itek"",
""\xE9ik"",
):
word = suffix_replace(word, suffix, ""e"")
r1 = suffix_replace(r1, suffix, ""e"")
else:
word = word[: -len(suffix)]
r1 = r1[: -len(suffix)]
break
for suffix in self.__step9_suffixes:
if word.endswith(suffix):
if r1.endswith(suffix):
if suffix == ""\xE1k"":
word = suffix_replace(word, suffix, ""a"")
elif suffix == ""\xE9k"":
word = suffix_replace(word, suffix, ""e"")
else:
word = word[: -len(suffix)]
break
return word
",[],0,[],/stem/snowball.py_stem
3799,/home/amandapotts/git/nltk/nltk/stem/snowball.py___r1_hungarian,"def __r1_hungarian(self, word, vowels, digraphs):
""""""
Return the region R1 that is used by the Hungarian stemmer.
If the word begins with a vowel, R1 is defined as the region
after the first consonant or digraph (= two letters stand for
one phoneme) in the word. If the word begins with a consonant,
it is defined as the region after the first vowel in the word.
If the word does not contain both a vowel and consonant, R1
is the null region at the end of the word.
:param word: The Hungarian word whose region R1 is determined.
:type word: str or unicode
:param vowels: The Hungarian vowels that are used to determine
the region R1.
:type vowels: unicode
:param digraphs: The digraphs that are used to determine the
region R1.
:type digraphs: tuple
:return: the region R1 for the respective word.
:rtype: unicode
:note: This helper method is invoked by the stem method of the subclass
HungarianStemmer. It is not to be invoked directly!
""""""
r1 = """"
if word[0] in vowels:
for digraph in digraphs:
if digraph in word[1:]:
r1 = word[word.index(digraph[-1]) + 1 :]
return r1
for i in range(1, len(word)):
if word[i] not in vowels:
r1 = word[i + 1 :]
break
else:
for i in range(1, len(word)):
if word[i] in vowels:
r1 = word[i + 1 :]
break
return r1
",[],0,[],/stem/snowball.py___r1_hungarian
3800,/home/amandapotts/git/nltk/nltk/stem/snowball.py_stem,"def stem(self, word):
""""""
Stem an Italian word and return the stemmed form.
:param word: The word that is stemmed.
:type word: str or unicode
:return: The stemmed form.
:rtype: unicode
""""""
word = word.lower()
if word in self.stopwords:
return word
step1_success = False
word = (
word.replace(""\xE1"", ""\xE0"")
.replace(""\xE9"", ""\xE8"")
.replace(""\xED"", ""\xEC"")
.replace(""\xF3"", ""\xF2"")
.replace(""\xFA"", ""\xF9"")
)
for i in range(1, len(word)):
if word[i - 1] == ""q"" and word[i] == ""u"":
word = """".join((word[:i], ""U"", word[i + 1 :]))
for i in range(1, len(word) - 1):
if word[i - 1] in self.__vowels and word[i + 1] in self.__vowels:
if word[i] == ""u"":
word = """".join((word[:i], ""U"", word[i + 1 :]))
elif word[i] == ""i"":
word = """".join((word[:i], ""I"", word[i + 1 :]))
r1, r2 = self._r1r2_standard(word, self.__vowels)
rv = self._rv_standard(word, self.__vowels)
for suffix in self.__step0_suffixes:
if rv.endswith(suffix):
if rv[-len(suffix) - 4 : -len(suffix)] in (""ando"", ""endo""):
word = word[: -len(suffix)]
r1 = r1[: -len(suffix)]
r2 = r2[: -len(suffix)]
rv = rv[: -len(suffix)]
elif rv[-len(suffix) - 2 : -len(suffix)] in (""ar"", ""er"", ""ir""):
word = suffix_replace(word, suffix, ""e"")
r1 = suffix_replace(r1, suffix, ""e"")
r2 = suffix_replace(r2, suffix, ""e"")
rv = suffix_replace(rv, suffix, ""e"")
break
for suffix in self.__step1_suffixes:
if word.endswith(suffix):
if suffix == ""amente"" and r1.endswith(suffix):
step1_success = True
word = word[:-6]
r2 = r2[:-6]
rv = rv[:-6]
if r2.endswith(""iv""):
word = word[:-2]
r2 = r2[:-2]
rv = rv[:-2]
if r2.endswith(""at""):
word = word[:-2]
rv = rv[:-2]
elif r2.endswith((""os"", ""ic"")):
word = word[:-2]
rv = rv[:-2]
elif r2.endswith(""abil""):
word = word[:-4]
rv = rv[:-4]
elif suffix in (""amento"", ""amenti"", ""imento"", ""imenti"") and rv.endswith(
suffix
):
step1_success = True
word = word[:-6]
rv = rv[:-6]
elif r2.endswith(suffix):
step1_success = True
if suffix in (""azione"", ""azioni"", ""atore"", ""atori""):
word = word[: -len(suffix)]
r2 = r2[: -len(suffix)]
rv = rv[: -len(suffix)]
if r2.endswith(""ic""):
word = word[:-2]
rv = rv[:-2]
elif suffix in (""logia"", ""logie""):
word = word[:-2]
rv = word[:-2]
elif suffix in (""uzione"", ""uzioni"", ""usione"", ""usioni""):
word = word[:-5]
rv = rv[:-5]
elif suffix in (""enza"", ""enze""):
word = suffix_replace(word, suffix, ""te"")
rv = suffix_replace(rv, suffix, ""te"")
elif suffix == ""it\xE0"":
word = word[:-3]
r2 = r2[:-3]
rv = rv[:-3]
if r2.endswith((""ic"", ""iv"")):
word = word[:-2]
rv = rv[:-2]
elif r2.endswith(""abil""):
word = word[:-4]
rv = rv[:-4]
elif suffix in (""ivo"", ""ivi"", ""iva"", ""ive""):
word = word[:-3]
r2 = r2[:-3]
rv = rv[:-3]
if r2.endswith(""at""):
word = word[:-2]
r2 = r2[:-2]
rv = rv[:-2]
if r2.endswith(""ic""):
word = word[:-2]
rv = rv[:-2]
else:
word = word[: -len(suffix)]
rv = rv[: -len(suffix)]
break
if not step1_success:
for suffix in self.__step2_suffixes:
if rv.endswith(suffix):
word = word[: -len(suffix)]
rv = rv[: -len(suffix)]
break
if rv.endswith((""a"", ""e"", ""i"", ""o"", ""\xE0"", ""\xE8"", ""\xEC"", ""\xF2"")):
word = word[:-1]
rv = rv[:-1]
if rv.endswith(""i""):
word = word[:-1]
rv = rv[:-1]
if rv.endswith((""ch"", ""gh"")):
word = word[:-1]
word = word.replace(""I"", ""i"").replace(""U"", ""u"")
return word
",[],0,[],/stem/snowball.py_stem
3801,/home/amandapotts/git/nltk/nltk/stem/snowball.py_stem,"def stem(self, word):
""""""
Stem a Norwegian word and return the stemmed form.
:param word: The word that is stemmed.
:type word: str or unicode
:return: The stemmed form.
:rtype: unicode
""""""
word = word.lower()
if word in self.stopwords:
return word
r1 = self._r1_scandinavian(word, self.__vowels)
for suffix in self.__step1_suffixes:
if r1.endswith(suffix):
if suffix in (""erte"", ""ert""):
word = suffix_replace(word, suffix, ""er"")
r1 = suffix_replace(r1, suffix, ""er"")
elif suffix == ""s"":
if word[-2] in self.__s_ending or (
word[-2] == ""k"" and word[-3] not in self.__vowels
):
word = word[:-1]
r1 = r1[:-1]
else:
word = word[: -len(suffix)]
r1 = r1[: -len(suffix)]
break
for suffix in self.__step2_suffixes:
if r1.endswith(suffix):
word = word[:-1]
r1 = r1[:-1]
break
for suffix in self.__step3_suffixes:
if r1.endswith(suffix):
word = word[: -len(suffix)]
break
return word
",[],0,[],/stem/snowball.py_stem
3802,/home/amandapotts/git/nltk/nltk/stem/snowball.py_stem,"def stem(self, word):
""""""
Stem a Portuguese word and return the stemmed form.
:param word: The word that is stemmed.
:type word: str or unicode
:return: The stemmed form.
:rtype: unicode
""""""
word = word.lower()
if word in self.stopwords:
return word
step1_success = False
step2_success = False
word = (
word.replace(""\xE3"", ""a~"")
.replace(""\xF5"", ""o~"")
.replace(""q\xFC"", ""qu"")
.replace(""g\xFC"", ""gu"")
)
r1, r2 = self._r1r2_standard(word, self.__vowels)
rv = self._rv_standard(word, self.__vowels)
for suffix in self.__step1_suffixes:
if word.endswith(suffix):
if suffix == ""amente"" and r1.endswith(suffix):
step1_success = True
word = word[:-6]
r2 = r2[:-6]
rv = rv[:-6]
if r2.endswith(""iv""):
word = word[:-2]
r2 = r2[:-2]
rv = rv[:-2]
if r2.endswith(""at""):
word = word[:-2]
rv = rv[:-2]
elif r2.endswith((""os"", ""ic"", ""ad"")):
word = word[:-2]
rv = rv[:-2]
elif (
suffix in (""ira"", ""iras"")
and rv.endswith(suffix)
and word[-len(suffix) - 1 : -len(suffix)] == ""e""
):
step1_success = True
word = suffix_replace(word, suffix, ""ir"")
rv = suffix_replace(rv, suffix, ""ir"")
elif r2.endswith(suffix):
step1_success = True
if suffix in (""logia"", ""logias""):
word = suffix_replace(word, suffix, ""log"")
rv = suffix_replace(rv, suffix, ""log"")
elif suffix in (""uça~o"", ""uço~es""):
word = suffix_replace(word, suffix, ""u"")
rv = suffix_replace(rv, suffix, ""u"")
elif suffix in (""\xEAncia"", ""\xEAncias""):
word = suffix_replace(word, suffix, ""ente"")
rv = suffix_replace(rv, suffix, ""ente"")
elif suffix == ""mente"":
word = word[:-5]
r2 = r2[:-5]
rv = rv[:-5]
if r2.endswith((""ante"", ""avel"", ""ivel"")):
word = word[:-4]
rv = rv[:-4]
elif suffix in (""idade"", ""idades""):
word = word[: -len(suffix)]
r2 = r2[: -len(suffix)]
rv = rv[: -len(suffix)]
if r2.endswith((""ic"", ""iv"")):
word = word[:-2]
rv = rv[:-2]
elif r2.endswith(""abil""):
word = word[:-4]
rv = rv[:-4]
elif suffix in (""iva"", ""ivo"", ""ivas"", ""ivos""):
word = word[: -len(suffix)]
r2 = r2[: -len(suffix)]
rv = rv[: -len(suffix)]
if r2.endswith(""at""):
word = word[:-2]
rv = rv[:-2]
else:
word = word[: -len(suffix)]
rv = rv[: -len(suffix)]
break
if not step1_success:
for suffix in self.__step2_suffixes:
if rv.endswith(suffix):
step2_success = True
word = word[: -len(suffix)]
rv = rv[: -len(suffix)]
break
if step1_success or step2_success:
if rv.endswith(""i"") and word[-2] == ""c"":
word = word[:-1]
rv = rv[:-1]
if not step1_success and not step2_success:
for suffix in self.__step4_suffixes:
if rv.endswith(suffix):
word = word[: -len(suffix)]
rv = rv[: -len(suffix)]
break
if rv.endswith((""e"", ""\xE9"", ""\xEA"")):
word = word[:-1]
rv = rv[:-1]
if (word.endswith(""gu"") and rv.endswith(""u"")) or (
word.endswith(""ci"") and rv.endswith(""i"")
):
word = word[:-1]
elif word.endswith(""\xE7""):
word = suffix_replace(word, ""\xE7"", ""c"")
word = word.replace(""a~"", ""\xE3"").replace(""o~"", ""\xF5"")
return word
",[],0,[],/stem/snowball.py_stem
3803,/home/amandapotts/git/nltk/nltk/stem/snowball.py_stem,"def stem(self, word):
""""""
Stem a Romanian word and return the stemmed form.
:param word: The word that is stemmed.
:type word: str or unicode
:return: The stemmed form.
:rtype: unicode
""""""
word = word.lower()
if word in self.stopwords:
return word
step1_success = False
step2_success = False
for i in range(1, len(word) - 1):
if word[i - 1] in self.__vowels and word[i + 1] in self.__vowels:
if word[i] == ""u"":
word = """".join((word[:i], ""U"", word[i + 1 :]))
elif word[i] == ""i"":
word = """".join((word[:i], ""I"", word[i + 1 :]))
r1, r2 = self._r1r2_standard(word, self.__vowels)
rv = self._rv_standard(word, self.__vowels)
for suffix in self.__step0_suffixes:
if word.endswith(suffix):
if suffix in r1:
if suffix in (""ul"", ""ului""):
word = word[: -len(suffix)]
if suffix in rv:
rv = rv[: -len(suffix)]
else:
rv = """"
elif (
suffix == ""aua""
or suffix == ""atei""
or (suffix == ""ile"" and word[-5:-3] != ""ab"")
):
word = word[:-2]
elif suffix in (""ea"", ""ele"", ""elor""):
word = suffix_replace(word, suffix, ""e"")
if suffix in rv:
rv = suffix_replace(rv, suffix, ""e"")
else:
rv = """"
elif suffix in (""ii"", ""iua"", ""iei"", ""iile"", ""iilor"", ""ilor""):
word = suffix_replace(word, suffix, ""i"")
if suffix in rv:
rv = suffix_replace(rv, suffix, ""i"")
else:
rv = """"
elif suffix in (""a\u0163ie"", ""a\u0163ia""):
word = word[:-1]
break
while True:
replacement_done = False
for suffix in self.__step1_suffixes:
if word.endswith(suffix):
if suffix in r1:
step1_success = True
replacement_done = True
if suffix in (
""abilitate"",
""abilitati"",
""abilit\u0103i"",
""abilit\u0103\u0163i"",
):
word = suffix_replace(word, suffix, ""abil"")
elif suffix == ""ibilitate"":
word = word[:-5]
elif suffix in (
""ivitate"",
""ivitati"",
""ivit\u0103i"",
""ivit\u0103\u0163i"",
):
word = suffix_replace(word, suffix, ""iv"")
elif suffix in (
""icitate"",
""icitati"",
""icit\u0103i"",
""icit\u0103\u0163i"",
""icator"",
""icatori"",
""iciv"",
""iciva"",
""icive"",
""icivi"",
""iciv\u0103"",
""ical"",
""icala"",
""icale"",
""icali"",
""ical\u0103"",
):
word = suffix_replace(word, suffix, ""ic"")
elif suffix in (
""ativ"",
""ativa"",
""ative"",
""ativi"",
""ativ\u0103"",
""a\u0163iune"",
""atoare"",
""ator"",
""atori"",
""\u0103toare"",
""\u0103tor"",
""\u0103tori"",
):
word = suffix_replace(word, suffix, ""at"")
if suffix in r2:
r2 = suffix_replace(r2, suffix, ""at"")
elif suffix in (
""itiv"",
""itiva"",
""itive"",
""itivi"",
""itiv\u0103"",
""i\u0163iune"",
""itoare"",
""itor"",
""itori"",
):
word = suffix_replace(word, suffix, ""it"")
if suffix in r2:
r2 = suffix_replace(r2, suffix, ""it"")
else:
step1_success = False
break
if not replacement_done:
break
for suffix in self.__step2_suffixes:
if word.endswith(suffix):
if suffix in r2:
step2_success = True
if suffix in (""iune"", ""iuni""):
if word[-5] == ""\u0163"":
word = """".join((word[:-5], ""t""))
elif suffix in (
""ism"",
""isme"",
""ist"",
""ista"",
""iste"",
""isti"",
""ist\u0103"",
""i\u015Fti"",
):
word = suffix_replace(word, suffix, ""ist"")
else:
word = word[: -len(suffix)]
break
if not step1_success and not step2_success:
for suffix in self.__step3_suffixes:
if word.endswith(suffix):
if suffix in rv:
if suffix in (
""seser\u0103\u0163i"",
""seser\u0103m"",
""ser\u0103\u0163i"",
""sese\u015Fi"",
""seser\u0103"",
""ser\u0103m"",
""sesem"",
""se\u015Fi"",
""ser\u0103"",
""sese"",
""a\u0163i"",
""e\u0163i"",
""i\u0163i"",
""\xE2\u0163i"",
""sei"",
""\u0103m"",
""em"",
""im"",
""\xE2m"",
""se"",
):
word = word[: -len(suffix)]
rv = rv[: -len(suffix)]
else:
if (
not rv.startswith(suffix)
and rv[rv.index(suffix) - 1] not in ""aeio\u0103\xE2\xEE""
):
word = word[: -len(suffix)]
break
for suffix in (""ie"", ""a"", ""e"", ""i"", ""\u0103""):
if word.endswith(suffix):
if suffix in rv:
word = word[: -len(suffix)]
break
word = word.replace(""I"", ""i"").replace(""U"", ""u"")
return word
",[],0,[],/stem/snowball.py_stem
3804,/home/amandapotts/git/nltk/nltk/stem/snowball.py_stem,"def stem(self, word):
""""""
Stem a Russian word and return the stemmed form.
:param word: The word that is stemmed.
:type word: str or unicode
:return: The stemmed form.
:rtype: unicode
""""""
if word in self.stopwords:
return word
chr_exceeded = False
for i in range(len(word)):
if ord(word[i]) > 255:
chr_exceeded = True
break
if not chr_exceeded:
return word
word = self.__cyrillic_to_roman(word)
step1_success = False
adjectival_removed = False
verb_removed = False
undouble_success = False
superlative_removed = False
rv, r2 = self.__regions_russian(word)
for suffix in self.__perfective_gerund_suffixes:
if rv.endswith(suffix):
if suffix in (""v"", ""vshi"", ""vshis'""):
if (
rv[-len(suffix) - 3 : -len(suffix)] == ""i^a""
or rv[-len(suffix) - 1 : -len(suffix)] == ""a""
):
word = word[: -len(suffix)]
r2 = r2[: -len(suffix)]
rv = rv[: -len(suffix)]
step1_success = True
break
else:
word = word[: -len(suffix)]
r2 = r2[: -len(suffix)]
rv = rv[: -len(suffix)]
step1_success = True
break
if not step1_success:
for suffix in self.__reflexive_suffixes:
if rv.endswith(suffix):
word = word[: -len(suffix)]
r2 = r2[: -len(suffix)]
rv = rv[: -len(suffix)]
break
for suffix in self.__adjectival_suffixes:
if rv.endswith(suffix):
if suffix in (
""i^ushchi^ui^u"",
""i^ushchi^ai^a"",
""i^ushchui^u"",
""i^ushchai^a"",
""i^ushchoi^u"",
""i^ushchei^u"",
""i^ushchimi"",
""i^ushchymi"",
""i^ushchego"",
""i^ushchogo"",
""i^ushchemu"",
""i^ushchomu"",
""i^ushchikh"",
""i^ushchykh"",
""shchi^ui^u"",
""shchi^ai^a"",
""i^ushchee"",
""i^ushchie"",
""i^ushchye"",
""i^ushchoe"",
""i^ushchei`"",
""i^ushchii`"",
""i^ushchyi`"",
""i^ushchoi`"",
""i^ushchem"",
""i^ushchim"",
""i^ushchym"",
""i^ushchom"",
""vshi^ui^u"",
""vshi^ai^a"",
""shchui^u"",
""shchai^a"",
""shchoi^u"",
""shchei^u"",
""emi^ui^u"",
""emi^ai^a"",
""nni^ui^u"",
""nni^ai^a"",
""shchimi"",
""shchymi"",
""shchego"",
""shchogo"",
""shchemu"",
""shchomu"",
""shchikh"",
""shchykh"",
""vshui^u"",
""vshai^a"",
""vshoi^u"",
""vshei^u"",
""shchee"",
""shchie"",
""shchye"",
""shchoe"",
""shchei`"",
""shchii`"",
""shchyi`"",
""shchoi`"",
""shchem"",
""shchim"",
""shchym"",
""shchom"",
""vshimi"",
""vshymi"",
""vshego"",
""vshogo"",
""vshemu"",
""vshomu"",
""vshikh"",
""vshykh"",
""emui^u"",
""emai^a"",
""emoi^u"",
""emei^u"",
""nnui^u"",
""nnai^a"",
""nnoi^u"",
""nnei^u"",
""vshee"",
""vshie"",
""vshye"",
""vshoe"",
""vshei`"",
""vshii`"",
""vshyi`"",
""vshoi`"",
""vshem"",
""vshim"",
""vshym"",
""vshom"",
""emimi"",
""emymi"",
""emego"",
""emogo"",
""ememu"",
""emomu"",
""emikh"",
""emykh"",
""nnimi"",
""nnymi"",
""nnego"",
""nnogo"",
""nnemu"",
""nnomu"",
""nnikh"",
""nnykh"",
""emee"",
""emie"",
""emye"",
""emoe"",
""emei`"",
""emii`"",
""emyi`"",
""emoi`"",
""emem"",
""emim"",
""emym"",
""emom"",
""nnee"",
""nnie"",
""nnye"",
""nnoe"",
""nnei`"",
""nnii`"",
""nnyi`"",
""nnoi`"",
""nnem"",
""nnim"",
""nnym"",
""nnom"",
):
if (
rv[-len(suffix) - 3 : -len(suffix)] == ""i^a""
or rv[-len(suffix) - 1 : -len(suffix)] == ""a""
):
word = word[: -len(suffix)]
r2 = r2[: -len(suffix)]
rv = rv[: -len(suffix)]
adjectival_removed = True
break
else:
word = word[: -len(suffix)]
r2 = r2[: -len(suffix)]
rv = rv[: -len(suffix)]
adjectival_removed = True
break
if not adjectival_removed:
for suffix in self.__verb_suffixes:
if rv.endswith(suffix):
if suffix in (
""la"",
""na"",
""ete"",
""i`te"",
""li"",
""i`"",
""l"",
""em"",
""n"",
""lo"",
""no"",
""et"",
""i^ut"",
""ny"",
""t'"",
""esh'"",
""nno"",
):
if (
rv[-len(suffix) - 3 : -len(suffix)] == ""i^a""
or rv[-len(suffix) - 1 : -len(suffix)] == ""a""
):
word = word[: -len(suffix)]
r2 = r2[: -len(suffix)]
rv = rv[: -len(suffix)]
verb_removed = True
break
else:
word = word[: -len(suffix)]
r2 = r2[: -len(suffix)]
rv = rv[: -len(suffix)]
verb_removed = True
break
if not adjectival_removed and not verb_removed:
for suffix in self.__noun_suffixes:
if rv.endswith(suffix):
word = word[: -len(suffix)]
r2 = r2[: -len(suffix)]
rv = rv[: -len(suffix)]
break
if rv.endswith(""i""):
word = word[:-1]
r2 = r2[:-1]
for suffix in self.__derivational_suffixes:
if r2.endswith(suffix):
word = word[: -len(suffix)]
break
if word.endswith(""nn""):
word = word[:-1]
undouble_success = True
if not undouble_success:
for suffix in self.__superlative_suffixes:
if word.endswith(suffix):
word = word[: -len(suffix)]
superlative_removed = True
break
if word.endswith(""nn""):
word = word[:-1]
if not undouble_success and not superlative_removed:
if word.endswith(""'""):
word = word[:-1]
word = self.__roman_to_cyrillic(word)
return word
",[],0,[],/stem/snowball.py_stem
3805,/home/amandapotts/git/nltk/nltk/stem/snowball.py___regions_russian,"def __regions_russian(self, word):
""""""
Return the regions RV and R2 which are used by the Russian stemmer.
In any word, RV is the region after the first vowel,
or the end of the word if it contains no vowel.
R2 is the region after the first non-vowel following
a vowel in R1, or the end of the word if there is no such non-vowel.
R1 is the region after the first non-vowel following a vowel,
or the end of the word if there is no such non-vowel.
:param word: The Russian word whose regions RV and R2 are determined.
:type word: str or unicode
:return: the regions RV and R2 for the respective Russian word.
:rtype: tuple
:note: This helper method is invoked by the stem method of the subclass
RussianStemmer. It is not to be invoked directly!
""""""
r1 = """"
r2 = """"
rv = """"
vowels = (""A"", ""U"", ""E"", ""a"", ""e"", ""i"", ""o"", ""u"", ""y"")
word = word.replace(""i^a"", ""A"").replace(""i^u"", ""U"").replace(""e`"", ""E"")
for i in range(1, len(word)):
if word[i] not in vowels and word[i - 1] in vowels:
r1 = word[i + 1 :]
break
for i in range(1, len(r1)):
if r1[i] not in vowels and r1[i - 1] in vowels:
r2 = r1[i + 1 :]
break
for i in range(len(word)):
if word[i] in vowels:
rv = word[i + 1 :]
break
r2 = r2.replace(""A"", ""i^a"").replace(""U"", ""i^u"").replace(""E"", ""e`"")
rv = rv.replace(""A"", ""i^a"").replace(""U"", ""i^u"").replace(""E"", ""e`"")
return (rv, r2)
",[],0,[],/stem/snowball.py___regions_russian
3806,/home/amandapotts/git/nltk/nltk/stem/snowball.py___cyrillic_to_roman,"def __cyrillic_to_roman(self, word):
""""""
Transliterate a Russian word into the Roman alphabet.
A Russian word whose letters consist of the Cyrillic
alphabet are transliterated into the Roman alphabet
in order to ease the forthcoming stemming process.
:param word: The word that is transliterated.
:type word: unicode
:return: the transliterated word.
:rtype: unicode
:note: This helper method is invoked by the stem method of the subclass
RussianStemmer. It is not to be invoked directly!
""""""
word = (
word.replace(""\u0410"", ""a"")
.replace(""\u0430"", ""a"")
.replace(""\u0411"", ""b"")
.replace(""\u0431"", ""b"")
.replace(""\u0412"", ""v"")
.replace(""\u0432"", ""v"")
.replace(""\u0413"", ""g"")
.replace(""\u0433"", ""g"")
.replace(""\u0414"", ""d"")
.replace(""\u0434"", ""d"")
.replace(""\u0415"", ""e"")
.replace(""\u0435"", ""e"")
.replace(""\u0401"", ""e"")
.replace(""\u0451"", ""e"")
.replace(""\u0416"", ""zh"")
.replace(""\u0436"", ""zh"")
.replace(""\u0417"", ""z"")
.replace(""\u0437"", ""z"")
.replace(""\u0418"", ""i"")
.replace(""\u0438"", ""i"")
.replace(""\u0419"", ""i`"")
.replace(""\u0439"", ""i`"")
.replace(""\u041A"", ""k"")
.replace(""\u043A"", ""k"")
.replace(""\u041B"", ""l"")
.replace(""\u043B"", ""l"")
.replace(""\u041C"", ""m"")
.replace(""\u043C"", ""m"")
.replace(""\u041D"", ""n"")
.replace(""\u043D"", ""n"")
.replace(""\u041E"", ""o"")
.replace(""\u043E"", ""o"")
.replace(""\u041F"", ""p"")
.replace(""\u043F"", ""p"")
.replace(""\u0420"", ""r"")
.replace(""\u0440"", ""r"")
.replace(""\u0421"", ""s"")
.replace(""\u0441"", ""s"")
.replace(""\u0422"", ""t"")
.replace(""\u0442"", ""t"")
.replace(""\u0423"", ""u"")
.replace(""\u0443"", ""u"")
.replace(""\u0424"", ""f"")
.replace(""\u0444"", ""f"")
.replace(""\u0425"", ""kh"")
.replace(""\u0445"", ""kh"")
.replace(""\u0426"", ""t^s"")
.replace(""\u0446"", ""t^s"")
.replace(""\u0427"", ""ch"")
.replace(""\u0447"", ""ch"")
.replace(""\u0428"", ""sh"")
.replace(""\u0448"", ""sh"")
.replace(""\u0429"", ""shch"")
.replace(""\u0449"", ""shch"")
.replace(""\u042A"", ""''"")
.replace(""\u044A"", ""''"")
.replace(""\u042B"", ""y"")
.replace(""\u044B"", ""y"")
.replace(""\u042C"", ""'"")
.replace(""\u044C"", ""'"")
.replace(""\u042D"", ""e`"")
.replace(""\u044D"", ""e`"")
.replace(""\u042E"", ""i^u"")
.replace(""\u044E"", ""i^u"")
.replace(""\u042F"", ""i^a"")
.replace(""\u044F"", ""i^a"")
)
return word
",[],0,[],/stem/snowball.py___cyrillic_to_roman
3807,/home/amandapotts/git/nltk/nltk/stem/snowball.py___roman_to_cyrillic,"def __roman_to_cyrillic(self, word):
""""""
Transliterate a Russian word back into the Cyrillic alphabet.
A Russian word formerly transliterated into the Roman alphabet
in order to ease the stemming process, is transliterated back
into the Cyrillic alphabet, its original form.
:param word: The word that is transliterated.
:type word: str or unicode
:return: word, the transliterated word.
:rtype: unicode
:note: This helper method is invoked by the stem method of the subclass
RussianStemmer. It is not to be invoked directly!
""""""
word = (
word.replace(""i^u"", ""\u044E"")
.replace(""i^a"", ""\u044F"")
.replace(""shch"", ""\u0449"")
.replace(""kh"", ""\u0445"")
.replace(""t^s"", ""\u0446"")
.replace(""ch"", ""\u0447"")
.replace(""e`"", ""\u044D"")
.replace(""i`"", ""\u0439"")
.replace(""sh"", ""\u0448"")
.replace(""k"", ""\u043A"")
.replace(""e"", ""\u0435"")
.replace(""zh"", ""\u0436"")
.replace(""a"", ""\u0430"")
.replace(""b"", ""\u0431"")
.replace(""v"", ""\u0432"")
.replace(""g"", ""\u0433"")
.replace(""d"", ""\u0434"")
.replace(""e"", ""\u0435"")
.replace(""z"", ""\u0437"")
.replace(""i"", ""\u0438"")
.replace(""l"", ""\u043B"")
.replace(""m"", ""\u043C"")
.replace(""n"", ""\u043D"")
.replace(""o"", ""\u043E"")
.replace(""p"", ""\u043F"")
.replace(""r"", ""\u0440"")
.replace(""s"", ""\u0441"")
.replace(""t"", ""\u0442"")
.replace(""u"", ""\u0443"")
.replace(""f"", ""\u0444"")
.replace(""''"", ""\u044A"")
.replace(""y"", ""\u044B"")
.replace(""'"", ""\u044C"")
)
return word
",[],0,[],/stem/snowball.py___roman_to_cyrillic
3808,/home/amandapotts/git/nltk/nltk/stem/snowball.py_stem,"def stem(self, word):
""""""
Stem a Spanish word and return the stemmed form.
:param word: The word that is stemmed.
:type word: str or unicode
:return: The stemmed form.
:rtype: unicode
""""""
word = word.lower()
if word in self.stopwords:
return word
step1_success = False
r1, r2 = self._r1r2_standard(word, self.__vowels)
rv = self._rv_standard(word, self.__vowels)
for suffix in self.__step0_suffixes:
if not (word.endswith(suffix) and rv.endswith(suffix)):
continue
if (
rv[: -len(suffix)].endswith(
(
""ando"",
""\xE1ndo"",
""ar"",
""\xE1r"",
""er"",
""\xE9r"",
""iendo"",
""i\xE9ndo"",
""ir"",
""\xEDr"",
)
)
) or (
rv[: -len(suffix)].endswith(""yendo"")
and word[: -len(suffix)].endswith(""uyendo"")
):
word = self.__replace_accented(word[: -len(suffix)])
r1 = self.__replace_accented(r1[: -len(suffix)])
r2 = self.__replace_accented(r2[: -len(suffix)])
rv = self.__replace_accented(rv[: -len(suffix)])
break
for suffix in self.__step1_suffixes:
if not word.endswith(suffix):
continue
if suffix == ""amente"" and r1.endswith(suffix):
step1_success = True
word = word[:-6]
r2 = r2[:-6]
rv = rv[:-6]
if r2.endswith(""iv""):
word = word[:-2]
r2 = r2[:-2]
rv = rv[:-2]
if r2.endswith(""at""):
word = word[:-2]
rv = rv[:-2]
elif r2.endswith((""os"", ""ic"", ""ad"")):
word = word[:-2]
rv = rv[:-2]
elif r2.endswith(suffix):
step1_success = True
if suffix in (
""adora"",
""ador"",
""aci\xF3n"",
""adoras"",
""adores"",
""acion"",
""aciones"",
""ante"",
""antes"",
""ancia"",
""ancias"",
):
word = word[: -len(suffix)]
r2 = r2[: -len(suffix)]
rv = rv[: -len(suffix)]
if r2.endswith(""ic""):
word = word[:-2]
rv = rv[:-2]
elif suffix in (""log\xEDa"", ""log\xEDas""):
word = suffix_replace(word, suffix, ""log"")
rv = suffix_replace(rv, suffix, ""log"")
elif suffix in (""uci\xF3n"", ""uciones""):
word = suffix_replace(word, suffix, ""u"")
rv = suffix_replace(rv, suffix, ""u"")
elif suffix in (""encia"", ""encias""):
word = suffix_replace(word, suffix, ""ente"")
rv = suffix_replace(rv, suffix, ""ente"")
elif suffix == ""mente"":
word = word[: -len(suffix)]
r2 = r2[: -len(suffix)]
rv = rv[: -len(suffix)]
if r2.endswith((""ante"", ""able"", ""ible"")):
word = word[:-4]
rv = rv[:-4]
elif suffix in (""idad"", ""idades""):
word = word[: -len(suffix)]
r2 = r2[: -len(suffix)]
rv = rv[: -len(suffix)]
for pre_suff in (""abil"", ""ic"", ""iv""):
if r2.endswith(pre_suff):
word = word[: -len(pre_suff)]
rv = rv[: -len(pre_suff)]
elif suffix in (""ivo"", ""iva"", ""ivos"", ""ivas""):
word = word[: -len(suffix)]
r2 = r2[: -len(suffix)]
rv = rv[: -len(suffix)]
if r2.endswith(""at""):
word = word[:-2]
rv = rv[:-2]
else:
word = word[: -len(suffix)]
rv = rv[: -len(suffix)]
break
if not step1_success:
for suffix in self.__step2a_suffixes:
if rv.endswith(suffix) and word[-len(suffix) - 1 : -len(suffix)] == ""u"":
word = word[: -len(suffix)]
rv = rv[: -len(suffix)]
break
for suffix in self.__step2b_suffixes:
if rv.endswith(suffix):
word = word[: -len(suffix)]
rv = rv[: -len(suffix)]
if suffix in (""en"", ""es"", ""\xE9is"", ""emos""):
if word.endswith(""gu""):
word = word[:-1]
if rv.endswith(""gu""):
rv = rv[:-1]
break
for suffix in self.__step3_suffixes:
if rv.endswith(suffix):
word = word[: -len(suffix)]
if suffix in (""e"", ""\xE9""):
rv = rv[: -len(suffix)]
if word[-2:] == ""gu"" and rv.endswith(""u""):
word = word[:-1]
break
word = self.__replace_accented(word)
return word
",[],0,[],/stem/snowball.py_stem
3809,/home/amandapotts/git/nltk/nltk/stem/snowball.py___replace_accented,"def __replace_accented(self, word):
""""""
Replaces all accented letters on a word with their non-accented
counterparts.
:param word: A spanish word, with or without accents
:type word: str or unicode
:return: a word with the accented letters (á, é, í, ó, ú) replaced with
their non-accented counterparts (a, e, i, o, u)
:rtype: str or unicode
""""""
return (
word.replace(""\xE1"", ""a"")
.replace(""\xE9"", ""e"")
.replace(""\xED"", ""i"")
.replace(""\xF3"", ""o"")
.replace(""\xFA"", ""u"")
)
",[],0,[],/stem/snowball.py___replace_accented
3810,/home/amandapotts/git/nltk/nltk/stem/snowball.py_stem,"def stem(self, word):
""""""
Stem a Swedish word and return the stemmed form.
:param word: The word that is stemmed.
:type word: str or unicode
:return: The stemmed form.
:rtype: unicode
""""""
word = word.lower()
if word in self.stopwords:
return word
r1 = self._r1_scandinavian(word, self.__vowels)
for suffix in self.__step1_suffixes:
if r1.endswith(suffix):
if suffix == ""s"":
if word[-2] in self.__s_ending:
word = word[:-1]
r1 = r1[:-1]
else:
word = word[: -len(suffix)]
r1 = r1[: -len(suffix)]
break
for suffix in self.__step2_suffixes:
if r1.endswith(suffix):
word = word[:-1]
r1 = r1[:-1]
break
for suffix in self.__step3_suffixes:
if r1.endswith(suffix):
if suffix in (""els"", ""lig"", ""ig""):
word = word[: -len(suffix)]
elif suffix in (""fullt"", ""l\xF6st""):
word = word[:-1]
break
return word
",[],0,[],/stem/snowball.py_stem
3811,/home/amandapotts/git/nltk/nltk/stem/snowball.py_demo,"def demo():
""""""
This function provides a demonstration of the Snowball stemmers.
After invoking this function and specifying a language,
it stems an excerpt of the Universal Declaration of Human Rights
(which is a part of the NLTK corpus collection) and then prints
out the original and the stemmed text.
""""""
from nltk.corpus import udhr
udhr_corpus = {
""arabic"": ""Arabic_Alarabia-Arabic"",
""danish"": ""Danish_Dansk-Latin1"",
""dutch"": ""Dutch_Nederlands-Latin1"",
""english"": ""English-Latin1"",
""finnish"": ""Finnish_Suomi-Latin1"",
""french"": ""French_Francais-Latin1"",
""german"": ""German_Deutsch-Latin1"",
""hungarian"": ""Hungarian_Magyar-UTF8"",
""italian"": ""Italian_Italiano-Latin1"",
""norwegian"": ""Norwegian-Latin1"",
""porter"": ""English-Latin1"",
""portuguese"": ""Portuguese_Portugues-Latin1"",
""romanian"": ""Romanian_Romana-Latin2"",
""russian"": ""Russian-UTF8"",
""spanish"": ""Spanish-Latin1"",
""swedish"": ""Swedish_Svenska-Latin1"",
}
print(""\n"")
print(""******************************"")
print(""Demo for the Snowball stemmers"")
print(""******************************"")
while True:
language = input(
""Please enter the name of the language ""
+ ""to be demonstrated\n""
+ ""/"".join(SnowballStemmer.languages)
+ ""\n""
+ ""(enter 'exit' in order to leave): ""
)
if language == ""exit"":
break
if language not in SnowballStemmer.languages:
print(
""\nOops, there is no stemmer for this language. ""
+ ""Please try again.\n""
)
continue
stemmer = SnowballStemmer(language)
excerpt = udhr.words(udhr_corpus[language])[:300]
stemmed = "" "".join(stemmer.stem(word) for word in excerpt)
stemmed = re.sub(r""(.{,70})\s"", r""\1\n"", stemmed + "" "").rstrip()
excerpt = "" "".join(excerpt)
excerpt = re.sub(r""(.{,70})\s"", r""\1\n"", excerpt + "" "").rstrip()
print(""\n"")
print(""-"" * 70)
print(""ORIGINAL"".center(70))
print(excerpt)
print(""\n\n"")
print(""STEMMED RESULTS"".center(70))
print(stemmed)
print(""-"" * 70)
print(""\n"")
",[],0,[],/stem/snowball.py_demo
3812,/home/amandapotts/git/nltk/nltk/stem/api.py_stem,"def stem(self, token):
""""""
Strip affixes from the token and return the stem.
:param token: The token that should be stemmed.
:type token: str
""""""
",[],0,[],/stem/api.py_stem
3813,/home/amandapotts/git/nltk/nltk/stem/arlstem2.py___init__,"def __init__(self):
self.re_hamzated_alif = re.compile(r""[\u0622\u0623\u0625]"")
self.re_alifMaqsura = re.compile(r""[\u0649]"")
self.re_diacritics = re.compile(r""[\u064B-\u065F]"")
self.pr2 = [""\u0627\u0644"", ""\u0644\u0644"", ""\u0641\u0644"", ""\u0641\u0628""]
self.pr3 = [""\u0628\u0627\u0644"", ""\u0643\u0627\u0644"", ""\u0648\u0627\u0644""]
self.pr32 = [""\u0641\u0644\u0644"", ""\u0648\u0644\u0644""]
self.pr4 = [
""\u0641\u0628\u0627\u0644"",
""\u0648\u0628\u0627\u0644"",
""\u0641\u0643\u0627\u0644"",
]
self.su2 = [""\u0643\u064A"", ""\u0643\u0645""]
self.su22 = [""\u0647\u0627"", ""\u0647\u0645""]
self.su3 = [""\u0643\u0645\u0627"", ""\u0643\u0646\u0651""]
self.su32 = [""\u0647\u0645\u0627"", ""\u0647\u0646\u0651""]
self.pl_si2 = [""\u0627\u0646"", ""\u064A\u0646"", ""\u0648\u0646""]
self.pl_si3 = [""\u062A\u0627\u0646"", ""\u062A\u064A\u0646""]
self.verb_su2 = [""\u0627\u0646"", ""\u0648\u0646""]
self.verb_pr2 = [""\u0633\u062A"", ""\u0633\u064A""]
self.verb_pr22 = [""\u0633\u0627"", ""\u0633\u0646""]
self.verb_pr33 = [
""\u0644\u0646"",
""\u0644\u062A"",
""\u0644\u064A"",
""\u0644\u0623"",
]
self.verb_suf3 = [""\u062A\u0645\u0627"", ""\u062A\u0646\u0651""]
self.verb_suf2 = [
""\u0646\u0627"",
""\u062A\u0645"",
""\u062A\u0627"",
""\u0648\u0627"",
]
self.verb_suf1 = [""\u062A"", ""\u0627"", ""\u0646""]
",[],0,[],/stem/arlstem2.py___init__
3814,/home/amandapotts/git/nltk/nltk/stem/arlstem2.py_stem1,"def stem1(self, token):
""""""
call this function to get the first stem
""""""
try:
if token is None:
raise ValueError(
""The word could not be stemmed, because \
it is empty !""
)
self.is_verb = False
token = self.norm(token)
pre = self.pref(token)
if pre is not None:
token = pre
fm = self.fem2masc(token)
if fm is not None:
return fm
adj = self.adjective(token)
if adj is not None:
return adj
token = self.suff(token)
ps = self.plur2sing(token)
if ps is None:
if pre is None:  # if the noun prefixes are not stripped
verb = self.verb(token)
if verb is not None:
self.is_verb = True
return verb
else:
return ps
return token
except ValueError as e:
print(e)
",[],0,[],/stem/arlstem2.py_stem1
3815,/home/amandapotts/git/nltk/nltk/stem/arlstem2.py_stem,"def stem(self, token):
try:
if token is None:
raise ValueError(
""The word could not be stemmed, because \
it is empty !""
)
token = self.stem1(token)
if len(token) > 4:
if token.startswith(""\u062A"") and token[-2] == ""\u064A"":
token = token[1:-2] + token[-1]
return token
if token.startswith(""\u0645"") and token[-2] == ""\u0648"":
token = token[1:-2] + token[-1]
return token
if len(token) > 3:
if not token.startswith(""\u0627"") and token.endswith(""\u064A""):
token = token[:-1]
return token
if token.startswith(""\u0644""):
return token[1:]
return token
except ValueError as e:
print(e)
",[],0,[],/stem/arlstem2.py_stem
3816,/home/amandapotts/git/nltk/nltk/stem/arlstem2.py_norm,"def norm(self, token):
""""""
normalize the word by removing diacritics, replace hamzated Alif
with Alif bare, replace AlifMaqsura with Yaa and remove Waaw at the
beginning.
""""""
token = self.re_diacritics.sub("""", token)
token = self.re_hamzated_alif.sub(""\u0627"", token)
token = self.re_alifMaqsura.sub(""\u064A"", token)
if token.startswith(""\u0648"") and len(token) > 3:
token = token[1:]
return token
",[],0,[],/stem/arlstem2.py_norm
3817,/home/amandapotts/git/nltk/nltk/stem/arlstem2.py_pref,"def pref(self, token):
""""""
remove prefixes from the words' beginning.
""""""
if len(token) > 5:
for p3 in self.pr3:
if token.startswith(p3):
return token[3:]
if len(token) > 6:
for p4 in self.pr4:
if token.startswith(p4):
return token[4:]
if len(token) > 5:
for p3 in self.pr32:
if token.startswith(p3):
return token[3:]
if len(token) > 4:
for p2 in self.pr2:
if token.startswith(p2):
return token[2:]
",[],0,[],/stem/arlstem2.py_pref
3818,/home/amandapotts/git/nltk/nltk/stem/arlstem2.py_adjective,"def adjective(self, token):
""""""
remove the infixes from adjectives
""""""
if len(token) > 5:
if (
token.startswith(""\u0627"")
and token[-3] == ""\u0627""
and token.endswith(""\u064A"")
):
return token[:-3] + token[-2]
",[],0,[],/stem/arlstem2.py_adjective
3819,/home/amandapotts/git/nltk/nltk/stem/arlstem2.py_suff,"def suff(self, token):
""""""
remove the suffixes from the word's ending.
""""""
if token.endswith(""\u0643"") and len(token) > 3:
return token[:-1]
if len(token) > 4:
for s2 in self.su2:
if token.endswith(s2):
return token[:-2]
if len(token) > 5:
for s3 in self.su3:
if token.endswith(s3):
return token[:-3]
if token.endswith(""\u0647"") and len(token) > 3:
token = token[:-1]
return token
if len(token) > 4:
for s2 in self.su22:
if token.endswith(s2):
return token[:-2]
if len(token) > 5:
for s3 in self.su32:
if token.endswith(s3):
return token[:-3]
if token.endswith(""\u0646\u0627"") and len(token) > 4:
return token[:-2]
return token
",[],0,[],/stem/arlstem2.py_suff
3820,/home/amandapotts/git/nltk/nltk/stem/arlstem2.py_fem2masc,"def fem2masc(self, token):
""""""
transform the word from the feminine form to the masculine form.
""""""
if len(token) > 6:
if (
token.startswith(""\u062A"")
and token[-4] == ""\u064A""
and token.endswith(""\u064A\u0629"")
):
return token[1:-4] + token[-3]
if (
token.startswith(""\u0627"")
and token[-4] == ""\u0627""
and token.endswith(""\u064A\u0629"")
):
return token[:-4] + token[-3]
if token.endswith(""\u0627\u064A\u0629"") and len(token) > 5:
return token[:-2]
if len(token) > 4:
if token[1] == ""\u0627"" and token.endswith(""\u0629""):
return token[0] + token[2:-1]
if token.endswith(""\u064A\u0629""):
return token[:-2]
if token.endswith(""\u0629"") and len(token) > 3:
return token[:-1]
",[],0,[],/stem/arlstem2.py_fem2masc
3821,/home/amandapotts/git/nltk/nltk/stem/arlstem2.py_plur2sing,"def plur2sing(self, token):
""""""
transform the word from the plural form to the singular form.
""""""
if len(token) > 5:
if token.startswith(""\u0645"") and token.endswith(""\u0648\u0646""):
return token[1:-2]
if len(token) > 4:
for ps2 in self.pl_si2:
if token.endswith(ps2):
return token[:-2]
if len(token) > 5:
for ps3 in self.pl_si3:
if token.endswith(ps3):
return token[:-3]
if len(token) > 4:
if token.endswith(""\u0627\u062A""):
return token[:-2]
if token.startswith(""\u0627"") and token[2] == ""\u0627"":
return token[:2] + token[3:]
if token.startswith(""\u0627"") and token[-2] == ""\u0627"":
return token[1:-2] + token[-1]
",[],0,[],/stem/arlstem2.py_plur2sing
3822,/home/amandapotts/git/nltk/nltk/stem/arlstem2.py_verb,"def verb(self, token):
""""""
stem the verb prefixes and suffixes or both
""""""
vb = self.verb_t1(token)
if vb is not None:
return vb
vb = self.verb_t2(token)
if vb is not None:
return vb
vb = self.verb_t3(token)
if vb is not None:
return vb
vb = self.verb_t4(token)
if vb is not None:
return vb
vb = self.verb_t5(token)
if vb is not None:
return vb
vb = self.verb_t6(token)
return vb
",[],0,[],/stem/arlstem2.py_verb
3823,/home/amandapotts/git/nltk/nltk/stem/arlstem2.py_verb_t1,"def verb_t1(self, token):
""""""
stem the present tense co-occurred prefixes and suffixes
""""""
if len(token) > 5 and token.startswith(""\u062A""):  # Taa
for s2 in self.pl_si2:
if token.endswith(s2):
return token[1:-2]
if len(token) > 5 and token.startswith(""\u064A""):  # Yaa
for s2 in self.verb_su2:
if token.endswith(s2):
return token[1:-2]
if len(token) > 4 and token.startswith(""\u0627""):  # Alif
if len(token) > 5 and token.endswith(""\u0648\u0627""):
return token[1:-2]
if token.endswith(""\u064A""):
return token[1:-1]
if token.endswith(""\u0627""):
return token[1:-1]
if token.endswith(""\u0646""):
return token[1:-1]
if len(token) > 4 and token.startswith(""\u064A"") and token.endswith(""\u0646""):
return token[1:-1]
if len(token) > 4 and token.startswith(""\u062A"") and token.endswith(""\u0646""):
return token[1:-1]
",[],0,[],/stem/arlstem2.py_verb_t1
3824,/home/amandapotts/git/nltk/nltk/stem/arlstem2.py_verb_t2,"def verb_t2(self, token):
""""""
stem the future tense co-occurred prefixes and suffixes
""""""
if len(token) > 6:
for s2 in self.pl_si2:
if token.startswith(self.verb_pr2[0]) and token.endswith(s2):
return token[2:-2]
if token.startswith(self.verb_pr2[1]) and token.endswith(self.pl_si2[0]):
return token[2:-2]
if token.startswith(self.verb_pr2[1]) and token.endswith(self.pl_si2[2]):
return token[2:-2]
if (
len(token) > 5
and token.startswith(self.verb_pr2[0])
and token.endswith(""\u0646"")
):
return token[2:-1]
if (
len(token) > 5
and token.startswith(self.verb_pr2[1])
and token.endswith(""\u0646"")
):
return token[2:-1]
",[],0,[],/stem/arlstem2.py_verb_t2
3825,/home/amandapotts/git/nltk/nltk/stem/arlstem2.py_verb_t3,"def verb_t3(self, token):
""""""
stem the present tense suffixes
""""""
if len(token) > 5:
for su3 in self.verb_suf3:
if token.endswith(su3):
return token[:-3]
if len(token) > 4:
for su2 in self.verb_suf2:
if token.endswith(su2):
return token[:-2]
if len(token) > 3:
for su1 in self.verb_suf1:
if token.endswith(su1):
return token[:-1]
",[],0,[],/stem/arlstem2.py_verb_t3
3826,/home/amandapotts/git/nltk/nltk/stem/arlstem2.py_verb_t4,"def verb_t4(self, token):
""""""
stem the present tense prefixes
""""""
if len(token) > 3:
for pr1 in self.verb_suf1:
if token.startswith(pr1):
return token[1:]
if token.startswith(""\u064A""):
return token[1:]
",[],0,[],/stem/arlstem2.py_verb_t4
3827,/home/amandapotts/git/nltk/nltk/stem/arlstem2.py_verb_t5,"def verb_t5(self, token):
""""""
stem the future tense prefixes
""""""
if len(token) > 4:
for pr2 in self.verb_pr22:
if token.startswith(pr2):
return token[2:]
for pr2 in self.verb_pr2:
if token.startswith(pr2):
return token[2:]
",[],0,[],/stem/arlstem2.py_verb_t5
3828,/home/amandapotts/git/nltk/nltk/stem/arlstem2.py_verb_t6,"def verb_t6(self, token):
""""""
stem the imperative tense prefixes
""""""
if len(token) > 4:
for pr3 in self.verb_pr33:
if token.startswith(pr3):
return token[2:]
return token
",[],0,[],/stem/arlstem2.py_verb_t6
3829,/home/amandapotts/git/nltk/nltk/stem/rslp.py___init__,"def __init__(self):
self._model = []
self._model.append(self.read_rule(""step0.pt""))
self._model.append(self.read_rule(""step1.pt""))
self._model.append(self.read_rule(""step2.pt""))
self._model.append(self.read_rule(""step3.pt""))
self._model.append(self.read_rule(""step4.pt""))
self._model.append(self.read_rule(""step5.pt""))
self._model.append(self.read_rule(""step6.pt""))
",[],0,[],/stem/rslp.py___init__
3830,/home/amandapotts/git/nltk/nltk/stem/rslp.py_read_rule,"def read_rule(self, filename):
rules = load(""nltk:stemmers/rslp/"" + filename, format=""raw"").decode(""utf8"")
lines = rules.split(""\n"")
lines = [line for line in lines if line != """"]  # remove blank lines
lines = [line for line in lines if line[0] != ""#""]  # remove comments
lines = [line.replace(""\t\t"", ""\t"") for line in lines]
rules = []
for line in lines:
rule = []
tokens = line.split(""\t"")
rule.append(tokens[0][1:-1])  # remove quotes
rule.append(int(tokens[1]))
rule.append(tokens[2][1:-1])  # remove quotes
rule.append([token[1:-1] for token in tokens[3].split("","")])
rules.append(rule)
return rules
",[],0,[],/stem/rslp.py_read_rule
3831,/home/amandapotts/git/nltk/nltk/stem/rslp.py_stem,"def stem(self, word):
word = word.lower()
if word[-1] == ""s"":
word = self.apply_rule(word, 0)
if word[-1] == ""a"":
word = self.apply_rule(word, 1)
word = self.apply_rule(word, 3)
word = self.apply_rule(word, 2)
prev_word = word
word = self.apply_rule(word, 4)
if word == prev_word:
prev_word = word
word = self.apply_rule(word, 5)
if word == prev_word:
word = self.apply_rule(word, 6)
return word
",[],0,[],/stem/rslp.py_stem
3832,/home/amandapotts/git/nltk/nltk/stem/rslp.py_apply_rule,"def apply_rule(self, word, rule_index):
rules = self._model[rule_index]
for rule in rules:
suffix_length = len(rule[0])
if word[-suffix_length:] == rule[0]:  # if suffix matches
if len(word) >= suffix_length + rule[1]:  # if we have minimum size
if word not in rule[3]:  # if not an exception
word = word[:-suffix_length] + rule[2]
break
return word
",[],0,[],/stem/rslp.py_apply_rule
3833,/home/amandapotts/git/nltk/nltk/stem/wordnet.py_lemmatize,"def lemmatize(self, word: str, pos: str = ""n"") -> str:
""""""Lemmatize `word` using WordNet's built-in morphy function.
Returns the input word unchanged if it cannot be found in WordNet.
:param word: The input word to lemmatize.
:type word: str
:param pos: The Part Of Speech tag. Valid options are `""n""` for nouns,
`""v""` for verbs, `""a""` for adjectives, `""r""` for adverbs and `""s""`
for satellite adjectives.
:type pos: str
:return: The lemma of `word`, for the given `pos`.
""""""
lemmas = wn._morphy(word, pos)
return min(lemmas, key=len) if lemmas else word
",[],0,[],/stem/wordnet.py_lemmatize
3834,/home/amandapotts/git/nltk/nltk/stem/wordnet.py___repr__,"def __repr__(self):
return ""<WordNetLemmatizer>""
",[],0,[],/stem/wordnet.py___repr__
3835,/home/amandapotts/git/nltk/nltk/misc/babelfish.py_babelize_shell,"def babelize_shell():
print(""Babelfish online translation service is no longer available."")
",[],0,[],/misc/babelfish.py_babelize_shell
3836,/home/amandapotts/git/nltk/nltk/misc/sort.py_selection,"def selection(a):
""""""
Selection Sort: scan the list to find its smallest element, then
swap it with the first element.  The remainder of the list is one
element smaller
""""""
count = 0
for i in range(len(a) - 1):
min = i
for j in range(i + 1, len(a)):
if a[j] < a[min]:
min = j
count += 1
a[min], a[i] = a[i], a[min]
return count
",[],0,[],/misc/sort.py_selection
3837,/home/amandapotts/git/nltk/nltk/misc/sort.py_bubble,"def bubble(a):
""""""
Bubble Sort: compare adjacent elements of the list left-to-right,
and swap them if they are out of order.  After one pass through
the list swapping adjacent items, the largest item will be in
the rightmost position.  The remainder is one element smaller
apply the same method to this list, and so on.
""""""
count = 0
for i in range(len(a) - 1):
for j in range(len(a) - i - 1):
if a[j + 1] < a[j]:
a[j], a[j + 1] = a[j + 1], a[j]
count += 1
return count
",[],0,[],/misc/sort.py_bubble
3838,/home/amandapotts/git/nltk/nltk/misc/sort.py__merge_lists,"def _merge_lists(b, c):
count = 0
i = j = 0
a = []
while i < len(b) and j < len(c):
count += 1
if b[i] <= c[j]:
a.append(b[i])
i += 1
else:
a.append(c[j])
j += 1
if i == len(b):
a += c[j:]
else:
a += b[i:]
return a, count
",[],0,[],/misc/sort.py__merge_lists
3839,/home/amandapotts/git/nltk/nltk/misc/sort.py_merge,"def merge(a):
""""""
Merge Sort: split the list in half, and sort each half, then
combine the sorted halves.
""""""
count = 0
if len(a) > 1:
midpoint = len(a) // 2
b = a[:midpoint]
c = a[midpoint:]
count_b = merge(b)
count_c = merge(c)
result, count_a = _merge_lists(b, c)
a[:] = result  # copy the result back into a.
count = count_a + count_b + count_c
return count
",[],0,[],/misc/sort.py_merge
3840,/home/amandapotts/git/nltk/nltk/misc/sort.py__partition,"def _partition(a, l, r):
p = a[l]
i = l
j = r + 1
count = 0
while True:
while i < r:
i += 1
if a[i] >= p:
break
while j > l:
j -= 1
if j < l or a[j] <= p:
break
a[i], a[j] = a[j], a[i]  # swap
count += 1
if i >= j:
break
a[i], a[j] = a[j], a[i]  # undo last swap
a[l], a[j] = a[j], a[l]
return j, count
",[],0,[],/misc/sort.py__partition
3841,/home/amandapotts/git/nltk/nltk/misc/sort.py__quick,"def _quick(a, l, r):
count = 0
if l < r:
s, count = _partition(a, l, r)
count += _quick(a, l, s - 1)
count += _quick(a, s + 1, r)
return count
",[],0,[],/misc/sort.py__quick
3842,/home/amandapotts/git/nltk/nltk/misc/sort.py_quick,"def quick(a):
return _quick(a, 0, len(a) - 1)
",[],0,[],/misc/sort.py_quick
3843,/home/amandapotts/git/nltk/nltk/misc/sort.py_demo,"def demo():
from random import shuffle
for size in (10, 20, 50, 100, 200, 500, 1000):
a = list(range(size))
shuffle(a)
count_selection = selection(a)
shuffle(a)
count_bubble = bubble(a)
shuffle(a)
count_merge = merge(a)
shuffle(a)
count_quick = quick(a)
print(
(""size=%5d:  selection=%8d,  bubble=%8d,  "" ""merge=%6d,  quick=%6d"")
% (size, count_selection, count_bubble, count_merge, count_quick)
)
",[],0,[],/misc/sort.py_demo
3844,/home/amandapotts/git/nltk/nltk/misc/chomsky.py_generate_chomsky,"def generate_chomsky(times=5, line_length=72):
parts = []
for part in (leadins, subjects, verbs, objects):
phraselist = list(map(str.strip, part.splitlines()))
random.shuffle(phraselist)
parts.append(phraselist)
output = chain.from_iterable(islice(zip(*parts), 0, times))
print(textwrap.fill("" "".join(output), line_length))
",[],0,[],/misc/chomsky.py_generate_chomsky
3845,/home/amandapotts/git/nltk/nltk/misc/minimalset.py___init__,"def __init__(self, parameters=None):
""""""
Create a new minimal set.
:param parameters: The (context, target, display) tuples for the item
:type parameters: list(tuple(str, str, str))
""""""
self._targets = set()  # the contrastive information
self._contexts = set()  # what we are controlling for
self._seen = defaultdict(set)  # to record what we have seen
self._displays = {}  # what we will display
if parameters:
for context, target, display in parameters:
self.add(context, target, display)
",[],0,[],/misc/minimalset.py___init__
3846,/home/amandapotts/git/nltk/nltk/misc/minimalset.py_add,"def add(self, context, target, display):
""""""
Add a new item to the minimal set, having the specified
context, target, and display form.
:param context: The context in which the item of interest appears
:type context: str
:param target: The item of interest
:type target: str
:param display: The information to be reported for each item
:type display: str
""""""
self._seen[context].add(target)
self._contexts.add(context)
self._targets.add(target)
self._displays[(context, target)] = display
",[],0,[],/misc/minimalset.py_add
3847,/home/amandapotts/git/nltk/nltk/misc/minimalset.py_contexts,"def contexts(self, minimum=2):
""""""
Determine which contexts occurred with enough distinct targets.
:param minimum: the minimum number of distinct target forms
:type minimum: int
:rtype: list
""""""
return [c for c in self._contexts if len(self._seen[c]) >= minimum]
",[],0,[],/misc/minimalset.py_contexts
3848,/home/amandapotts/git/nltk/nltk/misc/minimalset.py_display,"def display(self, context, target, default=""""):
if (context, target) in self._displays:
return self._displays[(context, target)]
else:
return default
",[],0,[],/misc/minimalset.py_display
3849,/home/amandapotts/git/nltk/nltk/misc/minimalset.py_display_all,"def display_all(self, context):
result = []
for target in self._targets:
x = self.display(context, target)
if x:
result.append(x)
return result
",[],0,[],/misc/minimalset.py_display_all
3850,/home/amandapotts/git/nltk/nltk/misc/minimalset.py_targets,"def targets(self):
return self._targets
",[],0,[],/misc/minimalset.py_targets
3851,/home/amandapotts/git/nltk/nltk/misc/wordfinder.py_revword,"def revword(word):
if random.randint(1, 2) == 1:
return word[::-1]
return word
",[],0,[],/misc/wordfinder.py_revword
3852,/home/amandapotts/git/nltk/nltk/misc/wordfinder.py_step,"def step(word, x, xf, y, yf, grid):
for i in range(len(word)):
if grid[xf(i)][yf(i)] != """" and grid[xf(i)][yf(i)] != word[i]:
return False
for i in range(len(word)):
grid[xf(i)][yf(i)] = word[i]
return True
",[],0,[],/misc/wordfinder.py_step
3853,/home/amandapotts/git/nltk/nltk/misc/wordfinder.py_wordfinder,"def wordfinder(words, rows=20, cols=20, attempts=50, alph=""ABCDEFGHIJKLMNOPQRSTUVWXYZ""):
""""""
Attempt to arrange words into a letter-grid with the specified
number of rows and columns.  Try each word in several positions
and directions, until it can be fitted into the grid, or the
maximum number of allowable attempts is exceeded.  Returns a tuple
consisting of the grid and the words that were successfully
placed.
:param words: the list of words to be put into the grid
:type words: list
:param rows: the number of rows in the grid
:type rows: int
:param cols: the number of columns in the grid
:type cols: int
:param attempts: the number of times to attempt placing a word
:type attempts: int
:param alph: the alphabet, to be used for filling blank cells
:type alph: list
:rtype: tuple
""""""
words = sorted(words, key=len, reverse=True)
grid = []  # the letter grid
used = []  # the words we used
for i in range(rows):
grid.append([""""] * cols)
for word in words:
word = word.strip().upper()  # normalize
save = word  # keep a record of the word
word = revword(word)
for attempt in range(attempts):
r = random.randint(0, len(word))
dir = random.choice([1, 2, 3, 4])
x = random.randint(0, rows)
y = random.randint(0, cols)
if dir == 1:
x += r
y += r
elif dir == 2:
x += r
elif dir == 3:
x += r
y -= r
elif dir == 4:
y += r
if 0 <= x < rows and 0 <= y < cols:
if check(word, dir, x, y, grid, rows, cols):
used.append(save)
break
for i in range(rows):
for j in range(cols):
if grid[i][j] == """":
grid[i][j] = random.choice(alph)
return grid, used
",[],0,[],/misc/wordfinder.py_wordfinder
3854,/home/amandapotts/git/nltk/nltk/misc/wordfinder.py_word_finder,"def word_finder():
from nltk.corpus import words
wordlist = words.words()
random.shuffle(wordlist)
wordlist = wordlist[:200]
wordlist = [w for w in wordlist if 3 <= len(w) <= 12]
grid, used = wordfinder(wordlist)
print(""Word Finder\n"")
for i in range(len(grid)):
for j in range(len(grid[i])):
print(grid[i][j], end="" "")
print()
print()
for i in range(len(used)):
print(""%d:"" % (i + 1), used[i])
",[],0,[],/misc/wordfinder.py_word_finder
3855,/home/amandapotts/git/nltk/nltk/corpus/__init__.py_demo,"def demo():
abc.demo()
brown.demo()
cmudict.demo()
conll2000.demo()
conll2002.demo()
genesis.demo()
gutenberg.demo()
ieer.demo()
inaugural.demo()
indian.demo()
names.demo()
ppattach.demo()
senseval.demo()
shakespeare.demo()
sinica_treebank.demo()
state_union.demo()
stopwords.demo()
timit.demo()
toolbox.demo()
treebank.demo()
udhr.demo()
webtext.demo()
words.demo()
",[],0,[],/corpus/__init__.py_demo
3856,/home/amandapotts/git/nltk/nltk/corpus/util.py___init__,"def __init__(self, name, reader_cls, *args, **kwargs):
from nltk.corpus.reader.api import CorpusReader
assert issubclass(reader_cls, CorpusReader)
self.__name = self.__name__ = name
self.__reader_cls = reader_cls
if ""nltk_data_subdir"" in kwargs:
self.subdir = kwargs[""nltk_data_subdir""]
kwargs.pop(""nltk_data_subdir"", None)
else:  # Otherwise use 'nltk_data/corpora'
self.subdir = ""corpora""
self.__args = args
self.__kwargs = kwargs
",[],0,[],/corpus/util.py___init__
3857,/home/amandapotts/git/nltk/nltk/corpus/util.py___load,"def __load(self):
zip_name = re.sub(r""(([^/]+)(/.*)?)"", r""\2.zip/\1/"", self.__name)
if TRY_ZIPFILE_FIRST:
try:
root = nltk.data.find(f""{self.subdir}/{zip_name}"")
except LookupError as e:
try:
root = nltk.data.find(f""{self.subdir}/{self.__name}"")
except LookupError:
raise e
else:
try:
root = nltk.data.find(f""{self.subdir}/{self.__name}"")
except LookupError as e:
try:
root = nltk.data.find(f""{self.subdir}/{zip_name}"")
except LookupError:
raise e
corpus = self.__reader_cls(root, *self.__args, **self.__kwargs)
args, kwargs = self.__args, self.__kwargs
name, reader_cls = self.__name, self.__reader_cls
self.__dict__ = corpus.__dict__
self.__class__ = corpus.__class__
",[],0,[],/corpus/util.py___load
3858,/home/amandapotts/git/nltk/nltk/corpus/util.py__unload,"def _unload(self):
lazy_reader = LazyCorpusLoader(name, reader_cls, *args, **kwargs)
self.__dict__ = lazy_reader.__dict__
self.__class__ = lazy_reader.__class__
gc.collect()
",[],0,[],/corpus/util.py__unload
3859,/home/amandapotts/git/nltk/nltk/corpus/util.py___getattr__,"def __getattr__(self, attr):
if attr == ""__bases__"":
raise AttributeError(""LazyCorpusLoader object has no attribute '__bases__'"")
self.__load()
return getattr(self, attr)
",[],0,[],/corpus/util.py___getattr__
3860,/home/amandapotts/git/nltk/nltk/corpus/util.py___repr__,"def __repr__(self):
return ""<{} in {!r} (not loaded yet)>"".format(
self.__reader_cls.__name__,
"".../corpora/"" + self.__name,
)
",[],0,[],/corpus/util.py___repr__
3861,/home/amandapotts/git/nltk/nltk/corpus/util.py__unload,"def _unload(self):
pass
",[],0,[],/corpus/util.py__unload
3862,/home/amandapotts/git/nltk/nltk/corpus/util.py__make_bound_method,"def _make_bound_method(func, self):
""""""
Magic for creating bound methods (used for _unload).
""""""
",[],0,[],/corpus/util.py__make_bound_method
3863,/home/amandapotts/git/nltk/nltk/corpus/util.py_Foo:,"class Foo:
",[],0,[],/corpus/util.py_Foo
3864,/home/amandapotts/git/nltk/nltk/corpus/util.py_meth,"def meth(self):
pass
",[],0,[],/corpus/util.py_meth
3865,/home/amandapotts/git/nltk/nltk/corpus/reader/tagged.py___init__,"def __init__(
self,
root,
fileids,
sep=""/"",
word_tokenizer=WhitespaceTokenizer(),
sent_tokenizer=RegexpTokenizer(""\n"", gaps=True),
para_block_reader=read_blankline_block,
encoding=""utf8"",
tagset=None,
",[],0,[],/corpus/reader/tagged.py___init__
3866,/home/amandapotts/git/nltk/nltk/corpus/reader/tagged.py_words,"def words(self, fileids=None):
""""""
:return: the given file(s) as a list of words
and punctuation symbols.
:rtype: list(str)
""""""
return concat(
[
TaggedCorpusView(
fileid,
enc,
False,
False,
False,
self._sep,
self._word_tokenizer,
self._sent_tokenizer,
self._para_block_reader,
None,
)
for (fileid, enc) in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/tagged.py_words
3867,/home/amandapotts/git/nltk/nltk/corpus/reader/tagged.py_sents,"def sents(self, fileids=None):
""""""
:return: the given file(s) as a list of
sentences or utterances, each encoded as a list of word
strings.
:rtype: list(list(str))
""""""
return concat(
[
TaggedCorpusView(
fileid,
enc,
False,
True,
False,
self._sep,
self._word_tokenizer,
self._sent_tokenizer,
self._para_block_reader,
None,
)
for (fileid, enc) in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/tagged.py_sents
3868,/home/amandapotts/git/nltk/nltk/corpus/reader/tagged.py_paras,"def paras(self, fileids=None):
""""""
:return: the given file(s) as a list of
paragraphs, each encoded as a list of sentences, which are
in turn encoded as lists of word strings.
:rtype: list(list(list(str)))
""""""
return concat(
[
TaggedCorpusView(
fileid,
enc,
False,
True,
True,
self._sep,
self._word_tokenizer,
self._sent_tokenizer,
self._para_block_reader,
None,
)
for (fileid, enc) in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/tagged.py_paras
3869,/home/amandapotts/git/nltk/nltk/corpus/reader/tagged.py_tagged_words,"def tagged_words(self, fileids=None, tagset=None):
""""""
:return: the given file(s) as a list of tagged
words and punctuation symbols, encoded as tuples
``(word,tag)``.
:rtype: list(tuple(str,str))
""""""
if tagset and tagset != self._tagset:
",[],0,[],/corpus/reader/tagged.py_tagged_words
3870,/home/amandapotts/git/nltk/nltk/corpus/reader/tagged.py_tagged_sents,"def tagged_sents(self, fileids=None, tagset=None):
""""""
:return: the given file(s) as a list of
sentences, each encoded as a list of ``(word,tag)`` tuples.
:rtype: list(list(tuple(str,str)))
""""""
if tagset and tagset != self._tagset:
",[],0,[],/corpus/reader/tagged.py_tagged_sents
3871,/home/amandapotts/git/nltk/nltk/corpus/reader/tagged.py_tagged_paras,"def tagged_paras(self, fileids=None, tagset=None):
""""""
:return: the given file(s) as a list of
paragraphs, each encoded as a list of sentences, which are
in turn encoded as lists of ``(word,tag)`` tuples.
:rtype: list(list(list(tuple(str,str))))
""""""
if tagset and tagset != self._tagset:
",[],0,[],/corpus/reader/tagged.py_tagged_paras
3872,/home/amandapotts/git/nltk/nltk/corpus/reader/tagged.py___init__,"def __init__(self, *args, **kwargs):
""""""
Initialize the corpus reader.  Categorization arguments
(``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to
the ``CategorizedCorpusReader`` constructor.  The remaining arguments
are passed to the ``TaggedCorpusReader``.
""""""
CategorizedCorpusReader.__init__(self, kwargs)
TaggedCorpusReader.__init__(self, *args, **kwargs)
",[],0,[],/corpus/reader/tagged.py___init__
3873,/home/amandapotts/git/nltk/nltk/corpus/reader/tagged.py_tagged_words,"def tagged_words(self, fileids=None, categories=None, tagset=None):
return super().tagged_words(self._resolve(fileids, categories), tagset)
",[],0,[],/corpus/reader/tagged.py_tagged_words
3874,/home/amandapotts/git/nltk/nltk/corpus/reader/tagged.py_tagged_sents,"def tagged_sents(self, fileids=None, categories=None, tagset=None):
return super().tagged_sents(self._resolve(fileids, categories), tagset)
",[],0,[],/corpus/reader/tagged.py_tagged_sents
3875,/home/amandapotts/git/nltk/nltk/corpus/reader/tagged.py_tagged_paras,"def tagged_paras(self, fileids=None, categories=None, tagset=None):
return super().tagged_paras(self._resolve(fileids, categories), tagset)
",[],0,[],/corpus/reader/tagged.py_tagged_paras
3876,/home/amandapotts/git/nltk/nltk/corpus/reader/tagged.py___init__,"def __init__(
self,
corpus_file,
encoding,
tagged,
group_by_sent,
group_by_para,
sep,
word_tokenizer,
sent_tokenizer,
para_block_reader,
tag_mapping_function=None,
",[],0,[],/corpus/reader/tagged.py___init__
3877,/home/amandapotts/git/nltk/nltk/corpus/reader/tagged.py_read_block,"def read_block(self, stream):
""""""Reads one paragraph at a time.""""""
block = []
for para_str in self._para_block_reader(stream):
para = []
for sent_str in self._sent_tokenizer.tokenize(para_str):
sent = [
str2tuple(s, self._sep)
for s in self._word_tokenizer.tokenize(sent_str)
]
if self._tag_mapping_function:
sent = [(w, self._tag_mapping_function(t)) for (w, t) in sent]
if not self._tagged:
sent = [w for (w, t) in sent]
if self._group_by_sent:
para.append(sent)
else:
para.extend(sent)
if self._group_by_para:
block.append(para)
else:
block.extend(para)
return block
",[],0,[],/corpus/reader/tagged.py_read_block
3878,/home/amandapotts/git/nltk/nltk/corpus/reader/tagged.py___init__,"def __init__(self, root, fileids, encoding=""utf8"", tagset=None):
TaggedCorpusReader.__init__(
self,
root,
fileids,
sep=""_"",
word_tokenizer=LineTokenizer(),
sent_tokenizer=RegexpTokenizer("".*\n""),
para_block_reader=self._read_block,
encoding=encoding,
tagset=tagset,
)
",[],0,[],/corpus/reader/tagged.py___init__
3879,/home/amandapotts/git/nltk/nltk/corpus/reader/tagged.py__read_block,"def _read_block(self, stream):
return read_regexp_block(stream, r"".*"", r"".*_\."")
",[],0,[],/corpus/reader/tagged.py__read_block
3880,/home/amandapotts/git/nltk/nltk/corpus/reader/tagged.py___init__,"def __init__(self, *args, **kwargs):
TaggedCorpusReader.__init__(
self, para_block_reader=read_timit_block, *args, **kwargs
)
",[],0,[],/corpus/reader/tagged.py___init__
3881,/home/amandapotts/git/nltk/nltk/corpus/reader/tagged.py_paras,"def paras(self):
raise NotImplementedError(""use sents() instead"")
",[],0,[],/corpus/reader/tagged.py_paras
3882,/home/amandapotts/git/nltk/nltk/corpus/reader/tagged.py_tagged_paras,"def tagged_paras(self):
raise NotImplementedError(""use tagged_sents() instead"")
",[],0,[],/corpus/reader/tagged.py_tagged_paras
3883,/home/amandapotts/git/nltk/nltk/corpus/reader/knbc.py___init__,"def __init__(self, root, fileids, encoding=""utf8"", morphs2str=_morphs2str_default):
""""""
Initialize KNBCorpusReader
morphs2str is a function to convert morphlist to str for tree representation
for _parse()
""""""
SyntaxCorpusReader.__init__(self, root, fileids, encoding)
self.morphs2str = morphs2str
",[],0,[],/corpus/reader/knbc.py___init__
3884,/home/amandapotts/git/nltk/nltk/corpus/reader/knbc.py__read_block,"def _read_block(self, stream):
return read_blankline_block(stream)
",[],0,[],/corpus/reader/knbc.py__read_block
3885,/home/amandapotts/git/nltk/nltk/corpus/reader/knbc.py__word,"def _word(self, t):
res = []
for line in t.splitlines():
if not re.match(r""EOS|\*|\#|\+"", line):
cells = line.strip().split("" "")
res.append(cells[0])
return res
",[],0,[],/corpus/reader/knbc.py__word
3886,/home/amandapotts/git/nltk/nltk/corpus/reader/knbc.py__tag,"def _tag(self, t, tagset=None):
res = []
for line in t.splitlines():
if not re.match(r""EOS|\*|\#|\+"", line):
cells = line.strip().split("" "")
res.append((cells[0], "" "".join(cells[1:])))
return res
",[],0,[],/corpus/reader/knbc.py__tag
3887,/home/amandapotts/git/nltk/nltk/corpus/reader/knbc.py__parse,"def _parse(self, t):
dg = DependencyGraph()
i = 0
for line in t.splitlines():
if line[0] in ""*+"":
cells = line.strip().split("" "", 3)
m = re.match(r""([\-0-9]*)([ADIP])"", cells[1])
assert m is not None
node = dg.nodes[i]
node.update({""address"": i, ""rel"": m.group(2), ""word"": []})
dep_parent = int(m.group(1))
if dep_parent == -1:
dg.root = node
else:
dg.nodes[dep_parent][""deps""].append(i)
i += 1
elif line[0] != ""#"":
cells = line.strip().split("" "")
morph = cells[0], "" "".join(cells[1:])
dg.nodes[i - 1][""word""].append(morph)
if self.morphs2str:
for node in dg.nodes.values():
node[""word""] = self.morphs2str(node[""word""])
return dg.tree()
",[],0,[],/corpus/reader/knbc.py__parse
3888,/home/amandapotts/git/nltk/nltk/corpus/reader/knbc.py_demo,"def demo():
import nltk
from nltk.corpus.util import LazyCorpusLoader
root = nltk.data.find(""corpora/knbc/corpus1"")
fileids = [
f
for f in find_corpus_fileids(FileSystemPathPointer(root), "".*"")
if re.search(r""\d\-\d\-[\d]+\-[\d]+"", f)
]
",[],0,[],/corpus/reader/knbc.py_demo
3889,/home/amandapotts/git/nltk/nltk/corpus/reader/knbc.py_test,"def test():
from nltk.corpus.util import LazyCorpusLoader
knbc = LazyCorpusLoader(
""knbc/corpus1"", KNBCorpusReader, r"".*/KN.*"", encoding=""euc-jp""
)
assert isinstance(knbc.words()[0], str)
assert isinstance(knbc.sents()[0][0], str)
assert isinstance(knbc.tagged_words()[0], tuple)
assert isinstance(knbc.tagged_sents()[0][0], tuple)
",[],0,[],/corpus/reader/knbc.py_test
3890,/home/amandapotts/git/nltk/nltk/corpus/reader/ycoe.py___init__,"def __init__(self, root, encoding=""utf8""):
CorpusReader.__init__(self, root, [], encoding)
self._psd_reader = YCOEParseCorpusReader(
self.root.join(""psd""), "".*"", "".psd"", encoding=encoding
)
self._pos_reader = YCOETaggedCorpusReader(self.root.join(""pos""), "".*"", "".pos"")
documents = {f[:-4] for f in self._psd_reader.fileids()}
if {f[:-4] for f in self._pos_reader.fileids()} != documents:
raise ValueError('Items in ""psd"" and ""pos"" ' ""subdirectories do not match."")
fileids = sorted(
[""%s.psd"" % doc for doc in documents]
+ [""%s.pos"" % doc for doc in documents]
)
CorpusReader.__init__(self, root, fileids, encoding)
self._documents = sorted(documents)
",[],0,[],/corpus/reader/ycoe.py___init__
3891,/home/amandapotts/git/nltk/nltk/corpus/reader/ycoe.py_documents,"def documents(self, fileids=None):
""""""
Return a list of document identifiers for all documents in
this corpus, or for the documents with the given file(s) if
specified.
""""""
if fileids is None:
return self._documents
if isinstance(fileids, str):
fileids = [fileids]
for f in fileids:
if f not in self._fileids:
raise KeyError(""File id %s not found"" % fileids)
return sorted({f[:-4] for f in fileids})
",[],0,[],/corpus/reader/ycoe.py_documents
3892,/home/amandapotts/git/nltk/nltk/corpus/reader/ycoe.py_fileids,"def fileids(self, documents=None):
""""""
Return a list of file identifiers for the files that make up
this corpus, or that store the given document(s) if specified.
""""""
if documents is None:
return self._fileids
elif isinstance(documents, str):
documents = [documents]
return sorted(
set(
[""%s.pos"" % doc for doc in documents]
+ [""%s.psd"" % doc for doc in documents]
)
)
",[],0,[],/corpus/reader/ycoe.py_fileids
3893,/home/amandapotts/git/nltk/nltk/corpus/reader/ycoe.py__getfileids,"def _getfileids(self, documents, subcorpus):
""""""
Helper that selects the appropriate fileids for a given set of
documents from a given subcorpus (pos or psd).
""""""
if documents is None:
documents = self._documents
else:
if isinstance(documents, str):
documents = [documents]
for document in documents:
if document not in self._documents:
if document[-4:] in ("".pos"", "".psd""):
raise ValueError(
""Expected a document identifier, not a file ""
""identifier.  (Use corpus.documents() to get ""
""a list of document identifiers.""
)
else:
raise ValueError(""Document identifier %s not found"" % document)
return [f""{d}.{subcorpus}"" for d in documents]
",[],0,[],/corpus/reader/ycoe.py__getfileids
3894,/home/amandapotts/git/nltk/nltk/corpus/reader/ycoe.py_words,"def words(self, documents=None):
return self._pos_reader.words(self._getfileids(documents, ""pos""))
",[],0,[],/corpus/reader/ycoe.py_words
3895,/home/amandapotts/git/nltk/nltk/corpus/reader/ycoe.py_sents,"def sents(self, documents=None):
return self._pos_reader.sents(self._getfileids(documents, ""pos""))
",[],0,[],/corpus/reader/ycoe.py_sents
3896,/home/amandapotts/git/nltk/nltk/corpus/reader/ycoe.py_paras,"def paras(self, documents=None):
return self._pos_reader.paras(self._getfileids(documents, ""pos""))
",[],0,[],/corpus/reader/ycoe.py_paras
3897,/home/amandapotts/git/nltk/nltk/corpus/reader/ycoe.py_tagged_words,"def tagged_words(self, documents=None):
return self._pos_reader.tagged_words(self._getfileids(documents, ""pos""))
",[],0,[],/corpus/reader/ycoe.py_tagged_words
3898,/home/amandapotts/git/nltk/nltk/corpus/reader/ycoe.py_tagged_sents,"def tagged_sents(self, documents=None):
return self._pos_reader.tagged_sents(self._getfileids(documents, ""pos""))
",[],0,[],/corpus/reader/ycoe.py_tagged_sents
3899,/home/amandapotts/git/nltk/nltk/corpus/reader/ycoe.py_tagged_paras,"def tagged_paras(self, documents=None):
return self._pos_reader.tagged_paras(self._getfileids(documents, ""pos""))
",[],0,[],/corpus/reader/ycoe.py_tagged_paras
3900,/home/amandapotts/git/nltk/nltk/corpus/reader/ycoe.py_parsed_sents,"def parsed_sents(self, documents=None):
return self._psd_reader.parsed_sents(self._getfileids(documents, ""psd""))
",[],0,[],/corpus/reader/ycoe.py_parsed_sents
3901,/home/amandapotts/git/nltk/nltk/corpus/reader/ycoe.py__parse,"def _parse(self, t):
t = re.sub(r""(?u)\((CODE|ID)[^\)]*\)"", """", t)
if re.match(r""\s*\(\s*\)\s*$"", t):
return None
return BracketParseCorpusReader._parse(self, t)
",[],0,[],/corpus/reader/ycoe.py__parse
3902,/home/amandapotts/git/nltk/nltk/corpus/reader/ycoe.py___init__,"def __init__(self, root, items, encoding=""utf8""):
gaps_re = r""(?u)(?<=/\.)\s+|\s*\S*_CODE\s*|\s*\S*_ID\s*""
sent_tokenizer = RegexpTokenizer(gaps_re, gaps=True)
TaggedCorpusReader.__init__(
self, root, items, sep=""_"", sent_tokenizer=sent_tokenizer
)
",[],0,[],/corpus/reader/ycoe.py___init__
3903,/home/amandapotts/git/nltk/nltk/corpus/reader/comparative_sents.py___init__,"def __init__(
self,
text=None,
comp_type=None,
entity_1=None,
entity_2=None,
feature=None,
keyword=None,
",[],0,[],/corpus/reader/comparative_sents.py___init__
3904,/home/amandapotts/git/nltk/nltk/corpus/reader/comparative_sents.py___repr__,"def __repr__(self):
return (
'Comparison(text=""{}"", comp_type={}, entity_1=""{}"", entity_2=""{}"", '
'feature=""{}"", keyword=""{}"")'
).format(
self.text,
self.comp_type,
self.entity_1,
self.entity_2,
self.feature,
self.keyword,
)
",[],0,[],/corpus/reader/comparative_sents.py___repr__
3905,/home/amandapotts/git/nltk/nltk/corpus/reader/comparative_sents.py___init__,"def __init__(
self,
root,
fileids,
word_tokenizer=WhitespaceTokenizer(),
sent_tokenizer=None,
encoding=""utf8"",
",[],0,[],/corpus/reader/comparative_sents.py___init__
3906,/home/amandapotts/git/nltk/nltk/corpus/reader/comparative_sents.py_comparisons,"def comparisons(self, fileids=None):
""""""
Return all comparisons in the corpus.
:param fileids: a list or regexp specifying the ids of the files whose
comparisons have to be returned.
:return: the given file(s) as a list of Comparison objects.
:rtype: list(Comparison)
""""""
if fileids is None:
fileids = self._fileids
elif isinstance(fileids, str):
fileids = [fileids]
return concat(
[
self.CorpusView(path, self._read_comparison_block, encoding=enc)
for (path, enc, fileid) in self.abspaths(fileids, True, True)
]
)
",[],0,[],/corpus/reader/comparative_sents.py_comparisons
3907,/home/amandapotts/git/nltk/nltk/corpus/reader/comparative_sents.py_keywords,"def keywords(self, fileids=None):
""""""
Return a set of all keywords used in the corpus.
:param fileids: a list or regexp specifying the ids of the files whose
keywords have to be returned.
:return: the set of keywords and comparative phrases used in the corpus.
:rtype: set(str)
""""""
all_keywords = concat(
[
self.CorpusView(path, self._read_keyword_block, encoding=enc)
for (path, enc, fileid) in self.abspaths(fileids, True, True)
]
)
keywords_set = {keyword.lower() for keyword in all_keywords if keyword}
return keywords_set
",[],0,[],/corpus/reader/comparative_sents.py_keywords
3908,/home/amandapotts/git/nltk/nltk/corpus/reader/comparative_sents.py_keywords_readme,"def keywords_readme(self):
""""""
Return the list of words and constituents considered as clues of a
comparison (from listOfkeywords.txt).
""""""
keywords = []
with self.open(""listOfkeywords.txt"") as fp:
raw_text = fp.read()
for line in raw_text.split(""\n""):
if not line or line.startswith(""//""):
continue
keywords.append(line.strip())
return keywords
",[],0,[],/corpus/reader/comparative_sents.py_keywords_readme
3909,/home/amandapotts/git/nltk/nltk/corpus/reader/comparative_sents.py_sents,"def sents(self, fileids=None):
""""""
Return all sentences in the corpus.
:param fileids: a list or regexp specifying the ids of the files whose
sentences have to be returned.
:return: all sentences of the corpus as lists of tokens (or as plain
strings, if no word tokenizer is specified).
:rtype: list(list(str)) or list(str)
""""""
return concat(
[
self.CorpusView(path, self._read_sent_block, encoding=enc)
for (path, enc, fileid) in self.abspaths(fileids, True, True)
]
)
",[],0,[],/corpus/reader/comparative_sents.py_sents
3910,/home/amandapotts/git/nltk/nltk/corpus/reader/comparative_sents.py_words,"def words(self, fileids=None):
""""""
Return all words and punctuation symbols in the corpus.
:param fileids: a list or regexp specifying the ids of the files whose
words have to be returned.
:return: the given file(s) as a list of words and punctuation symbols.
:rtype: list(str)
""""""
return concat(
[
self.CorpusView(path, self._read_word_block, encoding=enc)
for (path, enc, fileid) in self.abspaths(fileids, True, True)
]
)
",[],0,[],/corpus/reader/comparative_sents.py_words
3911,/home/amandapotts/git/nltk/nltk/corpus/reader/comparative_sents.py__read_comparison_block,"def _read_comparison_block(self, stream):
while True:
line = stream.readline()
if not line:
return []  # end of file.
comparison_tags = re.findall(COMPARISON, line)
if comparison_tags:
grad_comparisons = re.findall(GRAD_COMPARISON, line)
non_grad_comparisons = re.findall(NON_GRAD_COMPARISON, line)
comparison_text = stream.readline().strip()
if self._word_tokenizer:
comparison_text = self._word_tokenizer.tokenize(comparison_text)
stream.readline()
comparison_bundle = []
if grad_comparisons:
for comp in grad_comparisons:
comp_type = int(re.match(r""<cs-(\d)>"", comp).group(1))
comparison = Comparison(
text=comparison_text, comp_type=comp_type
)
line = stream.readline()
entities_feats = ENTITIES_FEATS.findall(line)
if entities_feats:
for code, entity_feat in entities_feats:
if code == ""1"":
comparison.entity_1 = entity_feat.strip()
elif code == ""2"":
comparison.entity_2 = entity_feat.strip()
elif code == ""3"":
comparison.feature = entity_feat.strip()
keyword = KEYWORD.findall(line)
if keyword:
comparison.keyword = keyword[0]
comparison_bundle.append(comparison)
if non_grad_comparisons:
for comp in non_grad_comparisons:
comp_type = int(re.match(r""<cs-(\d)>"", comp).group(1))
comparison = Comparison(
text=comparison_text, comp_type=comp_type
)
comparison_bundle.append(comparison)
return comparison_bundle
",[],0,[],/corpus/reader/comparative_sents.py__read_comparison_block
3912,/home/amandapotts/git/nltk/nltk/corpus/reader/comparative_sents.py__read_keyword_block,"def _read_keyword_block(self, stream):
keywords = []
for comparison in self._read_comparison_block(stream):
keywords.append(comparison.keyword)
return keywords
",[],0,[],/corpus/reader/comparative_sents.py__read_keyword_block
3913,/home/amandapotts/git/nltk/nltk/corpus/reader/comparative_sents.py__read_sent_block,"def _read_sent_block(self, stream):
while True:
line = stream.readline()
if re.match(STARS, line):
while True:
line = stream.readline()
if re.match(STARS, line):
break
continue
if (
not re.findall(COMPARISON, line)
and not ENTITIES_FEATS.findall(line)
and not re.findall(CLOSE_COMPARISON, line)
):
if self._sent_tokenizer:
return [
self._word_tokenizer.tokenize(sent)
for sent in self._sent_tokenizer.tokenize(line)
]
else:
return [self._word_tokenizer.tokenize(line)]
",[],0,[],/corpus/reader/comparative_sents.py__read_sent_block
3914,/home/amandapotts/git/nltk/nltk/corpus/reader/comparative_sents.py__read_word_block,"def _read_word_block(self, stream):
words = []
for sent in self._read_sent_block(stream):
words.extend(sent)
return words
",[],0,[],/corpus/reader/comparative_sents.py__read_word_block
3915,/home/amandapotts/git/nltk/nltk/corpus/reader/dependency.py___init__,"def __init__(
self,
root,
fileids,
encoding=""utf8"",
word_tokenizer=TabTokenizer(),
sent_tokenizer=RegexpTokenizer(""\n"", gaps=True),
para_block_reader=read_blankline_block,
",[],0,[],/corpus/reader/dependency.py___init__
3916,/home/amandapotts/git/nltk/nltk/corpus/reader/dependency.py_words,"def words(self, fileids=None):
return concat(
[
DependencyCorpusView(fileid, False, False, False, encoding=enc)
for fileid, enc in self.abspaths(fileids, include_encoding=True)
]
)
",[],0,[],/corpus/reader/dependency.py_words
3917,/home/amandapotts/git/nltk/nltk/corpus/reader/dependency.py_tagged_words,"def tagged_words(self, fileids=None):
return concat(
[
DependencyCorpusView(fileid, True, False, False, encoding=enc)
for fileid, enc in self.abspaths(fileids, include_encoding=True)
]
)
",[],0,[],/corpus/reader/dependency.py_tagged_words
3918,/home/amandapotts/git/nltk/nltk/corpus/reader/dependency.py_sents,"def sents(self, fileids=None):
return concat(
[
DependencyCorpusView(fileid, False, True, False, encoding=enc)
for fileid, enc in self.abspaths(fileids, include_encoding=True)
]
)
",[],0,[],/corpus/reader/dependency.py_sents
3919,/home/amandapotts/git/nltk/nltk/corpus/reader/dependency.py_tagged_sents,"def tagged_sents(self, fileids=None):
return concat(
[
DependencyCorpusView(fileid, True, True, False, encoding=enc)
for fileid, enc in self.abspaths(fileids, include_encoding=True)
]
)
",[],0,[],/corpus/reader/dependency.py_tagged_sents
3920,/home/amandapotts/git/nltk/nltk/corpus/reader/dependency.py_parsed_sents,"def parsed_sents(self, fileids=None):
sents = concat(
[
DependencyCorpusView(fileid, False, True, True, encoding=enc)
for fileid, enc in self.abspaths(fileids, include_encoding=True)
]
)
return [DependencyGraph(sent) for sent in sents]
",[],0,[],/corpus/reader/dependency.py_parsed_sents
3921,/home/amandapotts/git/nltk/nltk/corpus/reader/dependency.py___init__,"def __init__(
self,
corpus_file,
tagged,
group_by_sent,
dependencies,
chunk_types=None,
encoding=""utf8"",
",[],0,[],/corpus/reader/dependency.py___init__
3922,/home/amandapotts/git/nltk/nltk/corpus/reader/dependency.py_read_block,"def read_block(self, stream):
sent = read_blankline_block(stream)[0].strip()
if sent.startswith(self._DOCSTART):
sent = sent[len(self._DOCSTART) :].lstrip()
if not self._dependencies:
lines = [line.split(""\t"") for line in sent.split(""\n"")]
if len(lines[0]) == 3 or len(lines[0]) == 4:
sent = [(line[0], line[1]) for line in lines]
elif len(lines[0]) == 10:
sent = [(line[1], line[4]) for line in lines]
else:
raise ValueError(""Unexpected number of fields in dependency tree file"")
if not self._tagged:
sent = [word for (word, tag) in sent]
if self._group_by_sent:
return [sent]
else:
return list(sent)
",[],0,[],/corpus/reader/dependency.py_read_block
3923,/home/amandapotts/git/nltk/nltk/corpus/reader/chunked.py___init__,"def __init__(
self,
root,
fileids,
extension="""",
str2chunktree=tagstr2tree,
sent_tokenizer=RegexpTokenizer(""\n"", gaps=True),
para_block_reader=read_blankline_block,
encoding=""utf8"",
tagset=None,
",[],0,[],/corpus/reader/chunked.py___init__
3924,/home/amandapotts/git/nltk/nltk/corpus/reader/chunked.py_words,"def words(self, fileids=None):
""""""
:return: the given file(s) as a list of words
and punctuation symbols.
:rtype: list(str)
""""""
return concat(
[
ChunkedCorpusView(f, enc, 0, 0, 0, 0, *self._cv_args)
for (f, enc) in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/chunked.py_words
3925,/home/amandapotts/git/nltk/nltk/corpus/reader/chunked.py_sents,"def sents(self, fileids=None):
""""""
:return: the given file(s) as a list of
sentences or utterances, each encoded as a list of word
strings.
:rtype: list(list(str))
""""""
return concat(
[
ChunkedCorpusView(f, enc, 0, 1, 0, 0, *self._cv_args)
for (f, enc) in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/chunked.py_sents
3926,/home/amandapotts/git/nltk/nltk/corpus/reader/chunked.py_paras,"def paras(self, fileids=None):
""""""
:return: the given file(s) as a list of
paragraphs, each encoded as a list of sentences, which are
in turn encoded as lists of word strings.
:rtype: list(list(list(str)))
""""""
return concat(
[
ChunkedCorpusView(f, enc, 0, 1, 1, 0, *self._cv_args)
for (f, enc) in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/chunked.py_paras
3927,/home/amandapotts/git/nltk/nltk/corpus/reader/chunked.py_tagged_words,"def tagged_words(self, fileids=None, tagset=None):
""""""
:return: the given file(s) as a list of tagged
words and punctuation symbols, encoded as tuples
``(word,tag)``.
:rtype: list(tuple(str,str))
""""""
return concat(
[
ChunkedCorpusView(
f, enc, 1, 0, 0, 0, *self._cv_args, target_tagset=tagset
)
for (f, enc) in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/chunked.py_tagged_words
3928,/home/amandapotts/git/nltk/nltk/corpus/reader/chunked.py_tagged_sents,"def tagged_sents(self, fileids=None, tagset=None):
""""""
:return: the given file(s) as a list of
sentences, each encoded as a list of ``(word,tag)`` tuples.
:rtype: list(list(tuple(str,str)))
""""""
return concat(
[
ChunkedCorpusView(
f, enc, 1, 1, 0, 0, *self._cv_args, target_tagset=tagset
)
for (f, enc) in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/chunked.py_tagged_sents
3929,/home/amandapotts/git/nltk/nltk/corpus/reader/chunked.py_tagged_paras,"def tagged_paras(self, fileids=None, tagset=None):
""""""
:return: the given file(s) as a list of
paragraphs, each encoded as a list of sentences, which are
in turn encoded as lists of ``(word,tag)`` tuples.
:rtype: list(list(list(tuple(str,str))))
""""""
return concat(
[
ChunkedCorpusView(
f, enc, 1, 1, 1, 0, *self._cv_args, target_tagset=tagset
)
for (f, enc) in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/chunked.py_tagged_paras
3930,/home/amandapotts/git/nltk/nltk/corpus/reader/chunked.py_chunked_words,"def chunked_words(self, fileids=None, tagset=None):
""""""
:return: the given file(s) as a list of tagged
words and chunks.  Words are encoded as ``(word, tag)``
tuples (if the corpus has tags) or word strings (if the
corpus has no tags).  Chunks are encoded as depth-one
trees over ``(word,tag)`` tuples or word strings.
:rtype: list(tuple(str,str) and Tree)
""""""
return concat(
[
ChunkedCorpusView(
f, enc, 1, 0, 0, 1, *self._cv_args, target_tagset=tagset
)
for (f, enc) in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/chunked.py_chunked_words
3931,/home/amandapotts/git/nltk/nltk/corpus/reader/chunked.py_chunked_sents,"def chunked_sents(self, fileids=None, tagset=None):
""""""
:return: the given file(s) as a list of
sentences, each encoded as a shallow Tree.  The leaves
of these trees are encoded as ``(word, tag)`` tuples (if
the corpus has tags) or word strings (if the corpus has no
tags).
:rtype: list(Tree)
""""""
return concat(
[
ChunkedCorpusView(
f, enc, 1, 1, 0, 1, *self._cv_args, target_tagset=tagset
)
for (f, enc) in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/chunked.py_chunked_sents
3932,/home/amandapotts/git/nltk/nltk/corpus/reader/chunked.py_chunked_paras,"def chunked_paras(self, fileids=None, tagset=None):
""""""
:return: the given file(s) as a list of
paragraphs, each encoded as a list of sentences, which are
in turn encoded as a shallow Tree.  The leaves of these
trees are encoded as ``(word, tag)`` tuples (if the corpus
has tags) or word strings (if the corpus has no tags).
:rtype: list(list(Tree))
""""""
return concat(
[
ChunkedCorpusView(
f, enc, 1, 1, 1, 1, *self._cv_args, target_tagset=tagset
)
for (f, enc) in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/chunked.py_chunked_paras
3933,/home/amandapotts/git/nltk/nltk/corpus/reader/chunked.py__read_block,"def _read_block(self, stream):
return [tagstr2tree(t) for t in read_blankline_block(stream)]
",[],0,[],/corpus/reader/chunked.py__read_block
3934,/home/amandapotts/git/nltk/nltk/corpus/reader/chunked.py___init__,"def __init__(
self,
fileid,
encoding,
tagged,
group_by_sent,
group_by_para,
chunked,
str2chunktree,
sent_tokenizer,
para_block_reader,
source_tagset=None,
target_tagset=None,
",[],0,[],/corpus/reader/chunked.py___init__
3935,/home/amandapotts/git/nltk/nltk/corpus/reader/chunked.py_read_block,"def read_block(self, stream):
block = []
for para_str in self._para_block_reader(stream):
para = []
for sent_str in self._sent_tokenizer.tokenize(para_str):
sent = self._str2chunktree(
sent_str,
source_tagset=self._source_tagset,
target_tagset=self._target_tagset,
)
if not self._tagged:
sent = self._untag(sent)
if not self._chunked:
sent = sent.leaves()
if self._group_by_sent:
para.append(sent)
else:
para.extend(sent)
if self._group_by_para:
block.append(para)
else:
block.extend(para)
return block
",[],0,[],/corpus/reader/chunked.py_read_block
3936,/home/amandapotts/git/nltk/nltk/corpus/reader/chunked.py__untag,"def _untag(self, tree):
for i, child in enumerate(tree):
if isinstance(child, Tree):
self._untag(child)
elif isinstance(child, tuple):
tree[i] = child[0]
else:
raise ValueError(""expected child to be Tree or tuple"")
return tree
",[],0,[],/corpus/reader/chunked.py__untag
3937,/home/amandapotts/git/nltk/nltk/corpus/reader/opinion_lexicon.py___init__,"def __init__(self, *args, **kwargs):
StreamBackedCorpusView.__init__(self, *args, **kwargs)
self._open()
read_blankline_block(self._stream)
self._filepos = [self._stream.tell()]
",[],0,[],/corpus/reader/opinion_lexicon.py___init__
3938,/home/amandapotts/git/nltk/nltk/corpus/reader/opinion_lexicon.py_words,"def words(self, fileids=None):
""""""
Return all words in the opinion lexicon. Note that these words are not
sorted in alphabetical order.
:param fileids: a list or regexp specifying the ids of the files whose
words have to be returned.
:return: the given file(s) as a list of words and punctuation symbols.
:rtype: list(str)
""""""
if fileids is None:
fileids = self._fileids
elif isinstance(fileids, str):
fileids = [fileids]
return concat(
[
self.CorpusView(path, self._read_word_block, encoding=enc)
for (path, enc, fileid) in self.abspaths(fileids, True, True)
]
)
",[],0,[],/corpus/reader/opinion_lexicon.py_words
3939,/home/amandapotts/git/nltk/nltk/corpus/reader/opinion_lexicon.py_positive,"def positive(self):
""""""
Return all positive words in alphabetical order.
:return: a list of positive words.
:rtype: list(str)
""""""
return self.words(""positive-words.txt"")
",[],0,[],/corpus/reader/opinion_lexicon.py_positive
3940,/home/amandapotts/git/nltk/nltk/corpus/reader/opinion_lexicon.py_negative,"def negative(self):
""""""
Return all negative words in alphabetical order.
:return: a list of negative words.
:rtype: list(str)
""""""
return self.words(""negative-words.txt"")
",[],0,[],/corpus/reader/opinion_lexicon.py_negative
3941,/home/amandapotts/git/nltk/nltk/corpus/reader/opinion_lexicon.py__read_word_block,"def _read_word_block(self, stream):
words = []
for i in range(20):  # Read 20 lines at a time.
line = stream.readline()
if not line:
continue
words.append(line.strip())
return words
",[],0,[],/corpus/reader/opinion_lexicon.py__read_word_block
3942,/home/amandapotts/git/nltk/nltk/corpus/reader/verbnet.py___init__,"def __init__(self, root, fileids, wrap_etree=False):
XMLCorpusReader.__init__(self, root, fileids, wrap_etree)
self._lemma_to_class = defaultdict(list)
""""""A dictionary mapping from verb lemma strings to lists of
VerbNet class identifiers.""""""
self._wordnet_to_class = defaultdict(list)
""""""A dictionary mapping from wordnet identifier strings to
lists of VerbNet class identifiers.""""""
self._class_to_fileid = {}
""""""A dictionary mapping from class identifiers to
corresponding file identifiers.  The keys of this dictionary
provide a complete list of all classes and subclasses.""""""
self._shortid_to_longid = {}
self._quick_index()
",[],0,[],/corpus/reader/verbnet.py___init__
3943,/home/amandapotts/git/nltk/nltk/corpus/reader/verbnet.py_lemmas,"def lemmas(self, vnclass=None):
""""""
Return a list of all verb lemmas that appear in any class, or
in the ``classid`` if specified.
""""""
if vnclass is None:
return sorted(self._lemma_to_class.keys())
else:
if isinstance(vnclass, str):
vnclass = self.vnclass(vnclass)
return [member.get(""name"") for member in vnclass.findall(""MEMBERS/MEMBER"")]
",[],0,[],/corpus/reader/verbnet.py_lemmas
3944,/home/amandapotts/git/nltk/nltk/corpus/reader/verbnet.py_wordnetids,"def wordnetids(self, vnclass=None):
""""""
Return a list of all wordnet identifiers that appear in any
class, or in ``classid`` if specified.
""""""
if vnclass is None:
return sorted(self._wordnet_to_class.keys())
else:
if isinstance(vnclass, str):
vnclass = self.vnclass(vnclass)
return sum(
(
member.get(""wn"", """").split()
for member in vnclass.findall(""MEMBERS/MEMBER"")
),
[],
)
",[],0,[],/corpus/reader/verbnet.py_wordnetids
3945,/home/amandapotts/git/nltk/nltk/corpus/reader/verbnet.py_classids,"def classids(self, lemma=None, wordnetid=None, fileid=None, classid=None):
""""""
Return a list of the VerbNet class identifiers.  If a file
identifier is specified, then return only the VerbNet class
identifiers for classes (and subclasses) defined by that file.
If a lemma is specified, then return only VerbNet class
identifiers for classes that contain that lemma as a member.
If a wordnetid is specified, then return only identifiers for
classes that contain that wordnetid as a member.  If a classid
is specified, then return only identifiers for subclasses of
the specified VerbNet class.
If nothing is specified, return all classids within VerbNet
""""""
if fileid is not None:
return [c for (c, f) in self._class_to_fileid.items() if f == fileid]
elif lemma is not None:
return self._lemma_to_class[lemma]
elif wordnetid is not None:
return self._wordnet_to_class[wordnetid]
elif classid is not None:
xmltree = self.vnclass(classid)
return [
subclass.get(""ID"")
for subclass in xmltree.findall(""SUBCLASSES/VNSUBCLASS"")
]
else:
return sorted(self._class_to_fileid.keys())
",[],0,[],/corpus/reader/verbnet.py_classids
3946,/home/amandapotts/git/nltk/nltk/corpus/reader/verbnet.py_vnclass,"def vnclass(self, fileid_or_classid):
""""""Returns VerbNet class ElementTree
Return an ElementTree containing the xml for the specified
VerbNet class.
:param fileid_or_classid: An identifier specifying which class
should be returned.  Can be a file identifier (such as
``'put-9.1.xml'``), or a VerbNet class identifier (such as
``'put-9.1'``) or a short VerbNet class identifier (such as
``'9.1'``).
""""""
if fileid_or_classid in self._fileids:
return self.xml(fileid_or_classid)
classid = self.longid(fileid_or_classid)
if classid in self._class_to_fileid:
fileid = self._class_to_fileid[self.longid(classid)]
tree = self.xml(fileid)
if classid == tree.get(""ID""):
return tree
else:
for subclass in tree.findall("".//VNSUBCLASS""):
if classid == subclass.get(""ID""):
return subclass
else:
assert False  # we saw it during _index()!
else:
raise ValueError(f""Unknown identifier {fileid_or_classid}"")
",[],0,[],/corpus/reader/verbnet.py_vnclass
3947,/home/amandapotts/git/nltk/nltk/corpus/reader/verbnet.py_fileids,"def fileids(self, vnclass_ids=None):
""""""
Return a list of fileids that make up this corpus.  If
``vnclass_ids`` is specified, then return the fileids that make
up the specified VerbNet class(es).
""""""
if vnclass_ids is None:
return self._fileids
elif isinstance(vnclass_ids, str):
return [self._class_to_fileid[self.longid(vnclass_ids)]]
else:
return [
self._class_to_fileid[self.longid(vnclass_id)]
for vnclass_id in vnclass_ids
]
",[],0,[],/corpus/reader/verbnet.py_fileids
3948,/home/amandapotts/git/nltk/nltk/corpus/reader/verbnet.py_frames,"def frames(self, vnclass):
""""""Given a VerbNet class, this method returns VerbNet frames
The members returned are:
1) Example
2) Description
3) Syntax
4) Semantics
:param vnclass: A VerbNet class identifier
containing the xml contents of a VerbNet class.
:return: frames - a list of frame dictionaries
""""""
if isinstance(vnclass, str):
vnclass = self.vnclass(vnclass)
frames = []
vnframes = vnclass.findall(""FRAMES/FRAME"")
for vnframe in vnframes:
frames.append(
{
""example"": self._get_example_within_frame(vnframe),
""description"": self._get_description_within_frame(vnframe),
""syntax"": self._get_syntactic_list_within_frame(vnframe),
""semantics"": self._get_semantics_within_frame(vnframe),
}
)
return frames
",[],0,[],/corpus/reader/verbnet.py_frames
3949,/home/amandapotts/git/nltk/nltk/corpus/reader/verbnet.py_subclasses,"def subclasses(self, vnclass):
""""""Returns subclass ids, if any exist
Given a VerbNet class, this method returns subclass ids (if they exist)
in a list of strings.
:param vnclass: A VerbNet class identifier
containing the xml contents of a VerbNet class.
:return: list of subclasses
""""""
if isinstance(vnclass, str):
vnclass = self.vnclass(vnclass)
subclasses = [
subclass.get(""ID"") for subclass in vnclass.findall(""SUBCLASSES/VNSUBCLASS"")
]
return subclasses
",[],0,[],/corpus/reader/verbnet.py_subclasses
3950,/home/amandapotts/git/nltk/nltk/corpus/reader/verbnet.py_themroles,"def themroles(self, vnclass):
""""""Returns thematic roles participating in a VerbNet class
Members returned as part of roles are-
1) Type
2) Modifiers
:param vnclass: A VerbNet class identifier
containing the xml contents of a VerbNet class.
:return: themroles: A list of thematic roles in the VerbNet class
""""""
if isinstance(vnclass, str):
vnclass = self.vnclass(vnclass)
themroles = []
for trole in vnclass.findall(""THEMROLES/THEMROLE""):
themroles.append(
{
""type"": trole.get(""type""),
""modifiers"": [
{""value"": restr.get(""Value""), ""type"": restr.get(""type"")}
for restr in trole.findall(""SELRESTRS/SELRESTR"")
],
}
)
return themroles
",[],0,[],/corpus/reader/verbnet.py_themroles
3951,/home/amandapotts/git/nltk/nltk/corpus/reader/verbnet.py__index,"def _index(self):
""""""
Initialize the indexes ``_lemma_to_class``,
``_wordnet_to_class``, and ``_class_to_fileid`` by scanning
through the corpus fileids.  This is fast if ElementTree
uses the C implementation (<0.1 secs), but quite slow (>10 secs)
if only the python implementation is available.
""""""
for fileid in self._fileids:
self._index_helper(self.xml(fileid), fileid)
",[],0,[],/corpus/reader/verbnet.py__index
3952,/home/amandapotts/git/nltk/nltk/corpus/reader/verbnet.py__index_helper,"def _index_helper(self, xmltree, fileid):
""""""Helper for ``_index()``""""""
vnclass = xmltree.get(""ID"")
self._class_to_fileid[vnclass] = fileid
self._shortid_to_longid[self.shortid(vnclass)] = vnclass
for member in xmltree.findall(""MEMBERS/MEMBER""):
self._lemma_to_class[member.get(""name"")].append(vnclass)
for wn in member.get(""wn"", """").split():
self._wordnet_to_class[wn].append(vnclass)
for subclass in xmltree.findall(""SUBCLASSES/VNSUBCLASS""):
self._index_helper(subclass, fileid)
",[],0,[],/corpus/reader/verbnet.py__index_helper
3953,/home/amandapotts/git/nltk/nltk/corpus/reader/verbnet.py__quick_index,"def _quick_index(self):
""""""
Initialize the indexes ``_lemma_to_class``,
``_wordnet_to_class``, and ``_class_to_fileid`` by scanning
through the corpus fileids.  This doesn't do proper xml parsing,
but is good enough to find everything in the standard VerbNet
corpus -- and it runs about 30 times faster than xml parsing
(with the python ElementTree
if ElementTree uses the C implementation).
""""""
for fileid in self._fileids:
vnclass = fileid[:-4]  # strip the '.xml'
self._class_to_fileid[vnclass] = fileid
self._shortid_to_longid[self.shortid(vnclass)] = vnclass
with self.open(fileid) as fp:
for m in self._INDEX_RE.finditer(fp.read()):
groups = m.groups()
if groups[0] is not None:
self._lemma_to_class[groups[0]].append(vnclass)
for wn in groups[1].split():
self._wordnet_to_class[wn].append(vnclass)
elif groups[2] is not None:
self._class_to_fileid[groups[2]] = fileid
vnclass = groups[2]  # for <MEMBER> elts.
self._shortid_to_longid[self.shortid(vnclass)] = vnclass
else:
assert False, ""unexpected match condition""
",[],0,[],/corpus/reader/verbnet.py__quick_index
3954,/home/amandapotts/git/nltk/nltk/corpus/reader/verbnet.py_longid,"def longid(self, shortid):
""""""Returns longid of a VerbNet class
Given a short VerbNet class identifier (eg '37.10'), map it
to a long id (eg 'confess-37.10').  If ``shortid`` is already a
long id, then return it as-is""""""
if self._LONGID_RE.match(shortid):
return shortid  # it's already a longid.
elif not self._SHORTID_RE.match(shortid):
raise ValueError(""vnclass identifier %r not found"" % shortid)
try:
return self._shortid_to_longid[shortid]
except KeyError as e:
raise ValueError(""vnclass identifier %r not found"" % shortid) from e
",[],0,[],/corpus/reader/verbnet.py_longid
3955,/home/amandapotts/git/nltk/nltk/corpus/reader/verbnet.py_shortid,"def shortid(self, longid):
""""""Returns shortid of a VerbNet class
Given a long VerbNet class identifier (eg 'confess-37.10'),
map it to a short id (eg '37.10').  If ``longid`` is already a
short id, then return it as-is.""""""
if self._SHORTID_RE.match(longid):
return longid  # it's already a shortid.
m = self._LONGID_RE.match(longid)
if m:
return m.group(2)
else:
raise ValueError(""vnclass identifier %r not found"" % longid)
",[],0,[],/corpus/reader/verbnet.py_shortid
3956,/home/amandapotts/git/nltk/nltk/corpus/reader/verbnet.py__get_semantics_within_frame,"def _get_semantics_within_frame(self, vnframe):
""""""Returns semantics within a single frame
A utility function to retrieve semantics within a frame in VerbNet
Members of the semantics dictionary:
1) Predicate value
2) Arguments
:param vnframe: An ElementTree containing the xml contents of
a VerbNet frame.
:return: semantics: semantics dictionary
""""""
semantics_within_single_frame = []
for pred in vnframe.findall(""SEMANTICS/PRED""):
arguments = [
{""type"": arg.get(""type""), ""value"": arg.get(""value"")}
for arg in pred.findall(""ARGS/ARG"")
]
semantics_within_single_frame.append(
{
""predicate_value"": pred.get(""value""),
""arguments"": arguments,
""negated"": pred.get(""bool"") == ""!"",
}
)
return semantics_within_single_frame
",[],0,[],/corpus/reader/verbnet.py__get_semantics_within_frame
3957,/home/amandapotts/git/nltk/nltk/corpus/reader/verbnet.py__get_example_within_frame,"def _get_example_within_frame(self, vnframe):
""""""Returns example within a frame
A utility function to retrieve an example within a frame in VerbNet.
:param vnframe: An ElementTree containing the xml contents of
a VerbNet frame.
:return: example_text: The example sentence for this particular frame
""""""
example_element = vnframe.find(""EXAMPLES/EXAMPLE"")
if example_element is not None:
example_text = example_element.text
else:
example_text = """"
return example_text
",[],0,[],/corpus/reader/verbnet.py__get_example_within_frame
3958,/home/amandapotts/git/nltk/nltk/corpus/reader/verbnet.py__get_description_within_frame,"def _get_description_within_frame(self, vnframe):
""""""Returns member description within frame
A utility function to retrieve a description of participating members
within a frame in VerbNet.
:param vnframe: An ElementTree containing the xml contents of
a VerbNet frame.
:return: description: a description dictionary with members - primary and secondary
""""""
description_element = vnframe.find(""DESCRIPTION"")
return {
""primary"": description_element.attrib[""primary""],
""secondary"": description_element.get(""secondary"", """"),
}
",[],0,[],/corpus/reader/verbnet.py__get_description_within_frame
3959,/home/amandapotts/git/nltk/nltk/corpus/reader/verbnet.py__get_syntactic_list_within_frame,"def _get_syntactic_list_within_frame(self, vnframe):
""""""Returns semantics within a frame
A utility function to retrieve semantics within a frame in VerbNet.
Members of the syntactic dictionary:
1) POS Tag
2) Modifiers
:param vnframe: An ElementTree containing the xml contents of
a VerbNet frame.
:return: syntax_within_single_frame
""""""
syntax_within_single_frame = []
for elt in vnframe.find(""SYNTAX""):
pos_tag = elt.tag
modifiers = dict()
modifiers[""value""] = elt.get(""value"") if ""value"" in elt.attrib else """"
modifiers[""selrestrs""] = [
{""value"": restr.get(""Value""), ""type"": restr.get(""type"")}
for restr in elt.findall(""SELRESTRS/SELRESTR"")
]
modifiers[""synrestrs""] = [
{""value"": restr.get(""Value""), ""type"": restr.get(""type"")}
for restr in elt.findall(""SYNRESTRS/SYNRESTR"")
]
syntax_within_single_frame.append(
{""pos_tag"": pos_tag, ""modifiers"": modifiers}
)
return syntax_within_single_frame
",[],0,[],/corpus/reader/verbnet.py__get_syntactic_list_within_frame
3960,/home/amandapotts/git/nltk/nltk/corpus/reader/verbnet.py_pprint,"def pprint(self, vnclass):
""""""Returns pretty printed version of a VerbNet class
Return a string containing a pretty-printed representation of
the given VerbNet class.
:param vnclass: A VerbNet class identifier
containing the xml contents of a VerbNet class.
""""""
if isinstance(vnclass, str):
vnclass = self.vnclass(vnclass)
s = vnclass.get(""ID"") + ""\n""
s += self.pprint_subclasses(vnclass, indent=""  "") + ""\n""
s += self.pprint_members(vnclass, indent=""  "") + ""\n""
s += ""  Thematic roles:\n""
s += self.pprint_themroles(vnclass, indent=""    "") + ""\n""
s += ""  Frames:\n""
s += self.pprint_frames(vnclass, indent=""    "")
return s
",[],0,[],/corpus/reader/verbnet.py_pprint
3961,/home/amandapotts/git/nltk/nltk/corpus/reader/verbnet.py_pprint_subclasses,"def pprint_subclasses(self, vnclass, indent=""""):
""""""Returns pretty printed version of subclasses of VerbNet class
Return a string containing a pretty-printed representation of
the given VerbNet class's subclasses.
:param vnclass: A VerbNet class identifier
containing the xml contents of a VerbNet class.
""""""
if isinstance(vnclass, str):
vnclass = self.vnclass(vnclass)
subclasses = self.subclasses(vnclass)
if not subclasses:
subclasses = [""(none)""]
s = ""Subclasses: "" + "" "".join(subclasses)
return textwrap.fill(
s, 70, initial_indent=indent, subsequent_indent=indent + ""  ""
)
",[],0,[],/corpus/reader/verbnet.py_pprint_subclasses
3962,/home/amandapotts/git/nltk/nltk/corpus/reader/verbnet.py_pprint_members,"def pprint_members(self, vnclass, indent=""""):
""""""Returns pretty printed version of members in a VerbNet class
Return a string containing a pretty-printed representation of
the given VerbNet class's member verbs.
:param vnclass: A VerbNet class identifier
containing the xml contents of a VerbNet class.
""""""
if isinstance(vnclass, str):
vnclass = self.vnclass(vnclass)
members = self.lemmas(vnclass)
if not members:
members = [""(none)""]
s = ""Members: "" + "" "".join(members)
return textwrap.fill(
s, 70, initial_indent=indent, subsequent_indent=indent + ""  ""
)
",[],0,[],/corpus/reader/verbnet.py_pprint_members
3963,/home/amandapotts/git/nltk/nltk/corpus/reader/verbnet.py_pprint_themroles,"def pprint_themroles(self, vnclass, indent=""""):
""""""Returns pretty printed version of thematic roles in a VerbNet class
Return a string containing a pretty-printed representation of
the given VerbNet class's thematic roles.
:param vnclass: A VerbNet class identifier
containing the xml contents of a VerbNet class.
""""""
if isinstance(vnclass, str):
vnclass = self.vnclass(vnclass)
pieces = []
for themrole in self.themroles(vnclass):
piece = indent + ""* "" + themrole.get(""type"")
modifiers = [
modifier[""value""] + modifier[""type""]
for modifier in themrole[""modifiers""]
]
if modifiers:
piece += ""[{}]"".format("" "".join(modifiers))
pieces.append(piece)
return ""\n"".join(pieces)
",[],0,[],/corpus/reader/verbnet.py_pprint_themroles
3964,/home/amandapotts/git/nltk/nltk/corpus/reader/verbnet.py_pprint_frames,"def pprint_frames(self, vnclass, indent=""""):
""""""Returns pretty version of all frames in a VerbNet class
Return a string containing a pretty-printed representation of
the list of frames within the VerbNet class.
:param vnclass: A VerbNet class identifier
containing the xml contents of a VerbNet class.
""""""
if isinstance(vnclass, str):
vnclass = self.vnclass(vnclass)
pieces = []
for vnframe in self.frames(vnclass):
pieces.append(self._pprint_single_frame(vnframe, indent))
return ""\n"".join(pieces)
",[],0,[],/corpus/reader/verbnet.py_pprint_frames
3965,/home/amandapotts/git/nltk/nltk/corpus/reader/verbnet.py__pprint_single_frame,"def _pprint_single_frame(self, vnframe, indent=""""):
""""""Returns pretty printed version of a single frame in a VerbNet class
Returns a string containing a pretty-printed representation of
the given frame.
:param vnframe: An ElementTree containing the xml contents of
a VerbNet frame.
""""""
frame_string = self._pprint_description_within_frame(vnframe, indent) + ""\n""
frame_string += self._pprint_example_within_frame(vnframe, indent + "" "") + ""\n""
frame_string += (
self._pprint_syntax_within_frame(vnframe, indent + ""  Syntax: "") + ""\n""
)
frame_string += indent + ""  Semantics:\n""
frame_string += self._pprint_semantics_within_frame(vnframe, indent + ""    "")
return frame_string
",[],0,[],/corpus/reader/verbnet.py__pprint_single_frame
3966,/home/amandapotts/git/nltk/nltk/corpus/reader/verbnet.py__pprint_example_within_frame,"def _pprint_example_within_frame(self, vnframe, indent=""""):
""""""Returns pretty printed version of example within frame in a VerbNet class
Return a string containing a pretty-printed representation of
the given VerbNet frame example.
:param vnframe: An ElementTree containing the xml contents of
a Verbnet frame.
""""""
if vnframe[""example""]:
return indent + "" Example: "" + vnframe[""example""]
",[],0,[],/corpus/reader/verbnet.py__pprint_example_within_frame
3967,/home/amandapotts/git/nltk/nltk/corpus/reader/verbnet.py__pprint_description_within_frame,"def _pprint_description_within_frame(self, vnframe, indent=""""):
""""""Returns pretty printed version of a VerbNet frame description
Return a string containing a pretty-printed representation of
the given VerbNet frame description.
:param vnframe: An ElementTree containing the xml contents of
a VerbNet frame.
""""""
description = indent + vnframe[""description""][""primary""]
if vnframe[""description""][""secondary""]:
description += "" ({})"".format(vnframe[""description""][""secondary""])
return description
",[],0,[],/corpus/reader/verbnet.py__pprint_description_within_frame
3968,/home/amandapotts/git/nltk/nltk/corpus/reader/verbnet.py__pprint_syntax_within_frame,"def _pprint_syntax_within_frame(self, vnframe, indent=""""):
""""""Returns pretty printed version of syntax within a frame in a VerbNet class
Return a string containing a pretty-printed representation of
the given VerbNet frame syntax.
:param vnframe: An ElementTree containing the xml contents of
a VerbNet frame.
""""""
pieces = []
for element in vnframe[""syntax""]:
piece = element[""pos_tag""]
modifier_list = []
if ""value"" in element[""modifiers""] and element[""modifiers""][""value""]:
modifier_list.append(element[""modifiers""][""value""])
modifier_list += [
""{}{}"".format(restr[""value""], restr[""type""])
for restr in (
element[""modifiers""][""selrestrs""]
+ element[""modifiers""][""synrestrs""]
)
]
if modifier_list:
piece += ""[{}]"".format("" "".join(modifier_list))
pieces.append(piece)
return indent + "" "".join(pieces)
",[],0,[],/corpus/reader/verbnet.py__pprint_syntax_within_frame
3969,/home/amandapotts/git/nltk/nltk/corpus/reader/verbnet.py__pprint_semantics_within_frame,"def _pprint_semantics_within_frame(self, vnframe, indent=""""):
""""""Returns a pretty printed version of semantics within frame in a VerbNet class
Return a string containing a pretty-printed representation of
the given VerbNet frame semantics.
:param vnframe: An ElementTree containing the xml contents of
a VerbNet frame.
""""""
pieces = []
for predicate in vnframe[""semantics""]:
arguments = [argument[""value""] for argument in predicate[""arguments""]]
pieces.append(
f""{'¬' if predicate['negated'] else ''}{predicate['predicate_value']}({', '.join(arguments)})""
)
return ""\n"".join(f""{indent}* {piece}"" for piece in pieces)
",[],0,[],/corpus/reader/verbnet.py__pprint_semantics_within_frame
3970,/home/amandapotts/git/nltk/nltk/corpus/reader/cmudict.py_entries,"def entries(self):
""""""
:return: the cmudict lexicon as a list of entries
containing (word, transcriptions) tuples.
""""""
return concat(
[
StreamBackedCorpusView(fileid, read_cmudict_block, encoding=enc)
for fileid, enc in self.abspaths(None, True)
]
)
",[],0,[],/corpus/reader/cmudict.py_entries
3971,/home/amandapotts/git/nltk/nltk/corpus/reader/cmudict.py_words,"def words(self):
""""""
:return: a list of all words defined in the cmudict lexicon.
""""""
return [word.lower() for (word, _) in self.entries()]
",[],0,[],/corpus/reader/cmudict.py_words
3972,/home/amandapotts/git/nltk/nltk/corpus/reader/cmudict.py_dict,"def dict(self):
""""""
:return: the cmudict lexicon as a dictionary, whose keys are
lowercase words and whose values are lists of pronunciations.
""""""
return dict(Index(self.entries()))
",[],0,[],/corpus/reader/cmudict.py_dict
3973,/home/amandapotts/git/nltk/nltk/corpus/reader/cmudict.py_read_cmudict_block,"def read_cmudict_block(stream):
entries = []
while len(entries) < 100:  # Read 100 at a time.
line = stream.readline()
if line == """":
return entries  # end of file.
pieces = line.split()
entries.append((pieces[0].lower(), pieces[2:]))
return entries
",[],0,[],/corpus/reader/cmudict.py_read_cmudict_block
3974,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py_mimic_wrap,"def mimic_wrap(lines, wrap_at=65, **kwargs):
""""""
Wrap the first of 'lines' with textwrap and the remaining lines at exactly the same
positions as the first.
""""""
l0 = textwrap.fill(lines[0], wrap_at, drop_whitespace=False).split(""\n"")
yield l0
",[],0,[],/corpus/reader/framenet.py_mimic_wrap
3975,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__,"def _(line):
il0 = 0
while line and il0 < len(l0) - 1:
yield line[: len(l0[il0])]
line = line[len(l0[il0]) :]
il0 += 1
if line:  # Remaining stuff on this line past the end of the mimicked line.
yield from textwrap.fill(line, wrap_at, drop_whitespace=False).split(""\n"")
",[],0,[],/corpus/reader/framenet.py__
3976,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__pretty_longstring,"def _pretty_longstring(defstr, prefix="""", wrap_at=65):
""""""
Helper function for pretty-printing a long string.
:param defstr: The string to be printed.
:type defstr: str
:return: A nicely formatted string representation of the long string.
:rtype: str
""""""
return ""\n"".join(
[prefix + line for line in textwrap.fill(defstr, wrap_at).split(""\n"")]
)
",[],0,[],/corpus/reader/framenet.py__pretty_longstring
3977,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__pretty_any,"def _pretty_any(obj):
""""""
Helper function for pretty-printing any AttrDict object.
:param obj: The obj to be printed.
:type obj: AttrDict
:return: A nicely formatted string representation of the AttrDict object.
:rtype: str
""""""
outstr = """"
for k in obj:
if isinstance(obj[k], str) and len(obj[k]) > 65:
outstr += f""[{k}]\n""
outstr += ""{}"".format(_pretty_longstring(obj[k], prefix=""  ""))
outstr += ""\n""
else:
outstr += f""[{k}] {obj[k]}\n""
return outstr
",[],0,[],/corpus/reader/framenet.py__pretty_any
3978,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__pretty_semtype,"def _pretty_semtype(st):
""""""
Helper function for pretty-printing a semantic type.
:param st: The semantic type to be printed.
:type st: AttrDict
:return: A nicely formatted string representation of the semantic type.
:rtype: str
""""""
semkeys = st.keys()
if len(semkeys) == 1:
return ""<None>""
outstr = """"
outstr += ""semantic type ({0.ID}): {0.name}\n"".format(st)
if ""abbrev"" in semkeys:
outstr += f""[abbrev] {st.abbrev}\n""
if ""definition"" in semkeys:
outstr += ""[definition]\n""
outstr += _pretty_longstring(st.definition, ""  "")
outstr += f""[rootType] {st.rootType.name}({st.rootType.ID})\n""
if st.superType is None:
outstr += ""[superType] <None>\n""
else:
outstr += f""[superType] {st.superType.name}({st.superType.ID})\n""
outstr += f""[subTypes] {len(st.subTypes)} subtypes\n""
outstr += (
""  ""
+ "", "".join(f""{x.name}({x.ID})"" for x in st.subTypes)
+ ""\n"" * (len(st.subTypes) > 0)
)
return outstr
",[],0,[],/corpus/reader/framenet.py__pretty_semtype
3979,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__pretty_frame_relation_type,"def _pretty_frame_relation_type(freltyp):
""""""
Helper function for pretty-printing a frame relation type.
:param freltyp: The frame relation type to be printed.
:type freltyp: AttrDict
:return: A nicely formatted string representation of the frame relation type.
:rtype: str
""""""
outstr = ""<frame relation type ({0.ID}): {0.superFrameName} -- {0.name} -> {0.subFrameName}>"".format(
freltyp
)
return outstr
",[],0,[],/corpus/reader/framenet.py__pretty_frame_relation_type
3980,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__pretty_frame_relation,"def _pretty_frame_relation(frel):
""""""
Helper function for pretty-printing a frame relation.
:param frel: The frame relation to be printed.
:type frel: AttrDict
:return: A nicely formatted string representation of the frame relation.
:rtype: str
""""""
outstr = ""<{0.type.superFrameName}={0.superFrameName} -- {0.type.name} -> {0.type.subFrameName}={0.subFrameName}>"".format(
frel
)
return outstr
",[],0,[],/corpus/reader/framenet.py__pretty_frame_relation
3981,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__pretty_fe_relation,"def _pretty_fe_relation(ferel):
""""""
Helper function for pretty-printing an FE relation.
:param ferel: The FE relation to be printed.
:type ferel: AttrDict
:return: A nicely formatted string representation of the FE relation.
:rtype: str
""""""
outstr = ""<{0.type.superFrameName}={0.frameRelation.superFrameName}.{0.superFEName} -- {0.type.name} -> {0.type.subFrameName}={0.frameRelation.subFrameName}.{0.subFEName}>"".format(
ferel
)
return outstr
",[],0,[],/corpus/reader/framenet.py__pretty_fe_relation
3982,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__pretty_lu,"def _pretty_lu(lu):
""""""
Helper function for pretty-printing a lexical unit.
:param lu: The lu to be printed.
:type lu: AttrDict
:return: A nicely formatted string representation of the lexical unit.
:rtype: str
""""""
lukeys = lu.keys()
outstr = """"
outstr += ""lexical unit ({0.ID}): {0.name}\n\n"".format(lu)
if ""definition"" in lukeys:
outstr += ""[definition]\n""
outstr += _pretty_longstring(lu.definition, ""  "")
if ""frame"" in lukeys:
outstr += f""\n[frame] {lu.frame.name}({lu.frame.ID})\n""
if ""incorporatedFE"" in lukeys:
outstr += f""\n[incorporatedFE] {lu.incorporatedFE}\n""
if ""POS"" in lukeys:
outstr += f""\n[POS] {lu.POS}\n""
if ""status"" in lukeys:
outstr += f""\n[status] {lu.status}\n""
if ""totalAnnotated"" in lukeys:
outstr += f""\n[totalAnnotated] {lu.totalAnnotated} annotated examples\n""
if ""lexemes"" in lukeys:
outstr += ""\n[lexemes] {}\n"".format(
"" "".join(f""{lex.name}/{lex.POS}"" for lex in lu.lexemes)
)
if ""semTypes"" in lukeys:
outstr += f""\n[semTypes] {len(lu.semTypes)} semantic types\n""
outstr += (
""  "" * (len(lu.semTypes) > 0)
+ "", "".join(f""{x.name}({x.ID})"" for x in lu.semTypes)
+ ""\n"" * (len(lu.semTypes) > 0)
)
if ""URL"" in lukeys:
outstr += f""\n[URL] {lu.URL}\n""
if ""subCorpus"" in lukeys:
subc = [x.name for x in lu.subCorpus]
outstr += f""\n[subCorpus] {len(lu.subCorpus)} subcorpora\n""
for line in textwrap.fill("", "".join(sorted(subc)), 60).split(""\n""):
outstr += f""  {line}\n""
if ""exemplars"" in lukeys:
outstr += ""\n[exemplars] {} sentences across all subcorpora\n"".format(
len(lu.exemplars)
)
return outstr
",[],0,[],/corpus/reader/framenet.py__pretty_lu
3983,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__pretty_exemplars,"def _pretty_exemplars(exemplars, lu):
""""""
Helper function for pretty-printing a list of exemplar sentences for a lexical unit.
:param sent: The list of exemplar sentences to be printed.
:type sent: list(AttrDict)
:return: An index of the text of the exemplar sentences.
:rtype: str
""""""
outstr = """"
outstr += ""exemplar sentences for {0.name} in {0.frame.name}:\n\n"".format(lu)
for i, sent in enumerate(exemplars):
outstr += f""[{i}] {sent.text}\n""
outstr += ""\n""
return outstr
",[],0,[],/corpus/reader/framenet.py__pretty_exemplars
3984,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__pretty_fulltext_sentences,"def _pretty_fulltext_sentences(sents):
""""""
Helper function for pretty-printing a list of annotated sentences for a full-text document.
:param sent: The list of sentences to be printed.
:type sent: list(AttrDict)
:return: An index of the text of the sentences.
:rtype: str
""""""
outstr = """"
outstr += ""full-text document ({0.ID}) {0.name}:\n\n"".format(sents)
outstr += ""[corpid] {0.corpid}\n[corpname] {0.corpname}\n[description] {0.description}\n[URL] {0.URL}\n\n"".format(
sents
)
outstr += f""[sentence]\n""
for i, sent in enumerate(sents.sentence):
outstr += f""[{i}] {sent.text}\n""
outstr += ""\n""
return outstr
",[],0,[],/corpus/reader/framenet.py__pretty_fulltext_sentences
3985,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__pretty_fulltext_sentence,"def _pretty_fulltext_sentence(sent):
""""""
Helper function for pretty-printing an annotated sentence from a full-text document.
:param sent: The sentence to be printed.
:type sent: list(AttrDict)
:return: The text of the sentence with annotation set indices on frame targets.
:rtype: str
""""""
outstr = """"
outstr += ""full-text sentence ({0.ID}) in {1}:\n\n"".format(
sent, sent.doc.get(""name"", sent.doc.description)
)
outstr += f""\n[POS] {len(sent.POS)} tags\n""
outstr += f""\n[POS_tagset] {sent.POS_tagset}\n\n""
outstr += ""[text] + [annotationSet]\n\n""
outstr += sent._ascii()  # -> _annotation_ascii()
outstr += ""\n""
return outstr
",[],0,[],/corpus/reader/framenet.py__pretty_fulltext_sentence
3986,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__pretty_pos,"def _pretty_pos(aset):
""""""
Helper function for pretty-printing a sentence with its POS tags.
:param aset: The POS annotation set of the sentence to be printed.
:type sent: list(AttrDict)
:return: The text of the sentence and its POS tags.
:rtype: str
""""""
outstr = """"
outstr += ""POS annotation set ({0.ID}) {0.POS_tagset} in sentence {0.sent.ID}:\n\n"".format(
aset
)
overt = sorted(aset.POS)
sent = aset.sent
s0 = sent.text
s1 = """"
s2 = """"
i = 0
adjust = 0
for j, k, lbl in overt:
assert j >= i, (""Overlapping targets?"", (j, k, lbl))
s1 += "" "" * (j - i) + ""-"" * (k - j)
if len(lbl) > (k - j):
amt = len(lbl) - (k - j)
s0 = (
s0[: k + adjust] + ""~"" * amt + s0[k + adjust :]
)  # '~' to prevent line wrapping
s1 = s1[: k + adjust] + "" "" * amt + s1[k + adjust :]
adjust += amt
s2 += "" "" * (j - i) + lbl.ljust(k - j)
i = k
long_lines = [s0, s1, s2]
outstr += ""\n\n"".join(
map(""\n"".join, zip_longest(*mimic_wrap(long_lines), fillvalue="" ""))
).replace(""~"", "" "")
outstr += ""\n""
return outstr
",[],0,[],/corpus/reader/framenet.py__pretty_pos
3987,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__pretty_annotation,"def _pretty_annotation(sent, aset_level=False):
""""""
Helper function for pretty-printing an exemplar sentence for a lexical unit.
:param sent: An annotation set or exemplar sentence to be printed.
:param aset_level: If True, 'sent' is actually an annotation set within a sentence.
:type sent: AttrDict
:return: A nicely formatted string representation of the exemplar sentence
with its target, frame, and FE annotations.
:rtype: str
""""""
sentkeys = sent.keys()
outstr = ""annotation set"" if aset_level else ""exemplar sentence""
outstr += f"" ({sent.ID}):\n""
if aset_level:  # TODO: any UNANN exemplars?
outstr += f""\n[status] {sent.status}\n""
for k in (""corpID"", ""docID"", ""paragNo"", ""sentNo"", ""aPos""):
if k in sentkeys:
outstr += f""[{k}] {sent[k]}\n""
outstr += (
""\n[LU] ({0.ID}) {0.name} in {0.frame.name}\n"".format(sent.LU)
if sent.LU
else ""\n[LU] Not found!""
)
outstr += ""\n[frame] ({0.ID}) {0.name}\n"".format(
sent.frame
)  # redundant with above, but .frame is convenient
if not aset_level:
outstr += ""\n[annotationSet] {} annotation sets\n"".format(
len(sent.annotationSet)
)
outstr += f""\n[POS] {len(sent.POS)} tags\n""
outstr += f""\n[POS_tagset] {sent.POS_tagset}\n""
outstr += ""\n[GF] {} relation{}\n"".format(
len(sent.GF), ""s"" if len(sent.GF) != 1 else """"
)
outstr += ""\n[PT] {} phrase{}\n"".format(
len(sent.PT), ""s"" if len(sent.PT) != 1 else """"
)
""""""
Special Layers
--------------
The 'NER' layer contains, for some of the data, named entity labels.
The 'WSL' (word status layer) contains, for some of the data,
spans which should not in principle be considered targets (NT).
The 'Other' layer records relative clause constructions (Rel=relativizer, Ant=antecedent),
pleonastic 'it' (Null), and existential 'there' (Exist).
On occasion they are duplicated by accident (e.g., annotationSet 1467275 in lu6700.xml).
The 'Sent' layer appears to contain labels that the annotator has flagged the
sentence with for their convenience: values include
'sense1', 'sense2', 'sense3', etc.
'Blend', 'Canonical', 'Idiom', 'Metaphor', 'Special-Sent',
'keepS', 'deleteS', 'reexamine'
(sometimes they are duplicated for no apparent reason).
The POS-specific layers may contain the following kinds of spans:
Asp (aspectual particle), Non-Asp (non-aspectual particle),
Cop (copula), Supp (support), Ctrlr (controller),
Gov (governor), X. Gov and X always cooccur.
>>> from nltk.corpus import framenet as fn
>>> def f(luRE, lyr, ignore=set()):
...   for i,ex in enumerate(fn.exemplars(luRE)):
...     if lyr in ex and ex[lyr] and set(zip(*ex[lyr])[2]) - ignore:
...       print(i,ex[lyr])
- Verb: Asp, Non-Asp
- Noun: Cop, Supp, Ctrlr, Gov, X
- Adj: Cop, Supp, Ctrlr, Gov, X
- Prep: Cop, Supp, Ctrlr
- Adv: Ctrlr
- Scon: (none)
- Art: (none)
""""""
for lyr in (""NER"", ""WSL"", ""Other"", ""Sent""):
if lyr in sent and sent[lyr]:
outstr += ""\n[{}] {} entr{}\n"".format(
lyr, len(sent[lyr]), ""ies"" if len(sent[lyr]) != 1 else ""y""
)
outstr += ""\n[text] + [Target] + [FE]""
for lyr in (""Verb"", ""Noun"", ""Adj"", ""Adv"", ""Prep"", ""Scon"", ""Art""):
if lyr in sent and sent[lyr]:
outstr += f"" + [{lyr}]""
if ""FE2"" in sentkeys:
outstr += "" + [FE2]""
if ""FE3"" in sentkeys:
outstr += "" + [FE3]""
outstr += ""\n\n""
outstr += sent._ascii()  # -> _annotation_ascii()
outstr += ""\n""
return outstr
",[],0,[],/corpus/reader/framenet.py__pretty_annotation
3988,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__annotation_ascii,"def _annotation_ascii(sent):
""""""
Given a sentence or FE annotation set, construct the width-limited string showing
an ASCII visualization of the sentence's annotations, calling either
_annotation_ascii_frames() or _annotation_ascii_FEs() as appropriate.
This will be attached as a method to appropriate AttrDict instances
and called in the full pretty-printing of the instance.
""""""
if sent._type == ""fulltext_sentence"" or (
""annotationSet"" in sent and len(sent.annotationSet) > 2
):
return _annotation_ascii_frames(sent)
else:  # an FE annotation set, or an LU sentence with 1 target
return _annotation_ascii_FEs(sent)
",[],0,[],/corpus/reader/framenet.py__annotation_ascii
3989,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__annotation_ascii_frames,"def _annotation_ascii_frames(sent):
""""""
ASCII string rendering of the sentence along with its targets and frame names.
Called for all full-text sentences, as well as the few LU sentences with multiple
targets (e.g., fn.lu(6412).exemplars[82] has two want.v targets).
Line-wrapped to limit the display width.
""""""
overt = []
for a, aset in enumerate(sent.annotationSet[1:]):
for j, k in aset.Target:
indexS = f""[{a + 1}]""
if aset.status == ""UNANN"" or aset.LU.status == ""Problem"":
indexS += "" ""
if aset.status == ""UNANN"":
indexS += ""!""  # warning indicator that there is a frame annotation but no FE annotation
if aset.LU.status == ""Problem"":
indexS += ""?""  # warning indicator that there is a missing LU definition (because the LU has Problem status)
overt.append((j, k, aset.LU.frame.name, indexS))
overt = sorted(overt)
duplicates = set()
for o, (j, k, fname, asetIndex) in enumerate(overt):
if o > 0 and j <= overt[o - 1][1]:
if (
overt[o - 1][:2] == (j, k) and overt[o - 1][2] == fname
):  # same target, same frame
combinedIndex = (
overt[o - 1][3] + asetIndex
)  # e.g., '[1][2]', '[1]! [2]'
combinedIndex = combinedIndex.replace("" !"", ""! "").replace("" ?"", ""? "")
overt[o - 1] = overt[o - 1][:3] + (combinedIndex,)
duplicates.add(o)
else:  # different frames, same or overlapping targets
s = sent.text
for j, k, fname, asetIndex in overt:
s += ""\n"" + asetIndex + "" "" + sent.text[j:k] + "" :: "" + fname
s += ""\n(Unable to display sentence with targets marked inline due to overlap)""
return s
for o in reversed(sorted(duplicates)):
del overt[o]
s0 = sent.text
s1 = """"
s11 = """"
s2 = """"
i = 0
adjust = 0
fAbbrevs = OrderedDict()
for j, k, fname, asetIndex in overt:
if not j >= i:
assert j >= i, (
""Overlapping targets?""
+ (
"" UNANN""
if any(aset.status == ""UNANN"" for aset in sent.annotationSet[1:])
else """"
),
(j, k, asetIndex),
)
s1 += "" "" * (j - i) + ""*"" * (k - j)
short = fname[: k - j]
if (k - j) < len(fname):
r = 0
while short in fAbbrevs:
if fAbbrevs[short] == fname:
break
r += 1
short = fname[: k - j - 1] + str(r)
else:  # short not in fAbbrevs
fAbbrevs[short] = fname
s11 += "" "" * (j - i) + short.ljust(k - j)
if len(asetIndex) > (k - j):
amt = len(asetIndex) - (k - j)
s0 = (
s0[: k + adjust] + ""~"" * amt + s0[k + adjust :]
)  # '~' to prevent line wrapping
s1 = s1[: k + adjust] + "" "" * amt + s1[k + adjust :]
s11 = s11[: k + adjust] + "" "" * amt + s11[k + adjust :]
adjust += amt
s2 += "" "" * (j - i) + asetIndex.ljust(k - j)
i = k
long_lines = [s0, s1, s11, s2]
outstr = ""\n\n"".join(
map(""\n"".join, zip_longest(*mimic_wrap(long_lines), fillvalue="" ""))
).replace(""~"", "" "")
outstr += ""\n""
if fAbbrevs:
outstr += "" ("" + "", "".join(""="".join(pair) for pair in fAbbrevs.items()) + "")""
assert len(fAbbrevs) == len(dict(fAbbrevs)), ""Abbreviation clash""
return outstr
",[],0,[],/corpus/reader/framenet.py__annotation_ascii_frames
3990,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__annotation_ascii_FE_layer,"def _annotation_ascii_FE_layer(overt, ni, feAbbrevs):
""""""Helper for _annotation_ascii_FEs().""""""
s1 = """"
s2 = """"
i = 0
for j, k, fename in overt:
s1 += "" "" * (j - i) + (""^"" if fename.islower() else ""-"") * (k - j)
short = fename[: k - j]
if len(fename) > len(short):
r = 0
while short in feAbbrevs:
if feAbbrevs[short] == fename:
break
r += 1
short = fename[: k - j - 1] + str(r)
else:  # short not in feAbbrevs
feAbbrevs[short] = fename
s2 += "" "" * (j - i) + short.ljust(k - j)
i = k
sNI = """"
if ni:
sNI += "" ["" + "", "".join("":"".join(x) for x in sorted(ni.items())) + ""]""
return [s1, s2, sNI]
",[],0,[],/corpus/reader/framenet.py__annotation_ascii_FE_layer
3991,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__annotation_ascii_FEs,"def _annotation_ascii_FEs(sent):
""""""
ASCII string rendering of the sentence along with a single target and its FEs.
Secondary and tertiary FE layers are included if present.
'sent' can be an FE annotation set or an LU sentence with a single target.
Line-wrapped to limit the display width.
""""""
feAbbrevs = OrderedDict()
posspec = []  # POS-specific layer spans (e.g., Supp[ort], Cop[ula])
posspec_separate = False
for lyr in (""Verb"", ""Noun"", ""Adj"", ""Adv"", ""Prep"", ""Scon"", ""Art""):
if lyr in sent and sent[lyr]:
for a, b, lbl in sent[lyr]:
if (
lbl == ""X""
):  # skip this, which covers an entire phrase typically containing the target and all its FEs
continue
if any(1 for x, y, felbl in sent.FE[0] if x <= a < y or a <= x < b):
posspec_separate = (
True  # show POS-specific layers on a separate line
)
posspec.append(
(a, b, lbl.lower().replace(""-"", """"))
)  # lowercase Cop=>cop, Non-Asp=>nonasp, etc. to distinguish from FE names
if posspec_separate:
POSSPEC = _annotation_ascii_FE_layer(posspec, {}, feAbbrevs)
FE1 = _annotation_ascii_FE_layer(
sorted(sent.FE[0] + (posspec if not posspec_separate else [])),
sent.FE[1],
feAbbrevs,
)
FE2 = FE3 = None
if ""FE2"" in sent:
FE2 = _annotation_ascii_FE_layer(sent.FE2[0], sent.FE2[1], feAbbrevs)
if ""FE3"" in sent:
FE3 = _annotation_ascii_FE_layer(sent.FE3[0], sent.FE3[1], feAbbrevs)
for i, j in sent.Target:
FE1span, FE1name, FE1exp = FE1
if len(FE1span) < j:
FE1span += "" "" * (j - len(FE1span))
if len(FE1name) < j:
FE1name += "" "" * (j - len(FE1name))
FE1[1] = FE1name
FE1[0] = (
FE1span[:i] + FE1span[i:j].replace("" "", ""*"").replace(""-"", ""="") + FE1span[j:]
)
long_lines = [sent.text]
if posspec_separate:
long_lines.extend(POSSPEC[:2])
long_lines.extend([FE1[0], FE1[1] + FE1[2]])  # lines with no length limit
if FE2:
long_lines.extend([FE2[0], FE2[1] + FE2[2]])
if FE3:
long_lines.extend([FE3[0], FE3[1] + FE3[2]])
long_lines.append("""")
outstr = ""\n"".join(
map(""\n"".join, zip_longest(*mimic_wrap(long_lines), fillvalue="" ""))
)
if feAbbrevs:
outstr += ""("" + "", "".join(""="".join(pair) for pair in feAbbrevs.items()) + "")""
assert len(feAbbrevs) == len(dict(feAbbrevs)), ""Abbreviation clash""
outstr += ""\n""
return outstr
",[],0,[],/corpus/reader/framenet.py__annotation_ascii_FEs
3992,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__pretty_fe,"def _pretty_fe(fe):
""""""
Helper function for pretty-printing a frame element.
:param fe: The frame element to be printed.
:type fe: AttrDict
:return: A nicely formatted string representation of the frame element.
:rtype: str
""""""
fekeys = fe.keys()
outstr = """"
outstr += ""frame element ({0.ID}): {0.name}\n    of {1.name}({1.ID})\n"".format(
fe, fe.frame
)
if ""definition"" in fekeys:
outstr += ""[definition]\n""
outstr += _pretty_longstring(fe.definition, ""  "")
if ""abbrev"" in fekeys:
outstr += f""[abbrev] {fe.abbrev}\n""
if ""coreType"" in fekeys:
outstr += f""[coreType] {fe.coreType}\n""
if ""requiresFE"" in fekeys:
outstr += ""[requiresFE] ""
if fe.requiresFE is None:
outstr += ""<None>\n""
else:
outstr += f""{fe.requiresFE.name}({fe.requiresFE.ID})\n""
if ""excludesFE"" in fekeys:
outstr += ""[excludesFE] ""
if fe.excludesFE is None:
outstr += ""<None>\n""
else:
outstr += f""{fe.excludesFE.name}({fe.excludesFE.ID})\n""
if ""semType"" in fekeys:
outstr += ""[semType] ""
if fe.semType is None:
outstr += ""<None>\n""
else:
outstr += ""\n  "" + f""{fe.semType.name}({fe.semType.ID})"" + ""\n""
return outstr
",[],0,[],/corpus/reader/framenet.py__pretty_fe
3993,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py___init__,"def __init__(self, *args, **kwargs):
super().__init__(*args, **kwargs)
",[],0,[],/corpus/reader/framenet.py___init__
3994,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py___setattr__,"def __setattr__(self, name, value):
self[name] = value
",[],0,[],/corpus/reader/framenet.py___setattr__
3995,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py___getattr__,"def __getattr__(self, name):
if name == ""_short_repr"":
return self._short_repr
return self[name]
",[],0,[],/corpus/reader/framenet.py___getattr__
3996,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py___getitem__,"def __getitem__(self, name):
v = super().__getitem__(name)
if isinstance(v, Future):
return v._data()
return v
",[],0,[],/corpus/reader/framenet.py___getitem__
3997,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__short_repr,"def _short_repr(self):
if ""_type"" in self:
if self[""_type""].endswith(""relation""):
return self.__repr__()
try:
return ""<{} ID={} name={}>"".format(
self[""_type""], self[""ID""], self[""name""]
)
except KeyError:
try:  # no ID--e.g., for _type=lusubcorpus
return ""<{} name={}>"".format(self[""_type""], self[""name""])
except KeyError:  # no name--e.g., for _type=lusentence
return ""<{} ID={}>"".format(self[""_type""], self[""ID""])
else:
return self.__repr__()
",[],0,[],/corpus/reader/framenet.py__short_repr
3998,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__str,"def _str(self):
outstr = """"
if ""_type"" not in self:
outstr = _pretty_any(self)
elif self[""_type""] == ""frame"":
outstr = _pretty_frame(self)
elif self[""_type""] == ""fe"":
outstr = _pretty_fe(self)
elif self[""_type""] == ""lu"":
outstr = _pretty_lu(self)
elif self[""_type""] == ""luexemplars"":  # list of ALL exemplars for LU
outstr = _pretty_exemplars(self, self[0].LU)
elif (
self[""_type""] == ""fulltext_annotation""
):  # list of all sentences for full-text doc
outstr = _pretty_fulltext_sentences(self)
elif self[""_type""] == ""lusentence"":
outstr = _pretty_annotation(self)
elif self[""_type""] == ""fulltext_sentence"":
outstr = _pretty_fulltext_sentence(self)
elif self[""_type""] in (""luannotationset"", ""fulltext_annotationset""):
outstr = _pretty_annotation(self, aset_level=True)
elif self[""_type""] == ""posannotationset"":
outstr = _pretty_pos(self)
elif self[""_type""] == ""semtype"":
outstr = _pretty_semtype(self)
elif self[""_type""] == ""framerelationtype"":
outstr = _pretty_frame_relation_type(self)
elif self[""_type""] == ""framerelation"":
outstr = _pretty_frame_relation(self)
elif self[""_type""] == ""ferelation"":
outstr = _pretty_fe_relation(self)
else:
outstr = _pretty_any(self)
return outstr
",[],0,[],/corpus/reader/framenet.py__str
3999,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py___str__,"def __str__(self):
return self._str()
",[],0,[],/corpus/reader/framenet.py___str__
4000,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py___repr__,"def __repr__(self):
return self.__str__()
",[],0,[],/corpus/reader/framenet.py___repr__
4001,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py___init__,"def __init__(self, typ, *args, **kwargs):
super().__init__(*args, **kwargs)
self._type = typ
",[],0,[],/corpus/reader/framenet.py___init__
4002,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__str,"def _str(self):
outstr = """"
assert self._type
if len(self) == 0:
outstr = ""[]""
elif self._type == ""luexemplars"":  # list of ALL exemplars for LU
outstr = _pretty_exemplars(self, self[0].LU)
else:
assert False, self._type
return outstr
",[],0,[],/corpus/reader/framenet.py__str
4003,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py___str__,"def __str__(self):
return self._str()
",[],0,[],/corpus/reader/framenet.py___str__
4004,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py___repr__,"def __repr__(self):
return self.__str__()
",[],0,[],/corpus/reader/framenet.py___repr__
4005,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py___init__,"def __init__(self, loader, *args, **kwargs):
""""""
:param loader: when called with no arguments, returns the value to be stored
:type loader: callable
""""""
super().__init__(*args, **kwargs)
self._loader = loader
self._d = None
",[],0,[],/corpus/reader/framenet.py___init__
4006,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__data,"def _data(self):
if callable(self._loader):
self._d = self._loader()
self._loader = None  # the data is now cached
return self._d
",[],0,[],/corpus/reader/framenet.py__data
4007,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py___nonzero__,"def __nonzero__(self):
return bool(self._data())
",[],0,[],/corpus/reader/framenet.py___nonzero__
4008,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py___len__,"def __len__(self):
return len(self._data())
",[],0,[],/corpus/reader/framenet.py___len__
4009,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py___setitem__,"def __setitem__(self, key, value):
return self._data().__setitem__(key, value)
",[],0,[],/corpus/reader/framenet.py___setitem__
4010,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py___getitem__,"def __getitem__(self, key):
return self._data().__getitem__(key)
",[],0,[],/corpus/reader/framenet.py___getitem__
4011,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py___getattr__,"def __getattr__(self, key):
return self._data().__getattr__(key)
",[],0,[],/corpus/reader/framenet.py___getattr__
4012,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py___str__,"def __str__(self):
return self._data().__str__()
",[],0,[],/corpus/reader/framenet.py___str__
4013,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py___repr__,"def __repr__(self):
return self._data().__repr__()
",[],0,[],/corpus/reader/framenet.py___repr__
4014,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py___init__,"def __init__(self, *args, **kwargs):
_BREAK_LINES = kwargs.pop(""breakLines"", False)
super().__init__(*args, **kwargs)
dict.__setattr__(self, ""_BREAK_LINES"", _BREAK_LINES)
",[],0,[],/corpus/reader/framenet.py___init__
4015,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py___repr__,"def __repr__(self):
parts = []
for k, v in sorted(self.items()):
kv = repr(k) + "": ""
try:
kv += v._short_repr()
except AttributeError:
kv += repr(v)
parts.append(kv)
return ""{"" + ("",\n "" if self._BREAK_LINES else "", "").join(parts) + ""}""
",[],0,[],/corpus/reader/framenet.py___repr__
4016,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py___init__,"def __init__(self, *args, **kwargs):
self._MAX_REPR_SIZE = kwargs.pop(""maxReprSize"", 60)
self._BREAK_LINES = kwargs.pop(""breakLines"", False)
super().__init__(*args, **kwargs)
",[],0,[],/corpus/reader/framenet.py___init__
4017,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py___repr__,"def __repr__(self):
""""""
Return a string representation for this corpus view that is
similar to a list's representation
than 60 characters long, it is truncated.
""""""
pieces = []
length = 5
for elt in self:
pieces.append(
elt._short_repr()
)  # key difference from inherited version: call to _short_repr()
length += len(pieces[-1]) + 2
if self._MAX_REPR_SIZE and length > self._MAX_REPR_SIZE and len(pieces) > 2:
return ""[%s, ...]"" % str("",\n "" if self._BREAK_LINES else "", "").join(
pieces[:-1]
)
return ""[%s]"" % str("",\n "" if self._BREAK_LINES else "", "").join(pieces)
",[],0,[],/corpus/reader/framenet.py___repr__
4018,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py___repr__,"def __repr__(self):
""""""
Return a string representation for this corpus view that is
similar to a list's representation
than 60 characters long, it is truncated.
""""""
pieces = []
length = 5
for elt in self:
pieces.append(
elt._short_repr()
)  # key difference from inherited version: call to _short_repr()
length += len(pieces[-1]) + 2
if length > self._MAX_REPR_SIZE and len(pieces) > 2:
return ""[%s, ...]"" % "", "".join(pieces[:-1])
return ""[%s]"" % "", "".join(pieces)
",[],0,[],/corpus/reader/framenet.py___repr__
4019,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py___repr__,"def __repr__(self):
""""""
Return a string representation for this corpus view that is
similar to a list's representation
than 60 characters long, it is truncated.
""""""
pieces = []
length = 5
for elt in self:
pieces.append(
elt._short_repr()
)  # key difference from inherited version: call to _short_repr()
length += len(pieces[-1]) + 2
if length > self._MAX_REPR_SIZE and len(pieces) > 2:
return ""[%s, ...]"" % "", "".join(pieces[:-1])
return ""[%s]"" % "", "".join(pieces)
",[],0,[],/corpus/reader/framenet.py___repr__
4020,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py___repr__,"def __repr__(self):
""""""
Return a string representation for this corpus view that is
similar to a list's representation
than 60 characters long, it is truncated.
""""""
pieces = []
length = 5
for elt in self:
pieces.append(
elt._short_repr()
)  # key difference from inherited version: call to _short_repr()
length += len(pieces[-1]) + 2
if length > self._MAX_REPR_SIZE and len(pieces) > 2:
return ""[%s, ...]"" % "", "".join(pieces[:-1])
return ""[%s]"" % "", "".join(pieces)
",[],0,[],/corpus/reader/framenet.py___repr__
4021,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py___add__,"def __add__(self, other):
""""""Return a list concatenating self with other.""""""
return PrettyLazyIteratorList(itertools.chain(self, other))
",[],0,[],/corpus/reader/framenet.py___add__
4022,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py___radd__,"def __radd__(self, other):
""""""Return a list concatenating other with self.""""""
return PrettyLazyIteratorList(itertools.chain(other, self))
",[],0,[],/corpus/reader/framenet.py___radd__
4023,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py_warnings,"def warnings(self, v):
""""""Enable or disable warnings of data integrity issues as they are encountered.
If v is truthy, warnings will be enabled.
(This is a function rather than just an attribute/property to ensure that if
enabling warnings is the first action taken, the corpus reader is instantiated first.)
""""""
self._warnings = v
",[],0,[],/corpus/reader/framenet.py_warnings
4024,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py___init__,"def __init__(self, root, fileids):
XMLCorpusReader.__init__(self, root, fileids)
self._frame_dir = ""frame""
self._lu_dir = ""lu""
self._fulltext_dir = ""fulltext""
self._fnweb_url = ""https://framenet2.icsi.berkeley.edu/fnReports/data""
self._frame_idx = None
self._cached_frames = {}  # name -> ID
self._lu_idx = None
self._fulltext_idx = None
self._semtypes = None
self._freltyp_idx = None  # frame relation types (Inheritance, Using, etc.)
self._frel_idx = None  # frame-to-frame relation instances
self._ferel_idx = None  # FE-to-FE relation instances
self._frel_f_idx = None  # frame-to-frame relations associated with each frame
self._readme = ""README.txt""
",[],0,[],/corpus/reader/framenet.py___init__
4025,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py_help,"def help(self, attrname=None):
""""""Display help information summarizing the main methods.""""""
if attrname is not None:
return help(self.__getattribute__(attrname))
msg = """"""
",[],0,[],/corpus/reader/framenet.py_help
4026,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__buildframeindex,"def _buildframeindex(self):
if not self._frel_idx:
self._buildrelationindex()  # always load frame relations before frames,
self._frame_idx = {}
with XMLCorpusView(
self.abspath(""frameIndex.xml""), ""frameIndex/frame"", self._handle_elt
) as view:
for f in view:
self._frame_idx[f[""ID""]] = f
",[],0,[],/corpus/reader/framenet.py__buildframeindex
4027,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__buildcorpusindex,"def _buildcorpusindex(self):
self._fulltext_idx = {}
with XMLCorpusView(
self.abspath(""fulltextIndex.xml""),
""fulltextIndex/corpus"",
self._handle_fulltextindex_elt,
) as view:
for doclist in view:
for doc in doclist:
self._fulltext_idx[doc.ID] = doc
",[],0,[],/corpus/reader/framenet.py__buildcorpusindex
4028,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__buildluindex,"def _buildluindex(self):
self._lu_idx = {}
with XMLCorpusView(
self.abspath(""luIndex.xml""), ""luIndex/lu"", self._handle_elt
) as view:
for lu in view:
self._lu_idx[
lu[""ID""]
] = lu  # populate with LU index entries. if any of these
",[],0,[],/corpus/reader/framenet.py__buildluindex
4029,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__warn,"def _warn(self, *message, **kwargs):
if self._warnings:
kwargs.setdefault(""file"", sys.stderr)
print(*message, **kwargs)
",[],0,[],/corpus/reader/framenet.py__warn
4030,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py_buildindexes,"def buildindexes(self):
""""""
Build the internal indexes to make look-ups faster.
""""""
self._buildframeindex()
self._buildluindex()
self._buildcorpusindex()
self._buildrelationindex()
",[],0,[],/corpus/reader/framenet.py_buildindexes
4031,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py_doc,"def doc(self, fn_docid):
""""""
Returns the annotated document whose id number is
``fn_docid``. This id number can be obtained by calling the
Documents() function.
The dict that is returned from this function will contain the
following keys:
- '_type'      : 'fulltextannotation'
- 'sentence'   : a list of sentences in the document
- Each item in the list is a dict containing the following keys:
- 'ID'    : the ID number of the sentence
- '_type' : 'sentence'
- 'text'  : the text of the sentence
- 'paragNo' : the paragraph number
- 'sentNo'  : the sentence number
- 'docID'   : the document ID number
- 'corpID'  : the corpus ID number
- 'aPos'    : the annotation position
- 'annotationSet' : a list of annotation layers for the sentence
- Each item in the list is a dict containing the following keys:
- 'ID'       : the ID number of the annotation set
- '_type'    : 'annotationset'
- 'status'   : either 'MANUAL' or 'UNANN'
- 'luName'   : (only if status is 'MANUAL')
- 'luID'     : (only if status is 'MANUAL')
- 'frameID'  : (only if status is 'MANUAL')
- 'frameName': (only if status is 'MANUAL')
- 'layer' : a list of labels for the layer
- Each item in the layer is a dict containing the following keys:
- '_type': 'layer'
- 'rank'
- 'name'
- 'label' : a list of labels in the layer
- Each item is a dict containing the following keys:
- 'start'
- 'end'
- 'name'
- 'feID' (optional)
:param fn_docid: The Framenet id number of the document
:type fn_docid: int
:return: Information about the annotated document
:rtype: dict
""""""
try:
xmlfname = self._fulltext_idx[fn_docid].filename
except TypeError:  # happens when self._fulltext_idx == None
self._buildcorpusindex()
xmlfname = self._fulltext_idx[fn_docid].filename
except KeyError as e:  # probably means that fn_docid was not in the index
raise FramenetError(f""Unknown document id: {fn_docid}"") from e
locpath = os.path.join(f""{self._root}"", self._fulltext_dir, xmlfname)
with XMLCorpusView(locpath, ""fullTextAnnotation"") as view:
elt = view[0]
info = self._handle_fulltextannotation_elt(elt)
for k, v in self._fulltext_idx[fn_docid].items():
info[k] = v
return info
",[],0,[],/corpus/reader/framenet.py_doc
4032,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py_frame_by_id,"def frame_by_id(self, fn_fid, ignorekeys=[]):
""""""
Get the details for the specified Frame using the frame's id
number.
Usage examples:
>>> from nltk.corpus import framenet as fn
>>> f = fn.frame_by_id(256)
>>> f.ID
256
>>> f.name
'Medical_specialties'
>>> f.definition # doctest: +NORMALIZE_WHITESPACE
""This frame includes words that name medical specialties and is closely related to the
Medical_professionals frame.  The FE Type characterizing a sub-are in a Specialty may also be
expressed. 'Ralph practices paediatric oncology.'""
:param fn_fid: The Framenet id number of the frame
:type fn_fid: int
:param ignorekeys: The keys to ignore. These keys will not be
included in the output. (optional)
:type ignorekeys: list(str)
:return: Information about a frame
:rtype: dict
Also see the ``frame()`` function for details about what is
contained in the dict that is returned.
""""""
try:
fentry = self._frame_idx[fn_fid]
if ""_type"" in fentry:
return fentry  # full frame object is cached
name = fentry[""name""]
except TypeError:
self._buildframeindex()
name = self._frame_idx[fn_fid][""name""]
except KeyError as e:
raise FramenetError(f""Unknown frame id: {fn_fid}"") from e
return self.frame_by_name(name, ignorekeys, check_cache=False)
",[],0,[],/corpus/reader/framenet.py_frame_by_id
4033,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py_frame,"def frame(self, fn_fid_or_fname, ignorekeys=[]):
""""""
Get the details for the specified Frame using the frame's name
or id number.
Usage examples:
>>> from nltk.corpus import framenet as fn
>>> f = fn.frame(256)
>>> f.name
'Medical_specialties'
>>> f = fn.frame('Medical_specialties')
>>> f.ID
256
>>> # ensure non-ASCII character in definition doesn't trigger an encoding error:
>>> fn.frame('Imposing_obligation') # doctest: +ELLIPSIS
frame (1494): Imposing_obligation...
The dict that is returned from this function will contain the
following information about the Frame:
- 'name'       : the name of the Frame (e.g. 'Birth', 'Apply_heat', etc.)
- 'definition' : textual definition of the Frame
- 'ID'         : the internal ID number of the Frame
- 'semTypes'   : a list of semantic types for this frame
- Each item in the list is a dict containing the following keys:
- 'name' : can be used with the semtype() function
- 'ID'   : can be used with the semtype() function
- 'lexUnit'    : a dict containing all of the LUs for this frame.
The keys in this dict are the names of the LUs and
the value for each key is itself a dict containing
info about the LU (see the lu() function for more info.)
- 'FE' : a dict containing the Frame Elements that are part of this frame
The keys in this dict are the names of the FEs (e.g. 'Body_system')
and the values are dicts containing the following keys
- 'definition' : The definition of the FE
- 'name'       : The name of the FE e.g. 'Body_system'
- 'ID'         : The id number
- '_type'      : 'fe'
- 'abbrev'     : Abbreviation e.g. 'bod'
- 'coreType'   : one of ""Core"", ""Peripheral"", or ""Extra-Thematic""
- 'semType'    : if not None, a dict with the following two keys:
- 'name' : name of the semantic type. can be used with
the semtype() function
- 'ID'   : id number of the semantic type. can be used with
the semtype() function
- 'requiresFE' : if not None, a dict with the following two keys:
- 'name' : the name of another FE in this frame
- 'ID'   : the id of the other FE in this frame
- 'excludesFE' : if not None, a dict with the following two keys:
- 'name' : the name of another FE in this frame
- 'ID'   : the id of the other FE in this frame
- 'frameRelation'      : a list of objects describing frame relations
- 'FEcoreSets'  : a list of Frame Element core sets for this frame
- Each item in the list is a list of FE objects
:param fn_fid_or_fname: The Framenet name or id number of the frame
:type fn_fid_or_fname: int or str
:param ignorekeys: The keys to ignore. These keys will not be
included in the output. (optional)
:type ignorekeys: list(str)
:return: Information about a frame
:rtype: dict
""""""
if isinstance(fn_fid_or_fname, str):
f = self.frame_by_name(fn_fid_or_fname, ignorekeys)
else:
f = self.frame_by_id(fn_fid_or_fname, ignorekeys)
return f
",[],0,[],/corpus/reader/framenet.py_frame
4034,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py_frames_by_lemma,"def frames_by_lemma(self, pat):
""""""
Returns a list of all frames that contain LUs in which the
``name`` attribute of the LU matches the given regular expression
``pat``. Note that LU names are composed of ""lemma.POS"", where
the ""lemma"" part can be made up of either a single lexeme
(e.g. 'run') or multiple lexemes (e.g. 'a little').
Note: if you are going to be doing a lot of this type of
searching, you'd want to build an index that maps from lemmas to
frames because each time frames_by_lemma() is called, it has to
search through ALL of the frame XML files in the db.
>>> from nltk.corpus import framenet as fn
>>> from nltk.corpus.reader.framenet import PrettyList
>>> PrettyList(sorted(fn.frames_by_lemma(r'(?i)a little'), key=itemgetter('ID'))) # doctest: +ELLIPSIS
[<frame ID=189 name=Quanti...>, <frame ID=2001 name=Degree>]
:return: A list of frame objects.
:rtype: list(AttrDict)
""""""
return PrettyList(
f
for f in self.frames()
if any(re.search(pat, luName) for luName in f.lexUnit)
)
",[],0,[],/corpus/reader/framenet.py_frames_by_lemma
4035,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py_lu_basic,"def lu_basic(self, fn_luid):
""""""
Returns basic information about the LU whose id is
``fn_luid``. This is basically just a wrapper around the
``lu()`` function with ""subCorpus"" info excluded.
>>> from nltk.corpus import framenet as fn
>>> lu = PrettyDict(fn.lu_basic(256), breakLines=True)
>>> # ellipses account for differences between FN 1.5 and 1.7
>>> lu # doctest: +ELLIPSIS
{'ID': 256,
'POS': 'V',
'URL': 'https://framenet2.icsi.berkeley.edu/fnReports/data/lu/lu256.xml',
'_type': 'lu',
'cBy': ...,
'cDate': '02/08/2001 01:27:50 PST Thu',
'definition': 'COD: be aware of beforehand
'definitionMarkup': 'COD: be aware of beforehand
'frame': <frame ID=26 name=Expectation>,
'lemmaID': 15082,
'lexemes': [{'POS': 'V', 'breakBefore': 'false', 'headword': 'false', 'name': 'foresee', 'order': 1}],
'name': 'foresee.v',
'semTypes': [],
'sentenceCount': {'annotated': ..., 'total': ...},
'status': 'FN1_Sent'}
:param fn_luid: The id number of the desired LU
:type fn_luid: int
:return: Basic information about the lexical unit
:rtype: dict
""""""
return self.lu(fn_luid, ignorekeys=[""subCorpus"", ""exemplars""])
",[],0,[],/corpus/reader/framenet.py_lu_basic
4036,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py_lu,"def lu(self, fn_luid, ignorekeys=[], luName=None, frameID=None, frameName=None):
""""""
Access a lexical unit by its ID. luName, frameID, and frameName are used
only in the event that the LU does not have a file in the database
(which is the case for LUs with ""Problem"" status)
a placeholder LU is created which just contains its name, ID, and frame.
Usage examples:
>>> from nltk.corpus import framenet as fn
>>> fn.lu(256).name
'foresee.v'
>>> fn.lu(256).definition
'COD: be aware of beforehand
>>> fn.lu(256).frame.name
'Expectation'
>>> list(map(PrettyDict, fn.lu(256).lexemes))
[{'POS': 'V', 'breakBefore': 'false', 'headword': 'false', 'name': 'foresee', 'order': 1}]
>>> fn.lu(227).exemplars[23] # doctest: +NORMALIZE_WHITESPACE
exemplar sentence (352962):
[sentNo] 0
[aPos] 59699508
<BLANKLINE>
[LU] (227) guess.v in Coming_to_believe
<BLANKLINE>
[frame] (23) Coming_to_believe
<BLANKLINE>
[annotationSet] 2 annotation sets
<BLANKLINE>
[POS] 18 tags
<BLANKLINE>
[POS_tagset] BNC
<BLANKLINE>
[GF] 3 relations
<BLANKLINE>
[PT] 3 phrases
<BLANKLINE>
[Other] 1 entry
<BLANKLINE>
[text] + [Target] + [FE]
<BLANKLINE>
When he was inside the house , Culley noticed the characteristic
------------------
Content
<BLANKLINE>
he would n't have guessed at .
--                ******* --
Co                        C1 [Evidence:INI]
(Co=Cognizer, C1=Content)
<BLANKLINE>
<BLANKLINE>
The dict that is returned from this function will contain most of the
following information about the LU. Note that some LUs do not contain
all of these pieces of information - particularly 'totalAnnotated' and
'incorporatedFE' may be missing in some LUs:
- 'name'       : the name of the LU (e.g. 'merger.n')
- 'definition' : textual definition of the LU
- 'ID'         : the internal ID number of the LU
- '_type'      : 'lu'
- 'status'     : e.g. 'Created'
- 'frame'      : Frame that this LU belongs to
- 'POS'        : the part of speech of this LU (e.g. 'N')
- 'totalAnnotated' : total number of examples annotated with this LU
- 'incorporatedFE' : FE that incorporates this LU (e.g. 'Ailment')
- 'sentenceCount'  : a dict with the following two keys:
- 'annotated': number of sentences annotated with this LU
- 'total'    : total number of sentences with this LU
- 'lexemes'  : a list of dicts describing the lemma of this LU.
Each dict in the list contains these keys:
- 'POS'     : part of speech e.g. 'N'
- 'name'    : either single-lexeme e.g. 'merger' or
multi-lexeme e.g. 'a little'
- 'order': the order of the lexeme in the lemma (starting from 1)
- 'headword': a boolean ('true' or 'false')
- 'breakBefore': Can this lexeme be separated from the previous lexeme?
Consider: ""take over.v"" as in::
Germany took over the Netherlands in 2 days.
Germany took the Netherlands over in 2 days.
In this case, 'breakBefore' would be ""true"" for the lexeme
""over"". Contrast this with ""take after.v"" as in::
Mary takes after her grandmother.
In this case, 'breakBefore' would be ""false"" for the lexeme ""after""
- 'lemmaID'    : Can be used to connect lemmas in different LUs
- 'semTypes'   : a list of semantic type objects for this LU
- 'subCorpus'  : a list of subcorpora
- Each item in the list is a dict containing the following keys:
- 'name' :
- 'sentence' : a list of sentences in the subcorpus
- each item in the list is a dict with the following keys:
- 'ID':
- 'sentNo':
- 'text': the text of the sentence
- 'aPos':
- 'annotationSet': a list of annotation sets
- each item in the list is a dict with the following keys:
- 'ID':
- 'status':
- 'layer': a list of layers
- each layer is a dict containing the following keys:
- 'name': layer name (e.g. 'BNC')
- 'rank':
- 'label': a list of labels for the layer
- each label is a dict containing the following keys:
- 'start': start pos of label in sentence 'text' (0-based)
- 'end': end pos of label in sentence 'text' (0-based)
- 'name': name of label (e.g. 'NN1')
Under the hood, this implementation looks up the lexical unit information
in the *frame* definition file. That file does not contain
corpus annotations, so the LU files will be accessed on demand if those are
needed. In principle, valence patterns could be loaded here too,
though these are not currently supported.
:param fn_luid: The id number of the lexical unit
:type fn_luid: int
:param ignorekeys: The keys to ignore. These keys will not be
included in the output. (optional)
:type ignorekeys: list(str)
:return: All information about the lexical unit
:rtype: dict
""""""
if not self._lu_idx:
self._buildluindex()
OOV = object()
luinfo = self._lu_idx.get(fn_luid, OOV)
if luinfo is OOV:
self._warn(
""LU ID not found: {} ({}) in {} ({})"".format(
luName, fn_luid, frameName, frameID
)
)
luinfo = AttrDict(
{
""_type"": ""lu"",
""ID"": fn_luid,
""name"": luName,
""frameID"": frameID,
""status"": ""Problem"",
}
)
f = self.frame_by_id(luinfo.frameID)
assert f.name == frameName, (f.name, frameName)
luinfo[""frame""] = f
self._lu_idx[fn_luid] = luinfo
elif ""_type"" not in luinfo:
f = self.frame_by_id(luinfo.frameID)
luinfo = self._lu_idx[fn_luid]
if ignorekeys:
return AttrDict({k: v for k, v in luinfo.items() if k not in ignorekeys})
return luinfo
",[],0,[],/corpus/reader/framenet.py_lu
4037,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__lu_file,"def _lu_file(self, lu, ignorekeys=[]):
""""""
Augment the LU information that was loaded from the frame file
with additional information from the LU file.
""""""
fn_luid = lu.ID
fname = f""lu{fn_luid}.xml""
locpath = os.path.join(f""{self._root}"", self._lu_dir, fname)
if not self._lu_idx:
self._buildluindex()
try:
with XMLCorpusView(locpath, ""lexUnit"") as view:
elt = view[0]
except OSError as e:
raise FramenetError(f""Unknown LU id: {fn_luid}"") from e
lu2 = self._handle_lexunit_elt(elt, ignorekeys)
lu.URL = self._fnweb_url + ""/"" + self._lu_dir + ""/"" + fname
lu.subCorpus = lu2.subCorpus
lu.exemplars = SpecialList(
""luexemplars"", [sent for subc in lu.subCorpus for sent in subc.sentence]
)
for sent in lu.exemplars:
sent[""LU""] = lu
sent[""frame""] = lu.frame
for aset in sent.annotationSet:
aset[""LU""] = lu
aset[""frame""] = lu.frame
return lu
",[],0,[],/corpus/reader/framenet.py__lu_file
4038,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__loadsemtypes,"def _loadsemtypes(self):
""""""Create the semantic types index.""""""
self._semtypes = AttrDict()
with XMLCorpusView(
self.abspath(""semTypes.xml""),
""semTypes/semType"",
self._handle_semtype_elt,
) as view:
for st in view:
n = st[""name""]
a = st[""abbrev""]
i = st[""ID""]
self._semtypes[n] = i
self._semtypes[a] = i
self._semtypes[i] = st
roots = []
for st in self.semtypes():
if st.superType:
st.superType = self.semtype(st.superType.supID)
st.superType.subTypes.append(st)
else:
if st not in roots:
roots.append(st)
st.rootType = st
queue = list(roots)
assert queue
while queue:
st = queue.pop(0)
for child in st.subTypes:
child.rootType = st.rootType
queue.append(child)
",[],0,[],/corpus/reader/framenet.py__loadsemtypes
4039,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py_propagate_semtypes,"def propagate_semtypes(self):
""""""
Apply inference rules to distribute semtypes over relations between FEs.
For FrameNet 1.5, this results in 1011 semtypes being propagated.
(Not done by default because it requires loading all frame files,
which takes several seconds. If this needed to be fast, it could be rewritten
to traverse the neighboring relations on demand for each FE semtype.)
>>> from nltk.corpus import framenet as fn
>>> x = sum(1 for f in fn.frames() for fe in f.FE.values() if fe.semType)
>>> fn.propagate_semtypes()
>>> y = sum(1 for f in fn.frames() for fe in f.FE.values() if fe.semType)
>>> y-x > 1000
True
""""""
if not self._semtypes:
self._loadsemtypes()
if not self._ferel_idx:
self._buildrelationindex()
changed = True
i = 0
nPropagations = 0
while changed:
i += 1
changed = False
for ferel in self.fe_relations():
superST = ferel.superFE.semType
subST = ferel.subFE.semType
try:
if superST and superST is not subST:
assert subST is None or self.semtype_inherits(subST, superST), (
superST.name,
ferel,
subST.name,
)
if subST is None:
ferel.subFE.semType = subST = superST
changed = True
nPropagations += 1
if (
ferel.type.name in [""Perspective_on"", ""Subframe"", ""Precedes""]
and subST
and subST is not superST
):
assert superST is None, (superST.name, ferel, subST.name)
ferel.superFE.semType = superST = subST
changed = True
nPropagations += 1
except AssertionError as ex:
continue
",[],0,[],/corpus/reader/framenet.py_propagate_semtypes
4040,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py_semtype,"def semtype(self, key):
""""""
>>> from nltk.corpus import framenet as fn
>>> fn.semtype(233).name
'Temperature'
>>> fn.semtype(233).abbrev
'Temp'
>>> fn.semtype('Temperature').ID
233
:param key: The name, abbreviation, or id number of the semantic type
:type key: string or int
:return: Information about a semantic type
:rtype: dict
""""""
if isinstance(key, int):
stid = key
else:
try:
stid = self._semtypes[key]
except TypeError:
self._loadsemtypes()
stid = self._semtypes[key]
try:
st = self._semtypes[stid]
except TypeError:
self._loadsemtypes()
st = self._semtypes[stid]
return st
",[],0,[],/corpus/reader/framenet.py_semtype
4041,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py_semtype_inherits,"def semtype_inherits(self, st, superST):
if not isinstance(st, dict):
st = self.semtype(st)
if not isinstance(superST, dict):
superST = self.semtype(superST)
par = st.superType
while par:
if par is superST:
return True
par = par.superType
return False
",[],0,[],/corpus/reader/framenet.py_semtype_inherits
4042,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py_frames,"def frames(self, name=None):
""""""
Obtain details for a specific frame.
>>> from nltk.corpus import framenet as fn
>>> len(fn.frames()) in (1019, 1221)    # FN 1.5 and 1.7, resp.
True
>>> x = PrettyList(fn.frames(r'(?i)crim'), maxReprSize=0, breakLines=True)
>>> x.sort(key=itemgetter('ID'))
>>> x
[<frame ID=200 name=Criminal_process>,
<frame ID=500 name=Criminal_investigation>,
<frame ID=692 name=Crime_scenario>,
<frame ID=700 name=Committing_crime>]
A brief intro to Frames (excerpted from ""FrameNet II: Extended
Theory and Practice"" by Ruppenhofer et. al., 2010):
A Frame is a script-like conceptual structure that describes a
particular type of situation, object, or event along with the
participants and props that are needed for that Frame. For
example, the ""Apply_heat"" frame describes a common situation
involving a Cook, some Food, and a Heating_Instrument, and is
evoked by words such as bake, blanch, boil, broil, brown,
simmer, steam, etc.
We call the roles of a Frame ""frame elements"" (FEs) and the
frame-evoking words are called ""lexical units"" (LUs).
FrameNet includes relations between Frames. Several types of
relations are defined, of which the most important are:
- Inheritance: An IS-A relation. The child frame is a subtype
of the parent frame, and each FE in the parent is bound to
a corresponding FE in the child. An example is the
""Revenge"" frame which inherits from the
""Rewards_and_punishments"" frame.
- Using: The child frame presupposes the parent frame as
background, e.g the ""Speed"" frame ""uses"" (or presupposes)
the ""Motion"" frame
bound to child FEs.
- Subframe: The child frame is a subevent of a complex event
represented by the parent, e.g. the ""Criminal_process"" frame
has subframes of ""Arrest"", ""Arraignment"", ""Trial"", and
""Sentencing"".
- Perspective_on: The child frame provides a particular
perspective on an un-perspectivized parent frame. A pair of
examples consists of the ""Hiring"" and ""Get_a_job"" frames,
which perspectivize the ""Employment_start"" frame from the
Employer's and the Employee's point of view, respectively.
:param name: A regular expression pattern used to match against
Frame names. If 'name' is None, then a list of all
Framenet Frames will be returned.
:type name: str
:return: A list of matching Frames (or all Frames).
:rtype: list(AttrDict)
""""""
try:
fIDs = list(self._frame_idx.keys())
except AttributeError:
self._buildframeindex()
fIDs = list(self._frame_idx.keys())
if name is not None:
return PrettyList(
self.frame(fID) for fID, finfo in self.frame_ids_and_names(name).items()
)
else:
return PrettyLazyMap(self.frame, fIDs)
",[],0,[],/corpus/reader/framenet.py_frames
4043,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py_frame_ids_and_names,"def frame_ids_and_names(self, name=None):
""""""
Uses the frame index, which is much faster than looking up each frame definition
if only the names and IDs are needed.
""""""
if not self._frame_idx:
self._buildframeindex()
return {
fID: finfo.name
for fID, finfo in self._frame_idx.items()
if name is None or re.search(name, finfo.name) is not None
}
",[],0,[],/corpus/reader/framenet.py_frame_ids_and_names
4044,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py_fes,"def fes(self, name=None, frame=None):
""""""
Lists frame element objects. If 'name' is provided, this is treated as
a case-insensitive regular expression to filter by frame name.
(Case-insensitivity is because casing of frame element names is not always
consistent across frames.) Specify 'frame' to filter by a frame name pattern,
ID, or object.
>>> from nltk.corpus import framenet as fn
>>> fn.fes('Noise_maker')
[<fe ID=6043 name=Noise_maker>]
>>> sorted([(fe.frame.name,fe.name) for fe in fn.fes('sound')]) # doctest: +NORMALIZE_WHITESPACE
[('Cause_to_make_noise', 'Sound_maker'), ('Make_noise', 'Sound'),
('Make_noise', 'Sound_source'), ('Sound_movement', 'Location_of_sound_source'),
('Sound_movement', 'Sound'), ('Sound_movement', 'Sound_source'),
('Sounds', 'Component_sound'), ('Sounds', 'Location_of_sound_source'),
('Sounds', 'Sound_source'), ('Vocalizations', 'Location_of_sound_source'),
('Vocalizations', 'Sound_source')]
>>> sorted([(fe.frame.name,fe.name) for fe in fn.fes('sound',r'(?i)make_noise')]) # doctest: +NORMALIZE_WHITESPACE
[('Cause_to_make_noise', 'Sound_maker'),
('Make_noise', 'Sound'),
('Make_noise', 'Sound_source')]
>>> sorted(set(fe.name for fe in fn.fes('^sound')))
['Sound', 'Sound_maker', 'Sound_source']
>>> len(fn.fes('^sound$'))
2
:param name: A regular expression pattern used to match against
frame element names. If 'name' is None, then a list of all
frame elements will be returned.
:type name: str
:return: A list of matching frame elements
:rtype: list(AttrDict)
""""""
if frame is not None:
if isinstance(frame, int):
frames = [self.frame(frame)]
elif isinstance(frame, str):
frames = self.frames(frame)
else:
frames = [frame]
else:
frames = self.frames()
return PrettyList(
fe
for f in frames
for fename, fe in f.FE.items()
if name is None or re.search(name, fename, re.I)
)
",[],0,[],/corpus/reader/framenet.py_fes
4045,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py_lus,"def lus(self, name=None, frame=None):
""""""
Obtain details for lexical units.
Optionally restrict by lexical unit name pattern, and/or to a certain frame
or frames whose name matches a pattern.
>>> from nltk.corpus import framenet as fn
>>> len(fn.lus()) in (11829, 13572) # FN 1.5 and 1.7, resp.
True
>>> PrettyList(sorted(fn.lus(r'(?i)a little'), key=itemgetter('ID')), maxReprSize=0, breakLines=True)
[<lu ID=14733 name=a little.n>,
<lu ID=14743 name=a little.adv>,
<lu ID=14744 name=a little bit.adv>]
>>> PrettyList(sorted(fn.lus(r'interest', r'(?i)stimulus'), key=itemgetter('ID')))
[<lu ID=14894 name=interested.a>, <lu ID=14920 name=interesting.a>]
A brief intro to Lexical Units (excerpted from ""FrameNet II:
Extended Theory and Practice"" by Ruppenhofer et. al., 2010):
A lexical unit (LU) is a pairing of a word with a meaning. For
example, the ""Apply_heat"" Frame describes a common situation
involving a Cook, some Food, and a Heating Instrument, and is
_evoked_ by words such as bake, blanch, boil, broil, brown,
simmer, steam, etc. These frame-evoking words are the LUs in the
Apply_heat frame. Each sense of a polysemous word is a different
LU.
We have used the word ""word"" in talking about LUs. The reality
is actually rather complex. When we say that the word ""bake"" is
polysemous, we mean that the lemma ""bake.v"" (which has the
word-forms ""bake"", ""bakes"", ""baked"", and ""baking"") is linked to
three different frames:
- Apply_heat: ""Michelle baked the potatoes for 45 minutes.""
- Cooking_creation: ""Michelle baked her mother a cake for her birthday.""
- Absorb_heat: ""The potatoes have to bake for more than 30 minutes.""
These constitute three different LUs, with different
definitions.
Multiword expressions such as ""given name"" and hyphenated words
like ""shut-eye"" can also be LUs. Idiomatic phrases such as
""middle of nowhere"" and ""give the slip (to)"" are also defined as
LUs in the appropriate frames (""Isolated_places"" and ""Evading"",
respectively), and their internal structure is not analyzed.
Framenet provides multiple annotated examples of each sense of a
word (i.e. each LU).  Moreover, the set of examples
(approximately 20 per LU) illustrates all of the combinatorial
possibilities of the lexical unit.
Each LU is linked to a Frame, and hence to the other words which
evoke that Frame. This makes the FrameNet database similar to a
thesaurus, grouping together semantically similar words.
In the simplest case, frame-evoking words are verbs such as
""fried"" in:
""Matilde fried the catfish in a heavy iron skillet.""
Sometimes event nouns may evoke a Frame. For example,
""reduction"" evokes ""Cause_change_of_scalar_position"" in:
""...the reduction of debt levels to $665 million from $2.6 billion.""
Adjectives may also evoke a Frame. For example, ""asleep"" may
evoke the ""Sleep"" frame as in:
""They were asleep for hours.""
Many common nouns, such as artifacts like ""hat"" or ""tower"",
typically serve as dependents rather than clearly evoking their
own frames.
:param name: A regular expression pattern used to search the LU
names. Note that LU names take the form of a dotted
string (e.g. ""run.v"" or ""a little.adv"") in which a
lemma precedes the ""."" and a POS follows the
dot. The lemma may be composed of a single lexeme
(e.g. ""run"") or of multiple lexemes (e.g. ""a
little""). If 'name' is not given, then all LUs will
be returned.
The valid POSes are:
v    - verb
n    - noun
a    - adjective
adv  - adverb
prep - preposition
num  - numbers
intj - interjection
art  - article
c    - conjunction
scon - subordinating conjunction
:type name: str
:type frame: str or int or frame
:return: A list of selected (or all) lexical units
:rtype: list of LU objects (dicts). See the lu() function for info
about the specifics of LU objects.
""""""
if not self._lu_idx:
self._buildluindex()
if name is not None:  # match LUs, then restrict by frame
result = PrettyList(
self.lu(luID) for luID, luName in self.lu_ids_and_names(name).items()
)
if frame is not None:
if isinstance(frame, int):
frameIDs = {frame}
elif isinstance(frame, str):
frameIDs = {f.ID for f in self.frames(frame)}
else:
frameIDs = {frame.ID}
result = PrettyList(lu for lu in result if lu.frame.ID in frameIDs)
elif frame is not None:  # all LUs in matching frames
if isinstance(frame, int):
frames = [self.frame(frame)]
elif isinstance(frame, str):
frames = self.frames(frame)
else:
frames = [frame]
result = PrettyLazyIteratorList(
iter(LazyConcatenation(list(f.lexUnit.values()) for f in frames))
)
else:  # all LUs
luIDs = [
luID
for luID, lu in self._lu_idx.items()
if lu.status not in self._bad_statuses
]
result = PrettyLazyMap(self.lu, luIDs)
return result
",[],0,[],/corpus/reader/framenet.py_lus
4046,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py_lu_ids_and_names,"def lu_ids_and_names(self, name=None):
""""""
Uses the LU index, which is much faster than looking up each LU definition
if only the names and IDs are needed.
""""""
if not self._lu_idx:
self._buildluindex()
return {
luID: luinfo.name
for luID, luinfo in self._lu_idx.items()
if luinfo.status not in self._bad_statuses
and (name is None or re.search(name, luinfo.name) is not None)
}
",[],0,[],/corpus/reader/framenet.py_lu_ids_and_names
4047,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py_docs_metadata,"def docs_metadata(self, name=None):
""""""
Return an index of the annotated documents in Framenet.
Details for a specific annotated document can be obtained using this
class's doc() function and pass it the value of the 'ID' field.
>>> from nltk.corpus import framenet as fn
>>> len(fn.docs()) in (78, 107) # FN 1.5 and 1.7, resp.
True
>>> set([x.corpname for x in fn.docs_metadata()])>=set(['ANC', 'KBEval', \
'LUCorpus-v0.3', 'Miscellaneous', 'NTI', 'PropBank'])
True
:param name: A regular expression pattern used to search the
file name of each annotated document. The document's
file name contains the name of the corpus that the
document is from, followed by two underscores ""__""
followed by the document name. So, for example, the
file name ""LUCorpus-v0.3__20000410_nyt-NEW.xml"" is
from the corpus named ""LUCorpus-v0.3"" and the
document name is ""20000410_nyt-NEW.xml"".
:type name: str
:return: A list of selected (or all) annotated documents
:rtype: list of dicts, where each dict object contains the following
keys:
- 'name'
- 'ID'
- 'corpid'
- 'corpname'
- 'description'
- 'filename'
""""""
try:
ftlist = PrettyList(self._fulltext_idx.values())
except AttributeError:
self._buildcorpusindex()
ftlist = PrettyList(self._fulltext_idx.values())
if name is None:
return ftlist
else:
return PrettyList(
x for x in ftlist if re.search(name, x[""filename""]) is not None
)
",[],0,[],/corpus/reader/framenet.py_docs_metadata
4048,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py_sents,"def sents(self, exemplars=True, full_text=True):
""""""
Annotated sentences matching the specified criteria.
""""""
if exemplars:
if full_text:
return self.exemplars() + self.ft_sents()
else:
return self.exemplars()
elif full_text:
return self.ft_sents()
",[],0,[],/corpus/reader/framenet.py_sents
4049,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py_annotations,"def annotations(self, luNamePattern=None, exemplars=True, full_text=True):
""""""
Frame annotation sets matching the specified criteria.
""""""
if exemplars:
epart = PrettyLazyIteratorList(
sent.frameAnnotation for sent in self.exemplars(luNamePattern)
)
else:
epart = []
if full_text:
if luNamePattern is not None:
matchedLUIDs = set(self.lu_ids_and_names(luNamePattern).keys())
ftpart = PrettyLazyIteratorList(
aset
for sent in self.ft_sents()
for aset in sent.annotationSet[1:]
if luNamePattern is None or aset.get(""luID"", ""CXN_ASET"") in matchedLUIDs
)
else:
ftpart = []
if exemplars:
if full_text:
return epart + ftpart
else:
return epart
elif full_text:
return ftpart
",[],0,[],/corpus/reader/framenet.py_annotations
4050,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py_exemplars,"def exemplars(self, luNamePattern=None, frame=None, fe=None, fe2=None):
""""""
Lexicographic exemplar sentences, optionally filtered by LU name and/or 1-2 FEs that
are realized overtly. 'frame' may be a name pattern, frame ID, or frame instance.
'fe' may be a name pattern or FE instance
be specified to retrieve sentences with both overt FEs (in either order).
""""""
if fe is None and fe2 is not None:
raise FramenetError(""exemplars(..., fe=None, fe2=<value>) is not allowed"")
elif fe is not None and fe2 is not None:
if not isinstance(fe2, str):
if isinstance(fe, str):
fe, fe2 = fe2, fe
elif fe.frame is not fe2.frame:  # ensure frames match
raise FramenetError(
""exemplars() call with inconsistent `fe` and `fe2` specification (frames must match)""
)
if frame is None and fe is not None and not isinstance(fe, str):
frame = fe.frame
lusByFrame = defaultdict(
list
)  # frame name -> matching LUs, if luNamePattern is specified
if frame is not None or luNamePattern is not None:
if frame is None or isinstance(frame, str):
if luNamePattern is not None:
frames = set()
for lu in self.lus(luNamePattern, frame=frame):
frames.add(lu.frame.ID)
lusByFrame[lu.frame.name].append(lu)
frames = LazyMap(self.frame, list(frames))
else:
frames = self.frames(frame)
else:
if isinstance(frame, int):
frames = [self.frame(frame)]
else:  # frame object
frames = [frame]
if luNamePattern is not None:
lusByFrame = {frame.name: self.lus(luNamePattern, frame=frame)}
if fe is not None:  # narrow to frames that define this FE
if isinstance(fe, str):
frames = PrettyLazyIteratorList(
f
for f in frames
if fe in f.FE
or any(re.search(fe, ffe, re.I) for ffe in f.FE.keys())
)
else:
if fe.frame not in frames:
raise FramenetError(
""exemplars() call with inconsistent `frame` and `fe` specification""
)
frames = [fe.frame]
if fe2 is not None:  # narrow to frames that ALSO define this FE
if isinstance(fe2, str):
frames = PrettyLazyIteratorList(
f
for f in frames
if fe2 in f.FE
or any(re.search(fe2, ffe, re.I) for ffe in f.FE.keys())
)
else:  # frame, luNamePattern are None. fe, fe2 are None or strings
if fe is not None:
frames = {ffe.frame.ID for ffe in self.fes(fe)}
if fe2 is not None:
frames2 = {ffe.frame.ID for ffe in self.fes(fe2)}
frames = frames & frames2
frames = LazyMap(self.frame, list(frames))
else:
frames = self.frames()
",[],0,[],/corpus/reader/framenet.py_exemplars
4051,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__matching_exs,"def _matching_exs():
for f in frames:
fes = fes2 = None  # FEs of interest
if fe is not None:
fes = (
{ffe for ffe in f.FE.keys() if re.search(fe, ffe, re.I)}
if isinstance(fe, str)
else {fe.name}
)
if fe2 is not None:
fes2 = (
{ffe for ffe in f.FE.keys() if re.search(fe2, ffe, re.I)}
if isinstance(fe2, str)
else {fe2.name}
)
for lu in (
lusByFrame[f.name]
if luNamePattern is not None
else f.lexUnit.values()
):
for ex in lu.exemplars:
if (fes is None or self._exemplar_of_fes(ex, fes)) and (
fes2 is None or self._exemplar_of_fes(ex, fes2)
):
yield ex
",[],0,[],/corpus/reader/framenet.py__matching_exs
4052,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__exemplar_of_fes,"def _exemplar_of_fes(self, ex, fes=None):
""""""
Given an exemplar sentence and a set of FE names, return the subset of FE names
that are realized overtly in the sentence on the FE, FE2, or FE3 layer.
If 'fes' is None, returns all overt FE names.
""""""
overtNames = set(list(zip(*ex.FE[0]))[2]) if ex.FE[0] else set()
if ""FE2"" in ex:
overtNames |= set(list(zip(*ex.FE2[0]))[2]) if ex.FE2[0] else set()
if ""FE3"" in ex:
overtNames |= set(list(zip(*ex.FE3[0]))[2]) if ex.FE3[0] else set()
return overtNames & fes if fes is not None else overtNames
",[],0,[],/corpus/reader/framenet.py__exemplar_of_fes
4053,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py_ft_sents,"def ft_sents(self, docNamePattern=None):
""""""
Full-text annotation sentences, optionally filtered by document name.
""""""
return PrettyLazyIteratorList(
sent for d in self.docs(docNamePattern) for sent in d.sentence
)
",[],0,[],/corpus/reader/framenet.py_ft_sents
4054,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py_frame_relation_types,"def frame_relation_types(self):
""""""
Obtain a list of frame relation types.
>>> from nltk.corpus import framenet as fn
>>> frts = sorted(fn.frame_relation_types(), key=itemgetter('ID'))
>>> isinstance(frts, list)
True
>>> len(frts) in (9, 10)    # FN 1.5 and 1.7, resp.
True
>>> PrettyDict(frts[0], breakLines=True)
{'ID': 1,
'_type': 'framerelationtype',
'frameRelations': [<Parent=Event -- Inheritance -> Child=Change_of_consistency>, <Parent=Event -- Inheritance -> Child=Rotting>, ...],
'name': 'Inheritance',
'subFrameName': 'Child',
'superFrameName': 'Parent'}
:return: A list of all of the frame relation types in framenet
:rtype: list(dict)
""""""
if not self._freltyp_idx:
self._buildrelationindex()
return self._freltyp_idx.values()
",[],0,[],/corpus/reader/framenet.py_frame_relation_types
4055,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py_semtypes,"def semtypes(self):
""""""
Obtain a list of semantic types.
>>> from nltk.corpus import framenet as fn
>>> stypes = fn.semtypes()
>>> len(stypes) in (73, 109) # FN 1.5 and 1.7, resp.
True
>>> sorted(stypes[0].keys())
['ID', '_type', 'abbrev', 'definition', 'definitionMarkup', 'name', 'rootType', 'subTypes', 'superType']
:return: A list of all of the semantic types in framenet
:rtype: list(dict)
""""""
if not self._semtypes:
self._loadsemtypes()
return PrettyList(
self._semtypes[i] for i in self._semtypes if isinstance(i, int)
)
",[],0,[],/corpus/reader/framenet.py_semtypes
4056,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__load_xml_attributes,"def _load_xml_attributes(self, d, elt):
""""""
Extracts a subset of the attributes from the given element and
returns them in a dictionary.
:param d: A dictionary in which to store the attributes.
:type d: dict
:param elt: An ElementTree Element
:type elt: Element
:return: Returns the input dict ``d`` possibly including attributes from ``elt``
:rtype: dict
""""""
d = type(d)(d)
try:
attr_dict = elt.attrib
except AttributeError:
return d
if attr_dict is None:
return d
ignore_attrs = [  #'cBy', 'cDate', 'mDate', # <-- annotation metadata that could be of interest
""xsi"",
""schemaLocation"",
""xmlns"",
""bgColor"",
""fgColor"",
]
for attr in attr_dict:
if any(attr.endswith(x) for x in ignore_attrs):
continue
val = attr_dict[attr]
if val.isdigit():
d[attr] = int(val)
else:
d[attr] = val
return d
",[],0,[],/corpus/reader/framenet.py__load_xml_attributes
4057,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__strip_tags,"def _strip_tags(self, data):
""""""
Gets rid of all tags and newline characters from the given input
:return: A cleaned-up version of the input string
:rtype: str
""""""
try:
r""""""
m = re.search(r'\w[<][^/]|[<][/][^>]+[>](s\w|[a-rt-z0-9])', data)
if m:
print('Markup boundary:', data[max(0,m.start(0)-10):m.end(0)+10].replace('\n',' '), file=sys.stderr)
""""""
data = data.replace(""<t>"", """")
data = data.replace(""</t>"", """")
data = re.sub('<fex name=""[^""]+"">', """", data)
data = data.replace(""</fex>"", """")
data = data.replace(""<fen>"", """")
data = data.replace(""</fen>"", """")
data = data.replace(""<m>"", """")
data = data.replace(""</m>"", """")
data = data.replace(""<ment>"", """")
data = data.replace(""</ment>"", """")
data = data.replace(""<ex>"", ""'"")
data = data.replace(""</ex>"", ""'"")
data = data.replace(""<gov>"", """")
data = data.replace(""</gov>"", """")
data = data.replace(""<x>"", """")
data = data.replace(""</x>"", """")
data = data.replace(""<def-root>"", """")
data = data.replace(""</def-root>"", """")
data = data.replace(""\n"", "" "")
except AttributeError:
pass
return data
",[],0,[],/corpus/reader/framenet.py__strip_tags
4058,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__handle_elt,"def _handle_elt(self, elt, tagspec=None):
""""""Extracts and returns the attributes of the given element""""""
return self._load_xml_attributes(AttrDict(), elt)
",[],0,[],/corpus/reader/framenet.py__handle_elt
4059,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__handle_fulltextindex_elt,"def _handle_fulltextindex_elt(self, elt, tagspec=None):
""""""
Extracts corpus/document info from the fulltextIndex.xml file.
Note that this function ""flattens"" the information contained
in each of the ""corpus"" elements, so that each ""document""
element will contain attributes for the corpus and
corpusid. Also, each of the ""document"" items will contain a
new attribute called ""filename"" that is the base file name of
the xml file for the document in the ""fulltext"" subdir of the
Framenet corpus.
""""""
ftinfo = self._load_xml_attributes(AttrDict(), elt)
corpname = ftinfo.name
corpid = ftinfo.ID
retlist = []
for sub in elt:
if sub.tag.endswith(""document""):
doc = self._load_xml_attributes(AttrDict(), sub)
if ""name"" in doc:
docname = doc.name
else:
docname = doc.description
doc.filename = f""{corpname}__{docname}.xml""
doc.URL = (
self._fnweb_url + ""/"" + self._fulltext_dir + ""/"" + doc.filename
)
doc.corpname = corpname
doc.corpid = corpid
retlist.append(doc)
return retlist
",[],0,[],/corpus/reader/framenet.py__handle_fulltextindex_elt
4060,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__handle_fecoreset_elt,"def _handle_fecoreset_elt(self, elt):
""""""Load fe coreset info from xml.""""""
info = self._load_xml_attributes(AttrDict(), elt)
tmp = []
for sub in elt:
tmp.append(self._load_xml_attributes(AttrDict(), sub))
return tmp
",[],0,[],/corpus/reader/framenet.py__handle_fecoreset_elt
4061,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__handle_framerelationtype_elt,"def _handle_framerelationtype_elt(self, elt, *args):
""""""Load frame-relation element and its child fe-relation elements from frRelation.xml.""""""
info = self._load_xml_attributes(AttrDict(), elt)
info[""_type""] = ""framerelationtype""
info[""frameRelations""] = PrettyList()
for sub in elt:
if sub.tag.endswith(""frameRelation""):
frel = self._handle_framerelation_elt(sub)
frel[""type""] = info  # backpointer
for ferel in frel.feRelations:
ferel[""type""] = info
info[""frameRelations""].append(frel)
return info
",[],0,[],/corpus/reader/framenet.py__handle_framerelationtype_elt
4062,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__handle_framerelation_elt,"def _handle_framerelation_elt(self, elt):
""""""Load frame-relation element and its child fe-relation elements from frRelation.xml.""""""
info = self._load_xml_attributes(AttrDict(), elt)
assert info[""superFrameName""] != info[""subFrameName""], (elt, info)
info[""_type""] = ""framerelation""
info[""feRelations""] = PrettyList()
for sub in elt:
if sub.tag.endswith(""FERelation""):
ferel = self._handle_elt(sub)
ferel[""_type""] = ""ferelation""
ferel[""frameRelation""] = info  # backpointer
info[""feRelations""].append(ferel)
return info
",[],0,[],/corpus/reader/framenet.py__handle_framerelation_elt
4063,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__handle_fulltextannotation_elt,"def _handle_fulltextannotation_elt(self, elt):
""""""Load full annotation info for a document from its xml
file. The main element (fullTextAnnotation) contains a 'header'
element (which we ignore here) and a bunch of 'sentence'
elements.""""""
info = AttrDict()
info[""_type""] = ""fulltext_annotation""
info[""sentence""] = []
for sub in elt:
if sub.tag.endswith(""header""):
continue  # not used
elif sub.tag.endswith(""sentence""):
s = self._handle_fulltext_sentence_elt(sub)
s.doc = info
info[""sentence""].append(s)
return info
",[],0,[],/corpus/reader/framenet.py__handle_fulltextannotation_elt
4064,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__handle_fulltext_sentence_elt,"def _handle_fulltext_sentence_elt(self, elt):
""""""Load information from the given 'sentence' element. Each
'sentence' element contains a ""text"" and ""annotationSet"" sub
elements.""""""
info = self._load_xml_attributes(AttrDict(), elt)
info[""_type""] = ""fulltext_sentence""
info[""annotationSet""] = []
info[""targets""] = []
target_spans = set()
info[""_ascii""] = types.MethodType(
_annotation_ascii, info
)  # attach a method for this instance
info[""text""] = """"
for sub in elt:
if sub.tag.endswith(""text""):
info[""text""] = self._strip_tags(sub.text)
elif sub.tag.endswith(""annotationSet""):
a = self._handle_fulltextannotationset_elt(
sub, is_pos=(len(info[""annotationSet""]) == 0)
)
if ""cxnID"" in a:  # ignoring construction annotations for now
continue
a.sent = info
a.text = info.text
info[""annotationSet""].append(a)
if ""Target"" in a:
for tspan in a.Target:
if tspan in target_spans:
self._warn(
'Duplicate target span ""{}""'.format(
info.text[slice(*tspan)]
),
tspan,
""in sentence"",
info[""ID""],
info.text,
)
else:
target_spans.add(tspan)
info[""targets""].append((a.Target, a.luName, a.frameName))
assert info[""annotationSet""][0].status == ""UNANN""
info[""POS""] = info[""annotationSet""][0].POS
info[""POS_tagset""] = info[""annotationSet""][0].POS_tagset
return info
",[],0,[],/corpus/reader/framenet.py__handle_fulltext_sentence_elt
4065,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__handle_fulltextannotationset_elt,"def _handle_fulltextannotationset_elt(self, elt, is_pos=False):
""""""Load information from the given 'annotationSet' element. Each
'annotationSet' contains several ""layer"" elements.""""""
info = self._handle_luannotationset_elt(elt, is_pos=is_pos)
if not is_pos:
info[""_type""] = ""fulltext_annotationset""
if ""cxnID"" not in info:  # ignoring construction annotations for now
info[""LU""] = self.lu(
info.luID,
luName=info.luName,
frameID=info.frameID,
frameName=info.frameName,
)
info[""frame""] = info.LU.frame
return info
",[],0,[],/corpus/reader/framenet.py__handle_fulltextannotationset_elt
4066,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__handle_fulltextlayer_elt,"def _handle_fulltextlayer_elt(self, elt):
""""""Load information from the given 'layer' element. Each
'layer' contains several ""label"" elements.""""""
info = self._load_xml_attributes(AttrDict(), elt)
info[""_type""] = ""layer""
info[""label""] = []
for sub in elt:
if sub.tag.endswith(""label""):
l = self._load_xml_attributes(AttrDict(), sub)
info[""label""].append(l)
return info
",[],0,[],/corpus/reader/framenet.py__handle_fulltextlayer_elt
4067,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__handle_lexunit_elt,"def _handle_lexunit_elt(self, elt, ignorekeys):
""""""
Load full info for a lexical unit from its xml file.
This should only be called when accessing corpus annotations
(which are not included in frame files).
""""""
luinfo = self._load_xml_attributes(AttrDict(), elt)
luinfo[""_type""] = ""lu""
luinfo[""definition""] = """"
luinfo[""definitionMarkup""] = """"
luinfo[""subCorpus""] = PrettyList()
luinfo[""lexemes""] = PrettyList()  # multiword LUs have multiple lexemes
luinfo[""semTypes""] = PrettyList()  # an LU can have multiple semtypes
for k in ignorekeys:
if k in luinfo:
del luinfo[k]
for sub in elt:
if sub.tag.endswith(""header""):
continue  # not used
elif sub.tag.endswith(""valences""):
continue  # not used
elif sub.tag.endswith(""definition"") and ""definition"" not in ignorekeys:
luinfo[""definitionMarkup""] = sub.text
luinfo[""definition""] = self._strip_tags(sub.text)
elif sub.tag.endswith(""subCorpus"") and ""subCorpus"" not in ignorekeys:
sc = self._handle_lusubcorpus_elt(sub)
if sc is not None:
luinfo[""subCorpus""].append(sc)
elif sub.tag.endswith(""lexeme"") and ""lexeme"" not in ignorekeys:
luinfo[""lexemes""].append(self._load_xml_attributes(PrettyDict(), sub))
elif sub.tag.endswith(""semType"") and ""semType"" not in ignorekeys:
semtypeinfo = self._load_xml_attributes(AttrDict(), sub)
luinfo[""semTypes""].append(self.semtype(semtypeinfo.ID))
return luinfo
",[],0,[],/corpus/reader/framenet.py__handle_lexunit_elt
4068,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__handle_lusubcorpus_elt,"def _handle_lusubcorpus_elt(self, elt):
""""""Load a subcorpus of a lexical unit from the given xml.""""""
sc = AttrDict()
try:
sc[""name""] = elt.get(""name"")
except AttributeError:
return None
sc[""_type""] = ""lusubcorpus""
sc[""sentence""] = []
for sub in elt:
if sub.tag.endswith(""sentence""):
s = self._handle_lusentence_elt(sub)
if s is not None:
sc[""sentence""].append(s)
return sc
",[],0,[],/corpus/reader/framenet.py__handle_lusubcorpus_elt
4069,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__handle_lusentence_elt,"def _handle_lusentence_elt(self, elt):
""""""Load a sentence from a subcorpus of an LU from xml.""""""
info = self._load_xml_attributes(AttrDict(), elt)
info[""_type""] = ""lusentence""
info[""annotationSet""] = []
info[""_ascii""] = types.MethodType(
_annotation_ascii, info
)  # attach a method for this instance
for sub in elt:
if sub.tag.endswith(""text""):
info[""text""] = self._strip_tags(sub.text)
elif sub.tag.endswith(""annotationSet""):
annset = self._handle_luannotationset_elt(
sub, is_pos=(len(info[""annotationSet""]) == 0)
)
if annset is not None:
assert annset.status == ""UNANN"" or ""FE"" in annset, annset
if annset.status != ""UNANN"":
info[""frameAnnotation""] = annset
for k in (
""Target"",
""FE"",
""FE2"",
""FE3"",
""GF"",
""PT"",
""POS"",
""POS_tagset"",
""Other"",
""Sent"",
""Verb"",
""Noun"",
""Adj"",
""Adv"",
""Prep"",
""Scon"",
""Art"",
):
if k in annset:
info[k] = annset[k]
info[""annotationSet""].append(annset)
annset[""sent""] = info
annset[""text""] = info.text
return info
",[],0,[],/corpus/reader/framenet.py__handle_lusentence_elt
4070,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__handle_luannotationset_elt,"def _handle_luannotationset_elt(self, elt, is_pos=False):
""""""Load an annotation set from a sentence in an subcorpus of an LU""""""
info = self._load_xml_attributes(AttrDict(), elt)
info[""_type""] = ""posannotationset"" if is_pos else ""luannotationset""
info[""layer""] = []
info[""_ascii""] = types.MethodType(
_annotation_ascii, info
)  # attach a method for this instance
if ""cxnID"" in info:  # ignoring construction annotations for now.
return info
for sub in elt:
if sub.tag.endswith(""layer""):
l = self._handle_lulayer_elt(sub)
if l is not None:
overt = []
ni = {}  # null instantiations
info[""layer""].append(l)
for lbl in l.label:
if ""start"" in lbl:
thespan = (lbl.start, lbl.end + 1, lbl.name)
if l.name not in (
""Sent"",
""Other"",
):  # 'Sent' and 'Other' layers sometimes contain accidental duplicate spans
assert thespan not in overt, (info.ID, l.name, thespan)
overt.append(thespan)
else:  # null instantiation
if lbl.name in ni:
self._warn(
""FE with multiple NI entries:"",
lbl.name,
ni[lbl.name],
lbl.itype,
)
else:
ni[lbl.name] = lbl.itype
overt = sorted(overt)
if l.name == ""Target"":
if not overt:
self._warn(
""Skipping empty Target layer in annotation set ID={}"".format(
info.ID
)
)
continue
assert all(lblname == ""Target"" for i, j, lblname in overt)
if ""Target"" in info:
self._warn(
""Annotation set {} has multiple Target layers"".format(
info.ID
)
)
else:
info[""Target""] = [(i, j) for (i, j, _) in overt]
elif l.name == ""FE"":
if l.rank == 1:
assert ""FE"" not in info
info[""FE""] = (overt, ni)
else:
assert 2 <= l.rank <= 3, l.rank
k = ""FE"" + str(l.rank)
assert k not in info
info[k] = (overt, ni)
elif l.name in (""GF"", ""PT""):
assert l.rank == 1
info[l.name] = overt
elif l.name in (""BNC"", ""PENN""):
assert l.rank == 1
info[""POS""] = overt
info[""POS_tagset""] = l.name
else:
if is_pos:
if l.name not in (""NER"", ""WSL""):
self._warn(
""Unexpected layer in sentence annotationset:"",
l.name,
)
else:
if l.name not in (
""Sent"",
""Verb"",
""Noun"",
""Adj"",
""Adv"",
""Prep"",
""Scon"",
""Art"",
""Other"",
):
self._warn(
""Unexpected layer in frame annotationset:"", l.name
)
info[l.name] = overt
if not is_pos and ""cxnID"" not in info:
if ""Target"" not in info:
self._warn(f""Missing target in annotation set ID={info.ID}"")
assert ""FE"" in info
if ""FE3"" in info:
assert ""FE2"" in info
return info
",[],0,[],/corpus/reader/framenet.py__handle_luannotationset_elt
4071,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__handle_lulayer_elt,"def _handle_lulayer_elt(self, elt):
""""""Load a layer from an annotation set""""""
layer = self._load_xml_attributes(AttrDict(), elt)
layer[""_type""] = ""lulayer""
layer[""label""] = []
for sub in elt:
if sub.tag.endswith(""label""):
l = self._load_xml_attributes(AttrDict(), sub)
if l is not None:
layer[""label""].append(l)
return layer
",[],0,[],/corpus/reader/framenet.py__handle_lulayer_elt
4072,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__handle_fe_elt,"def _handle_fe_elt(self, elt):
feinfo = self._load_xml_attributes(AttrDict(), elt)
feinfo[""_type""] = ""fe""
feinfo[""definition""] = """"
feinfo[""definitionMarkup""] = """"
feinfo[""semType""] = None
feinfo[""requiresFE""] = None
feinfo[""excludesFE""] = None
for sub in elt:
if sub.tag.endswith(""definition""):
feinfo[""definitionMarkup""] = sub.text
feinfo[""definition""] = self._strip_tags(sub.text)
elif sub.tag.endswith(""semType""):
stinfo = self._load_xml_attributes(AttrDict(), sub)
feinfo[""semType""] = self.semtype(stinfo.ID)
elif sub.tag.endswith(""requiresFE""):
feinfo[""requiresFE""] = self._load_xml_attributes(AttrDict(), sub)
elif sub.tag.endswith(""excludesFE""):
feinfo[""excludesFE""] = self._load_xml_attributes(AttrDict(), sub)
return feinfo
",[],0,[],/corpus/reader/framenet.py__handle_fe_elt
4073,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py__handle_semtype_elt,"def _handle_semtype_elt(self, elt, tagspec=None):
semt = self._load_xml_attributes(AttrDict(), elt)
semt[""_type""] = ""semtype""
semt[""superType""] = None
semt[""subTypes""] = PrettyList()
for sub in elt:
if sub.text is not None:
semt[""definitionMarkup""] = sub.text
semt[""definition""] = self._strip_tags(sub.text)
else:
supertypeinfo = self._load_xml_attributes(AttrDict(), sub)
semt[""superType""] = supertypeinfo
return semt
",[],0,[],/corpus/reader/framenet.py__handle_semtype_elt
4074,/home/amandapotts/git/nltk/nltk/corpus/reader/framenet.py_demo,"def demo():
from nltk.corpus import framenet as fn
print(""Building the indexes..."")
fn.buildindexes()
print(""Number of Frames:"", len(fn.frames()))
print(""Number of Lexical Units:"", len(fn.lus()))
print(""Number of annotated documents:"", len(fn.docs()))
print()
print(
'getting frames whose name matches the (case insensitive) regex: ""(?i)medical""'
)
medframes = fn.frames(r""(?i)medical"")
print(f'Found {len(medframes)} Frames whose name matches ""(?i)medical"":')
print([(f.name, f.ID) for f in medframes])
tmp_id = medframes[0].ID
m_frame = fn.frame(tmp_id)  # reads all info for the frame
print(
'\nNumber of frame relations for the ""{}"" ({}) frame:'.format(
m_frame.name, m_frame.ID
),
len(m_frame.frameRelations),
)
for fr in m_frame.frameRelations:
print(""   "", fr)
print(
f'\nNumber of Frame Elements in the ""{m_frame.name}"" frame:',
len(m_frame.FE),
)
print(""   "", [x for x in m_frame.FE])
print(f'\nThe ""core"" Frame Elements in the ""{m_frame.name}"" frame:')
print(""   "", [x.name for x in m_frame.FE.values() if x.coreType == ""Core""])
print('\nAll Lexical Units that are incorporated in the ""Ailment"" FE:')
m_frame = fn.frame(239)
ailment_lus = [
x
for x in m_frame.lexUnit.values()
if ""incorporatedFE"" in x and x.incorporatedFE == ""Ailment""
]
print(""   "", [x.name for x in ailment_lus])
print(
f'\nNumber of Lexical Units in the ""{m_frame.name}"" frame:',
len(m_frame.lexUnit),
)
print(""  "", [x.name for x in m_frame.lexUnit.values()][:5], ""..."")
tmp_id = m_frame.lexUnit[""ailment.n""].ID  # grab the id of the specified LU
luinfo = fn.lu_basic(tmp_id)  # get basic info on the LU
print(f""\nInformation on the LU: {luinfo.name}"")
pprint(luinfo)
print(""\nNames of all of the corpora used for fulltext annotation:"")
allcorpora = {x.corpname for x in fn.docs_metadata()}
pprint(list(allcorpora))
firstcorp = list(allcorpora)[0]
firstcorp_docs = fn.docs(firstcorp)
print(f'\nNames of the annotated documents in the ""{firstcorp}"" corpus:')
pprint([x.filename for x in firstcorp_docs])
print(
'\nSearching for all Frames that have a lemma that matches the regexp: ""^run.v$"":'
)
pprint(fn.frames_by_lemma(r""^run.v$""))
",[],0,[],/corpus/reader/framenet.py_demo
4075,/home/amandapotts/git/nltk/nltk/corpus/reader/udhr.py___init__,"def __init__(self, root=""udhr""):
fileids = find_corpus_fileids(root, r""(?!README|\.).*"")
super().__init__(
root,
[fileid for fileid in fileids if fileid not in self.SKIP],
encoding=self.ENCODINGS,
)
",[],0,[],/corpus/reader/udhr.py___init__
4076,/home/amandapotts/git/nltk/nltk/corpus/reader/nombank.py___init__,"def __init__(
self,
root,
nomfile,
framefiles="""",
nounsfile=None,
parse_fileid_xform=None,
parse_corpus=None,
encoding=""utf8"",
",[],0,[],/corpus/reader/nombank.py___init__
4077,/home/amandapotts/git/nltk/nltk/corpus/reader/nombank.py_lines,"def lines(self):
""""""
:return: a corpus view that acts as a list of strings, one for
each line in the predicate-argument annotation file.
""""""
return StreamBackedCorpusView(
self.abspath(self._nomfile),
read_line_block,
encoding=self.encoding(self._nomfile),
)
",[],0,[],/corpus/reader/nombank.py_lines
4078,/home/amandapotts/git/nltk/nltk/corpus/reader/nombank.py_roleset,"def roleset(self, roleset_id):
""""""
:return: the xml description for the given roleset.
""""""
baseform = roleset_id.split(""."")[0]
baseform = baseform.replace(""perc-sign"", ""%"")
baseform = baseform.replace(""oneslashonezero"", ""1/10"").replace(
""1/10"", ""1-slash-10""
)
framefile = ""frames/%s.xml"" % baseform
if framefile not in self.fileids():
raise ValueError(""Frameset file for %s not found"" % roleset_id)
with self.abspath(framefile).open() as fp:
etree = ElementTree.parse(fp).getroot()
for roleset in etree.findall(""predicate/roleset""):
if roleset.attrib[""id""] == roleset_id:
return roleset
raise ValueError(f""Roleset {roleset_id} not found in {framefile}"")
",[],0,[],/corpus/reader/nombank.py_roleset
4079,/home/amandapotts/git/nltk/nltk/corpus/reader/nombank.py_rolesets,"def rolesets(self, baseform=None):
""""""
:return: list of xml descriptions for rolesets.
""""""
if baseform is not None:
framefile = ""frames/%s.xml"" % baseform
if framefile not in self.fileids():
raise ValueError(""Frameset file for %s not found"" % baseform)
framefiles = [framefile]
else:
framefiles = self.fileids()
rsets = []
for framefile in framefiles:
with self.abspath(framefile).open() as fp:
etree = ElementTree.parse(fp).getroot()
rsets.append(etree.findall(""predicate/roleset""))
return LazyConcatenation(rsets)
",[],0,[],/corpus/reader/nombank.py_rolesets
4080,/home/amandapotts/git/nltk/nltk/corpus/reader/nombank.py_nouns,"def nouns(self):
""""""
:return: a corpus view that acts as a list of all noun lemmas
in this corpus (from the nombank.1.0.words file).
""""""
return StreamBackedCorpusView(
self.abspath(self._nounsfile),
read_line_block,
encoding=self.encoding(self._nounsfile),
)
",[],0,[],/corpus/reader/nombank.py_nouns
4081,/home/amandapotts/git/nltk/nltk/corpus/reader/nombank.py___init__,"def __init__(
self,
fileid,
sentnum,
wordnum,
baseform,
sensenumber,
predicate,
predid,
arguments,
parse_corpus=None,
",[],0,[],/corpus/reader/nombank.py___init__
4082,/home/amandapotts/git/nltk/nltk/corpus/reader/nombank.py_roleset,"def roleset(self):
""""""The name of the roleset used by this instance's predicate.
Use ``nombank.roleset() <NombankCorpusReader.roleset>`` to
look up information about the roleset.""""""
r = self.baseform.replace(""%"", ""perc-sign"")
r = r.replace(""1/10"", ""1-slash-10"").replace(""1-slash-10"", ""oneslashonezero"")
return f""{r}.{self.sensenumber}""
",[],0,[],/corpus/reader/nombank.py_roleset
4083,/home/amandapotts/git/nltk/nltk/corpus/reader/nombank.py___repr__,"def __repr__(self):
return ""<NombankInstance: {}, sent {}, word {}>"".format(
self.fileid,
self.sentnum,
self.wordnum,
)
",[],0,[],/corpus/reader/nombank.py___repr__
4084,/home/amandapotts/git/nltk/nltk/corpus/reader/nombank.py___str__,"def __str__(self):
s = ""{} {} {} {} {}"".format(
self.fileid,
self.sentnum,
self.wordnum,
self.baseform,
self.sensenumber,
)
items = self.arguments + ((self.predicate, ""rel""),)
for argloc, argid in sorted(items):
s += f"" {argloc}-{argid}""
return s
",[],0,[],/corpus/reader/nombank.py___str__
4085,/home/amandapotts/git/nltk/nltk/corpus/reader/nombank.py__get_tree,"def _get_tree(self):
if self.parse_corpus is None:
return None
if self.fileid not in self.parse_corpus.fileids():
return None
return self.parse_corpus.parsed_sents(self.fileid)[self.sentnum]
",[],0,[],/corpus/reader/nombank.py__get_tree
4086,/home/amandapotts/git/nltk/nltk/corpus/reader/nombank.py_parse,"def parse(s, parse_fileid_xform=None, parse_corpus=None):
pieces = s.split()
if len(pieces) < 6:
raise ValueError(""Badly formatted nombank line: %r"" % s)
(fileid, sentnum, wordnum, baseform, sensenumber) = pieces[:5]
args = pieces[5:]
rel = [args.pop(i) for i, p in enumerate(args) if ""-rel"" in p]
if len(rel) != 1:
raise ValueError(""Badly formatted nombank line: %r"" % s)
if parse_fileid_xform is not None:
fileid = parse_fileid_xform(fileid)
sentnum = int(sentnum)
wordnum = int(wordnum)
predloc, predid = rel[0].split(""-"", 1)
predicate = NombankTreePointer.parse(predloc)
arguments = []
for arg in args:
argloc, argid = arg.split(""-"", 1)
arguments.append((NombankTreePointer.parse(argloc), argid))
return NombankInstance(
fileid,
sentnum,
wordnum,
baseform,
sensenumber,
predicate,
predid,
arguments,
parse_corpus,
)
",[],0,[],/corpus/reader/nombank.py_parse
4087,/home/amandapotts/git/nltk/nltk/corpus/reader/nombank.py___init__,"def __init__(self):
if self.__class__ == NombankPointer:
raise NotImplementedError()
",[],0,[],/corpus/reader/nombank.py___init__
4088,/home/amandapotts/git/nltk/nltk/corpus/reader/nombank.py___init__,"def __init__(self, pieces):
self.pieces = pieces
""""""A list of the pieces that make up this chain.  Elements may
be either ``NombankSplitTreePointer`` or
``NombankTreePointer`` pointers.""""""
",[],0,[],/corpus/reader/nombank.py___init__
4089,/home/amandapotts/git/nltk/nltk/corpus/reader/nombank.py___str__,"def __str__(self):
return ""*"".join(""%s"" % p for p in self.pieces)
",[],0,[],/corpus/reader/nombank.py___str__
4090,/home/amandapotts/git/nltk/nltk/corpus/reader/nombank.py___repr__,"def __repr__(self):
return ""<NombankChainTreePointer: %s>"" % self
",[],0,[],/corpus/reader/nombank.py___repr__
4091,/home/amandapotts/git/nltk/nltk/corpus/reader/nombank.py_select,"def select(self, tree):
if tree is None:
raise ValueError(""Parse tree not available"")
return Tree(""*CHAIN*"", [p.select(tree) for p in self.pieces])
",[],0,[],/corpus/reader/nombank.py_select
4092,/home/amandapotts/git/nltk/nltk/corpus/reader/nombank.py___init__,"def __init__(self, pieces):
self.pieces = pieces
""""""A list of the pieces that make up this chain.  Elements are
all ``NombankTreePointer`` pointers.""""""
",[],0,[],/corpus/reader/nombank.py___init__
4093,/home/amandapotts/git/nltk/nltk/corpus/reader/nombank.py___str__,"def __str__(self):
return "","".join(""%s"" % p for p in self.pieces)
",[],0,[],/corpus/reader/nombank.py___str__
4094,/home/amandapotts/git/nltk/nltk/corpus/reader/nombank.py___repr__,"def __repr__(self):
return ""<NombankSplitTreePointer: %s>"" % self
",[],0,[],/corpus/reader/nombank.py___repr__
4095,/home/amandapotts/git/nltk/nltk/corpus/reader/nombank.py_select,"def select(self, tree):
if tree is None:
raise ValueError(""Parse tree not available"")
return Tree(""*SPLIT*"", [p.select(tree) for p in self.pieces])
",[],0,[],/corpus/reader/nombank.py_select
4096,/home/amandapotts/git/nltk/nltk/corpus/reader/nombank.py___init__,"def __init__(self, wordnum, height):
self.wordnum = wordnum
self.height = height
",[],0,[],/corpus/reader/nombank.py___init__
4097,/home/amandapotts/git/nltk/nltk/corpus/reader/nombank.py_parse,"def parse(s):
pieces = s.split(""*"")
if len(pieces) > 1:
return NombankChainTreePointer(
[NombankTreePointer.parse(elt) for elt in pieces]
)
pieces = s.split("","")
if len(pieces) > 1:
return NombankSplitTreePointer(
[NombankTreePointer.parse(elt) for elt in pieces]
)
pieces = s.split("":"")
if len(pieces) != 2:
raise ValueError(""bad nombank pointer %r"" % s)
return NombankTreePointer(int(pieces[0]), int(pieces[1]))
",[],0,[],/corpus/reader/nombank.py_parse
4098,/home/amandapotts/git/nltk/nltk/corpus/reader/nombank.py___str__,"def __str__(self):
return f""{self.wordnum}:{self.height}""
",[],0,[],/corpus/reader/nombank.py___str__
4099,/home/amandapotts/git/nltk/nltk/corpus/reader/nombank.py___repr__,"def __repr__(self):
return ""NombankTreePointer(%d, %d)"" % (self.wordnum, self.height)
",[],0,[],/corpus/reader/nombank.py___repr__
4100,/home/amandapotts/git/nltk/nltk/corpus/reader/nombank.py___eq__,"def __eq__(self, other):
while isinstance(other, (NombankChainTreePointer, NombankSplitTreePointer)):
other = other.pieces[0]
if not isinstance(other, NombankTreePointer):
return self is other
return self.wordnum == other.wordnum and self.height == other.height
",[],0,[],/corpus/reader/nombank.py___eq__
4101,/home/amandapotts/git/nltk/nltk/corpus/reader/nombank.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/corpus/reader/nombank.py___ne__
4102,/home/amandapotts/git/nltk/nltk/corpus/reader/nombank.py___lt__,"def __lt__(self, other):
while isinstance(other, (NombankChainTreePointer, NombankSplitTreePointer)):
other = other.pieces[0]
if not isinstance(other, NombankTreePointer):
return id(self) < id(other)
return (self.wordnum, -self.height) < (other.wordnum, -other.height)
",[],0,[],/corpus/reader/nombank.py___lt__
4103,/home/amandapotts/git/nltk/nltk/corpus/reader/nombank.py_select,"def select(self, tree):
if tree is None:
raise ValueError(""Parse tree not available"")
return tree[self.treepos(tree)]
",[],0,[],/corpus/reader/nombank.py_select
4104,/home/amandapotts/git/nltk/nltk/corpus/reader/nombank.py_treepos,"def treepos(self, tree):
""""""
Convert this pointer to a standard 'tree position' pointer,
given that it points to the given tree.
""""""
if tree is None:
raise ValueError(""Parse tree not available"")
stack = [tree]
treepos = []
wordnum = 0
while True:
if isinstance(stack[-1], Tree):
if len(treepos) < len(stack):
treepos.append(0)
else:
treepos[-1] += 1
if treepos[-1] < len(stack[-1]):
stack.append(stack[-1][treepos[-1]])
else:
stack.pop()
treepos.pop()
else:
if wordnum == self.wordnum:
return tuple(treepos[: len(treepos) - self.height - 1])
else:
wordnum += 1
stack.pop()
",[],0,[],/corpus/reader/nombank.py_treepos
4105,/home/amandapotts/git/nltk/nltk/corpus/reader/switchboard.py___init__,"def __init__(self, words, speaker, id):
list.__init__(self, words)
self.speaker = speaker
self.id = int(id)
",[],0,[],/corpus/reader/switchboard.py___init__
4106,/home/amandapotts/git/nltk/nltk/corpus/reader/switchboard.py___repr__,"def __repr__(self):
if len(self) == 0:
text = """"
elif isinstance(self[0], tuple):
text = "" "".join(""%s/%s"" % w for w in self)
else:
text = "" "".join(self)
return f""<{self.speaker}.{self.id}: {text!r}>""
",[],0,[],/corpus/reader/switchboard.py___repr__
4107,/home/amandapotts/git/nltk/nltk/corpus/reader/switchboard.py___init__,"def __init__(self, root, tagset=None):
CorpusReader.__init__(self, root, self._FILES)
self._tagset = tagset
",[],0,[],/corpus/reader/switchboard.py___init__
4108,/home/amandapotts/git/nltk/nltk/corpus/reader/switchboard.py_words,"def words(self):
return StreamBackedCorpusView(self.abspath(""tagged""), self._words_block_reader)
",[],0,[],/corpus/reader/switchboard.py_words
4109,/home/amandapotts/git/nltk/nltk/corpus/reader/switchboard.py_tagged_words,"def tagged_words(self, tagset=None):
",[],0,[],/corpus/reader/switchboard.py_tagged_words
4110,/home/amandapotts/git/nltk/nltk/corpus/reader/switchboard.py_tagged_words_block_reader,"def tagged_words_block_reader(stream):
return self._tagged_words_block_reader(stream, tagset)
",[],0,[],/corpus/reader/switchboard.py_tagged_words_block_reader
4111,/home/amandapotts/git/nltk/nltk/corpus/reader/switchboard.py_turns,"def turns(self):
return StreamBackedCorpusView(self.abspath(""tagged""), self._turns_block_reader)
",[],0,[],/corpus/reader/switchboard.py_turns
4112,/home/amandapotts/git/nltk/nltk/corpus/reader/switchboard.py_tagged_turns,"def tagged_turns(self, tagset=None):
",[],0,[],/corpus/reader/switchboard.py_tagged_turns
4113,/home/amandapotts/git/nltk/nltk/corpus/reader/switchboard.py_tagged_turns_block_reader,"def tagged_turns_block_reader(stream):
return self._tagged_turns_block_reader(stream, tagset)
",[],0,[],/corpus/reader/switchboard.py_tagged_turns_block_reader
4114,/home/amandapotts/git/nltk/nltk/corpus/reader/switchboard.py_discourses,"def discourses(self):
return StreamBackedCorpusView(
self.abspath(""tagged""), self._discourses_block_reader
)
",[],0,[],/corpus/reader/switchboard.py_discourses
4115,/home/amandapotts/git/nltk/nltk/corpus/reader/switchboard.py_tagged_discourses,"def tagged_discourses(self, tagset=False):
",[],0,[],/corpus/reader/switchboard.py_tagged_discourses
4116,/home/amandapotts/git/nltk/nltk/corpus/reader/switchboard.py_tagged_discourses_block_reader,"def tagged_discourses_block_reader(stream):
return self._tagged_discourses_block_reader(stream, tagset)
",[],0,[],/corpus/reader/switchboard.py_tagged_discourses_block_reader
4117,/home/amandapotts/git/nltk/nltk/corpus/reader/switchboard.py__discourses_block_reader,"def _discourses_block_reader(self, stream):
return [
[
self._parse_utterance(u, include_tag=False)
for b in read_blankline_block(stream)
for u in b.split(""\n"")
if u.strip()
]
]
",[],0,[],/corpus/reader/switchboard.py__discourses_block_reader
4118,/home/amandapotts/git/nltk/nltk/corpus/reader/switchboard.py__tagged_discourses_block_reader,"def _tagged_discourses_block_reader(self, stream, tagset=None):
return [
[
self._parse_utterance(u, include_tag=True, tagset=tagset)
for b in read_blankline_block(stream)
for u in b.split(""\n"")
if u.strip()
]
]
",[],0,[],/corpus/reader/switchboard.py__tagged_discourses_block_reader
4119,/home/amandapotts/git/nltk/nltk/corpus/reader/switchboard.py__turns_block_reader,"def _turns_block_reader(self, stream):
return self._discourses_block_reader(stream)[0]
",[],0,[],/corpus/reader/switchboard.py__turns_block_reader
4120,/home/amandapotts/git/nltk/nltk/corpus/reader/switchboard.py__tagged_turns_block_reader,"def _tagged_turns_block_reader(self, stream, tagset=None):
return self._tagged_discourses_block_reader(stream, tagset)[0]
",[],0,[],/corpus/reader/switchboard.py__tagged_turns_block_reader
4121,/home/amandapotts/git/nltk/nltk/corpus/reader/switchboard.py__words_block_reader,"def _words_block_reader(self, stream):
return sum(self._discourses_block_reader(stream)[0], [])
",[],0,[],/corpus/reader/switchboard.py__words_block_reader
4122,/home/amandapotts/git/nltk/nltk/corpus/reader/switchboard.py__tagged_words_block_reader,"def _tagged_words_block_reader(self, stream, tagset=None):
return sum(self._tagged_discourses_block_reader(stream, tagset)[0], [])
",[],0,[],/corpus/reader/switchboard.py__tagged_words_block_reader
4123,/home/amandapotts/git/nltk/nltk/corpus/reader/switchboard.py__parse_utterance,"def _parse_utterance(self, utterance, include_tag, tagset=None):
m = self._UTTERANCE_RE.match(utterance)
if m is None:
raise ValueError(""Bad utterance %r"" % utterance)
speaker, id, text = m.groups()
words = [str2tuple(s, self._SEP) for s in text.split()]
if not include_tag:
words = [w for (w, t) in words]
elif tagset and tagset != self._tagset:
words = [(w, map_tag(self._tagset, tagset, t)) for (w, t) in words]
return SwitchboardTurn(words, speaker, id)
",[],0,[],/corpus/reader/switchboard.py__parse_utterance
4124,/home/amandapotts/git/nltk/nltk/corpus/reader/plaintext.py___init__,"def __init__(
self,
root,
fileids,
word_tokenizer=WordPunctTokenizer(),
sent_tokenizer=nltk.data.LazyLoader(""tokenizers/punkt/english.pickle""),
para_block_reader=read_blankline_block,
encoding=""utf8"",
",[],0,[],/corpus/reader/plaintext.py___init__
4125,/home/amandapotts/git/nltk/nltk/corpus/reader/plaintext.py_words,"def words(self, fileids=None):
""""""
:return: the given file(s) as a list of words
and punctuation symbols.
:rtype: list(str)
""""""
return concat(
[
self.CorpusView(path, self._read_word_block, encoding=enc)
for (path, enc, fileid) in self.abspaths(fileids, True, True)
]
)
",[],0,[],/corpus/reader/plaintext.py_words
4126,/home/amandapotts/git/nltk/nltk/corpus/reader/plaintext.py_sents,"def sents(self, fileids=None):
""""""
:return: the given file(s) as a list of
sentences or utterances, each encoded as a list of word
strings.
:rtype: list(list(str))
""""""
if self._sent_tokenizer is None:
raise ValueError(""No sentence tokenizer for this corpus"")
return concat(
[
self.CorpusView(path, self._read_sent_block, encoding=enc)
for (path, enc, fileid) in self.abspaths(fileids, True, True)
]
)
",[],0,[],/corpus/reader/plaintext.py_sents
4127,/home/amandapotts/git/nltk/nltk/corpus/reader/plaintext.py_paras,"def paras(self, fileids=None):
""""""
:return: the given file(s) as a list of
paragraphs, each encoded as a list of sentences, which are
in turn encoded as lists of word strings.
:rtype: list(list(list(str)))
""""""
if self._sent_tokenizer is None:
raise ValueError(""No sentence tokenizer for this corpus"")
return concat(
[
self.CorpusView(path, self._read_para_block, encoding=enc)
for (path, enc, fileid) in self.abspaths(fileids, True, True)
]
)
",[],0,[],/corpus/reader/plaintext.py_paras
4128,/home/amandapotts/git/nltk/nltk/corpus/reader/plaintext.py__read_word_block,"def _read_word_block(self, stream):
words = []
for i in range(20):  # Read 20 lines at a time.
words.extend(self._word_tokenizer.tokenize(stream.readline()))
return words
",[],0,[],/corpus/reader/plaintext.py__read_word_block
4129,/home/amandapotts/git/nltk/nltk/corpus/reader/plaintext.py__read_sent_block,"def _read_sent_block(self, stream):
sents = []
for para in self._para_block_reader(stream):
sents.extend(
[
self._word_tokenizer.tokenize(sent)
for sent in self._sent_tokenizer.tokenize(para)
]
)
return sents
",[],0,[],/corpus/reader/plaintext.py__read_sent_block
4130,/home/amandapotts/git/nltk/nltk/corpus/reader/plaintext.py__read_para_block,"def _read_para_block(self, stream):
paras = []
for para in self._para_block_reader(stream):
paras.append(
[
self._word_tokenizer.tokenize(sent)
for sent in self._sent_tokenizer.tokenize(para)
]
)
return paras
",[],0,[],/corpus/reader/plaintext.py__read_para_block
4131,/home/amandapotts/git/nltk/nltk/corpus/reader/plaintext.py___init__,"def __init__(self, *args, **kwargs):
""""""
Initialize the corpus reader.  Categorization arguments
(``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to
the ``CategorizedCorpusReader`` constructor.  The remaining arguments
are passed to the ``PlaintextCorpusReader`` constructor.
""""""
CategorizedCorpusReader.__init__(self, kwargs)
PlaintextCorpusReader.__init__(self, *args, **kwargs)
",[],0,[],/corpus/reader/plaintext.py___init__
4132,/home/amandapotts/git/nltk/nltk/corpus/reader/plaintext.py___init__,"def __init__(self, *args, **kwargs):
CategorizedCorpusReader.__init__(self, kwargs)
kwargs[""sent_tokenizer""] = nltk.data.LazyLoader(
""tokenizers/punkt/portuguese.pickle""
)
PlaintextCorpusReader.__init__(self, *args, **kwargs)
",[],0,[],/corpus/reader/plaintext.py___init__
4133,/home/amandapotts/git/nltk/nltk/corpus/reader/plaintext.py__read_word_block,"def _read_word_block(self, stream):
words = []
for i in range(20):  # Read 20 lines at a time.
words.extend(stream.readline().split())
return words
",[],0,[],/corpus/reader/plaintext.py__read_word_block
4134,/home/amandapotts/git/nltk/nltk/corpus/reader/plaintext.py__read_sent_block,"def _read_sent_block(self, stream):
sents = []
for para in self._para_block_reader(stream):
sents.extend([sent.split() for sent in para.splitlines()])
return sents
",[],0,[],/corpus/reader/plaintext.py__read_sent_block
4135,/home/amandapotts/git/nltk/nltk/corpus/reader/plaintext.py__read_para_block,"def _read_para_block(self, stream):
paras = []
for para in self._para_block_reader(stream):
paras.append([sent.split() for sent in para.splitlines()])
return paras
",[],0,[],/corpus/reader/plaintext.py__read_para_block
4136,/home/amandapotts/git/nltk/nltk/corpus/reader/plaintext.py_chapters,"def chapters(self, fileids=None):
""""""
:return: the given file(s) as a list of
chapters, each encoded as a list of sentences, which are
in turn encoded as lists of word strings.
:rtype: list(list(list(str)))
""""""
return concat(
[
self.CorpusView(fileid, self._read_para_block, encoding=enc)
for (fileid, enc) in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/plaintext.py_chapters
4137,/home/amandapotts/git/nltk/nltk/corpus/reader/plaintext.py_paras,"def paras(self, fileids=None):
raise NotImplementedError(
""The Europarl corpus reader does not support paragraphs. Please use chapters() instead.""
)
",[],0,[],/corpus/reader/plaintext.py_paras
4138,/home/amandapotts/git/nltk/nltk/corpus/reader/indian.py_words,"def words(self, fileids=None):
return concat(
[
IndianCorpusView(fileid, enc, False, False)
for (fileid, enc) in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/indian.py_words
4139,/home/amandapotts/git/nltk/nltk/corpus/reader/indian.py_tagged_words,"def tagged_words(self, fileids=None, tagset=None):
if tagset and tagset != self._tagset:
",[],0,[],/corpus/reader/indian.py_tagged_words
4140,/home/amandapotts/git/nltk/nltk/corpus/reader/indian.py_sents,"def sents(self, fileids=None):
return concat(
[
IndianCorpusView(fileid, enc, False, True)
for (fileid, enc) in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/indian.py_sents
4141,/home/amandapotts/git/nltk/nltk/corpus/reader/indian.py_tagged_sents,"def tagged_sents(self, fileids=None, tagset=None):
if tagset and tagset != self._tagset:
",[],0,[],/corpus/reader/indian.py_tagged_sents
4142,/home/amandapotts/git/nltk/nltk/corpus/reader/indian.py___init__,"def __init__(
self, corpus_file, encoding, tagged, group_by_sent, tag_mapping_function=None
",[],0,[],/corpus/reader/indian.py___init__
4143,/home/amandapotts/git/nltk/nltk/corpus/reader/indian.py_read_block,"def read_block(self, stream):
line = stream.readline()
if line.startswith(""<""):
return []
sent = [str2tuple(word, sep=""_"") for word in line.split()]
if self._tag_mapping_function:
sent = [(w, self._tag_mapping_function(t)) for (w, t) in sent]
if not self._tagged:
sent = [w for (w, t) in sent]
if self._group_by_sent:
return [sent]
else:
return sent
",[],0,[],/corpus/reader/indian.py_read_block
4144,/home/amandapotts/git/nltk/nltk/corpus/reader/mte.py_xpath,"def xpath(root, path, ns):
return root.findall(path, ns)
",[],0,[],/corpus/reader/mte.py_xpath
4145,/home/amandapotts/git/nltk/nltk/corpus/reader/mte.py___init__,"def __init__(self, fileid, tagspec, elt_handler=None):
XMLCorpusView.__init__(self, fileid, tagspec, elt_handler)
",[],0,[],/corpus/reader/mte.py___init__
4146,/home/amandapotts/git/nltk/nltk/corpus/reader/mte.py___init__,"def __init__(self, file_path):
self.__file_path = file_path
",[],0,[],/corpus/reader/mte.py___init__
4147,/home/amandapotts/git/nltk/nltk/corpus/reader/mte.py__word_elt,"def _word_elt(cls, elt, context):
return elt.text
",[],0,[],/corpus/reader/mte.py__word_elt
4148,/home/amandapotts/git/nltk/nltk/corpus/reader/mte.py__sent_elt,"def _sent_elt(cls, elt, context):
return [cls._word_elt(w, None) for w in xpath(elt, ""*"", cls.ns)]
",[],0,[],/corpus/reader/mte.py__sent_elt
4149,/home/amandapotts/git/nltk/nltk/corpus/reader/mte.py__para_elt,"def _para_elt(cls, elt, context):
return [cls._sent_elt(s, None) for s in xpath(elt, ""*"", cls.ns)]
",[],0,[],/corpus/reader/mte.py__para_elt
4150,/home/amandapotts/git/nltk/nltk/corpus/reader/mte.py__tagged_word_elt,"def _tagged_word_elt(cls, elt, context):
if ""ana"" not in elt.attrib:
return (elt.text, """")
if cls.__tags == """" and cls.__tagset == ""msd"":
return (elt.text, elt.attrib[""ana""])
elif cls.__tags == """" and cls.__tagset == ""universal"":
return (elt.text, MTETagConverter.msd_to_universal(elt.attrib[""ana""]))
else:
tags = re.compile(""^"" + re.sub(""-"", ""."", cls.__tags) + "".*$"")
if tags.match(elt.attrib[""ana""]):
if cls.__tagset == ""msd"":
return (elt.text, elt.attrib[""ana""])
else:
return (
elt.text,
MTETagConverter.msd_to_universal(elt.attrib[""ana""]),
)
else:
return None
",[],0,[],/corpus/reader/mte.py__tagged_word_elt
4151,/home/amandapotts/git/nltk/nltk/corpus/reader/mte.py__lemma_word_elt,"def _lemma_word_elt(cls, elt, context):
if ""lemma"" not in elt.attrib:
return (elt.text, """")
else:
return (elt.text, elt.attrib[""lemma""])
",[],0,[],/corpus/reader/mte.py__lemma_word_elt
4152,/home/amandapotts/git/nltk/nltk/corpus/reader/mte.py__lemma_sent_elt,"def _lemma_sent_elt(cls, elt, context):
return [cls._lemma_word_elt(w, None) for w in xpath(elt, ""*"", cls.ns)]
",[],0,[],/corpus/reader/mte.py__lemma_sent_elt
4153,/home/amandapotts/git/nltk/nltk/corpus/reader/mte.py__lemma_para_elt,"def _lemma_para_elt(cls, elt, context):
return [cls._lemma_sent_elt(s, None) for s in xpath(elt, ""*"", cls.ns)]
",[],0,[],/corpus/reader/mte.py__lemma_para_elt
4154,/home/amandapotts/git/nltk/nltk/corpus/reader/mte.py_words,"def words(self):
return MTECorpusView(
self.__file_path, MTEFileReader.word_path, MTEFileReader._word_elt
)
",[],0,[],/corpus/reader/mte.py_words
4155,/home/amandapotts/git/nltk/nltk/corpus/reader/mte.py_sents,"def sents(self):
return MTECorpusView(
self.__file_path, MTEFileReader.sent_path, MTEFileReader._sent_elt
)
",[],0,[],/corpus/reader/mte.py_sents
4156,/home/amandapotts/git/nltk/nltk/corpus/reader/mte.py_paras,"def paras(self):
return MTECorpusView(
self.__file_path, MTEFileReader.para_path, MTEFileReader._para_elt
)
",[],0,[],/corpus/reader/mte.py_paras
4157,/home/amandapotts/git/nltk/nltk/corpus/reader/mte.py_lemma_words,"def lemma_words(self):
return MTECorpusView(
self.__file_path, MTEFileReader.word_path, MTEFileReader._lemma_word_elt
)
",[],0,[],/corpus/reader/mte.py_lemma_words
4158,/home/amandapotts/git/nltk/nltk/corpus/reader/mte.py_tagged_words,"def tagged_words(self, tagset, tags):
MTEFileReader.__tagset = tagset
MTEFileReader.__tags = tags
return MTECorpusView(
self.__file_path, MTEFileReader.word_path, MTEFileReader._tagged_word_elt
)
",[],0,[],/corpus/reader/mte.py_tagged_words
4159,/home/amandapotts/git/nltk/nltk/corpus/reader/mte.py_lemma_sents,"def lemma_sents(self):
return MTECorpusView(
self.__file_path, MTEFileReader.sent_path, MTEFileReader._lemma_sent_elt
)
",[],0,[],/corpus/reader/mte.py_lemma_sents
4160,/home/amandapotts/git/nltk/nltk/corpus/reader/mte.py_tagged_sents,"def tagged_sents(self, tagset, tags):
MTEFileReader.__tagset = tagset
MTEFileReader.__tags = tags
return MTECorpusView(
self.__file_path, MTEFileReader.sent_path, MTEFileReader._tagged_sent_elt
)
",[],0,[],/corpus/reader/mte.py_tagged_sents
4161,/home/amandapotts/git/nltk/nltk/corpus/reader/mte.py_lemma_paras,"def lemma_paras(self):
return MTECorpusView(
self.__file_path, MTEFileReader.para_path, MTEFileReader._lemma_para_elt
)
",[],0,[],/corpus/reader/mte.py_lemma_paras
4162,/home/amandapotts/git/nltk/nltk/corpus/reader/mte.py_tagged_paras,"def tagged_paras(self, tagset, tags):
MTEFileReader.__tagset = tagset
MTEFileReader.__tags = tags
return MTECorpusView(
self.__file_path, MTEFileReader.para_path, MTEFileReader._tagged_para_elt
)
",[],0,[],/corpus/reader/mte.py_tagged_paras
4163,/home/amandapotts/git/nltk/nltk/corpus/reader/mte.py_msd_to_universal,"def msd_to_universal(tag):
""""""
This function converts the annotation from the Multex-East to the universal tagset
as described in Chapter 5 of the NLTK-Book
Unknown Tags will be mapped to X. Punctuation marks are not supported in MSD tags, so
""""""
indicator = tag[0] if not tag[0] == ""#"" else tag[1]
if not indicator in MTETagConverter.mapping_msd_universal:
indicator = ""-""
return MTETagConverter.mapping_msd_universal[indicator]
",[],0,[],/corpus/reader/mte.py_msd_to_universal
4164,/home/amandapotts/git/nltk/nltk/corpus/reader/mte.py___init__,"def __init__(self, root=None, fileids=None, encoding=""utf8""):
""""""
Construct a new MTECorpusreader for a set of documents
located at the given root directory.  Example usage:
>>> root = '/...path to corpus.../'
>>> reader = MTECorpusReader(root, 'oana-*.xml', 'utf8') # doctest: +SKIP
:param root: The root directory for this corpus. (default points to location in multext config file)
:param fileids: A list or regexp specifying the fileids in this corpus. (default is oana-en.xml)
:param encoding: The encoding of the given files (default is utf8)
""""""
TaggedCorpusReader.__init__(self, root, fileids, encoding)
self._readme = ""00README.txt""
",[],0,[],/corpus/reader/mte.py___init__
4165,/home/amandapotts/git/nltk/nltk/corpus/reader/mte.py_words,"def words(self, fileids=None):
""""""
:param fileids: A list specifying the fileids that should be used.
:return: the given file(s) as a list of words and punctuation symbols.
:rtype: list(str)
""""""
return concat(
[
MTEFileReader(os.path.join(self._root, f)).words()
for f in self.__fileids(fileids)
]
)
",[],0,[],/corpus/reader/mte.py_words
4166,/home/amandapotts/git/nltk/nltk/corpus/reader/mte.py_sents,"def sents(self, fileids=None):
""""""
:param fileids: A list specifying the fileids that should be used.
:return: the given file(s) as a list of sentences or utterances,
each encoded as a list of word strings
:rtype: list(list(str))
""""""
return concat(
[
MTEFileReader(os.path.join(self._root, f)).sents()
for f in self.__fileids(fileids)
]
)
",[],0,[],/corpus/reader/mte.py_sents
4167,/home/amandapotts/git/nltk/nltk/corpus/reader/mte.py_paras,"def paras(self, fileids=None):
""""""
:param fileids: A list specifying the fileids that should be used.
:return: the given file(s) as a list of paragraphs, each encoded as a list
of sentences, which are in turn encoded as lists of word string
:rtype: list(list(list(str)))
""""""
return concat(
[
MTEFileReader(os.path.join(self._root, f)).paras()
for f in self.__fileids(fileids)
]
)
",[],0,[],/corpus/reader/mte.py_paras
4168,/home/amandapotts/git/nltk/nltk/corpus/reader/mte.py_lemma_words,"def lemma_words(self, fileids=None):
""""""
:param fileids: A list specifying the fileids that should be used.
:return: the given file(s) as a list of words, the corresponding lemmas
and punctuation symbols, encoded as tuples (word, lemma)
:rtype: list(tuple(str,str))
""""""
return concat(
[
MTEFileReader(os.path.join(self._root, f)).lemma_words()
for f in self.__fileids(fileids)
]
)
",[],0,[],/corpus/reader/mte.py_lemma_words
4169,/home/amandapotts/git/nltk/nltk/corpus/reader/mte.py_tagged_words,"def tagged_words(self, fileids=None, tagset=""msd"", tags=""""):
""""""
:param fileids: A list specifying the fileids that should be used.
:param tagset: The tagset that should be used in the returned object,
either ""universal"" or ""msd"", ""msd"" is the default
:param tags: An MSD Tag that is used to filter all parts of the used corpus
that are not more precise or at least equal to the given tag
:return: the given file(s) as a list of tagged words and punctuation symbols
encoded as tuples (word, tag)
:rtype: list(tuple(str, str))
""""""
if tagset == ""universal"" or tagset == ""msd"":
return concat(
[
MTEFileReader(os.path.join(self._root, f)).tagged_words(
tagset, tags
)
for f in self.__fileids(fileids)
]
)
else:
print(""Unknown tagset specified."")
",[],0,[],/corpus/reader/mte.py_tagged_words
4170,/home/amandapotts/git/nltk/nltk/corpus/reader/mte.py_lemma_sents,"def lemma_sents(self, fileids=None):
""""""
:param fileids: A list specifying the fileids that should be used.
:return: the given file(s) as a list of sentences or utterances, each
encoded as a list of tuples of the word and the corresponding
lemma (word, lemma)
:rtype: list(list(tuple(str, str)))
""""""
return concat(
[
MTEFileReader(os.path.join(self._root, f)).lemma_sents()
for f in self.__fileids(fileids)
]
)
",[],0,[],/corpus/reader/mte.py_lemma_sents
4171,/home/amandapotts/git/nltk/nltk/corpus/reader/mte.py_tagged_sents,"def tagged_sents(self, fileids=None, tagset=""msd"", tags=""""):
""""""
:param fileids: A list specifying the fileids that should be used.
:param tagset: The tagset that should be used in the returned object,
either ""universal"" or ""msd"", ""msd"" is the default
:param tags: An MSD Tag that is used to filter all parts of the used corpus
that are not more precise or at least equal to the given tag
:return: the given file(s) as a list of sentences or utterances, each
each encoded as a list of (word,tag) tuples
:rtype: list(list(tuple(str, str)))
""""""
if tagset == ""universal"" or tagset == ""msd"":
return concat(
[
MTEFileReader(os.path.join(self._root, f)).tagged_sents(
tagset, tags
)
for f in self.__fileids(fileids)
]
)
else:
print(""Unknown tagset specified."")
",[],0,[],/corpus/reader/mte.py_tagged_sents
4172,/home/amandapotts/git/nltk/nltk/corpus/reader/mte.py_lemma_paras,"def lemma_paras(self, fileids=None):
""""""
:param fileids: A list specifying the fileids that should be used.
:return: the given file(s) as a list of paragraphs, each encoded as a
list of sentences, which are in turn encoded as a list of
tuples of the word and the corresponding lemma (word, lemma)
:rtype: list(List(List(tuple(str, str))))
""""""
return concat(
[
MTEFileReader(os.path.join(self._root, f)).lemma_paras()
for f in self.__fileids(fileids)
]
)
",[],0,[],/corpus/reader/mte.py_lemma_paras
4173,/home/amandapotts/git/nltk/nltk/corpus/reader/mte.py_tagged_paras,"def tagged_paras(self, fileids=None, tagset=""msd"", tags=""""):
""""""
:param fileids: A list specifying the fileids that should be used.
:param tagset: The tagset that should be used in the returned object,
either ""universal"" or ""msd"", ""msd"" is the default
:param tags: An MSD Tag that is used to filter all parts of the used corpus
that are not more precise or at least equal to the given tag
:return: the given file(s) as a list of paragraphs, each encoded as a
list of sentences, which are in turn encoded as a list
of (word,tag) tuples
:rtype: list(list(list(tuple(str, str))))
""""""
if tagset == ""universal"" or tagset == ""msd"":
return concat(
[
MTEFileReader(os.path.join(self._root, f)).tagged_paras(
tagset, tags
)
for f in self.__fileids(fileids)
]
)
else:
print(""Unknown tagset specified."")
",[],0,[],/corpus/reader/mte.py_tagged_paras
4174,/home/amandapotts/git/nltk/nltk/corpus/reader/util.py_read_block,"def read_block(self, stream):
""""""
Read a block from the input stream.
:return: a block of tokens from the input stream
:rtype: list(any)
:param stream: an input stream
:type stream: stream
""""""
raise NotImplementedError(""Abstract Method"")
",[],0,[],/corpus/reader/util.py_read_block
4175,/home/amandapotts/git/nltk/nltk/corpus/reader/util.py__open,"def _open(self):
""""""
Open the file stream associated with this corpus view.  This
will be called performed if any value is read from the view
while its file stream is closed.
""""""
if isinstance(self._fileid, PathPointer):
self._stream = self._fileid.open(self._encoding)
elif self._encoding:
self._stream = SeekableUnicodeStreamReader(
open(self._fileid, ""rb""), self._encoding
)
else:
self._stream = open(self._fileid, ""rb"")
",[],0,[],/corpus/reader/util.py__open
4176,/home/amandapotts/git/nltk/nltk/corpus/reader/util.py_close,"def close(self):
""""""
Close the file stream associated with this corpus view.  This
can be useful if you are worried about running out of file
handles (although the stream should automatically be closed
upon garbage collection of the corpus view).  If the corpus
view is accessed after it is closed, it will be automatically
re-opened.
""""""
if self._stream is not None:
self._stream.close()
self._stream = None
",[],0,[],/corpus/reader/util.py_close
4177,/home/amandapotts/git/nltk/nltk/corpus/reader/util.py___enter__,"def __enter__(self):
return self
",[],0,[],/corpus/reader/util.py___enter__
4178,/home/amandapotts/git/nltk/nltk/corpus/reader/util.py___exit__,"def __exit__(self, type, value, traceback):
self.close()
",[],0,[],/corpus/reader/util.py___exit__
4179,/home/amandapotts/git/nltk/nltk/corpus/reader/util.py___len__,"def __len__(self):
if self._len is None:
for tok in self.iterate_from(self._toknum[-1]):
pass
return self._len
",[],0,[],/corpus/reader/util.py___len__
4180,/home/amandapotts/git/nltk/nltk/corpus/reader/util.py___getitem__,"def __getitem__(self, i):
if isinstance(i, slice):
start, stop = slice_bounds(self, i)
offset = self._cache[0]
if offset <= start and stop <= self._cache[1]:
return self._cache[2][start - offset : stop - offset]
return LazySubsequence(self, start, stop)
else:
if i < 0:
i += len(self)
if i < 0:
raise IndexError(""index out of range"")
offset = self._cache[0]
if offset <= i < self._cache[1]:
return self._cache[2][i - offset]
try:
return next(self.iterate_from(i))
except StopIteration as e:
raise IndexError(""index out of range"") from e
",[],0,[],/corpus/reader/util.py___getitem__
4181,/home/amandapotts/git/nltk/nltk/corpus/reader/util.py_iterate_from,"def iterate_from(self, start_tok):
if self._cache[0] <= start_tok < self._cache[1]:
for tok in self._cache[2][start_tok - self._cache[0] :]:
yield tok
start_tok += 1
if start_tok < self._toknum[-1]:
block_index = bisect.bisect_right(self._toknum, start_tok) - 1
toknum = self._toknum[block_index]
filepos = self._filepos[block_index]
else:
block_index = len(self._toknum) - 1
toknum = self._toknum[-1]
filepos = self._filepos[-1]
if self._stream is None:
self._open()
if self._eofpos == 0:
self._len = 0
while filepos < self._eofpos:
self._stream.seek(filepos)
self._current_toknum = toknum
self._current_blocknum = block_index
tokens = self.read_block(self._stream)
assert isinstance(tokens, (tuple, list, AbstractLazySequence)), (
""block reader %s() should return list or tuple.""
% self.read_block.__name__
)
num_toks = len(tokens)
new_filepos = self._stream.tell()
assert (
new_filepos > filepos
), ""block reader %s() should consume at least 1 byte (filepos=%d)"" % (
self.read_block.__name__,
filepos,
)
self._cache = (toknum, toknum + num_toks, list(tokens))
assert toknum <= self._toknum[-1]
if num_toks > 0:
block_index += 1
if toknum == self._toknum[-1]:
assert new_filepos > self._filepos[-1]  # monotonic!
self._filepos.append(new_filepos)
self._toknum.append(toknum + num_toks)
else:
assert (
new_filepos == self._filepos[block_index]
), ""inconsistent block reader (num chars read)""
assert (
toknum + num_toks == self._toknum[block_index]
), ""inconsistent block reader (num tokens returned)""
if new_filepos == self._eofpos:
self._len = toknum + num_toks
for tok in tokens[max(0, start_tok - toknum) :]:
yield tok
assert new_filepos <= self._eofpos
if new_filepos == self._eofpos:
break
toknum += num_toks
filepos = new_filepos
assert self._len is not None
self.close()
",[],0,[],/corpus/reader/util.py_iterate_from
4182,/home/amandapotts/git/nltk/nltk/corpus/reader/util.py___add__,"def __add__(self, other):
return concat([self, other])
",[],0,[],/corpus/reader/util.py___add__
4183,/home/amandapotts/git/nltk/nltk/corpus/reader/util.py___radd__,"def __radd__(self, other):
return concat([other, self])
",[],0,[],/corpus/reader/util.py___radd__
4184,/home/amandapotts/git/nltk/nltk/corpus/reader/util.py___mul__,"def __mul__(self, count):
return concat([self] * count)
",[],0,[],/corpus/reader/util.py___mul__
4185,/home/amandapotts/git/nltk/nltk/corpus/reader/util.py___rmul__,"def __rmul__(self, count):
return concat([self] * count)
",[],0,[],/corpus/reader/util.py___rmul__
4186,/home/amandapotts/git/nltk/nltk/corpus/reader/util.py___init__,"def __init__(self, corpus_views):
self._pieces = corpus_views
""""""A list of the corpus subviews that make up this
concatenation.""""""
self._offsets = [0]
""""""A list of offsets, indicating the index at which each
subview begins.  In particular::
offsets[i] = sum([len(p) for p in pieces[:i]])""""""
self._open_piece = None
""""""The most recently accessed corpus subview (or None).
Before a new subview is accessed, this subview will be closed.""""""
",[],0,[],/corpus/reader/util.py___init__
4187,/home/amandapotts/git/nltk/nltk/corpus/reader/util.py___len__,"def __len__(self):
if len(self._offsets) <= len(self._pieces):
for tok in self.iterate_from(self._offsets[-1]):
pass
return self._offsets[-1]
",[],0,[],/corpus/reader/util.py___len__
4188,/home/amandapotts/git/nltk/nltk/corpus/reader/util.py_close,"def close(self):
for piece in self._pieces:
piece.close()
",[],0,[],/corpus/reader/util.py_close
4189,/home/amandapotts/git/nltk/nltk/corpus/reader/util.py_iterate_from,"def iterate_from(self, start_tok):
piecenum = bisect.bisect_right(self._offsets, start_tok) - 1
while piecenum < len(self._pieces):
offset = self._offsets[piecenum]
piece = self._pieces[piecenum]
if self._open_piece is not piece:
if self._open_piece is not None:
self._open_piece.close()
self._open_piece = piece
yield from piece.iterate_from(max(0, start_tok - offset))
if piecenum + 1 == len(self._offsets):
self._offsets.append(self._offsets[-1] + len(piece))
piecenum += 1
",[],0,[],/corpus/reader/util.py_iterate_from
4190,/home/amandapotts/git/nltk/nltk/corpus/reader/util.py___init__,"def __init__(self, fileid, delete_on_gc=False):
""""""
Create a new corpus view that reads the pickle corpus
``fileid``.
:param delete_on_gc: If true, then ``fileid`` will be deleted
whenever this object gets garbage-collected.
""""""
self._delete_on_gc = delete_on_gc
StreamBackedCorpusView.__init__(self, fileid)
",[],0,[],/corpus/reader/util.py___init__
4191,/home/amandapotts/git/nltk/nltk/corpus/reader/util.py_read_block,"def read_block(self, stream):
result = []
for i in range(self.BLOCK_SIZE):
try:
result.append(pickle.load(stream))
except EOFError:
break
return result
",[],0,[],/corpus/reader/util.py_read_block
4192,/home/amandapotts/git/nltk/nltk/corpus/reader/util.py___del__,"def __del__(self):
""""""
If ``delete_on_gc`` was set to true when this
``PickleCorpusView`` was created, then delete the corpus view's
fileid.  (This method is called whenever a
``PickledCorpusView`` is garbage-collected.
""""""
if getattr(self, ""_delete_on_gc""):
if os.path.exists(self._fileid):
try:
os.remove(self._fileid)
except OSError:
pass
self.__dict__.clear()  # make the garbage collector's job easier
",[],0,[],/corpus/reader/util.py___del__
4193,/home/amandapotts/git/nltk/nltk/corpus/reader/util.py_write,"def write(cls, sequence, output_file):
if isinstance(output_file, str):
output_file = open(output_file, ""wb"")
for item in sequence:
pickle.dump(item, output_file, cls.PROTOCOL)
",[],0,[],/corpus/reader/util.py_write
4194,/home/amandapotts/git/nltk/nltk/corpus/reader/util.py_cache_to_tempfile,"def cache_to_tempfile(cls, sequence, delete_on_gc=True):
""""""
Write the given sequence to a temporary file as a pickle
corpus
temporary corpus file.
:param delete_on_gc: If true, then the temporary file will be
deleted whenever this object gets garbage-collected.
""""""
try:
fd, output_file_name = tempfile.mkstemp("".pcv"", ""nltk-"")
output_file = os.fdopen(fd, ""wb"")
cls.write(sequence, output_file)
output_file.close()
return PickleCorpusView(output_file_name, delete_on_gc)
except OSError as e:
raise ValueError(""Error while creating temp file: %s"" % e) from e
",[],0,[],/corpus/reader/util.py_cache_to_tempfile
4195,/home/amandapotts/git/nltk/nltk/corpus/reader/util.py_read_whitespace_block,"def read_whitespace_block(stream):
toks = []
for i in range(20):  # Read 20 lines at a time.
toks.extend(stream.readline().split())
return toks
",[],0,[],/corpus/reader/util.py_read_whitespace_block
4196,/home/amandapotts/git/nltk/nltk/corpus/reader/util.py_read_wordpunct_block,"def read_wordpunct_block(stream):
toks = []
for i in range(20):  # Read 20 lines at a time.
toks.extend(wordpunct_tokenize(stream.readline()))
return toks
",[],0,[],/corpus/reader/util.py_read_wordpunct_block
4197,/home/amandapotts/git/nltk/nltk/corpus/reader/util.py_read_line_block,"def read_line_block(stream):
toks = []
for i in range(20):
line = stream.readline()
if not line:
return toks
toks.append(line.rstrip(""\n""))
return toks
",[],0,[],/corpus/reader/util.py_read_line_block
4198,/home/amandapotts/git/nltk/nltk/corpus/reader/util.py_read_blankline_block,"def read_blankline_block(stream):
s = """"
while True:
line = stream.readline()
if not line:
if s:
return [s]
else:
return []
elif line and not line.strip():
if s:
return [s]
else:
s += line
",[],0,[],/corpus/reader/util.py_read_blankline_block
4199,/home/amandapotts/git/nltk/nltk/corpus/reader/util.py_read_alignedsent_block,"def read_alignedsent_block(stream):
s = """"
while True:
line = stream.readline()
if line[0] == ""="" or line[0] == ""\n"" or line[:2] == ""\r\n"":
continue
if not line:
if s:
return [s]
else:
return []
else:
s += line
if re.match(r""^\d+-\d+"", line) is not None:
return [s]
",[],0,[],/corpus/reader/util.py_read_alignedsent_block
4200,/home/amandapotts/git/nltk/nltk/corpus/reader/util.py_read_regexp_block,"def read_regexp_block(stream, start_re, end_re=None):
""""""
Read a sequence of tokens from a stream, where tokens begin with
lines that match ``start_re``.  If ``end_re`` is specified, then
tokens end with lines that match ``end_re``
whenever the next line matching ``start_re`` or EOF is found.
""""""
while True:
line = stream.readline()
if not line:
return []  # end of file.
if re.match(start_re, line):
break
lines = [line]
while True:
oldpos = stream.tell()
line = stream.readline()
if not line:
return ["""".join(lines)]
if end_re is not None and re.match(end_re, line):
return ["""".join(lines)]
if end_re is None and re.match(start_re, line):
stream.seek(oldpos)
return ["""".join(lines)]
lines.append(line)
",[],0,[],/corpus/reader/util.py_read_regexp_block
4201,/home/amandapotts/git/nltk/nltk/corpus/reader/util.py_read_sexpr_block,"def read_sexpr_block(stream, block_size=16384, comment_char=None):
""""""
Read a sequence of s-expressions from the stream, and leave the
stream's file position at the end the last complete s-expression
read.  This function will always return at least one s-expression,
unless there are no more s-expressions in the file.
If the file ends in in the middle of an s-expression, then that
incomplete s-expression is returned when the end of the file is
reached.
:param block_size: The default block size for reading.  If an
s-expression is longer than one block, then more than one
block will be read.
:param comment_char: A character that marks comments.  Any lines
that begin with this character will be stripped out.
(If spaces or tabs precede the comment character, then the
line will not be stripped.)
""""""
start = stream.tell()
block = stream.read(block_size)
encoding = getattr(stream, ""encoding"", None)
assert encoding is not None or isinstance(block, str)
if encoding not in (None, ""utf-8""):
import warnings
warnings.warn(
""Parsing may fail, depending on the properties ""
""of the %s encoding!"" % encoding
)
if comment_char:
COMMENT = re.compile(""(?m)^%s.*$"" % re.escape(comment_char))
while True:
try:
if comment_char:
block += stream.readline()
block = re.sub(COMMENT, _sub_space, block)
tokens, offset = _parse_sexpr_block(block)
offset = re.compile(r""\s*"").search(block, offset).end()
if encoding is None:
stream.seek(start + offset)
else:
stream.seek(start + len(block[:offset].encode(encoding)))
return tokens
except ValueError as e:
if e.args[0] == ""Block too small"":
next_block = stream.read(block_size)
if next_block:
block += next_block
continue
else:
return [block.strip()]
else:
raise
",[],0,[],/corpus/reader/util.py_read_sexpr_block
4202,/home/amandapotts/git/nltk/nltk/corpus/reader/util.py__sub_space,"def _sub_space(m):
""""""Helper function: given a regexp match, return a string of
spaces that's the same length as the matched string.""""""
return "" "" * (m.end() - m.start())
",[],0,[],/corpus/reader/util.py__sub_space
4203,/home/amandapotts/git/nltk/nltk/corpus/reader/util.py__parse_sexpr_block,"def _parse_sexpr_block(block):
tokens = []
start = end = 0
while end < len(block):
m = re.compile(r""\S"").search(block, end)
if not m:
return tokens, end
start = m.start()
if m.group() != ""("":
m2 = re.compile(r""[\s(]"").search(block, start)
if m2:
end = m2.start()
else:
if tokens:
return tokens, end
raise ValueError(""Block too small"")
else:
nesting = 0
for m in re.compile(r""[()]"").finditer(block, start):
if m.group() == ""("":
nesting += 1
else:
nesting -= 1
if nesting == 0:
end = m.end()
break
else:
if tokens:
return tokens, end
raise ValueError(""Block too small"")
tokens.append(block[start:end])
return tokens, end
",[],0,[],/corpus/reader/util.py__parse_sexpr_block
4204,/home/amandapotts/git/nltk/nltk/corpus/reader/util.py_find_corpus_fileids,"def find_corpus_fileids(root, regexp):
if not isinstance(root, PathPointer):
raise TypeError(""find_corpus_fileids: expected a PathPointer"")
regexp += ""$""
if isinstance(root, ZipFilePathPointer):
fileids = [
name[len(root.entry) :]
for name in root.zipfile.namelist()
if not name.endswith(""/"")
]
items = [name for name in fileids if re.match(regexp, name)]
return sorted(items)
elif isinstance(root, FileSystemPathPointer):
items = []
for dirname, subdirs, fileids in os.walk(root.path):
prefix = """".join(""%s/"" % p for p in _path_from(root.path, dirname))
items += [
prefix + fileid
for fileid in fileids
if re.match(regexp, prefix + fileid)
]
if "".svn"" in subdirs:
subdirs.remove("".svn"")
return sorted(items)
else:
raise AssertionError(""Don't know how to handle %r"" % root)
",[],0,[],/corpus/reader/util.py_find_corpus_fileids
4205,/home/amandapotts/git/nltk/nltk/corpus/reader/util.py__path_from,"def _path_from(parent, child):
if os.path.split(parent)[1] == """":
parent = os.path.split(parent)[0]
path = []
while parent != child:
child, dirname = os.path.split(child)
path.insert(0, dirname)
assert os.path.split(child)[0] != child
return path
",[],0,[],/corpus/reader/util.py__path_from
4206,/home/amandapotts/git/nltk/nltk/corpus/reader/util.py_tagged_treebank_para_block_reader,"def tagged_treebank_para_block_reader(stream):
para = """"
while True:
line = stream.readline()
if re.match(r""======+\s*$"", line):
if para.strip():
return [para]
elif line == """":
if para.strip():
return [para]
else:
return []
else:
para += line
",[],0,[],/corpus/reader/util.py_tagged_treebank_para_block_reader
4207,/home/amandapotts/git/nltk/nltk/corpus/reader/bcp47.py___init__,"def __init__(self, root, fileids):
""""""Read the BCP-47 database""""""
super().__init__(root, fileids)
self.langcode = {}
with self.open(""iana/language-subtag-registry.txt"") as fp:
self.db = self.data_dict(fp.read().split(""%%\n""))
with self.open(""cldr/common-subdivisions-en.xml"") as fp:
self.subdiv = self.subdiv_dict(
et.parse(fp).iterfind(""localeDisplayNames/subdivisions/subdivision"")
)
self.morphology()
",[],0,[],/corpus/reader/bcp47.py___init__
4208,/home/amandapotts/git/nltk/nltk/corpus/reader/bcp47.py_load_wiki_q,"def load_wiki_q(self):
""""""Load conversion table to Wikidata Q-codes (only if needed)""""""
with self.open(""cldr/tools-cldr-rdf-external-entityToCode.tsv"") as fp:
self.wiki_q = self.wiki_dict(fp.read().strip().split(""\n"")[1:])
",[],0,[],/corpus/reader/bcp47.py_load_wiki_q
4209,/home/amandapotts/git/nltk/nltk/corpus/reader/bcp47.py_wiki_dict,"def wiki_dict(self, lines):
""""""Convert Wikidata list of Q-codes to a BCP-47 dictionary""""""
return {
pair[1]: pair[0].split(""/"")[-1]
for pair in [line.strip().split(""\t"") for line in lines]
}
",[],0,[],/corpus/reader/bcp47.py_wiki_dict
4210,/home/amandapotts/git/nltk/nltk/corpus/reader/bcp47.py_subdiv_dict,"def subdiv_dict(self, subdivs):
""""""Convert the CLDR subdivisions list to a dictionary""""""
return {sub.attrib[""type""]: sub.text for sub in subdivs}
",[],0,[],/corpus/reader/bcp47.py_subdiv_dict
4211,/home/amandapotts/git/nltk/nltk/corpus/reader/bcp47.py_morphology,"def morphology(self):
self.casing = {
""language"": str.lower,
""extlang"": str.lower,
""script"": str.title,
""region"": str.upper,
""variant"": str.lower,
}
dig = ""[0-9]""
low = ""[a-z]""
up = ""[A-Z]""
alnum = ""[a-zA-Z0-9]""
self.format = {
""language"": re.compile(f""{low*3}?""),
""extlang"": re.compile(f""{low*3}""),
""script"": re.compile(f""{up}{low*3}""),
""region"": re.compile(f""({up*2})|({dig*3})""),
""variant"": re.compile(f""{alnum*4}{(alnum+'?')*4}""),
""singleton"": re.compile(f""{low}""),
}
",[],0,[],/corpus/reader/bcp47.py_morphology
4212,/home/amandapotts/git/nltk/nltk/corpus/reader/bcp47.py_data_dict,"def data_dict(self, records):
""""""Convert the BCP-47 language subtag registry to a dictionary""""""
self.version = records[0].replace(""File-Date:"", """").strip()
dic = {}
dic[""deprecated""] = {}
for label in [
""language"",
""extlang"",
""script"",
""region"",
""variant"",
""redundant"",
""grandfathered"",
]:
dic[""deprecated""][label] = {}
for record in records[1:]:
fields = [field.split("": "") for field in record.strip().split(""\n"")]
typ = fields[0][1]
tag = fields[1][1]
if typ not in dic:
dic[typ] = {}
subfields = {}
for field in fields[2:]:
if len(field) == 2:
[key, val] = field
if key not in subfields:
subfields[key] = [val]
else:  # multiple value
subfields[key].append(val)
else:  # multiline field
subfields[key][-1] += "" "" + field[0].strip()
if (
""Deprecated"" not in record
and typ == ""language""
and key == ""Description""
):
self.langcode[subfields[key][-1]] = tag
for key in subfields:
if len(subfields[key]) == 1:  # single value
subfields[key] = subfields[key][0]
if ""Deprecated"" in record:
dic[""deprecated""][typ][tag] = subfields
else:
dic[typ][tag] = subfields
return dic
",[],0,[],/corpus/reader/bcp47.py_data_dict
4213,/home/amandapotts/git/nltk/nltk/corpus/reader/bcp47.py_val2str,"def val2str(self, val):
""""""Return only first value""""""
if type(val) == list:
val = val[0]
return val
",[],0,[],/corpus/reader/bcp47.py_val2str
4214,/home/amandapotts/git/nltk/nltk/corpus/reader/bcp47.py_lang2str,"def lang2str(self, lg_record):
""""""Concatenate subtag values""""""
name = f""{lg_record['language']}""
for label in [""extlang"", ""script"", ""region"", ""variant"", ""extension""]:
if label in lg_record:
name += f"": {lg_record[label]}""
return name
",[],0,[],/corpus/reader/bcp47.py_lang2str
4215,/home/amandapotts/git/nltk/nltk/corpus/reader/bcp47.py_parse_tag,"def parse_tag(self, tag):
""""""Convert a BCP-47 tag to a dictionary of labelled subtags""""""
subtags = tag.split(""-"")
lang = {}
labels = [""language"", ""extlang"", ""script"", ""region"", ""variant"", ""variant""]
while subtags and labels:
subtag = subtags.pop(0)
found = False
while labels:
label = labels.pop(0)
subtag = self.casing[label](subtag)
if self.format[label].fullmatch(subtag):
if subtag in self.db[label]:
found = True
valstr = self.val2str(self.db[label][subtag][""Description""])
if label == ""variant"" and label in lang:
lang[label] += "": "" + valstr
else:
lang[label] = valstr
break
elif subtag in self.db[""deprecated""][label]:
found = True
note = f""The {subtag!r} {label} code is deprecated""
if ""Preferred-Value"" in self.db[""deprecated""][label][subtag]:
prefer = self.db[""deprecated""][label][subtag][
""Preferred-Value""
]
note += f""', prefer '{self.val2str(prefer)}'""
lang[label] = self.val2str(
self.db[""deprecated""][label][subtag][""Description""]
)
warn(note)
break
if not found:
if subtag == ""u"" and subtags[0] == ""sd"":  # CLDR regional subdivisions
sd = subtags[1]
if sd in self.subdiv:
ext = self.subdiv[sd]
else:
ext = f""<Unknown subdivision: {ext}>""
else:  # other extension subtags are not supported yet
ext = f""{subtag}{''.join(['-'+ext for ext in subtags])}"".lower()
if not self.format[""singleton""].fullmatch(subtag):
ext = f""<Invalid extension: {ext}>""
warn(ext)
lang[""extension""] = ext
subtags = []
return lang
",[],0,[],/corpus/reader/bcp47.py_parse_tag
4216,/home/amandapotts/git/nltk/nltk/corpus/reader/bcp47.py_name,"def name(self, tag):
""""""
Convert a BCP-47 tag to a colon-separated string of subtag names
>>> from nltk.corpus import bcp47
>>> bcp47.name('ca-Latn-ES-valencia')
'Catalan: Latin: Spain: Valencian'
""""""
for label in [""redundant"", ""grandfathered""]:
val = None
if tag in self.db[label]:
val = f""{self.db[label][tag]['Description']}""
note = f""The {tag!r} code is {label}""
elif tag in self.db[""deprecated""][label]:
val = f""{self.db['deprecated'][label][tag]['Description']}""
note = f""The {tag!r} code is {label} and deprecated""
if ""Preferred-Value"" in self.db[""deprecated""][label][tag]:
prefer = self.db[""deprecated""][label][tag][""Preferred-Value""]
note += f"", prefer {self.val2str(prefer)!r}""
if val:
warn(note)
return val
try:
return self.lang2str(self.parse_tag(tag))
except:
warn(f""Tag {tag!r} was not recognized"")
return None
",[],0,[],/corpus/reader/bcp47.py_name
4217,/home/amandapotts/git/nltk/nltk/corpus/reader/crubadan.py___init__,"def __init__(self, root, fileids, encoding=""utf8"", tagset=None):
super().__init__(root, fileids, encoding=""utf8"")
self._lang_mapping_data = []
self._load_lang_mapping_data()
",[],0,[],/corpus/reader/crubadan.py___init__
4218,/home/amandapotts/git/nltk/nltk/corpus/reader/crubadan.py_lang_freq,"def lang_freq(self, lang):
""""""Return n-gram FreqDist for a specific language
given ISO 639-3 language code""""""
if lang not in self._all_lang_freq:
self._all_lang_freq[lang] = self._load_lang_ngrams(lang)
return self._all_lang_freq[lang]
",[],0,[],/corpus/reader/crubadan.py_lang_freq
4219,/home/amandapotts/git/nltk/nltk/corpus/reader/crubadan.py_langs,"def langs(self):
""""""Return a list of supported languages as ISO 639-3 codes""""""
return [row[1] for row in self._lang_mapping_data]
",[],0,[],/corpus/reader/crubadan.py_langs
4220,/home/amandapotts/git/nltk/nltk/corpus/reader/crubadan.py_iso_to_crubadan,"def iso_to_crubadan(self, lang):
""""""Return internal Crubadan code based on ISO 639-3 code""""""
for i in self._lang_mapping_data:
if i[1].lower() == lang.lower():
return i[0]
",[],0,[],/corpus/reader/crubadan.py_iso_to_crubadan
4221,/home/amandapotts/git/nltk/nltk/corpus/reader/crubadan.py_crubadan_to_iso,"def crubadan_to_iso(self, lang):
""""""Return ISO 639-3 code given internal Crubadan code""""""
for i in self._lang_mapping_data:
if i[0].lower() == lang.lower():
return i[1]
",[],0,[],/corpus/reader/crubadan.py_crubadan_to_iso
4222,/home/amandapotts/git/nltk/nltk/corpus/reader/crubadan.py__load_lang_mapping_data,"def _load_lang_mapping_data(self):
""""""Load language mappings between codes and description from table.txt""""""
if isinstance(self.root, ZipFilePathPointer):
raise RuntimeError(
""Please install the 'crubadan' corpus first, use nltk.download()""
)
mapper_file = path.join(self.root, self._LANG_MAPPER_FILE)
if self._LANG_MAPPER_FILE not in self.fileids():
raise RuntimeError(""Could not find language mapper file: "" + mapper_file)
with open(mapper_file, encoding=""utf-8"") as raw:
strip_raw = raw.read().strip()
self._lang_mapping_data = [row.split(""\t"") for row in strip_raw.split(""\n"")]
",[],0,[],/corpus/reader/crubadan.py__load_lang_mapping_data
4223,/home/amandapotts/git/nltk/nltk/corpus/reader/crubadan.py__load_lang_ngrams,"def _load_lang_ngrams(self, lang):
""""""Load single n-gram language file given the ISO 639-3 language code
and return its FreqDist""""""
if lang not in self.langs():
raise RuntimeError(""Unsupported language."")
crubadan_code = self.iso_to_crubadan(lang)
ngram_file = path.join(self.root, crubadan_code + ""-3grams.txt"")
if not path.isfile(ngram_file):
raise RuntimeError(""No N-gram file found for requested language."")
counts = FreqDist()
with open(ngram_file, encoding=""utf-8"") as f:
for line in f:
data = line.split("" "")
ngram = data[1].strip(""\n"")
freq = int(data[0])
counts[ngram] = freq
return counts
",[],0,[],/corpus/reader/crubadan.py__load_lang_ngrams
4224,/home/amandapotts/git/nltk/nltk/corpus/reader/nps_chat.py___init__,"def __init__(self, root, fileids, wrap_etree=False, tagset=None):
XMLCorpusReader.__init__(self, root, fileids, wrap_etree)
self._tagset = tagset
",[],0,[],/corpus/reader/nps_chat.py___init__
4225,/home/amandapotts/git/nltk/nltk/corpus/reader/nps_chat.py_xml_posts,"def xml_posts(self, fileids=None):
if self._wrap_etree:
return concat(
[
XMLCorpusView(fileid, ""Session/Posts/Post"", self._wrap_elt)
for fileid in self.abspaths(fileids)
]
)
else:
return concat(
[
XMLCorpusView(fileid, ""Session/Posts/Post"")
for fileid in self.abspaths(fileids)
]
)
",[],0,[],/corpus/reader/nps_chat.py_xml_posts
4226,/home/amandapotts/git/nltk/nltk/corpus/reader/nps_chat.py_posts,"def posts(self, fileids=None):
return concat(
[
XMLCorpusView(
fileid, ""Session/Posts/Post/terminals"", self._elt_to_words
)
for fileid in self.abspaths(fileids)
]
)
",[],0,[],/corpus/reader/nps_chat.py_posts
4227,/home/amandapotts/git/nltk/nltk/corpus/reader/nps_chat.py_tagged_posts,"def tagged_posts(self, fileids=None, tagset=None):
",[],0,[],/corpus/reader/nps_chat.py_tagged_posts
4228,/home/amandapotts/git/nltk/nltk/corpus/reader/nps_chat.py_reader,"def reader(elt, handler):
return self._elt_to_tagged_words(elt, handler, tagset)
",[],0,[],/corpus/reader/nps_chat.py_reader
4229,/home/amandapotts/git/nltk/nltk/corpus/reader/nps_chat.py_words,"def words(self, fileids=None):
return LazyConcatenation(self.posts(fileids))
",[],0,[],/corpus/reader/nps_chat.py_words
4230,/home/amandapotts/git/nltk/nltk/corpus/reader/nps_chat.py_tagged_words,"def tagged_words(self, fileids=None, tagset=None):
return LazyConcatenation(self.tagged_posts(fileids, tagset))
",[],0,[],/corpus/reader/nps_chat.py_tagged_words
4231,/home/amandapotts/git/nltk/nltk/corpus/reader/nps_chat.py__wrap_elt,"def _wrap_elt(self, elt, handler):
return ElementWrapper(elt)
",[],0,[],/corpus/reader/nps_chat.py__wrap_elt
4232,/home/amandapotts/git/nltk/nltk/corpus/reader/nps_chat.py__elt_to_words,"def _elt_to_words(self, elt, handler):
return [self._simplify_username(t.attrib[""word""]) for t in elt.findall(""t"")]
",[],0,[],/corpus/reader/nps_chat.py__elt_to_words
4233,/home/amandapotts/git/nltk/nltk/corpus/reader/nps_chat.py__elt_to_tagged_words,"def _elt_to_tagged_words(self, elt, handler, tagset=None):
tagged_post = [
(self._simplify_username(t.attrib[""word""]), t.attrib[""pos""])
for t in elt.findall(""t"")
]
if tagset and tagset != self._tagset:
tagged_post = [
(w, map_tag(self._tagset, tagset, t)) for (w, t) in tagged_post
]
return tagged_post
",[],0,[],/corpus/reader/nps_chat.py__elt_to_tagged_words
4234,/home/amandapotts/git/nltk/nltk/corpus/reader/nps_chat.py__simplify_username,"def _simplify_username(word):
if ""User"" in word:
word = ""U"" + word.split(""User"", 1)[1]
elif isinstance(word, bytes):
word = word.decode(""ascii"")
return word
",[],0,[],/corpus/reader/nps_chat.py__simplify_username
4235,/home/amandapotts/git/nltk/nltk/corpus/reader/toolbox.py_xml,"def xml(self, fileids, key=None):
return concat(
[
ToolboxData(path, enc).parse(key=key)
for (path, enc) in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/toolbox.py_xml
4236,/home/amandapotts/git/nltk/nltk/corpus/reader/toolbox.py_fields,"def fields(
self,
fileids,
strip=True,
unwrap=True,
encoding=""utf8"",
errors=""strict"",
unicode_fields=None,
",[],0,[],/corpus/reader/toolbox.py_fields
4237,/home/amandapotts/git/nltk/nltk/corpus/reader/toolbox.py_entries,"def entries(self, fileids, **kwargs):
if ""key"" in kwargs:
key = kwargs[""key""]
del kwargs[""key""]
else:
key = ""lx""  # the default key in MDF
entries = []
for marker, contents in self.fields(fileids, **kwargs):
if marker == key:
entries.append((contents, []))
else:
try:
entries[-1][-1].append((marker, contents))
except IndexError:
pass
return entries
",[],0,[],/corpus/reader/toolbox.py_entries
4238,/home/amandapotts/git/nltk/nltk/corpus/reader/toolbox.py_words,"def words(self, fileids, key=""lx""):
return [contents for marker, contents in self.fields(fileids) if marker == key]
",[],0,[],/corpus/reader/toolbox.py_words
4239,/home/amandapotts/git/nltk/nltk/corpus/reader/toolbox.py_demo,"def demo():
pass
",[],0,[],/corpus/reader/toolbox.py_demo
4240,/home/amandapotts/git/nltk/nltk/corpus/reader/semcor.py___init__,"def __init__(self, root, fileids, wordnet, lazy=True):
XMLCorpusReader.__init__(self, root, fileids)
self._lazy = lazy
self._wordnet = wordnet
",[],0,[],/corpus/reader/semcor.py___init__
4241,/home/amandapotts/git/nltk/nltk/corpus/reader/semcor.py_words,"def words(self, fileids=None):
""""""
:return: the given file(s) as a list of words and punctuation symbols.
:rtype: list(str)
""""""
return self._items(fileids, ""word"", False, False, False)
",[],0,[],/corpus/reader/semcor.py_words
4242,/home/amandapotts/git/nltk/nltk/corpus/reader/semcor.py_chunks,"def chunks(self, fileids=None):
""""""
:return: the given file(s) as a list of chunks,
each of which is a list of words and punctuation symbols
that form a unit.
:rtype: list(list(str))
""""""
return self._items(fileids, ""chunk"", False, False, False)
",[],0,[],/corpus/reader/semcor.py_chunks
4243,/home/amandapotts/git/nltk/nltk/corpus/reader/semcor.py_tagged_chunks,"def tagged_chunks(self, fileids=None, tag=(""pos"" or ""sem"" or ""both"")):
""""""
:return: the given file(s) as a list of tagged chunks, represented
in tree form.
:rtype: list(Tree)
:param tag: `'pos'` (part of speech), `'sem'` (semantic), or `'both'`
to indicate the kind of tags to include.  Semantic tags consist of
WordNet lemma IDs, plus an `'NE'` node if the chunk is a named entity
without a specific entry in WordNet.  (Named entities of type 'other'
have no lemma.  Other chunks not in WordNet have no semantic tag.
Punctuation tokens have `None` for their part of speech tag.)
""""""
return self._items(fileids, ""chunk"", False, tag != ""sem"", tag != ""pos"")
",[],0,[],/corpus/reader/semcor.py_tagged_chunks
4244,/home/amandapotts/git/nltk/nltk/corpus/reader/semcor.py_sents,"def sents(self, fileids=None):
""""""
:return: the given file(s) as a list of sentences, each encoded
as a list of word strings.
:rtype: list(list(str))
""""""
return self._items(fileids, ""word"", True, False, False)
",[],0,[],/corpus/reader/semcor.py_sents
4245,/home/amandapotts/git/nltk/nltk/corpus/reader/semcor.py_chunk_sents,"def chunk_sents(self, fileids=None):
""""""
:return: the given file(s) as a list of sentences, each encoded
as a list of chunks.
:rtype: list(list(list(str)))
""""""
return self._items(fileids, ""chunk"", True, False, False)
",[],0,[],/corpus/reader/semcor.py_chunk_sents
4246,/home/amandapotts/git/nltk/nltk/corpus/reader/semcor.py_tagged_sents,"def tagged_sents(self, fileids=None, tag=(""pos"" or ""sem"" or ""both"")):
""""""
:return: the given file(s) as a list of sentences. Each sentence
is represented as a list of tagged chunks (in tree form).
:rtype: list(list(Tree))
:param tag: `'pos'` (part of speech), `'sem'` (semantic), or `'both'`
to indicate the kind of tags to include.  Semantic tags consist of
WordNet lemma IDs, plus an `'NE'` node if the chunk is a named entity
without a specific entry in WordNet.  (Named entities of type 'other'
have no lemma.  Other chunks not in WordNet have no semantic tag.
Punctuation tokens have `None` for their part of speech tag.)
""""""
return self._items(fileids, ""chunk"", True, tag != ""sem"", tag != ""pos"")
",[],0,[],/corpus/reader/semcor.py_tagged_sents
4247,/home/amandapotts/git/nltk/nltk/corpus/reader/semcor.py__items,"def _items(self, fileids, unit, bracket_sent, pos_tag, sem_tag):
if unit == ""word"" and not bracket_sent:
",[],0,[],/corpus/reader/semcor.py__items
4248,/home/amandapotts/git/nltk/nltk/corpus/reader/semcor.py__words,"def _words(self, fileid, unit, bracket_sent, pos_tag, sem_tag):
""""""
Helper used to implement the view methods -- returns a list of
tokens, (segmented) words, chunks, or sentences. The tokens
and chunks may optionally be tagged (with POS and sense
information).
:param fileid: The name of the underlying file.
:param unit: One of `'token'`, `'word'`, or `'chunk'`.
:param bracket_sent: If true, include sentence bracketing.
:param pos_tag: Whether to include part-of-speech tags.
:param sem_tag: Whether to include semantic tags, namely WordNet lemma
and OOV named entity status.
""""""
assert unit in (""token"", ""word"", ""chunk"")
result = []
xmldoc = ElementTree.parse(fileid).getroot()
for xmlsent in xmldoc.findall("".//s""):
sent = []
for xmlword in _all_xmlwords_in(xmlsent):
itm = SemcorCorpusReader._word(
xmlword, unit, pos_tag, sem_tag, self._wordnet
)
if unit == ""word"":
sent.extend(itm)
else:
sent.append(itm)
if bracket_sent:
result.append(SemcorSentence(xmlsent.attrib[""snum""], sent))
else:
result.extend(sent)
assert None not in result
return result
",[],0,[],/corpus/reader/semcor.py__words
4249,/home/amandapotts/git/nltk/nltk/corpus/reader/semcor.py__word,"def _word(xmlword, unit, pos_tag, sem_tag, wordnet):
tkn = xmlword.text
if not tkn:
tkn = """"  # fixes issue 337?
lemma = xmlword.get(""lemma"", tkn)  # lemma or NE class
lexsn = xmlword.get(""lexsn"")  # lex_sense (locator for the lemma's sense)
if lexsn is not None:
sense_key = lemma + ""%"" + lexsn
wnpos = (""n"", ""v"", ""a"", ""r"", ""s"")[
int(lexsn.split("":"")[0]) - 1
]  # see http://wordnet.princeton.edu/man/senseidx.5WN.html
else:
sense_key = wnpos = None
redef = xmlword.get(
""rdf"", tkn
)  # redefinition--this indicates the lookup string
sensenum = xmlword.get(""wnsn"")  # WordNet sense number
isOOVEntity = ""pn"" in xmlword.keys()  # a ""personal name"" (NE) not in WordNet
pos = xmlword.get(
""pos""
)  # part of speech for the whole chunk (None for punctuation)
if unit == ""token"":
if not pos_tag and not sem_tag:
itm = tkn
else:
itm = (
(tkn,)
+ ((pos,) if pos_tag else ())
+ ((lemma, wnpos, sensenum, isOOVEntity) if sem_tag else ())
)
return itm
else:
ww = tkn.split(""_"")  # TODO: case where punctuation intervenes in MWE
if unit == ""word"":
return ww
else:
if sensenum is not None:
try:
sense = wordnet.lemma_from_key(sense_key)  # Lemma object
except Exception:
try:
sense = ""%s.%s.%02d"" % (
lemma,
wnpos,
int(sensenum),
)  # e.g.: reach.v.02
except ValueError:
sense = (
lemma + ""."" + wnpos + ""."" + sensenum
)  # e.g. the sense number may be ""2
bottom = [Tree(pos, ww)] if pos_tag else ww
if sem_tag and isOOVEntity:
if sensenum is not None:
return Tree(sense, [Tree(""NE"", bottom)])
else:  # 'other' NE
return Tree(""NE"", bottom)
elif sem_tag and sensenum is not None:
return Tree(sense, bottom)
elif pos_tag:
return bottom[0]
else:
return bottom  # chunk as a list
",[],0,[],/corpus/reader/semcor.py__word
4250,/home/amandapotts/git/nltk/nltk/corpus/reader/semcor.py__all_xmlwords_in,"def _all_xmlwords_in(elt, result=None):
if result is None:
result = []
for child in elt:
if child.tag in (""wf"", ""punc""):
result.append(child)
else:
_all_xmlwords_in(child, result)
return result
",[],0,[],/corpus/reader/semcor.py__all_xmlwords_in
4251,/home/amandapotts/git/nltk/nltk/corpus/reader/semcor.py___init__,"def __init__(self, num, items):
self.num = num
list.__init__(self, items)
",[],0,[],/corpus/reader/semcor.py___init__
4252,/home/amandapotts/git/nltk/nltk/corpus/reader/semcor.py___init__,"def __init__(self, fileid, unit, bracket_sent, pos_tag, sem_tag, wordnet):
""""""
:param fileid: The name of the underlying file.
:param unit: One of `'token'`, `'word'`, or `'chunk'`.
:param bracket_sent: If true, include sentence bracketing.
:param pos_tag: Whether to include part-of-speech tags.
:param sem_tag: Whether to include semantic tags, namely WordNet lemma
and OOV named entity status.
""""""
if bracket_sent:
tagspec = "".*/s""
else:
tagspec = "".*/s/(punc|wf)""
self._unit = unit
self._sent = bracket_sent
self._pos_tag = pos_tag
self._sem_tag = sem_tag
self._wordnet = wordnet
XMLCorpusView.__init__(self, fileid, tagspec)
",[],0,[],/corpus/reader/semcor.py___init__
4253,/home/amandapotts/git/nltk/nltk/corpus/reader/semcor.py_handle_elt,"def handle_elt(self, elt, context):
if self._sent:
return self.handle_sent(elt)
else:
return self.handle_word(elt)
",[],0,[],/corpus/reader/semcor.py_handle_elt
4254,/home/amandapotts/git/nltk/nltk/corpus/reader/semcor.py_handle_word,"def handle_word(self, elt):
return SemcorCorpusReader._word(
elt, self._unit, self._pos_tag, self._sem_tag, self._wordnet
)
",[],0,[],/corpus/reader/semcor.py_handle_word
4255,/home/amandapotts/git/nltk/nltk/corpus/reader/semcor.py_handle_sent,"def handle_sent(self, elt):
sent = []
for child in elt:
if child.tag in (""wf"", ""punc""):
itm = self.handle_word(child)
if self._unit == ""word"":
sent.extend(itm)
else:
sent.append(itm)
else:
raise ValueError(""Unexpected element %s"" % child.tag)
return SemcorSentence(elt.attrib[""snum""], sent)
",[],0,[],/corpus/reader/semcor.py_handle_sent
4256,/home/amandapotts/git/nltk/nltk/corpus/reader/pl196x.py___init__,"def __init__(
self,
corpus_file,
tagged,
group_by_sent,
group_by_para,
tagset=None,
head_len=0,
textids=None,
",[],0,[],/corpus/reader/pl196x.py___init__
4257,/home/amandapotts/git/nltk/nltk/corpus/reader/pl196x.py_read_block,"def read_block(self, stream):
block = stream.readlines(self._pagesize)
block = concat(block)
while (block.count(""<text id"") > block.count(""</text>"")) or block.count(
""<text id""
) == 0:
tmp = stream.readline()
if len(tmp) <= 0:
break
block += tmp
block = block.replace(""\n"", """")
textids = TEXTID.findall(block)
if self._textids:
for tid in textids:
if tid not in self._textids:
beg = block.find(tid) - 1
end = block[beg:].find(""</text>"") + len(""</text>"")
block = block[:beg] + block[beg + end :]
output = []
for para_str in PARA.findall(block):
para = []
for sent_str in SENT.findall(para_str):
if not self._tagged:
sent = WORD.findall(sent_str)
else:
sent = list(map(self._parse_tag, TAGGEDWORD.findall(sent_str)))
if self._group_by_sent:
para.append(sent)
else:
para.extend(sent)
if self._group_by_para:
output.append(para)
else:
output.extend(para)
return output
",[],0,[],/corpus/reader/pl196x.py_read_block
4258,/home/amandapotts/git/nltk/nltk/corpus/reader/pl196x.py__parse_tag,"def _parse_tag(self, tag_word_tuple):
(tag, word) = tag_word_tuple
if tag.startswith(""w""):
tag = ANA.search(tag).group(1)
else:  # tag.startswith('c')
tag = TYPE.search(tag).group(1)
return word, tag
",[],0,[],/corpus/reader/pl196x.py__parse_tag
4259,/home/amandapotts/git/nltk/nltk/corpus/reader/pl196x.py___init__,"def __init__(self, *args, **kwargs):
if ""textid_file"" in kwargs:
self._textids = kwargs[""textid_file""]
else:
self._textids = None
XMLCorpusReader.__init__(self, *args)
CategorizedCorpusReader.__init__(self, kwargs)
self._init_textids()
",[],0,[],/corpus/reader/pl196x.py___init__
4260,/home/amandapotts/git/nltk/nltk/corpus/reader/pl196x.py__init_textids,"def _init_textids(self):
self._f2t = defaultdict(list)
self._t2f = defaultdict(list)
if self._textids is not None:
with open(self._textids) as fp:
for line in fp:
line = line.strip()
file_id, text_ids = line.split("" "", 1)
if file_id not in self.fileids():
raise ValueError(
""In text_id mapping file %s: %s not found""
% (self._textids, file_id)
)
for text_id in text_ids.split(self._delimiter):
self._add_textids(file_id, text_id)
",[],0,[],/corpus/reader/pl196x.py__init_textids
4261,/home/amandapotts/git/nltk/nltk/corpus/reader/pl196x.py__add_textids,"def _add_textids(self, file_id, text_id):
self._f2t[file_id].append(text_id)
self._t2f[text_id].append(file_id)
",[],0,[],/corpus/reader/pl196x.py__add_textids
4262,/home/amandapotts/git/nltk/nltk/corpus/reader/pl196x.py_decode_tag,"def decode_tag(self, tag):
return tag
",[],0,[],/corpus/reader/pl196x.py_decode_tag
4263,/home/amandapotts/git/nltk/nltk/corpus/reader/pl196x.py_textids,"def textids(self, fileids=None, categories=None):
""""""
In the pl196x corpus each category is stored in single
file and thus both methods provide identical functionality. In order
to accommodate finer granularity, a non-standard textids() method was
implemented. All the main functions can be supplied with a list
of required chunks---giving much more control to the user.
""""""
fileids, _ = self._resolve(fileids, categories)
if fileids is None:
return sorted(self._t2f)
if isinstance(fileids, str):
fileids = [fileids]
return sorted(sum((self._f2t[d] for d in fileids), []))
",[],0,[],/corpus/reader/pl196x.py_textids
4264,/home/amandapotts/git/nltk/nltk/corpus/reader/pl196x.py_words,"def words(self, fileids=None, categories=None, textids=None):
fileids, textids = self._resolve(fileids, categories, textids)
if fileids is None:
fileids = self._fileids
elif isinstance(fileids, str):
fileids = [fileids]
if textids:
return concat(
[
TEICorpusView(
self.abspath(fileid),
False,
False,
False,
head_len=self.head_len,
textids=textids[fileid],
)
for fileid in fileids
]
)
else:
return concat(
[
TEICorpusView(
self.abspath(fileid),
False,
False,
False,
head_len=self.head_len,
)
for fileid in fileids
]
)
",[],0,[],/corpus/reader/pl196x.py_words
4265,/home/amandapotts/git/nltk/nltk/corpus/reader/pl196x.py_sents,"def sents(self, fileids=None, categories=None, textids=None):
fileids, textids = self._resolve(fileids, categories, textids)
if fileids is None:
fileids = self._fileids
elif isinstance(fileids, str):
fileids = [fileids]
if textids:
return concat(
[
TEICorpusView(
self.abspath(fileid),
False,
True,
False,
head_len=self.head_len,
textids=textids[fileid],
)
for fileid in fileids
]
)
else:
return concat(
[
TEICorpusView(
self.abspath(fileid), False, True, False, head_len=self.head_len
)
for fileid in fileids
]
)
",[],0,[],/corpus/reader/pl196x.py_sents
4266,/home/amandapotts/git/nltk/nltk/corpus/reader/pl196x.py_paras,"def paras(self, fileids=None, categories=None, textids=None):
fileids, textids = self._resolve(fileids, categories, textids)
if fileids is None:
fileids = self._fileids
elif isinstance(fileids, str):
fileids = [fileids]
if textids:
return concat(
[
TEICorpusView(
self.abspath(fileid),
False,
True,
True,
head_len=self.head_len,
textids=textids[fileid],
)
for fileid in fileids
]
)
else:
return concat(
[
TEICorpusView(
self.abspath(fileid), False, True, True, head_len=self.head_len
)
for fileid in fileids
]
)
",[],0,[],/corpus/reader/pl196x.py_paras
4267,/home/amandapotts/git/nltk/nltk/corpus/reader/pl196x.py_tagged_words,"def tagged_words(self, fileids=None, categories=None, textids=None):
fileids, textids = self._resolve(fileids, categories, textids)
if fileids is None:
fileids = self._fileids
elif isinstance(fileids, str):
fileids = [fileids]
if textids:
return concat(
[
TEICorpusView(
self.abspath(fileid),
True,
False,
False,
head_len=self.head_len,
textids=textids[fileid],
)
for fileid in fileids
]
)
else:
return concat(
[
TEICorpusView(
self.abspath(fileid), True, False, False, head_len=self.head_len
)
for fileid in fileids
]
)
",[],0,[],/corpus/reader/pl196x.py_tagged_words
4268,/home/amandapotts/git/nltk/nltk/corpus/reader/pl196x.py_tagged_sents,"def tagged_sents(self, fileids=None, categories=None, textids=None):
fileids, textids = self._resolve(fileids, categories, textids)
if fileids is None:
fileids = self._fileids
elif isinstance(fileids, str):
fileids = [fileids]
if textids:
return concat(
[
TEICorpusView(
self.abspath(fileid),
True,
True,
False,
head_len=self.head_len,
textids=textids[fileid],
)
for fileid in fileids
]
)
else:
return concat(
[
TEICorpusView(
self.abspath(fileid), True, True, False, head_len=self.head_len
)
for fileid in fileids
]
)
",[],0,[],/corpus/reader/pl196x.py_tagged_sents
4269,/home/amandapotts/git/nltk/nltk/corpus/reader/pl196x.py_tagged_paras,"def tagged_paras(self, fileids=None, categories=None, textids=None):
fileids, textids = self._resolve(fileids, categories, textids)
if fileids is None:
fileids = self._fileids
elif isinstance(fileids, str):
fileids = [fileids]
if textids:
return concat(
[
TEICorpusView(
self.abspath(fileid),
True,
True,
True,
head_len=self.head_len,
textids=textids[fileid],
)
for fileid in fileids
]
)
else:
return concat(
[
TEICorpusView(
self.abspath(fileid), True, True, True, head_len=self.head_len
)
for fileid in fileids
]
)
",[],0,[],/corpus/reader/pl196x.py_tagged_paras
4270,/home/amandapotts/git/nltk/nltk/corpus/reader/pl196x.py_xml,"def xml(self, fileids=None, categories=None):
fileids, _ = self._resolve(fileids, categories)
if len(fileids) == 1:
return XMLCorpusReader.xml(self, fileids[0])
else:
raise TypeError(""Expected a single file"")
",[],0,[],/corpus/reader/pl196x.py_xml
4271,/home/amandapotts/git/nltk/nltk/corpus/reader/ipipan.py__parse_args,"def _parse_args(fun):
@functools.wraps(fun)
",[],0,[],/corpus/reader/ipipan.py__parse_args
4272,/home/amandapotts/git/nltk/nltk/corpus/reader/ipipan.py_decorator,"def decorator(self, fileids=None, **kwargs):
kwargs.pop(""tags"", None)
if not fileids:
fileids = self.fileids()
return fun(self, fileids, **kwargs)
",[],0,[],/corpus/reader/ipipan.py_decorator
4273,/home/amandapotts/git/nltk/nltk/corpus/reader/ipipan.py___init__,"def __init__(self, root, fileids):
CorpusReader.__init__(self, root, fileids, None, None)
",[],0,[],/corpus/reader/ipipan.py___init__
4274,/home/amandapotts/git/nltk/nltk/corpus/reader/ipipan.py_channels,"def channels(self, fileids=None):
if not fileids:
fileids = self.fileids()
return self._parse_header(fileids, ""channel"")
",[],0,[],/corpus/reader/ipipan.py_channels
4275,/home/amandapotts/git/nltk/nltk/corpus/reader/ipipan.py_domains,"def domains(self, fileids=None):
if not fileids:
fileids = self.fileids()
return self._parse_header(fileids, ""domain"")
",[],0,[],/corpus/reader/ipipan.py_domains
4276,/home/amandapotts/git/nltk/nltk/corpus/reader/ipipan.py_categories,"def categories(self, fileids=None):
if not fileids:
fileids = self.fileids()
return [
self._map_category(cat) for cat in self._parse_header(fileids, ""keyTerm"")
]
",[],0,[],/corpus/reader/ipipan.py_categories
4277,/home/amandapotts/git/nltk/nltk/corpus/reader/ipipan.py_fileids,"def fileids(self, channels=None, domains=None, categories=None):
if channels is not None and domains is not None and categories is not None:
raise ValueError(
""You can specify only one of channels, domains ""
""and categories parameter at once""
)
if channels is None and domains is None and categories is None:
return CorpusReader.fileids(self)
if isinstance(channels, str):
channels = [channels]
if isinstance(domains, str):
domains = [domains]
if isinstance(categories, str):
categories = [categories]
if channels:
return self._list_morph_files_by(""channel"", channels)
elif domains:
return self._list_morph_files_by(""domain"", domains)
else:
return self._list_morph_files_by(
""keyTerm"", categories, map=self._map_category
)
",[],0,[],/corpus/reader/ipipan.py_fileids
4278,/home/amandapotts/git/nltk/nltk/corpus/reader/ipipan.py_sents,"def sents(self, fileids=None, **kwargs):
return concat(
[
self._view(
fileid, mode=IPIPANCorpusView.SENTS_MODE, tags=False, **kwargs
)
for fileid in self._list_morph_files(fileids)
]
)
",[],0,[],/corpus/reader/ipipan.py_sents
4279,/home/amandapotts/git/nltk/nltk/corpus/reader/ipipan.py_paras,"def paras(self, fileids=None, **kwargs):
return concat(
[
self._view(
fileid, mode=IPIPANCorpusView.PARAS_MODE, tags=False, **kwargs
)
for fileid in self._list_morph_files(fileids)
]
)
",[],0,[],/corpus/reader/ipipan.py_paras
4280,/home/amandapotts/git/nltk/nltk/corpus/reader/ipipan.py_words,"def words(self, fileids=None, **kwargs):
return concat(
[
self._view(fileid, tags=False, **kwargs)
for fileid in self._list_morph_files(fileids)
]
)
",[],0,[],/corpus/reader/ipipan.py_words
4281,/home/amandapotts/git/nltk/nltk/corpus/reader/ipipan.py_tagged_sents,"def tagged_sents(self, fileids=None, **kwargs):
return concat(
[
self._view(fileid, mode=IPIPANCorpusView.SENTS_MODE, **kwargs)
for fileid in self._list_morph_files(fileids)
]
)
",[],0,[],/corpus/reader/ipipan.py_tagged_sents
4282,/home/amandapotts/git/nltk/nltk/corpus/reader/ipipan.py_tagged_paras,"def tagged_paras(self, fileids=None, **kwargs):
return concat(
[
self._view(fileid, mode=IPIPANCorpusView.PARAS_MODE, **kwargs)
for fileid in self._list_morph_files(fileids)
]
)
",[],0,[],/corpus/reader/ipipan.py_tagged_paras
4283,/home/amandapotts/git/nltk/nltk/corpus/reader/ipipan.py_tagged_words,"def tagged_words(self, fileids=None, **kwargs):
return concat(
[self._view(fileid, **kwargs) for fileid in self._list_morph_files(fileids)]
)
",[],0,[],/corpus/reader/ipipan.py_tagged_words
4284,/home/amandapotts/git/nltk/nltk/corpus/reader/ipipan.py__list_morph_files,"def _list_morph_files(self, fileids):
return [f for f in self.abspaths(fileids)]
",[],0,[],/corpus/reader/ipipan.py__list_morph_files
4285,/home/amandapotts/git/nltk/nltk/corpus/reader/ipipan.py__list_header_files,"def _list_header_files(self, fileids):
return [
f.replace(""morph.xml"", ""header.xml"")
for f in self._list_morph_files(fileids)
]
",[],0,[],/corpus/reader/ipipan.py__list_header_files
4286,/home/amandapotts/git/nltk/nltk/corpus/reader/ipipan.py__parse_header,"def _parse_header(self, fileids, tag):
values = set()
for f in self._list_header_files(fileids):
values_list = self._get_tag(f, tag)
for v in values_list:
values.add(v)
return list(values)
",[],0,[],/corpus/reader/ipipan.py__parse_header
4287,/home/amandapotts/git/nltk/nltk/corpus/reader/ipipan.py__list_morph_files_by,"def _list_morph_files_by(self, tag, values, map=None):
fileids = self.fileids()
ret_fileids = set()
for f in fileids:
fp = self.abspath(f).replace(""morph.xml"", ""header.xml"")
values_list = self._get_tag(fp, tag)
for value in values_list:
if map is not None:
value = map(value)
if value in values:
ret_fileids.add(f)
return list(ret_fileids)
",[],0,[],/corpus/reader/ipipan.py__list_morph_files_by
4288,/home/amandapotts/git/nltk/nltk/corpus/reader/ipipan.py__get_tag,"def _get_tag(self, f, tag):
tags = []
with open(f) as infile:
header = infile.read()
tag_end = 0
while True:
tag_pos = header.find(""<"" + tag, tag_end)
if tag_pos < 0:
return tags
tag_end = header.find(""</"" + tag + "">"", tag_pos)
tags.append(header[tag_pos + len(tag) + 2 : tag_end])
",[],0,[],/corpus/reader/ipipan.py__get_tag
4289,/home/amandapotts/git/nltk/nltk/corpus/reader/ipipan.py__map_category,"def _map_category(self, cat):
pos = cat.find("">"")
if pos == -1:
return cat
else:
return cat[pos + 1 :]
",[],0,[],/corpus/reader/ipipan.py__map_category
4290,/home/amandapotts/git/nltk/nltk/corpus/reader/ipipan.py__view,"def _view(self, filename, **kwargs):
tags = kwargs.pop(""tags"", True)
mode = kwargs.pop(""mode"", 0)
simplify_tags = kwargs.pop(""simplify_tags"", False)
one_tag = kwargs.pop(""one_tag"", True)
disamb_only = kwargs.pop(""disamb_only"", True)
append_no_space = kwargs.pop(""append_no_space"", False)
append_space = kwargs.pop(""append_space"", False)
replace_xmlentities = kwargs.pop(""replace_xmlentities"", True)
if len(kwargs) > 0:
raise ValueError(""Unexpected arguments: %s"" % kwargs.keys())
if not one_tag and not disamb_only:
raise ValueError(
""You cannot specify both one_tag=False and "" ""disamb_only=False""
)
if not tags and (simplify_tags or not one_tag or not disamb_only):
raise ValueError(
""You cannot specify simplify_tags, one_tag or ""
""disamb_only with functions other than tagged_*""
)
return IPIPANCorpusView(
filename,
tags=tags,
mode=mode,
simplify_tags=simplify_tags,
one_tag=one_tag,
disamb_only=disamb_only,
append_no_space=append_no_space,
append_space=append_space,
replace_xmlentities=replace_xmlentities,
)
",[],0,[],/corpus/reader/ipipan.py__view
4291,/home/amandapotts/git/nltk/nltk/corpus/reader/ipipan.py___init__,"def __init__(self, filename, startpos=0, **kwargs):
StreamBackedCorpusView.__init__(self, filename, None, startpos, None)
self.in_sentence = False
self.position = 0
self.show_tags = kwargs.pop(""tags"", True)
self.disamb_only = kwargs.pop(""disamb_only"", True)
self.mode = kwargs.pop(""mode"", IPIPANCorpusView.WORDS_MODE)
self.simplify_tags = kwargs.pop(""simplify_tags"", False)
self.one_tag = kwargs.pop(""one_tag"", True)
self.append_no_space = kwargs.pop(""append_no_space"", False)
self.append_space = kwargs.pop(""append_space"", False)
self.replace_xmlentities = kwargs.pop(""replace_xmlentities"", True)
",[],0,[],/corpus/reader/ipipan.py___init__
4292,/home/amandapotts/git/nltk/nltk/corpus/reader/ipipan.py_read_block,"def read_block(self, stream):
sentence = []
sentences = []
space = False
no_space = False
tags = set()
lines = self._read_data(stream)
while True:
if len(lines) <= 1:
self._seek(stream)
lines = self._read_data(stream)
if lines == [""""]:
assert not sentences
return []
line = lines.pop()
self.position += len(line) + 1
if line.startswith('<chunk type=""s""'):
self.in_sentence = True
elif line.startswith('<chunk type=""p""'):
pass
elif line.startswith(""<tok""):
if self.append_space and space and not no_space:
self._append_space(sentence)
space = True
no_space = False
orth = """"
tags = set()
elif line.startswith(""</chunk""):
if self.in_sentence:
self.in_sentence = False
self._seek(stream)
if self.mode == self.SENTS_MODE:
return [sentence]
elif self.mode == self.WORDS_MODE:
if self.append_space:
self._append_space(sentence)
return sentence
else:
sentences.append(sentence)
elif self.mode == self.PARAS_MODE:
self._seek(stream)
return [sentences]
elif line.startswith(""<orth""):
orth = line[6:-7]
if self.replace_xmlentities:
orth = orth.replace(""&quot
elif line.startswith(""<lex""):
if not self.disamb_only or line.find(""disamb="") != -1:
tag = line[line.index(""<ctag"") + 6 : line.index(""</ctag"")]
tags.add(tag)
elif line.startswith(""</tok""):
if self.show_tags:
if self.simplify_tags:
tags = [t.split("":"")[0] for t in tags]
if not self.one_tag or not self.disamb_only:
sentence.append((orth, tuple(tags)))
else:
sentence.append((orth, tags.pop()))
else:
sentence.append(orth)
elif line.startswith(""<ns/>""):
if self.append_space:
no_space = True
if self.append_no_space:
if self.show_tags:
sentence.append(("""", ""no-space""))
else:
sentence.append("""")
elif line.startswith(""</cesAna""):
pass
",[],0,[],/corpus/reader/ipipan.py_read_block
4293,/home/amandapotts/git/nltk/nltk/corpus/reader/ipipan.py__read_data,"def _read_data(self, stream):
self.position = stream.tell()
buff = stream.read(4096)
lines = buff.split(""\n"")
lines.reverse()
return lines
",[],0,[],/corpus/reader/ipipan.py__read_data
4294,/home/amandapotts/git/nltk/nltk/corpus/reader/ipipan.py__seek,"def _seek(self, stream):
stream.seek(self.position)
",[],0,[],/corpus/reader/ipipan.py__seek
4295,/home/amandapotts/git/nltk/nltk/corpus/reader/ipipan.py__append_space,"def _append_space(self, sentence):
if self.show_tags:
sentence.append(("" "", ""space""))
else:
sentence.append("" "")
",[],0,[],/corpus/reader/ipipan.py__append_space
4296,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py___init__,"def __init__(
self,
root,
fileids,
columntypes,
chunk_types=None,
root_label=""S"",
pos_in_tree=False,
srl_includes_roleset=True,
encoding=""utf8"",
tree_class=Tree,
tagset=None,
separator=None,
",[],0,[],/corpus/reader/conll.py___init__
4297,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py_words,"def words(self, fileids=None):
self._require(self.WORDS)
return LazyConcatenation(LazyMap(self._get_words, self._grids(fileids)))
",[],0,[],/corpus/reader/conll.py_words
4298,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py_sents,"def sents(self, fileids=None):
self._require(self.WORDS)
return LazyMap(self._get_words, self._grids(fileids))
",[],0,[],/corpus/reader/conll.py_sents
4299,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py_tagged_words,"def tagged_words(self, fileids=None, tagset=None):
self._require(self.WORDS, self.POS)
",[],0,[],/corpus/reader/conll.py_tagged_words
4300,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py_get_tagged_words,"def get_tagged_words(grid):
return self._get_tagged_words(grid, tagset)
",[],0,[],/corpus/reader/conll.py_get_tagged_words
4301,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py_tagged_sents,"def tagged_sents(self, fileids=None, tagset=None):
self._require(self.WORDS, self.POS)
",[],0,[],/corpus/reader/conll.py_tagged_sents
4302,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py_get_tagged_words,"def get_tagged_words(grid):
return self._get_tagged_words(grid, tagset)
",[],0,[],/corpus/reader/conll.py_get_tagged_words
4303,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py_chunked_words,"def chunked_words(self, fileids=None, chunk_types=None, tagset=None):
self._require(self.WORDS, self.POS, self.CHUNK)
if chunk_types is None:
chunk_types = self._chunk_types
",[],0,[],/corpus/reader/conll.py_chunked_words
4304,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py_get_chunked_words,"def get_chunked_words(grid):  # capture chunk_types as local var
return self._get_chunked_words(grid, chunk_types, tagset)
",[],0,[],/corpus/reader/conll.py_get_chunked_words
4305,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py_chunked_sents,"def chunked_sents(self, fileids=None, chunk_types=None, tagset=None):
self._require(self.WORDS, self.POS, self.CHUNK)
if chunk_types is None:
chunk_types = self._chunk_types
",[],0,[],/corpus/reader/conll.py_chunked_sents
4306,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py_get_chunked_words,"def get_chunked_words(grid):  # capture chunk_types as local var
return self._get_chunked_words(grid, chunk_types, tagset)
",[],0,[],/corpus/reader/conll.py_get_chunked_words
4307,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py_parsed_sents,"def parsed_sents(self, fileids=None, pos_in_tree=None, tagset=None):
self._require(self.WORDS, self.POS, self.TREE)
if pos_in_tree is None:
pos_in_tree = self._pos_in_tree
",[],0,[],/corpus/reader/conll.py_parsed_sents
4308,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py_get_parsed_sent,"def get_parsed_sent(grid):  # capture pos_in_tree as local var
return self._get_parsed_sent(grid, pos_in_tree, tagset)
",[],0,[],/corpus/reader/conll.py_get_parsed_sent
4309,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py_srl_spans,"def srl_spans(self, fileids=None):
self._require(self.SRL)
return LazyMap(self._get_srl_spans, self._grids(fileids))
",[],0,[],/corpus/reader/conll.py_srl_spans
4310,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py_srl_instances,"def srl_instances(self, fileids=None, pos_in_tree=None, flatten=True):
self._require(self.WORDS, self.POS, self.TREE, self.SRL)
if pos_in_tree is None:
pos_in_tree = self._pos_in_tree
",[],0,[],/corpus/reader/conll.py_srl_instances
4311,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py_get_srl_instances,"def get_srl_instances(grid):  # capture pos_in_tree as local var
return self._get_srl_instances(grid, pos_in_tree)
",[],0,[],/corpus/reader/conll.py_get_srl_instances
4312,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py_iob_words,"def iob_words(self, fileids=None, tagset=None):
""""""
:return: a list of word/tag/IOB tuples
:rtype: list(tuple)
:param fileids: the list of fileids that make up this corpus
:type fileids: None or str or list
""""""
self._require(self.WORDS, self.POS, self.CHUNK)
",[],0,[],/corpus/reader/conll.py_iob_words
4313,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py_get_iob_words,"def get_iob_words(grid):
return self._get_iob_words(grid, tagset)
",[],0,[],/corpus/reader/conll.py_get_iob_words
4314,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py_iob_sents,"def iob_sents(self, fileids=None, tagset=None):
""""""
:return: a list of lists of word/tag/IOB tuples
:rtype: list(list)
:param fileids: the list of fileids that make up this corpus
:type fileids: None or str or list
""""""
self._require(self.WORDS, self.POS, self.CHUNK)
",[],0,[],/corpus/reader/conll.py_iob_sents
4315,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py_get_iob_words,"def get_iob_words(grid):
return self._get_iob_words(grid, tagset)
",[],0,[],/corpus/reader/conll.py_get_iob_words
4316,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py__grids,"def _grids(self, fileids=None):
return concat(
[
StreamBackedCorpusView(fileid, self._read_grid_block, encoding=enc)
for (fileid, enc) in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/conll.py__grids
4317,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py__read_grid_block,"def _read_grid_block(self, stream):
grids = []
for block in read_blankline_block(stream):
block = block.strip()
if not block:
continue
grid = [line.split(self.sep) for line in block.split(""\n"")]
if grid[0][self._colmap.get(""words"", 0)] == ""-DOCSTART-"":
del grid[0]
for row in grid:
if len(row) != len(grid[0]):
raise ValueError(""Inconsistent number of columns:\n%s"" % block)
grids.append(grid)
return grids
",[],0,[],/corpus/reader/conll.py__read_grid_block
4318,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py__get_words,"def _get_words(self, grid):
return self._get_column(grid, self._colmap[""words""])
",[],0,[],/corpus/reader/conll.py__get_words
4319,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py__get_tagged_words,"def _get_tagged_words(self, grid, tagset=None):
pos_tags = self._get_column(grid, self._colmap[""pos""])
if tagset and tagset != self._tagset:
pos_tags = [map_tag(self._tagset, tagset, t) for t in pos_tags]
return list(zip(self._get_column(grid, self._colmap[""words""]), pos_tags))
",[],0,[],/corpus/reader/conll.py__get_tagged_words
4320,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py__get_iob_words,"def _get_iob_words(self, grid, tagset=None):
pos_tags = self._get_column(grid, self._colmap[""pos""])
if tagset and tagset != self._tagset:
pos_tags = [map_tag(self._tagset, tagset, t) for t in pos_tags]
return list(
zip(
self._get_column(grid, self._colmap[""words""]),
pos_tags,
self._get_column(grid, self._colmap[""chunk""]),
)
)
",[],0,[],/corpus/reader/conll.py__get_iob_words
4321,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py__get_chunked_words,"def _get_chunked_words(self, grid, chunk_types, tagset=None):
words = self._get_column(grid, self._colmap[""words""])
pos_tags = self._get_column(grid, self._colmap[""pos""])
if tagset and tagset != self._tagset:
pos_tags = [map_tag(self._tagset, tagset, t) for t in pos_tags]
chunk_tags = self._get_column(grid, self._colmap[""chunk""])
stack = [Tree(self._root_label, [])]
for word, pos_tag, chunk_tag in zip(words, pos_tags, chunk_tags):
if chunk_tag == ""O"":
state, chunk_type = ""O"", """"
else:
(state, chunk_type) = chunk_tag.split(""-"")
if chunk_types is not None and chunk_type not in chunk_types:
state = ""O""
if state == ""I"" and chunk_type != stack[-1].label():
state = ""B""
if state in ""BO"" and len(stack) == 2:
stack.pop()
if state == ""B"":
new_chunk = Tree(chunk_type, [])
stack[-1].append(new_chunk)
stack.append(new_chunk)
stack[-1].append((word, pos_tag))
return stack[0]
",[],0,[],/corpus/reader/conll.py__get_chunked_words
4322,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py__get_parsed_sent,"def _get_parsed_sent(self, grid, pos_in_tree, tagset=None):
words = self._get_column(grid, self._colmap[""words""])
pos_tags = self._get_column(grid, self._colmap[""pos""])
if tagset and tagset != self._tagset:
pos_tags = [map_tag(self._tagset, tagset, t) for t in pos_tags]
parse_tags = self._get_column(grid, self._colmap[""tree""])
treestr = """"
for word, pos_tag, parse_tag in zip(words, pos_tags, parse_tags):
if word == ""("":
word = ""-LRB-""
if word == "")"":
word = ""-RRB-""
if pos_tag == ""("":
pos_tag = ""-LRB-""
if pos_tag == "")"":
pos_tag = ""-RRB-""
(left, right) = parse_tag.split(""*"")
right = right.count("")"") * "")""  # only keep ')'.
treestr += f""{left} ({pos_tag} {word}) {right}""
try:
tree = self._tree_class.fromstring(treestr)
except (ValueError, IndexError):
tree = self._tree_class.fromstring(f""({self._root_label} {treestr})"")
if not pos_in_tree:
for subtree in tree.subtrees():
for i, child in enumerate(subtree):
if (
isinstance(child, Tree)
and len(child) == 1
and isinstance(child[0], str)
):
subtree[i] = (child[0], child.label())
return tree
",[],0,[],/corpus/reader/conll.py__get_parsed_sent
4323,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py__get_srl_spans,"def _get_srl_spans(self, grid):
""""""
list of list of (start, end), tag) tuples
""""""
if self._srl_includes_roleset:
predicates = self._get_column(grid, self._colmap[""srl""] + 1)
start_col = self._colmap[""srl""] + 2
else:
predicates = self._get_column(grid, self._colmap[""srl""])
start_col = self._colmap[""srl""] + 1
num_preds = len([p for p in predicates if p != ""-""])
spanlists = []
for i in range(num_preds):
col = self._get_column(grid, start_col + i)
spanlist = []
stack = []
for wordnum, srl_tag in enumerate(col):
(left, right) = srl_tag.split(""*"")
for tag in left.split(""(""):
if tag:
stack.append((tag, wordnum))
for i in range(right.count("")"")):
(tag, start) = stack.pop()
spanlist.append(((start, wordnum + 1), tag))
spanlists.append(spanlist)
return spanlists
",[],0,[],/corpus/reader/conll.py__get_srl_spans
4324,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py__get_srl_instances,"def _get_srl_instances(self, grid, pos_in_tree):
tree = self._get_parsed_sent(grid, pos_in_tree)
spanlists = self._get_srl_spans(grid)
if self._srl_includes_roleset:
predicates = self._get_column(grid, self._colmap[""srl""] + 1)
rolesets = self._get_column(grid, self._colmap[""srl""])
else:
predicates = self._get_column(grid, self._colmap[""srl""])
rolesets = [None] * len(predicates)
instances = ConllSRLInstanceList(tree)
for wordnum, predicate in enumerate(predicates):
if predicate == ""-"":
continue
for spanlist in spanlists:
for (start, end), tag in spanlist:
if wordnum in range(start, end) and tag in (""V"", ""C-V""):
break
else:
continue
break
else:
raise ValueError(""No srl column found for %r"" % predicate)
instances.append(
ConllSRLInstance(tree, wordnum, predicate, rolesets[wordnum], spanlist)
)
return instances
",[],0,[],/corpus/reader/conll.py__get_srl_instances
4325,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py__require,"def _require(self, *columntypes):
for columntype in columntypes:
if columntype not in self._colmap:
raise ValueError(
""This corpus does not contain a %s "" ""column."" % columntype
)
",[],0,[],/corpus/reader/conll.py__require
4326,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py__get_column,"def _get_column(grid, column_index):
return [grid[i][column_index] for i in range(len(grid))]
",[],0,[],/corpus/reader/conll.py__get_column
4327,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py___init__,"def __init__(self, tree, verb_head, verb_stem, roleset, tagged_spans):
self.verb = []
""""""A list of the word indices of the words that compose the
verb whose arguments are identified by this instance.
This will contain multiple word indices when multi-word
verbs are used (e.g. 'turn on').""""""
self.verb_head = verb_head
""""""The word index of the head word of the verb whose arguments
are identified by this instance.  E.g., for a sentence that
uses the verb 'turn on,' ``verb_head`` will be the word index
of the word 'turn'.""""""
self.verb_stem = verb_stem
self.roleset = roleset
self.arguments = []
""""""A list of ``(argspan, argid)`` tuples, specifying the location
and type for each of the arguments identified by this
instance.  ``argspan`` is a tuple ``start, end``, indicating
that the argument consists of the ``words[start:end]``.""""""
self.tagged_spans = tagged_spans
""""""A list of ``(span, id)`` tuples, specifying the location and
type for each of the arguments, as well as the verb pieces,
that make up this instance.""""""
self.tree = tree
""""""The parse tree for the sentence containing this instance.""""""
self.words = tree.leaves()
""""""A list of the words in the sentence containing this
instance.""""""
for (start, end), tag in tagged_spans:
if tag in (""V"", ""C-V""):
self.verb += list(range(start, end))
else:
self.arguments.append(((start, end), tag))
",[],0,[],/corpus/reader/conll.py___init__
4328,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py___repr__,"def __repr__(self):
plural = ""s"" if len(self.arguments) != 1 else """"
return ""<ConllSRLInstance for %r with %d argument%s>"" % (
(self.verb_stem, len(self.arguments), plural)
)
",[],0,[],/corpus/reader/conll.py___repr__
4329,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py_pprint,"def pprint(self):
verbstr = "" "".join(self.words[i][0] for i in self.verb)
hdr = f""SRL for {verbstr!r} (stem={self.verb_stem!r}):\n""
s = """"
for i, word in enumerate(self.words):
if isinstance(word, tuple):
word = word[0]
for (start, end), argid in self.arguments:
if i == start:
s += ""[%s "" % argid
if i == end:
s += ""] ""
if i in self.verb:
word = ""<<%s>>"" % word
s += word + "" ""
return hdr + textwrap.fill(
s.replace("" ]"", ""]""), initial_indent=""    "", subsequent_indent=""    ""
)
",[],0,[],/corpus/reader/conll.py_pprint
4330,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py___init__,"def __init__(self, tree, instances=()):
self.tree = tree
list.__init__(self, instances)
",[],0,[],/corpus/reader/conll.py___init__
4331,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py___str__,"def __str__(self):
return self.pprint()
",[],0,[],/corpus/reader/conll.py___str__
4332,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py_pprint,"def pprint(self, include_tree=False):
for inst in self:
if inst.tree != self.tree:
raise ValueError(""Tree mismatch!"")
if include_tree:
words = self.tree.leaves()
pos = [None] * len(words)
synt = [""*""] * len(words)
self._tree2conll(self.tree, 0, words, pos, synt)
s = """"
for i in range(len(words)):
if include_tree:
s += ""%-20s "" % words[i]
s += ""%-8s "" % pos[i]
s += ""%15s*%-8s "" % tuple(synt[i].split(""*""))
for inst in self:
if i == inst.verb_head:
s += ""%-20s "" % inst.verb_stem
break
else:
s += ""%-20s "" % ""-""
for inst in self:
argstr = ""*""
for (start, end), argid in inst.tagged_spans:
if i == start:
argstr = f""({argid}{argstr}""
if i == (end - 1):
argstr += "")""
s += ""%-12s "" % argstr
s += ""\n""
return s
",[],0,[],/corpus/reader/conll.py_pprint
4333,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py__tree2conll,"def _tree2conll(self, tree, wordnum, words, pos, synt):
assert isinstance(tree, Tree)
if len(tree) == 1 and isinstance(tree[0], str):
pos[wordnum] = tree.label()
assert words[wordnum] == tree[0]
return wordnum + 1
elif len(tree) == 1 and isinstance(tree[0], tuple):
assert len(tree[0]) == 2
pos[wordnum], pos[wordnum] = tree[0]
return wordnum + 1
else:
synt[wordnum] = f""({tree.label()}{synt[wordnum]}""
for child in tree:
wordnum = self._tree2conll(child, wordnum, words, pos, synt)
synt[wordnum - 1] += "")""
return wordnum
",[],0,[],/corpus/reader/conll.py__tree2conll
4334,/home/amandapotts/git/nltk/nltk/corpus/reader/conll.py___init__,"def __init__(
self, root, fileids, chunk_types, encoding=""utf8"", tagset=None, separator=None
",[],0,[],/corpus/reader/conll.py___init__
4335,/home/amandapotts/git/nltk/nltk/corpus/reader/sinica_treebank.py__read_block,"def _read_block(self, stream):
sent = stream.readline()
sent = IDENTIFIER.sub("""", sent)
sent = APPENDIX.sub("""", sent)
return [sent]
",[],0,[],/corpus/reader/sinica_treebank.py__read_block
4336,/home/amandapotts/git/nltk/nltk/corpus/reader/sinica_treebank.py__parse,"def _parse(self, sent):
return sinica_parse(sent)
",[],0,[],/corpus/reader/sinica_treebank.py__parse
4337,/home/amandapotts/git/nltk/nltk/corpus/reader/sinica_treebank.py__tag,"def _tag(self, sent, tagset=None):
tagged_sent = [(w, t) for (t, w) in TAGWORD.findall(sent)]
if tagset and tagset != self._tagset:
tagged_sent = [
(w, map_tag(self._tagset, tagset, t)) for (w, t) in tagged_sent
]
return tagged_sent
",[],0,[],/corpus/reader/sinica_treebank.py__tag
4338,/home/amandapotts/git/nltk/nltk/corpus/reader/sinica_treebank.py__word,"def _word(self, sent):
return WORD.findall(sent)
",[],0,[],/corpus/reader/sinica_treebank.py__word
4339,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py___init__,"def __init__(
self,
root,
propfile,
framefiles="""",
verbsfile=None,
parse_fileid_xform=None,
parse_corpus=None,
encoding=""utf8"",
",[],0,[],/corpus/reader/propbank.py___init__
4340,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py_lines,"def lines(self):
""""""
:return: a corpus view that acts as a list of strings, one for
each line in the predicate-argument annotation file.
""""""
return StreamBackedCorpusView(
self.abspath(self._propfile),
read_line_block,
encoding=self.encoding(self._propfile),
)
",[],0,[],/corpus/reader/propbank.py_lines
4341,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py_roleset,"def roleset(self, roleset_id):
""""""
:return: the xml description for the given roleset.
""""""
baseform = roleset_id.split(""."")[0]
framefile = ""frames/%s.xml"" % baseform
if framefile not in self._framefiles:
raise ValueError(""Frameset file for %s not found"" % roleset_id)
with self.abspath(framefile).open() as fp:
etree = ElementTree.parse(fp).getroot()
for roleset in etree.findall(""predicate/roleset""):
if roleset.attrib[""id""] == roleset_id:
return roleset
raise ValueError(f""Roleset {roleset_id} not found in {framefile}"")
",[],0,[],/corpus/reader/propbank.py_roleset
4342,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py_rolesets,"def rolesets(self, baseform=None):
""""""
:return: list of xml descriptions for rolesets.
""""""
if baseform is not None:
framefile = ""frames/%s.xml"" % baseform
if framefile not in self._framefiles:
raise ValueError(""Frameset file for %s not found"" % baseform)
framefiles = [framefile]
else:
framefiles = self._framefiles
rsets = []
for framefile in framefiles:
with self.abspath(framefile).open() as fp:
etree = ElementTree.parse(fp).getroot()
rsets.append(etree.findall(""predicate/roleset""))
return LazyConcatenation(rsets)
",[],0,[],/corpus/reader/propbank.py_rolesets
4343,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py_verbs,"def verbs(self):
""""""
:return: a corpus view that acts as a list of all verb lemmas
in this corpus (from the verbs.txt file).
""""""
return StreamBackedCorpusView(
self.abspath(self._verbsfile),
read_line_block,
encoding=self.encoding(self._verbsfile),
)
",[],0,[],/corpus/reader/propbank.py_verbs
4344,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py___init__,"def __init__(
self,
fileid,
sentnum,
wordnum,
tagger,
roleset,
inflection,
predicate,
arguments,
parse_corpus=None,
",[],0,[],/corpus/reader/propbank.py___init__
4345,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py_baseform,"def baseform(self):
""""""The baseform of the predicate.""""""
return self.roleset.split(""."")[0]
",[],0,[],/corpus/reader/propbank.py_baseform
4346,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py_sensenumber,"def sensenumber(self):
""""""The sense number of the predicate.""""""
return self.roleset.split(""."")[1]
",[],0,[],/corpus/reader/propbank.py_sensenumber
4347,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py_predid,"def predid(self):
""""""Identifier of the predicate.""""""
return ""rel""
",[],0,[],/corpus/reader/propbank.py_predid
4348,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py___repr__,"def __repr__(self):
return ""<PropbankInstance: {}, sent {}, word {}>"".format(
self.fileid,
self.sentnum,
self.wordnum,
)
",[],0,[],/corpus/reader/propbank.py___repr__
4349,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py___str__,"def __str__(self):
s = ""{} {} {} {} {} {}"".format(
self.fileid,
self.sentnum,
self.wordnum,
self.tagger,
self.roleset,
self.inflection,
)
items = self.arguments + ((self.predicate, ""rel""),)
for argloc, argid in sorted(items):
s += f"" {argloc}-{argid}""
return s
",[],0,[],/corpus/reader/propbank.py___str__
4350,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py__get_tree,"def _get_tree(self):
if self.parse_corpus is None:
return None
if self.fileid not in self.parse_corpus.fileids():
return None
return self.parse_corpus.parsed_sents(self.fileid)[self.sentnum]
",[],0,[],/corpus/reader/propbank.py__get_tree
4351,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py_parse,"def parse(s, parse_fileid_xform=None, parse_corpus=None):
pieces = s.split()
if len(pieces) < 7:
raise ValueError(""Badly formatted propbank line: %r"" % s)
(fileid, sentnum, wordnum, tagger, roleset, inflection) = pieces[:6]
rel = [p for p in pieces[6:] if p.endswith(""-rel"")]
args = [p for p in pieces[6:] if not p.endswith(""-rel"")]
if len(rel) != 1:
raise ValueError(""Badly formatted propbank line: %r"" % s)
if parse_fileid_xform is not None:
fileid = parse_fileid_xform(fileid)
sentnum = int(sentnum)
wordnum = int(wordnum)
inflection = PropbankInflection.parse(inflection)
predicate = PropbankTreePointer.parse(rel[0][:-4])
arguments = []
for arg in args:
argloc, argid = arg.split(""-"", 1)
arguments.append((PropbankTreePointer.parse(argloc), argid))
return PropbankInstance(
fileid,
sentnum,
wordnum,
tagger,
roleset,
inflection,
predicate,
arguments,
parse_corpus,
)
",[],0,[],/corpus/reader/propbank.py_parse
4352,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py___init__,"def __init__(self):
if self.__class__ == PropbankPointer:
raise NotImplementedError()
",[],0,[],/corpus/reader/propbank.py___init__
4353,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py___init__,"def __init__(self, pieces):
self.pieces = pieces
""""""A list of the pieces that make up this chain.  Elements may
be either ``PropbankSplitTreePointer`` or
``PropbankTreePointer`` pointers.""""""
",[],0,[],/corpus/reader/propbank.py___init__
4354,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py___str__,"def __str__(self):
return ""*"".join(""%s"" % p for p in self.pieces)
",[],0,[],/corpus/reader/propbank.py___str__
4355,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py___repr__,"def __repr__(self):
return ""<PropbankChainTreePointer: %s>"" % self
",[],0,[],/corpus/reader/propbank.py___repr__
4356,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py_select,"def select(self, tree):
if tree is None:
raise ValueError(""Parse tree not available"")
return Tree(""*CHAIN*"", [p.select(tree) for p in self.pieces])
",[],0,[],/corpus/reader/propbank.py_select
4357,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py___init__,"def __init__(self, pieces):
self.pieces = pieces
""""""A list of the pieces that make up this chain.  Elements are
all ``PropbankTreePointer`` pointers.""""""
",[],0,[],/corpus/reader/propbank.py___init__
4358,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py___str__,"def __str__(self):
return "","".join(""%s"" % p for p in self.pieces)
",[],0,[],/corpus/reader/propbank.py___str__
4359,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py___repr__,"def __repr__(self):
return ""<PropbankSplitTreePointer: %s>"" % self
",[],0,[],/corpus/reader/propbank.py___repr__
4360,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py_select,"def select(self, tree):
if tree is None:
raise ValueError(""Parse tree not available"")
return Tree(""*SPLIT*"", [p.select(tree) for p in self.pieces])
",[],0,[],/corpus/reader/propbank.py_select
4361,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py___init__,"def __init__(self, wordnum, height):
self.wordnum = wordnum
self.height = height
",[],0,[],/corpus/reader/propbank.py___init__
4362,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py_parse,"def parse(s):
pieces = s.split(""*"")
if len(pieces) > 1:
return PropbankChainTreePointer(
[PropbankTreePointer.parse(elt) for elt in pieces]
)
pieces = s.split("","")
if len(pieces) > 1:
return PropbankSplitTreePointer(
[PropbankTreePointer.parse(elt) for elt in pieces]
)
pieces = s.split("":"")
if len(pieces) != 2:
raise ValueError(""bad propbank pointer %r"" % s)
return PropbankTreePointer(int(pieces[0]), int(pieces[1]))
",[],0,[],/corpus/reader/propbank.py_parse
4363,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py___str__,"def __str__(self):
return f""{self.wordnum}:{self.height}""
",[],0,[],/corpus/reader/propbank.py___str__
4364,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py___repr__,"def __repr__(self):
return ""PropbankTreePointer(%d, %d)"" % (self.wordnum, self.height)
",[],0,[],/corpus/reader/propbank.py___repr__
4365,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py___eq__,"def __eq__(self, other):
while isinstance(other, (PropbankChainTreePointer, PropbankSplitTreePointer)):
other = other.pieces[0]
if not isinstance(other, PropbankTreePointer):
return self is other
return self.wordnum == other.wordnum and self.height == other.height
",[],0,[],/corpus/reader/propbank.py___eq__
4366,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py___ne__,"def __ne__(self, other):
return not self == other
",[],0,[],/corpus/reader/propbank.py___ne__
4367,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py___lt__,"def __lt__(self, other):
while isinstance(other, (PropbankChainTreePointer, PropbankSplitTreePointer)):
other = other.pieces[0]
if not isinstance(other, PropbankTreePointer):
return id(self) < id(other)
return (self.wordnum, -self.height) < (other.wordnum, -other.height)
",[],0,[],/corpus/reader/propbank.py___lt__
4368,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py_select,"def select(self, tree):
if tree is None:
raise ValueError(""Parse tree not available"")
return tree[self.treepos(tree)]
",[],0,[],/corpus/reader/propbank.py_select
4369,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py_treepos,"def treepos(self, tree):
""""""
Convert this pointer to a standard 'tree position' pointer,
given that it points to the given tree.
""""""
if tree is None:
raise ValueError(""Parse tree not available"")
stack = [tree]
treepos = []
wordnum = 0
while True:
if isinstance(stack[-1], Tree):
if len(treepos) < len(stack):
treepos.append(0)
else:
treepos[-1] += 1
if treepos[-1] < len(stack[-1]):
stack.append(stack[-1][treepos[-1]])
else:
stack.pop()
treepos.pop()
else:
if wordnum == self.wordnum:
return tuple(treepos[: len(treepos) - self.height - 1])
else:
wordnum += 1
stack.pop()
",[],0,[],/corpus/reader/propbank.py_treepos
4370,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py___init__,"def __init__(self, form=""-"", tense=""-"", aspect=""-"", person=""-"", voice=""-""):
self.form = form
self.tense = tense
self.aspect = aspect
self.person = person
self.voice = voice
",[],0,[],/corpus/reader/propbank.py___init__
4371,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py___str__,"def __str__(self):
return self.form + self.tense + self.aspect + self.person + self.voice
",[],0,[],/corpus/reader/propbank.py___str__
4372,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py___repr__,"def __repr__(self):
return ""<PropbankInflection: %s>"" % self
",[],0,[],/corpus/reader/propbank.py___repr__
4373,/home/amandapotts/git/nltk/nltk/corpus/reader/propbank.py_parse,"def parse(s):
if not isinstance(s, str):
raise TypeError(""expected a string"")
if len(s) != 5 or not PropbankInflection._VALIDATE.match(s):
raise ValueError(""Bad propbank inflection string %r"" % s)
return PropbankInflection(*s)
",[],0,[],/corpus/reader/propbank.py_parse
4374,/home/amandapotts/git/nltk/nltk/corpus/reader/bracket_parse.py___init__,"def __init__(
self,
root,
fileids,
comment_char=None,
detect_blocks=""unindented_paren"",
encoding=""utf8"",
tagset=None,
",[],0,[],/corpus/reader/bracket_parse.py___init__
4375,/home/amandapotts/git/nltk/nltk/corpus/reader/bracket_parse.py__read_block,"def _read_block(self, stream):
if self._detect_blocks == ""sexpr"":
return read_sexpr_block(stream, comment_char=self._comment_char)
elif self._detect_blocks == ""blankline"":
return read_blankline_block(stream)
elif self._detect_blocks == ""unindented_paren"":
toks = read_regexp_block(stream, start_re=r""^\("")
if self._comment_char:
toks = [
re.sub(""(?m)^%s.*"" % re.escape(self._comment_char), """", tok)
for tok in toks
]
return toks
else:
assert 0, ""bad block type""
",[],0,[],/corpus/reader/bracket_parse.py__read_block
4376,/home/amandapotts/git/nltk/nltk/corpus/reader/bracket_parse.py__normalize,"def _normalize(self, t):
t = re.sub(r""\((.)\)"", r""(\1 \1)"", t)
t = re.sub(r""\(([^\s()]+) ([^\s()]+) [^\s()]+\)"", r""(\1 \2)"", t)
return t
",[],0,[],/corpus/reader/bracket_parse.py__normalize
4377,/home/amandapotts/git/nltk/nltk/corpus/reader/bracket_parse.py__parse,"def _parse(self, t):
try:
tree = Tree.fromstring(self._normalize(t))
if tree.label() == """" and len(tree) == 1:
return tree[0]
else:
return tree
except ValueError as e:
sys.stderr.write(""Bad tree detected
if e.args == (""mismatched parens"",):
for n in range(1, 5):
try:
v = Tree(self._normalize(t + "")"" * n))
sys.stderr.write(
""  Recovered by adding %d close "" ""paren(s)\n"" % n
)
return v
except ValueError:
pass
sys.stderr.write(""  Recovered by returning a flat parse.\n"")
return Tree(""S"", self._tag(t))
",[],0,[],/corpus/reader/bracket_parse.py__parse
4378,/home/amandapotts/git/nltk/nltk/corpus/reader/bracket_parse.py__tag,"def _tag(self, t, tagset=None):
tagged_sent = [(w, p) for (p, w) in TAGWORD.findall(self._normalize(t))]
if tagset and tagset != self._tagset:
tagged_sent = [
(w, map_tag(self._tagset, tagset, p)) for (w, p) in tagged_sent
]
return tagged_sent
",[],0,[],/corpus/reader/bracket_parse.py__tag
4379,/home/amandapotts/git/nltk/nltk/corpus/reader/bracket_parse.py__word,"def _word(self, t):
return WORD.findall(self._normalize(t))
",[],0,[],/corpus/reader/bracket_parse.py__word
4380,/home/amandapotts/git/nltk/nltk/corpus/reader/bracket_parse.py___init__,"def __init__(self, *args, **kwargs):
""""""
Initialize the corpus reader.  Categorization arguments
(C{cat_pattern}, C{cat_map}, and C{cat_file}) are passed to
the L{CategorizedCorpusReader constructor
<CategorizedCorpusReader.__init__>}.  The remaining arguments
are passed to the L{BracketParseCorpusReader constructor
<BracketParseCorpusReader.__init__>}.
""""""
CategorizedCorpusReader.__init__(self, kwargs)
BracketParseCorpusReader.__init__(self, *args, **kwargs)
",[],0,[],/corpus/reader/bracket_parse.py___init__
4381,/home/amandapotts/git/nltk/nltk/corpus/reader/bracket_parse.py_tagged_words,"def tagged_words(self, fileids=None, categories=None, tagset=None):
return super().tagged_words(self._resolve(fileids, categories), tagset)
",[],0,[],/corpus/reader/bracket_parse.py_tagged_words
4382,/home/amandapotts/git/nltk/nltk/corpus/reader/bracket_parse.py_tagged_sents,"def tagged_sents(self, fileids=None, categories=None, tagset=None):
return super().tagged_sents(self._resolve(fileids, categories), tagset)
",[],0,[],/corpus/reader/bracket_parse.py_tagged_sents
4383,/home/amandapotts/git/nltk/nltk/corpus/reader/bracket_parse.py_tagged_paras,"def tagged_paras(self, fileids=None, categories=None, tagset=None):
return super().tagged_paras(self._resolve(fileids, categories), tagset)
",[],0,[],/corpus/reader/bracket_parse.py_tagged_paras
4384,/home/amandapotts/git/nltk/nltk/corpus/reader/bracket_parse.py_parsed_words,"def parsed_words(self, fileids=None, categories=None):
return super().parsed_words(self._resolve(fileids, categories))
",[],0,[],/corpus/reader/bracket_parse.py_parsed_words
4385,/home/amandapotts/git/nltk/nltk/corpus/reader/bracket_parse.py_parsed_sents,"def parsed_sents(self, fileids=None, categories=None):
return super().parsed_sents(self._resolve(fileids, categories))
",[],0,[],/corpus/reader/bracket_parse.py_parsed_sents
4386,/home/amandapotts/git/nltk/nltk/corpus/reader/bracket_parse.py_parsed_paras,"def parsed_paras(self, fileids=None, categories=None):
return super().parsed_paras(self._resolve(fileids, categories))
",[],0,[],/corpus/reader/bracket_parse.py_parsed_paras
4387,/home/amandapotts/git/nltk/nltk/corpus/reader/bracket_parse.py___init__,"def __init__(self, root, encoding=""ISO-8859-1"", tagset=None):
BracketParseCorpusReader.__init__(
self,
root,
r""alpino\.xml"",
detect_blocks=""blankline"",
encoding=encoding,
tagset=tagset,
)
",[],0,[],/corpus/reader/bracket_parse.py___init__
4388,/home/amandapotts/git/nltk/nltk/corpus/reader/bracket_parse.py__normalize,"def _normalize(self, t, ordered=False):
""""""Normalize the xml sentence element in t.
The sentence elements <alpino_ds>, although embedded in a few overall
xml elements, are separated by blank lines. That's how the reader can
deliver them one at a time.
Each sentence has a few category subnodes that are of no use to us.
The remaining word nodes may or may not appear in the proper order.
Each word node has attributes, among which:
- begin : the position of the word in the sentence
- pos   : Part of Speech: the Tag
- word  : the actual word
The return value is a string with all xml elementes replaced by
clauses: either a cat clause with nested clauses, or a word clause.
The order of the bracket clauses closely follows the xml.
If ordered == True, the word clauses include an order sequence number.
If ordered == False, the word clauses only have pos and word parts.
""""""
if t[:10] != ""<alpino_ds"":
return """"
t = re.sub(r'  <node .*? cat=""(\w+)"".*>', r""(\1"", t)
if ordered:
t = re.sub(
r'  <node. *?begin=""(\d+)"".*? pos=""(\w+)"".*? word=""([^""]+)"".*?/>',
r""(\1 \2 \3)"",
t,
)
else:
t = re.sub(r'  <node .*?pos=""(\w+)"".*? word=""([^""]+)"".*?/>', r""(\1 \2)"", t)
t = re.sub(r""  </node>"", r"")"", t)
t = re.sub(r""<sentence>.*</sentence>"", r"""", t)
t = re.sub(r""</?alpino_ds.*>"", r"""", t)
return t
",[],0,[],/corpus/reader/bracket_parse.py__normalize
4389,/home/amandapotts/git/nltk/nltk/corpus/reader/bracket_parse.py__tag,"def _tag(self, t, tagset=None):
tagged_sent = [
(int(o), w, p)
for (o, p, w) in SORTTAGWRD.findall(self._normalize(t, ordered=True))
]
tagged_sent.sort()
if tagset and tagset != self._tagset:
tagged_sent = [
(w, map_tag(self._tagset, tagset, p)) for (o, w, p) in tagged_sent
]
else:
tagged_sent = [(w, p) for (o, w, p) in tagged_sent]
return tagged_sent
",[],0,[],/corpus/reader/bracket_parse.py__tag
4390,/home/amandapotts/git/nltk/nltk/corpus/reader/bracket_parse.py__word,"def _word(self, t):
""""""Return a correctly ordered list if words""""""
tagged_sent = self._tag(t)
return [w for (w, p) in tagged_sent]
",[],0,[],/corpus/reader/bracket_parse.py__word
4391,/home/amandapotts/git/nltk/nltk/corpus/reader/ieer.py___init__,"def __init__(self, text, docno=None, doctype=None, date_time=None, headline=""""):
self.text = text
self.docno = docno
self.doctype = doctype
self.date_time = date_time
self.headline = headline
",[],0,[],/corpus/reader/ieer.py___init__
4392,/home/amandapotts/git/nltk/nltk/corpus/reader/ieer.py___repr__,"def __repr__(self):
if self.headline:
headline = "" "".join(self.headline.leaves())
else:
headline = (
"" "".join([w for w in self.text.leaves() if w[:1] != ""<""][:12]) + ""...""
)
if self.docno is not None:
return f""<IEERDocument {self.docno}: {headline!r}>""
else:
return ""<IEERDocument: %r>"" % headline
",[],0,[],/corpus/reader/ieer.py___repr__
4393,/home/amandapotts/git/nltk/nltk/corpus/reader/ieer.py_docs,"def docs(self, fileids=None):
return concat(
[
StreamBackedCorpusView(fileid, self._read_block, encoding=enc)
for (fileid, enc) in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/ieer.py_docs
4394,/home/amandapotts/git/nltk/nltk/corpus/reader/ieer.py_parsed_docs,"def parsed_docs(self, fileids=None):
return concat(
[
StreamBackedCorpusView(fileid, self._read_parsed_block, encoding=enc)
for (fileid, enc) in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/ieer.py_parsed_docs
4395,/home/amandapotts/git/nltk/nltk/corpus/reader/ieer.py__read_parsed_block,"def _read_parsed_block(self, stream):
return [
self._parse(doc)
for doc in self._read_block(stream)
if self._parse(doc).docno is not None
]
",[],0,[],/corpus/reader/ieer.py__read_parsed_block
4396,/home/amandapotts/git/nltk/nltk/corpus/reader/ieer.py__parse,"def _parse(self, doc):
val = nltk.chunk.ieerstr2tree(doc, root_label=""DOCUMENT"")
if isinstance(val, dict):
return IEERDocument(**val)
else:
return IEERDocument(val)
",[],0,[],/corpus/reader/ieer.py__parse
4397,/home/amandapotts/git/nltk/nltk/corpus/reader/ieer.py__read_block,"def _read_block(self, stream):
out = []
while True:
line = stream.readline()
if not line:
break
if line.strip() == ""<DOC>"":
break
out.append(line)
while True:
line = stream.readline()
if not line:
break
out.append(line)
if line.strip() == ""</DOC>"":
break
return [""\n"".join(out)]
",[],0,[],/corpus/reader/ieer.py__read_block
4398,/home/amandapotts/git/nltk/nltk/corpus/reader/twitter.py___init__,"def __init__(
self, root, fileids=None, word_tokenizer=TweetTokenizer(), encoding=""utf8""
",[],0,[],/corpus/reader/twitter.py___init__
4399,/home/amandapotts/git/nltk/nltk/corpus/reader/twitter.py_docs,"def docs(self, fileids=None):
""""""
Returns the full Tweet objects, as specified by `Twitter
documentation on Tweets
<https://dev.twitter.com/docs/platform-objects/tweets>`_
:return: the given file(s) as a list of dictionaries deserialised
from JSON.
:rtype: list(dict)
""""""
return concat(
[
self.CorpusView(path, self._read_tweets, encoding=enc)
for (path, enc, fileid) in self.abspaths(fileids, True, True)
]
)
",[],0,[],/corpus/reader/twitter.py_docs
4400,/home/amandapotts/git/nltk/nltk/corpus/reader/twitter.py_strings,"def strings(self, fileids=None):
""""""
Returns only the text content of Tweets in the file(s)
:return: the given file(s) as a list of Tweets.
:rtype: list(str)
""""""
fulltweets = self.docs(fileids)
tweets = []
for jsono in fulltweets:
try:
text = jsono[""text""]
if isinstance(text, bytes):
text = text.decode(self.encoding)
tweets.append(text)
except KeyError:
pass
return tweets
",[],0,[],/corpus/reader/twitter.py_strings
4401,/home/amandapotts/git/nltk/nltk/corpus/reader/twitter.py_tokenized,"def tokenized(self, fileids=None):
""""""
:return: the given file(s) as a list of the text content of Tweets as
as a list of words, screenanames, hashtags, URLs and punctuation symbols.
:rtype: list(list(str))
""""""
tweets = self.strings(fileids)
tokenizer = self._word_tokenizer
return [tokenizer.tokenize(t) for t in tweets]
",[],0,[],/corpus/reader/twitter.py_tokenized
4402,/home/amandapotts/git/nltk/nltk/corpus/reader/twitter.py__read_tweets,"def _read_tweets(self, stream):
""""""
Assumes that each line in ``stream`` is a JSON-serialised object.
""""""
tweets = []
for i in range(10):
line = stream.readline()
if not line:
return tweets
tweet = json.loads(line)
tweets.append(tweet)
return tweets
",[],0,[],/corpus/reader/twitter.py__read_tweets
4403,/home/amandapotts/git/nltk/nltk/corpus/reader/rte.py_norm,"def norm(value_string):
""""""
Normalize the string value in an RTE pair's ``value`` or ``entailment``
attribute as an integer (1, 0).
:param value_string: the label used to classify a text/hypothesis pair
:type value_string: str
:rtype: int
""""""
valdict = {""TRUE"": 1, ""FALSE"": 0, ""YES"": 1, ""NO"": 0}
return valdict[value_string.upper()]
",[],0,[],/corpus/reader/rte.py_norm
4404,/home/amandapotts/git/nltk/nltk/corpus/reader/rte.py___init__,"def __init__(
self,
pair,
challenge=None,
id=None,
text=None,
hyp=None,
value=None,
task=None,
length=None,
",[],0,[],/corpus/reader/rte.py___init__
4405,/home/amandapotts/git/nltk/nltk/corpus/reader/rte.py___repr__,"def __repr__(self):
if self.challenge:
return f""<RTEPair: gid={self.challenge}-{self.id}>""
else:
return ""<RTEPair: id=%s>"" % self.id
",[],0,[],/corpus/reader/rte.py___repr__
4406,/home/amandapotts/git/nltk/nltk/corpus/reader/rte.py__read_etree,"def _read_etree(self, doc):
""""""
Map the XML input into an RTEPair.
This uses the ``getiterator()`` method from the ElementTree package to
find all the ``<pair>`` elements.
:param doc: a parsed XML document
:rtype: list(RTEPair)
""""""
try:
challenge = doc.attrib[""challenge""]
except KeyError:
challenge = None
pairiter = doc.iter(""pair"")
return [RTEPair(pair, challenge=challenge) for pair in pairiter]
",[],0,[],/corpus/reader/rte.py__read_etree
4407,/home/amandapotts/git/nltk/nltk/corpus/reader/rte.py_pairs,"def pairs(self, fileids):
""""""
Build a list of RTEPairs from a RTE corpus.
:param fileids: a list of RTE corpus fileids
:type: list
:rtype: list(RTEPair)
""""""
if isinstance(fileids, str):
fileids = [fileids]
return concat([self._read_etree(self.xml(fileid)) for fileid in fileids])
",[],0,[],/corpus/reader/rte.py_pairs
4408,/home/amandapotts/git/nltk/nltk/corpus/reader/pros_cons.py___init__,"def __init__(
self,
root,
fileids,
word_tokenizer=WordPunctTokenizer(),
encoding=""utf8"",
",[],0,[],/corpus/reader/pros_cons.py___init__
4409,/home/amandapotts/git/nltk/nltk/corpus/reader/pros_cons.py_sents,"def sents(self, fileids=None, categories=None):
""""""
Return all sentences in the corpus or in the specified files/categories.
:param fileids: a list or regexp specifying the ids of the files whose
sentences have to be returned.
:param categories: a list specifying the categories whose sentences
have to be returned.
:return: the given file(s) as a list of sentences. Each sentence is
tokenized using the specified word_tokenizer.
:rtype: list(list(str))
""""""
fileids = self._resolve(fileids, categories)
if fileids is None:
fileids = self._fileids
elif isinstance(fileids, str):
fileids = [fileids]
return concat(
[
self.CorpusView(path, self._read_sent_block, encoding=enc)
for (path, enc, fileid) in self.abspaths(fileids, True, True)
]
)
",[],0,[],/corpus/reader/pros_cons.py_sents
4410,/home/amandapotts/git/nltk/nltk/corpus/reader/pros_cons.py_words,"def words(self, fileids=None, categories=None):
""""""
Return all words and punctuation symbols in the corpus or in the specified
files/categories.
:param fileids: a list or regexp specifying the ids of the files whose
words have to be returned.
:param categories: a list specifying the categories whose words have
to be returned.
:return: the given file(s) as a list of words and punctuation symbols.
:rtype: list(str)
""""""
fileids = self._resolve(fileids, categories)
if fileids is None:
fileids = self._fileids
elif isinstance(fileids, str):
fileids = [fileids]
return concat(
[
self.CorpusView(path, self._read_word_block, encoding=enc)
for (path, enc, fileid) in self.abspaths(fileids, True, True)
]
)
",[],0,[],/corpus/reader/pros_cons.py_words
4411,/home/amandapotts/git/nltk/nltk/corpus/reader/pros_cons.py__read_sent_block,"def _read_sent_block(self, stream):
sents = []
for i in range(20):  # Read 20 lines at a time.
line = stream.readline()
if not line:
continue
sent = re.match(r""^(?!\n)\s*<(Pros|Cons)>(.*)</(?:Pros|Cons)>"", line)
if sent:
sents.append(self._word_tokenizer.tokenize(sent.group(2).strip()))
return sents
",[],0,[],/corpus/reader/pros_cons.py__read_sent_block
4412,/home/amandapotts/git/nltk/nltk/corpus/reader/pros_cons.py__read_word_block,"def _read_word_block(self, stream):
words = []
for sent in self._read_sent_block(stream):
words.extend(sent)
return words
",[],0,[],/corpus/reader/pros_cons.py__read_word_block
4413,/home/amandapotts/git/nltk/nltk/corpus/reader/chasen.py___init__,"def __init__(self, root, fileids, encoding=""utf8"", sent_splitter=None):
self._sent_splitter = sent_splitter
CorpusReader.__init__(self, root, fileids, encoding)
",[],0,[],/corpus/reader/chasen.py___init__
4414,/home/amandapotts/git/nltk/nltk/corpus/reader/chasen.py_words,"def words(self, fileids=None):
return concat(
[
ChasenCorpusView(fileid, enc, False, False, False, self._sent_splitter)
for (fileid, enc) in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/chasen.py_words
4415,/home/amandapotts/git/nltk/nltk/corpus/reader/chasen.py_tagged_words,"def tagged_words(self, fileids=None):
return concat(
[
ChasenCorpusView(fileid, enc, True, False, False, self._sent_splitter)
for (fileid, enc) in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/chasen.py_tagged_words
4416,/home/amandapotts/git/nltk/nltk/corpus/reader/chasen.py_sents,"def sents(self, fileids=None):
return concat(
[
ChasenCorpusView(fileid, enc, False, True, False, self._sent_splitter)
for (fileid, enc) in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/chasen.py_sents
4417,/home/amandapotts/git/nltk/nltk/corpus/reader/chasen.py_tagged_sents,"def tagged_sents(self, fileids=None):
return concat(
[
ChasenCorpusView(fileid, enc, True, True, False, self._sent_splitter)
for (fileid, enc) in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/chasen.py_tagged_sents
4418,/home/amandapotts/git/nltk/nltk/corpus/reader/chasen.py_paras,"def paras(self, fileids=None):
return concat(
[
ChasenCorpusView(fileid, enc, False, True, True, self._sent_splitter)
for (fileid, enc) in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/chasen.py_paras
4419,/home/amandapotts/git/nltk/nltk/corpus/reader/chasen.py_tagged_paras,"def tagged_paras(self, fileids=None):
return concat(
[
ChasenCorpusView(fileid, enc, True, True, True, self._sent_splitter)
for (fileid, enc) in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/chasen.py_tagged_paras
4420,/home/amandapotts/git/nltk/nltk/corpus/reader/chasen.py___init__,"def __init__(
self,
corpus_file,
encoding,
tagged,
group_by_sent,
group_by_para,
sent_splitter=None,
",[],0,[],/corpus/reader/chasen.py___init__
4421,/home/amandapotts/git/nltk/nltk/corpus/reader/chasen.py_read_block,"def read_block(self, stream):
""""""Reads one paragraph at a time.""""""
block = []
for para_str in read_regexp_block(stream, r""."", r""^EOS\n""):
para = []
sent = []
for line in para_str.splitlines():
_eos = line.strip() == ""EOS""
_cells = line.split(""\t"")
w = (_cells[0], ""\t"".join(_cells[1:]))
if not _eos:
sent.append(w)
if _eos or (self._sent_splitter and self._sent_splitter(w)):
if not self._tagged:
sent = [w for (w, t) in sent]
if self._group_by_sent:
para.append(sent)
else:
para.extend(sent)
sent = []
if len(sent) > 0:
if not self._tagged:
sent = [w for (w, t) in sent]
if self._group_by_sent:
para.append(sent)
else:
para.extend(sent)
if self._group_by_para:
block.append(para)
else:
block.extend(para)
return block
",[],0,[],/corpus/reader/chasen.py_read_block
4422,/home/amandapotts/git/nltk/nltk/corpus/reader/chasen.py_demo,"def demo():
import nltk
from nltk.corpus.util import LazyCorpusLoader
jeita = LazyCorpusLoader(""jeita"", ChasenCorpusReader, r"".*chasen"", encoding=""utf-8"")
print(""/"".join(jeita.words()[22100:22140]))
print(
""\nEOS\n"".join(
""\n"".join(""{}/{}"".format(w[0], w[1].split(""\t"")[2]) for w in sent)
for sent in jeita.tagged_sents()[2170:2173]
)
)
",[],0,[],/corpus/reader/chasen.py_demo
4423,/home/amandapotts/git/nltk/nltk/corpus/reader/chasen.py_test,"def test():
from nltk.corpus.util import LazyCorpusLoader
jeita = LazyCorpusLoader(""jeita"", ChasenCorpusReader, r"".*chasen"", encoding=""utf-8"")
assert isinstance(jeita.tagged_words()[0][1], str)
",[],0,[],/corpus/reader/chasen.py_test
4424,/home/amandapotts/git/nltk/nltk/corpus/reader/senseval.py___init__,"def __init__(self, word, position, context, senses):
self.word = word
self.senses = tuple(senses)
self.position = position
self.context = context
",[],0,[],/corpus/reader/senseval.py___init__
4425,/home/amandapotts/git/nltk/nltk/corpus/reader/senseval.py___repr__,"def __repr__(self):
return ""SensevalInstance(word=%r, position=%r, "" ""context=%r, senses=%r)"" % (
self.word,
self.position,
self.context,
self.senses,
)
",[],0,[],/corpus/reader/senseval.py___repr__
4426,/home/amandapotts/git/nltk/nltk/corpus/reader/senseval.py_instances,"def instances(self, fileids=None):
return concat(
[
SensevalCorpusView(fileid, enc)
for (fileid, enc) in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/senseval.py_instances
4427,/home/amandapotts/git/nltk/nltk/corpus/reader/senseval.py__entry,"def _entry(self, tree):
elts = []
for lexelt in tree.findall(""lexelt""):
for inst in lexelt.findall(""instance""):
sense = inst[0].attrib[""senseid""]
context = [(w.text, w.attrib[""pos""]) for w in inst[1]]
elts.append((sense, context))
return elts
",[],0,[],/corpus/reader/senseval.py__entry
4428,/home/amandapotts/git/nltk/nltk/corpus/reader/senseval.py___init__,"def __init__(self, fileid, encoding):
StreamBackedCorpusView.__init__(self, fileid, encoding=encoding)
self._word_tokenizer = WhitespaceTokenizer()
self._lexelt_starts = [0]  # list of streampos
self._lexelts = [None]  # list of lexelt names
",[],0,[],/corpus/reader/senseval.py___init__
4429,/home/amandapotts/git/nltk/nltk/corpus/reader/senseval.py_read_block,"def read_block(self, stream):
lexelt_num = bisect.bisect_right(self._lexelt_starts, stream.tell()) - 1
lexelt = self._lexelts[lexelt_num]
instance_lines = []
in_instance = False
while True:
line = stream.readline()
if line == """":
assert instance_lines == []
return []
if line.lstrip().startswith(""<lexelt""):
lexelt_num += 1
m = re.search(""item=(\""[^\""]+\""|'[^']+')"", line)
assert m is not None  # <lexelt> has no 'item=...'
lexelt = m.group(1)[1:-1]
if lexelt_num < len(self._lexelts):
assert lexelt == self._lexelts[lexelt_num]
else:
self._lexelts.append(lexelt)
self._lexelt_starts.append(stream.tell())
if line.lstrip().startswith(""<instance""):
assert instance_lines == []
in_instance = True
if in_instance:
instance_lines.append(line)
if line.lstrip().startswith(""</instance""):
xml_block = ""\n"".join(instance_lines)
xml_block = _fixXML(xml_block)
inst = ElementTree.fromstring(xml_block)
return [self._parse_instance(inst, lexelt)]
",[],0,[],/corpus/reader/senseval.py_read_block
4430,/home/amandapotts/git/nltk/nltk/corpus/reader/senseval.py__parse_instance,"def _parse_instance(self, instance, lexelt):
senses = []
context = []
position = None
for child in instance:
if child.tag == ""answer"":
senses.append(child.attrib[""senseid""])
elif child.tag == ""context"":
context += self._word_tokenizer.tokenize(child.text)
for cword in child:
if cword.tag == ""compound"":
cword = cword[0]  # is this ok to do?
if cword.tag == ""head"":
assert position is None, ""head specified twice""
assert cword.text.strip() or len(cword) == 1
assert not (cword.text.strip() and len(cword) == 1)
position = len(context)
if cword.text.strip():
context.append(cword.text.strip())
elif cword[0].tag == ""wf"":
context.append((cword[0].text, cword[0].attrib[""pos""]))
if cword[0].tail:
context += self._word_tokenizer.tokenize(cword[0].tail)
else:
assert False, ""expected CDATA or wf in <head>""
elif cword.tag == ""wf"":
context.append((cword.text, cword.attrib[""pos""]))
elif cword.tag == ""s"":
pass  # Sentence boundary marker.
else:
print(""ACK"", cword.tag)
assert False, ""expected CDATA or <wf> or <head>""
if cword.tail:
context += self._word_tokenizer.tokenize(cword.tail)
else:
assert False, ""unexpected tag %s"" % child.tag
return SensevalInstance(lexelt, position, context, senses)
",[],0,[],/corpus/reader/senseval.py__parse_instance
4431,/home/amandapotts/git/nltk/nltk/corpus/reader/senseval.py__fixXML,"def _fixXML(text):
""""""
Fix the various issues with Senseval pseudo-XML.
""""""
text = re.sub(r""<([~\^])>"", r""\1"", text)
text = re.sub(r""(\s+)\&(\s+)"", r""\1&amp
text = re.sub(r'""""""', ""'\""'"", text)
text = re.sub(r'(<[^<]*snum=)([^"">]+)>', r'\1""\2""/>', text)
text = re.sub(r""<\&frasl>\s*<p[^>]*>"", ""FRASL"", text)
text = re.sub(r""<\&I[^>]*>"", """", text)
text = re.sub(r""<{([^}]+)}>"", r""\1"", text)
text = re.sub(r""<(@|/?p)>"", r"""", text)
text = re.sub(r""<&\w+ \.>"", r"""", text)
text = re.sub(r""<!DOCTYPE[^>]*>"", r"""", text)
text = re.sub(r""<\[\/?[^>]+\]*>"", r"""", text)
text = re.sub(r""<(\&\w+
text = re.sub(r""&(?!amp|gt|lt|apos|quot)"", r"""", text)
text = re.sub(
r'[ \t]*([^<>\s]+?)[ \t]*<p=""([^""]*""?)""/>', r' <wf pos=""\2"">\1</wf>', text
)
text = re.sub(r'\s*""\s*<p=\'""\'/>', "" <wf pos='\""'>\""</wf>"", text)
return text
",[],0,[],/corpus/reader/senseval.py__fixXML
4432,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py___init__,"def __init__(self, root, fileids, encoding=""utf8"", tagset=None):
""""""
:type root: PathPointer or str
:param root: A path pointer identifying the root directory for
this corpus.  If a string is specified, then it will be
converted to a ``PathPointer`` automatically.
:param fileids: A list of the files that make up this corpus.
This list can either be specified explicitly, as a list of
strings
paths.  The absolute path for each file will be constructed
by joining the reader's root to each file name.
:param encoding: The default unicode encoding for the files
that make up the corpus.  The value of ``encoding`` can be any
of the following:
- A string: ``encoding`` is the encoding name for all files.
- A dictionary: ``encoding[file_id]`` is the encoding
name for the file whose identifier is ``file_id``.  If
``file_id`` is not in ``encoding``, then the file
contents will be processed using non-unicode byte strings.
- A list: ``encoding`` should be a list of ``(regexp, encoding)``
tuples.  The encoding for a file whose identifier is ``file_id``
will be the ``encoding`` value for the first tuple whose
``regexp`` matches the ``file_id``.  If no tuple's ``regexp``
matches the ``file_id``, the file contents will be processed
using non-unicode byte strings.
- None: the file contents of all files will be
processed using non-unicode byte strings.
:param tagset: The name of the tagset used by this corpus, to be used
for normalizing or converting the POS tags returned by the
``tagged_...()`` methods.
""""""
if isinstance(root, str) and not isinstance(root, PathPointer):
m = re.match(r""(.*\.zip)/?(.*)$|"", root)
zipfile, zipentry = m.groups()
if zipfile:
root = ZipFilePathPointer(zipfile, zipentry)
else:
root = FileSystemPathPointer(root)
elif not isinstance(root, PathPointer):
raise TypeError(""CorpusReader: expected a string or a PathPointer"")
if isinstance(fileids, str):
fileids = find_corpus_fileids(root, fileids)
self._fileids = fileids
""""""A list of the relative paths for the fileids that make up
this corpus.""""""
self._root = root
""""""The root directory for this corpus.""""""
self._readme = ""README""
self._license = ""LICENSE""
self._citation = ""citation.bib""
if isinstance(encoding, list):
encoding_dict = {}
for fileid in self._fileids:
for x in encoding:
(regexp, enc) = x
if re.match(regexp, fileid):
encoding_dict[fileid] = enc
break
encoding = encoding_dict
self._encoding = encoding
""""""The default unicode encoding for the fileids that make up
this corpus.  If ``encoding`` is None, then the file
contents are processed using byte strings.""""""
self._tagset = tagset
",[],0,[],/corpus/reader/api.py___init__
4433,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py___repr__,"def __repr__(self):
if isinstance(self._root, ZipFilePathPointer):
path = f""{self._root.zipfile.filename}/{self._root.entry}""
else:
path = ""%s"" % self._root.path
return f""<{self.__class__.__name__} in {path!r}>""
",[],0,[],/corpus/reader/api.py___repr__
4434,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py_ensure_loaded,"def ensure_loaded(self):
""""""
Load this corpus (if it has not already been loaded).  This is
used by LazyCorpusLoader as a simple method that can be used to
make sure a corpus is loaded -- e.g., in case a user wants to
do help(some_corpus).
""""""
pass  # no need to actually do anything.
",[],0,[],/corpus/reader/api.py_ensure_loaded
4435,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py_readme,"def readme(self):
""""""
Return the contents of the corpus README file, if it exists.
""""""
with self.open(self._readme) as f:
return f.read()
",[],0,[],/corpus/reader/api.py_readme
4436,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py_license,"def license(self):
""""""
Return the contents of the corpus LICENSE file, if it exists.
""""""
with self.open(self._license) as f:
return f.read()
",[],0,[],/corpus/reader/api.py_license
4437,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py_citation,"def citation(self):
""""""
Return the contents of the corpus citation.bib file, if it exists.
""""""
with self.open(self._citation) as f:
return f.read()
",[],0,[],/corpus/reader/api.py_citation
4438,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py_fileids,"def fileids(self):
""""""
Return a list of file identifiers for the fileids that make up
this corpus.
""""""
return self._fileids
",[],0,[],/corpus/reader/api.py_fileids
4439,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py_abspath,"def abspath(self, fileid):
""""""
Return the absolute path for the given file.
:type fileid: str
:param fileid: The file identifier for the file whose path
should be returned.
:rtype: PathPointer
""""""
return self._root.join(fileid)
",[],0,[],/corpus/reader/api.py_abspath
4440,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py_abspaths,"def abspaths(self, fileids=None, include_encoding=False, include_fileid=False):
""""""
Return a list of the absolute paths for all fileids in this corpus
or for the given list of fileids, if specified.
:type fileids: None or str or list
:param fileids: Specifies the set of fileids for which paths should
be returned.  Can be None, for all fileids
file identifiers, for a specified set of fileids
file identifier, for a single file.  Note that the return
value is always a list of paths, even if ``fileids`` is a
single file identifier.
:param include_encoding: If true, then return a list of
``(path_pointer, encoding)`` tuples.
:rtype: list(PathPointer)
""""""
if fileids is None:
fileids = self._fileids
elif isinstance(fileids, str):
fileids = [fileids]
paths = [self._root.join(f) for f in fileids]
if include_encoding and include_fileid:
return list(zip(paths, [self.encoding(f) for f in fileids], fileids))
elif include_fileid:
return list(zip(paths, fileids))
elif include_encoding:
return list(zip(paths, [self.encoding(f) for f in fileids]))
else:
return paths
",[],0,[],/corpus/reader/api.py_abspaths
4441,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py_raw,"def raw(self, fileids=None):
""""""
:param fileids: A list specifying the fileids that should be used.
:return: the given file(s) as a single string.
:rtype: str
""""""
if fileids is None:
fileids = self._fileids
elif isinstance(fileids, str):
fileids = [fileids]
contents = []
for f in fileids:
with self.open(f) as fp:
contents.append(fp.read())
return concat(contents)
",[],0,[],/corpus/reader/api.py_raw
4442,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py_open,"def open(self, file):
""""""
Return an open stream that can be used to read the given file.
If the file's encoding is not None, then the stream will
automatically decode the file's contents into unicode.
:param file: The file identifier of the file to read.
""""""
encoding = self.encoding(file)
stream = self._root.join(file).open(encoding)
return stream
",[],0,[],/corpus/reader/api.py_open
4443,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py_encoding,"def encoding(self, file):
""""""
Return the unicode encoding for the given corpus file, if known.
If the encoding is unknown, or if the given file should be
processed using byte strings (str), then return None.
""""""
if isinstance(self._encoding, dict):
return self._encoding.get(file)
else:
return self._encoding
",[],0,[],/corpus/reader/api.py_encoding
4444,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py__get_root,"def _get_root(self):
return self._root
",[],0,[],/corpus/reader/api.py__get_root
4445,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py___init__,"def __init__(self, kwargs):
""""""
Initialize this mapping based on keyword arguments, as
follows:
- cat_pattern: A regular expression pattern used to find the
category for each file identifier.  The pattern will be
applied to each file identifier, and the first matching
group will be used as the category label for that file.
- cat_map: A dictionary, mapping from file identifiers to
category labels.
- cat_file: The name of a file that contains the mapping
from file identifiers to categories.  The argument
``cat_delimiter`` can be used to specify a delimiter.
The corresponding argument will be deleted from ``kwargs``.  If
more than one argument is specified, an exception will be
raised.
""""""
self._f2c = None  #: file-to-category mapping
self._c2f = None  #: category-to-file mapping
self._pattern = None  #: regexp specifying the mapping
self._map = None  #: dict specifying the mapping
self._file = None  #: fileid of file containing the mapping
self._delimiter = None  #: delimiter for ``self._file``
if ""cat_pattern"" in kwargs:
self._pattern = kwargs[""cat_pattern""]
del kwargs[""cat_pattern""]
elif ""cat_map"" in kwargs:
self._map = kwargs[""cat_map""]
del kwargs[""cat_map""]
elif ""cat_file"" in kwargs:
self._file = kwargs[""cat_file""]
del kwargs[""cat_file""]
if ""cat_delimiter"" in kwargs:
self._delimiter = kwargs[""cat_delimiter""]
del kwargs[""cat_delimiter""]
else:
raise ValueError(
""Expected keyword argument cat_pattern or "" ""cat_map or cat_file.""
)
if ""cat_pattern"" in kwargs or ""cat_map"" in kwargs or ""cat_file"" in kwargs:
raise ValueError(
""Specify exactly one of: cat_pattern, "" ""cat_map, cat_file.""
)
",[],0,[],/corpus/reader/api.py___init__
4446,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py__init,"def _init(self):
self._f2c = defaultdict(set)
self._c2f = defaultdict(set)
if self._pattern is not None:
for file_id in self._fileids:
category = re.match(self._pattern, file_id).group(1)
self._add(file_id, category)
elif self._map is not None:
for file_id, categories in self._map.items():
for category in categories:
self._add(file_id, category)
elif self._file is not None:
with self.open(self._file) as f:
for line in f.readlines():
line = line.strip()
file_id, categories = line.split(self._delimiter, 1)
if file_id not in self.fileids():
raise ValueError(
""In category mapping file %s: %s ""
""not found"" % (self._file, file_id)
)
for category in categories.split(self._delimiter):
self._add(file_id, category)
",[],0,[],/corpus/reader/api.py__init
4447,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py__add,"def _add(self, file_id, category):
self._f2c[file_id].add(category)
self._c2f[category].add(file_id)
",[],0,[],/corpus/reader/api.py__add
4448,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py_categories,"def categories(self, fileids=None):
""""""
Return a list of the categories that are defined for this corpus,
or for the file(s) if it is given.
""""""
if self._f2c is None:
self._init()
if fileids is None:
return sorted(self._c2f)
if isinstance(fileids, str):
fileids = [fileids]
return sorted(set.union(*(self._f2c[d] for d in fileids)))
",[],0,[],/corpus/reader/api.py_categories
4449,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py_fileids,"def fileids(self, categories=None):
""""""
Return a list of file identifiers for the files that make up
this corpus, or that make up the given category(s) if specified.
""""""
if categories is None:
return super().fileids()
elif isinstance(categories, str):
if self._f2c is None:
self._init()
if categories in self._c2f:
return sorted(self._c2f[categories])
else:
raise ValueError(""Category %s not found"" % categories)
else:
if self._f2c is None:
self._init()
return sorted(set.union(*(self._c2f[c] for c in categories)))
",[],0,[],/corpus/reader/api.py_fileids
4450,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py__resolve,"def _resolve(self, fileids, categories):
if fileids is not None and categories is not None:
raise ValueError(""Specify fileids or categories, not both"")
if categories is not None:
return self.fileids(categories)
else:
return fileids
",[],0,[],/corpus/reader/api.py__resolve
4451,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py_raw,"def raw(self, fileids=None, categories=None):
return super().raw(self._resolve(fileids, categories))
",[],0,[],/corpus/reader/api.py_raw
4452,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py_words,"def words(self, fileids=None, categories=None):
return super().words(self._resolve(fileids, categories))
",[],0,[],/corpus/reader/api.py_words
4453,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py_sents,"def sents(self, fileids=None, categories=None):
return super().sents(self._resolve(fileids, categories))
",[],0,[],/corpus/reader/api.py_sents
4454,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py_paras,"def paras(self, fileids=None, categories=None):
return super().paras(self._resolve(fileids, categories))
",[],0,[],/corpus/reader/api.py_paras
4455,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py__parse,"def _parse(self, s):
raise NotImplementedError()
",[],0,[],/corpus/reader/api.py__parse
4456,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py__word,"def _word(self, s):
raise NotImplementedError()
",[],0,[],/corpus/reader/api.py__word
4457,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py__tag,"def _tag(self, s):
raise NotImplementedError()
",[],0,[],/corpus/reader/api.py__tag
4458,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py__read_block,"def _read_block(self, stream):
raise NotImplementedError()
",[],0,[],/corpus/reader/api.py__read_block
4459,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py_parsed_sents,"def parsed_sents(self, fileids=None):
reader = self._read_parsed_sent_block
return concat(
[
StreamBackedCorpusView(fileid, reader, encoding=enc)
for fileid, enc in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/api.py_parsed_sents
4460,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py_tagged_sents,"def tagged_sents(self, fileids=None, tagset=None):
",[],0,[],/corpus/reader/api.py_tagged_sents
4461,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py_reader,"def reader(stream):
return self._read_tagged_sent_block(stream, tagset)
",[],0,[],/corpus/reader/api.py_reader
4462,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py_sents,"def sents(self, fileids=None):
reader = self._read_sent_block
return concat(
[
StreamBackedCorpusView(fileid, reader, encoding=enc)
for fileid, enc in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/api.py_sents
4463,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py_tagged_words,"def tagged_words(self, fileids=None, tagset=None):
",[],0,[],/corpus/reader/api.py_tagged_words
4464,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py_reader,"def reader(stream):
return self._read_tagged_word_block(stream, tagset)
",[],0,[],/corpus/reader/api.py_reader
4465,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py_words,"def words(self, fileids=None):
return concat(
[
StreamBackedCorpusView(fileid, self._read_word_block, encoding=enc)
for fileid, enc in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/api.py_words
4466,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py__read_word_block,"def _read_word_block(self, stream):
return list(chain.from_iterable(self._read_sent_block(stream)))
",[],0,[],/corpus/reader/api.py__read_word_block
4467,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py__read_tagged_word_block,"def _read_tagged_word_block(self, stream, tagset=None):
return list(chain.from_iterable(self._read_tagged_sent_block(stream, tagset)))
",[],0,[],/corpus/reader/api.py__read_tagged_word_block
4468,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py__read_sent_block,"def _read_sent_block(self, stream):
return list(filter(None, [self._word(t) for t in self._read_block(stream)]))
",[],0,[],/corpus/reader/api.py__read_sent_block
4469,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py__read_tagged_sent_block,"def _read_tagged_sent_block(self, stream, tagset=None):
return list(
filter(None, [self._tag(t, tagset) for t in self._read_block(stream)])
)
",[],0,[],/corpus/reader/api.py__read_tagged_sent_block
4470,/home/amandapotts/git/nltk/nltk/corpus/reader/api.py__read_parsed_sent_block,"def _read_parsed_sent_block(self, stream):
return list(filter(None, [self._parse(t) for t in self._read_block(stream)]))
",[],0,[],/corpus/reader/api.py__read_parsed_sent_block
4471,/home/amandapotts/git/nltk/nltk/corpus/reader/lin.py___defaultdict_factory,"def __defaultdict_factory():
""""""Factory for creating defaultdict of defaultdict(dict)s""""""
return defaultdict(dict)
",[],0,[],/corpus/reader/lin.py___defaultdict_factory
4472,/home/amandapotts/git/nltk/nltk/corpus/reader/lin.py___init__,"def __init__(self, root, badscore=0.0):
""""""
Initialize the thesaurus.
:param root: root directory containing thesaurus LISP files
:type root: C{string}
:param badscore: the score to give to words which do not appear in each other's sets of synonyms
:type badscore: C{float}
""""""
super().__init__(root, r""sim[A-Z]\.lsp"")
self._thesaurus = defaultdict(LinThesaurusCorpusReader.__defaultdict_factory)
self._badscore = badscore
for path, encoding, fileid in self.abspaths(
include_encoding=True, include_fileid=True
):
with open(path) as lin_file:
first = True
for line in lin_file:
line = line.strip()
if first:
key = LinThesaurusCorpusReader._key_re.sub(r""\1"", line)
first = False
elif line == ""))"":
first = True
else:
split_line = line.split(""\t"")
if len(split_line) == 2:
ngram, score = split_line
self._thesaurus[fileid][key][ngram.strip('""')] = float(
score
)
",[],0,[],/corpus/reader/lin.py___init__
4473,/home/amandapotts/git/nltk/nltk/corpus/reader/lin.py_similarity,"def similarity(self, ngram1, ngram2, fileid=None):
""""""
Returns the similarity score for two ngrams.
:param ngram1: first ngram to compare
:type ngram1: C{string}
:param ngram2: second ngram to compare
:type ngram2: C{string}
:param fileid: thesaurus fileid to search in. If None, search all fileids.
:type fileid: C{string}
:return: If fileid is specified, just the score for the two ngrams
list of tuples of fileids and scores.
""""""
if ngram1 == ngram2:
if fileid:
return 1.0
else:
return [(fid, 1.0) for fid in self._fileids]
else:
if fileid:
return (
self._thesaurus[fileid][ngram1][ngram2]
if ngram2 in self._thesaurus[fileid][ngram1]
else self._badscore
)
else:
return [
(
fid,
(
self._thesaurus[fid][ngram1][ngram2]
if ngram2 in self._thesaurus[fid][ngram1]
else self._badscore
),
)
for fid in self._fileids
]
",[],0,[],/corpus/reader/lin.py_similarity
4474,/home/amandapotts/git/nltk/nltk/corpus/reader/lin.py_scored_synonyms,"def scored_synonyms(self, ngram, fileid=None):
""""""
Returns a list of scored synonyms (tuples of synonyms and scores) for the current ngram
:param ngram: ngram to lookup
:type ngram: C{string}
:param fileid: thesaurus fileid to search in. If None, search all fileids.
:type fileid: C{string}
:return: If fileid is specified, list of tuples of scores and synonyms
list of tuples of fileids and lists, where inner lists consist of tuples of
scores and synonyms.
""""""
if fileid:
return self._thesaurus[fileid][ngram].items()
else:
return [
(fileid, self._thesaurus[fileid][ngram].items())
for fileid in self._fileids
]
",[],0,[],/corpus/reader/lin.py_scored_synonyms
4475,/home/amandapotts/git/nltk/nltk/corpus/reader/lin.py_synonyms,"def synonyms(self, ngram, fileid=None):
""""""
Returns a list of synonyms for the current ngram.
:param ngram: ngram to lookup
:type ngram: C{string}
:param fileid: thesaurus fileid to search in. If None, search all fileids.
:type fileid: C{string}
:return: If fileid is specified, list of synonyms
lists, where inner lists contain synonyms.
""""""
if fileid:
return self._thesaurus[fileid][ngram].keys()
else:
return [
(fileid, self._thesaurus[fileid][ngram].keys())
for fileid in self._fileids
]
",[],0,[],/corpus/reader/lin.py_synonyms
4476,/home/amandapotts/git/nltk/nltk/corpus/reader/lin.py_demo,"def demo():
from nltk.corpus import lin_thesaurus as thes
word1 = ""business""
word2 = ""enterprise""
print(""Getting synonyms for "" + word1)
print(thes.synonyms(word1))
print(""Getting scored synonyms for "" + word1)
print(thes.scored_synonyms(word1))
print(""Getting synonyms from simN.lsp (noun subsection) for "" + word1)
print(thes.synonyms(word1, fileid=""simN.lsp""))
print(""Getting synonyms from simN.lsp (noun subsection) for "" + word1)
print(thes.synonyms(word1, fileid=""simN.lsp""))
print(f""Similarity score for {word1} and {word2}:"")
print(thes.similarity(word1, word2))
",[],0,[],/corpus/reader/lin.py_demo
4477,/home/amandapotts/git/nltk/nltk/corpus/reader/ppattach.py___init__,"def __init__(self, sent, verb, noun1, prep, noun2, attachment):
self.sent = sent
self.verb = verb
self.noun1 = noun1
self.prep = prep
self.noun2 = noun2
self.attachment = attachment
",[],0,[],/corpus/reader/ppattach.py___init__
4478,/home/amandapotts/git/nltk/nltk/corpus/reader/ppattach.py___repr__,"def __repr__(self):
return (
""PPAttachment(sent=%r, verb=%r, noun1=%r, prep=%r, ""
""noun2=%r, attachment=%r)""
% (self.sent, self.verb, self.noun1, self.prep, self.noun2, self.attachment)
)
",[],0,[],/corpus/reader/ppattach.py___repr__
4479,/home/amandapotts/git/nltk/nltk/corpus/reader/ppattach.py_attachments,"def attachments(self, fileids):
return concat(
[
StreamBackedCorpusView(fileid, self._read_obj_block, encoding=enc)
for (fileid, enc) in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/ppattach.py_attachments
4480,/home/amandapotts/git/nltk/nltk/corpus/reader/ppattach.py_tuples,"def tuples(self, fileids):
return concat(
[
StreamBackedCorpusView(fileid, self._read_tuple_block, encoding=enc)
for (fileid, enc) in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/ppattach.py_tuples
4481,/home/amandapotts/git/nltk/nltk/corpus/reader/ppattach.py__read_tuple_block,"def _read_tuple_block(self, stream):
line = stream.readline()
if line:
return [tuple(line.split())]
else:
return []
",[],0,[],/corpus/reader/ppattach.py__read_tuple_block
4482,/home/amandapotts/git/nltk/nltk/corpus/reader/ppattach.py__read_obj_block,"def _read_obj_block(self, stream):
line = stream.readline()
if line:
return [PPAttachment(*line.split())]
else:
return []
",[],0,[],/corpus/reader/ppattach.py__read_obj_block
4483,/home/amandapotts/git/nltk/nltk/corpus/reader/nkjp.py__parse_args,"def _parse_args(fun):
""""""
Wraps function arguments:
if fileids not specified then function set NKJPCorpusReader paths.
""""""
@functools.wraps(fun)
",[],0,[],/corpus/reader/nkjp.py__parse_args
4484,/home/amandapotts/git/nltk/nltk/corpus/reader/nkjp.py_decorator,"def decorator(self, fileids=None, **kwargs):
if not fileids:
fileids = self._paths
return fun(self, fileids, **kwargs)
",[],0,[],/corpus/reader/nkjp.py_decorator
4485,/home/amandapotts/git/nltk/nltk/corpus/reader/nkjp.py___init__,"def __init__(self, root, fileids="".*""):
""""""
Corpus reader designed to work with National Corpus of Polish.
See http://nkjp.pl/ for more details about NKJP.
use example:
import nltk
import nkjp
from nkjp import NKJPCorpusReader
x = NKJPCorpusReader(root='/home/USER/nltk_data/corpora/nkjp/', fileids='') # obtain the whole corpus
x.header()
x.raw()
x.words()
x.tagged_words(tags=['subst', 'comp'])  #Link to find more tags: nkjp.pl/poliqarp/help/ense2.html
x.sents()
x = NKJPCorpusReader(root='/home/USER/nltk_data/corpora/nkjp/', fileids='Wilk*') # obtain particular file(s)
x.header(fileids=['WilkDom', '/home/USER/nltk_data/corpora/nkjp/WilkWilczy'])
x.tagged_words(fileids=['WilkDom', '/home/USER/nltk_data/corpora/nkjp/WilkWilczy'], tags=['subst', 'comp'])
""""""
if isinstance(fileids, str):
XMLCorpusReader.__init__(self, root, fileids + "".*/header.xml"")
else:
XMLCorpusReader.__init__(
self, root, [fileid + ""/header.xml"" for fileid in fileids]
)
self._paths = self.get_paths()
",[],0,[],/corpus/reader/nkjp.py___init__
4486,/home/amandapotts/git/nltk/nltk/corpus/reader/nkjp.py_get_paths,"def get_paths(self):
return [
os.path.join(str(self._root), f.split(""header.xml"")[0])
for f in self._fileids
]
",[],0,[],/corpus/reader/nkjp.py_get_paths
4487,/home/amandapotts/git/nltk/nltk/corpus/reader/nkjp.py_fileids,"def fileids(self):
""""""
Returns a list of file identifiers for the fileids that make up
this corpus.
""""""
return [f.split(""header.xml"")[0] for f in self._fileids]
",[],0,[],/corpus/reader/nkjp.py_fileids
4488,/home/amandapotts/git/nltk/nltk/corpus/reader/nkjp.py__view,"def _view(self, filename, tags=None, **kwargs):
""""""
Returns a view specialised for use with particular corpus file.
""""""
mode = kwargs.pop(""mode"", NKJPCorpusReader.WORDS_MODE)
if mode is NKJPCorpusReader.WORDS_MODE:
return NKJPCorpus_Morph_View(filename, tags=tags)
elif mode is NKJPCorpusReader.SENTS_MODE:
return NKJPCorpus_Segmentation_View(filename, tags=tags)
elif mode is NKJPCorpusReader.HEADER_MODE:
return NKJPCorpus_Header_View(filename, tags=tags)
elif mode is NKJPCorpusReader.RAW_MODE:
return NKJPCorpus_Text_View(
filename, tags=tags, mode=NKJPCorpus_Text_View.RAW_MODE
)
else:
raise NameError(""No such mode!"")
",[],0,[],/corpus/reader/nkjp.py__view
4489,/home/amandapotts/git/nltk/nltk/corpus/reader/nkjp.py_add_root,"def add_root(self, fileid):
""""""
Add root if necessary to specified fileid.
""""""
if self.root in fileid:
return fileid
return self.root + fileid
",[],0,[],/corpus/reader/nkjp.py_add_root
4490,/home/amandapotts/git/nltk/nltk/corpus/reader/nkjp.py_header,"def header(self, fileids=None, **kwargs):
""""""
Returns header(s) of specified fileids.
""""""
return concat(
[
self._view(
self.add_root(fileid), mode=NKJPCorpusReader.HEADER_MODE, **kwargs
).handle_query()
for fileid in fileids
]
)
",[],0,[],/corpus/reader/nkjp.py_header
4491,/home/amandapotts/git/nltk/nltk/corpus/reader/nkjp.py_sents,"def sents(self, fileids=None, **kwargs):
""""""
Returns sentences in specified fileids.
""""""
return concat(
[
self._view(
self.add_root(fileid), mode=NKJPCorpusReader.SENTS_MODE, **kwargs
).handle_query()
for fileid in fileids
]
)
",[],0,[],/corpus/reader/nkjp.py_sents
4492,/home/amandapotts/git/nltk/nltk/corpus/reader/nkjp.py_words,"def words(self, fileids=None, **kwargs):
""""""
Returns words in specified fileids.
""""""
return concat(
[
self._view(
self.add_root(fileid), mode=NKJPCorpusReader.WORDS_MODE, **kwargs
).handle_query()
for fileid in fileids
]
)
",[],0,[],/corpus/reader/nkjp.py_words
4493,/home/amandapotts/git/nltk/nltk/corpus/reader/nkjp.py_tagged_words,"def tagged_words(self, fileids=None, **kwargs):
""""""
Call with specified tags as a list, e.g. tags=['subst', 'comp'].
Returns tagged words in specified fileids.
""""""
tags = kwargs.pop(""tags"", [])
return concat(
[
self._view(
self.add_root(fileid),
mode=NKJPCorpusReader.WORDS_MODE,
tags=tags,
).handle_query()
for fileid in fileids
]
)
",[],0,[],/corpus/reader/nkjp.py_tagged_words
4494,/home/amandapotts/git/nltk/nltk/corpus/reader/nkjp.py_raw,"def raw(self, fileids=None, **kwargs):
""""""
Returns words in specified fileids.
""""""
return concat(
[
self._view(
self.add_root(fileid), mode=NKJPCorpusReader.RAW_MODE, **kwargs
).handle_query()
for fileid in fileids
]
)
",[],0,[],/corpus/reader/nkjp.py_raw
4495,/home/amandapotts/git/nltk/nltk/corpus/reader/nkjp.py___init__,"def __init__(self, filename, **kwargs):
""""""
HEADER_MODE
A stream backed corpus view specialized for use with
header.xml files in NKJP corpus.
""""""
self.tagspec = "".*/sourceDesc$""
XMLCorpusView.__init__(self, filename + ""header.xml"", self.tagspec)
",[],0,[],/corpus/reader/nkjp.py___init__
4496,/home/amandapotts/git/nltk/nltk/corpus/reader/nkjp.py_handle_query,"def handle_query(self):
self._open()
header = []
while True:
segm = XMLCorpusView.read_block(self, self._stream)
if len(segm) == 0:
break
header.extend(segm)
self.close()
return header
",[],0,[],/corpus/reader/nkjp.py_handle_query
4497,/home/amandapotts/git/nltk/nltk/corpus/reader/nkjp.py_handle_elt,"def handle_elt(self, elt, context):
titles = elt.findall(""bibl/title"")
title = []
if titles:
title = ""\n"".join(title.text.strip() for title in titles)
authors = elt.findall(""bibl/author"")
author = []
if authors:
author = ""\n"".join(author.text.strip() for author in authors)
dates = elt.findall(""bibl/date"")
date = []
if dates:
date = ""\n"".join(date.text.strip() for date in dates)
publishers = elt.findall(""bibl/publisher"")
publisher = []
if publishers:
publisher = ""\n"".join(publisher.text.strip() for publisher in publishers)
idnos = elt.findall(""bibl/idno"")
idno = []
if idnos:
idno = ""\n"".join(idno.text.strip() for idno in idnos)
notes = elt.findall(""bibl/note"")
note = []
if notes:
note = ""\n"".join(note.text.strip() for note in notes)
return {
""title"": title,
""author"": author,
""date"": date,
""publisher"": publisher,
""idno"": idno,
""note"": note,
}
",[],0,[],/corpus/reader/nkjp.py_handle_elt
4498,/home/amandapotts/git/nltk/nltk/corpus/reader/nkjp.py___init__,"def __init__(self, root, filename):
self.read_file = os.path.join(root, filename)
self.write_file = tempfile.NamedTemporaryFile(delete=False)
",[],0,[],/corpus/reader/nkjp.py___init__
4499,/home/amandapotts/git/nltk/nltk/corpus/reader/nkjp.py_build_preprocessed_file,"def build_preprocessed_file(self):
try:
fr = open(self.read_file)
fw = self.write_file
line = "" ""
while len(line):
line = fr.readline()
x = re.split(r""nkjp:[^ ]* "", line)  # in all files
ret = "" "".join(x)
x = re.split(""<nkjp:paren>"", ret)  # in ann_segmentation.xml
ret = "" "".join(x)
x = re.split(""</nkjp:paren>"", ret)  # in ann_segmentation.xml
ret = "" "".join(x)
x = re.split(""<choice>"", ret)  # in ann_segmentation.xml
ret = "" "".join(x)
x = re.split(""</choice>"", ret)  # in ann_segmentation.xml
ret = "" "".join(x)
fw.write(ret)
fr.close()
fw.close()
return self.write_file.name
except Exception as e:
self.remove_preprocessed_file()
raise Exception from e
",[],0,[],/corpus/reader/nkjp.py_build_preprocessed_file
4500,/home/amandapotts/git/nltk/nltk/corpus/reader/nkjp.py_remove_preprocessed_file,"def remove_preprocessed_file(self):
os.remove(self.write_file.name)
",[],0,[],/corpus/reader/nkjp.py_remove_preprocessed_file
4501,/home/amandapotts/git/nltk/nltk/corpus/reader/nkjp.py___init__,"def __init__(self, filename, **kwargs):
self.tagspec = "".*p/.*s""
self.text_view = NKJPCorpus_Text_View(
filename, mode=NKJPCorpus_Text_View.SENTS_MODE
)
self.text_view.handle_query()
self.xml_tool = XML_Tool(filename, ""ann_segmentation.xml"")
XMLCorpusView.__init__(
self, self.xml_tool.build_preprocessed_file(), self.tagspec
)
",[],0,[],/corpus/reader/nkjp.py___init__
4502,/home/amandapotts/git/nltk/nltk/corpus/reader/nkjp.py_get_segm_id,"def get_segm_id(self, example_word):
return example_word.split(""("")[1].split("","")[0]
",[],0,[],/corpus/reader/nkjp.py_get_segm_id
4503,/home/amandapotts/git/nltk/nltk/corpus/reader/nkjp.py_get_sent_beg,"def get_sent_beg(self, beg_word):
return int(beg_word.split("","")[1])
",[],0,[],/corpus/reader/nkjp.py_get_sent_beg
4504,/home/amandapotts/git/nltk/nltk/corpus/reader/nkjp.py_get_sent_end,"def get_sent_end(self, end_word):
splitted = end_word.split("")"")[0].split("","")
return int(splitted[1]) + int(splitted[2])
",[],0,[],/corpus/reader/nkjp.py_get_sent_end
4505,/home/amandapotts/git/nltk/nltk/corpus/reader/nkjp.py_get_sentences,"def get_sentences(self, sent_segm):
id = self.get_segm_id(sent_segm[0])
segm = self.text_view.segm_dict[id]  # text segment
beg = self.get_sent_beg(sent_segm[0])
end = self.get_sent_end(sent_segm[len(sent_segm) - 1])
return segm[beg:end]
",[],0,[],/corpus/reader/nkjp.py_get_sentences
4506,/home/amandapotts/git/nltk/nltk/corpus/reader/nkjp.py_remove_choice,"def remove_choice(self, segm):
ret = []
prev_txt_end = -1
prev_txt_nr = -1
for word in segm:
txt_nr = self.get_segm_id(word)
if self.get_sent_beg(word) > prev_txt_end - 1 or prev_txt_nr != txt_nr:
ret.append(word)
prev_txt_end = self.get_sent_end(word)
prev_txt_nr = txt_nr
return ret
",[],0,[],/corpus/reader/nkjp.py_remove_choice
4507,/home/amandapotts/git/nltk/nltk/corpus/reader/nkjp.py_handle_query,"def handle_query(self):
try:
self._open()
sentences = []
while True:
sent_segm = XMLCorpusView.read_block(self, self._stream)
if len(sent_segm) == 0:
break
for segm in sent_segm:
segm = self.remove_choice(segm)
sentences.append(self.get_sentences(segm))
self.close()
self.xml_tool.remove_preprocessed_file()
return sentences
except Exception as e:
self.xml_tool.remove_preprocessed_file()
raise Exception from e
",[],0,[],/corpus/reader/nkjp.py_handle_query
4508,/home/amandapotts/git/nltk/nltk/corpus/reader/nkjp.py_handle_elt,"def handle_elt(self, elt, context):
ret = []
for seg in elt:
ret.append(seg.get(""corresp""))
return ret
",[],0,[],/corpus/reader/nkjp.py_handle_elt
4509,/home/amandapotts/git/nltk/nltk/corpus/reader/nkjp.py___init__,"def __init__(self, filename, **kwargs):
self.mode = kwargs.pop(""mode"", 0)
self.tagspec = "".*/div/ab""
self.segm_dict = dict()
self.xml_tool = XML_Tool(filename, ""text.xml"")
XMLCorpusView.__init__(
self, self.xml_tool.build_preprocessed_file(), self.tagspec
)
",[],0,[],/corpus/reader/nkjp.py___init__
4510,/home/amandapotts/git/nltk/nltk/corpus/reader/nkjp.py_handle_query,"def handle_query(self):
try:
self._open()
x = self.read_block(self._stream)
self.close()
self.xml_tool.remove_preprocessed_file()
return x
except Exception as e:
self.xml_tool.remove_preprocessed_file()
raise Exception from e
",[],0,[],/corpus/reader/nkjp.py_handle_query
4511,/home/amandapotts/git/nltk/nltk/corpus/reader/nkjp.py_read_block,"def read_block(self, stream, tagspec=None, elt_handler=None):
""""""
Returns text as a list of sentences.
""""""
txt = []
while True:
segm = XMLCorpusView.read_block(self, stream)
if len(segm) == 0:
break
for part in segm:
txt.append(part)
return ["" "".join([segm for segm in txt])]
",[],0,[],/corpus/reader/nkjp.py_read_block
4512,/home/amandapotts/git/nltk/nltk/corpus/reader/nkjp.py_get_segm_id,"def get_segm_id(self, elt):
for attr in elt.attrib:
if attr.endswith(""id""):
return elt.get(attr)
",[],0,[],/corpus/reader/nkjp.py_get_segm_id
4513,/home/amandapotts/git/nltk/nltk/corpus/reader/nkjp.py_handle_elt,"def handle_elt(self, elt, context):
if self.mode is NKJPCorpus_Text_View.SENTS_MODE:
self.segm_dict[self.get_segm_id(elt)] = elt.text
return elt.text
",[],0,[],/corpus/reader/nkjp.py_handle_elt
4514,/home/amandapotts/git/nltk/nltk/corpus/reader/nkjp.py___init__,"def __init__(self, filename, **kwargs):
self.tags = kwargs.pop(""tags"", None)
self.tagspec = "".*/seg/fs""
self.xml_tool = XML_Tool(filename, ""ann_morphosyntax.xml"")
XMLCorpusView.__init__(
self, self.xml_tool.build_preprocessed_file(), self.tagspec
)
",[],0,[],/corpus/reader/nkjp.py___init__
4515,/home/amandapotts/git/nltk/nltk/corpus/reader/nkjp.py_handle_query,"def handle_query(self):
try:
self._open()
words = []
while True:
segm = XMLCorpusView.read_block(self, self._stream)
if len(segm) == 0:
break
for part in segm:
if part is not None:
words.append(part)
self.close()
self.xml_tool.remove_preprocessed_file()
return words
except Exception as e:
self.xml_tool.remove_preprocessed_file()
raise Exception from e
",[],0,[],/corpus/reader/nkjp.py_handle_query
4516,/home/amandapotts/git/nltk/nltk/corpus/reader/nkjp.py_handle_elt,"def handle_elt(self, elt, context):
word = """"
flag = False
is_not_interp = True
if self.tags is None:
flag = True
for child in elt:
if ""name"" in child.keys() and child.attrib[""name""] == ""orth"":
for symbol in child:
if symbol.tag == ""string"":
word = symbol.text
elif ""name"" in child.keys() and child.attrib[""name""] == ""interps"":
for symbol in child:
if ""type"" in symbol.keys() and symbol.attrib[""type""] == ""lex"":
for symbol2 in symbol:
if (
""name"" in symbol2.keys()
and symbol2.attrib[""name""] == ""ctag""
):
for symbol3 in symbol2:
if (
""value"" in symbol3.keys()
and self.tags is not None
and symbol3.attrib[""value""] in self.tags
):
flag = True
elif (
""value"" in symbol3.keys()
and symbol3.attrib[""value""] == ""interp""
):
is_not_interp = False
if flag and is_not_interp:
return word
",[],0,[],/corpus/reader/nkjp.py_handle_elt
4517,/home/amandapotts/git/nltk/nltk/corpus/reader/timit.py___init__,"def __init__(self, root, encoding=""utf8""):
""""""
Construct a new TIMIT corpus reader in the given directory.
:param root: The root directory for this corpus.
""""""
if isinstance(encoding, str):
encoding = [(r"".*\.wav"", None), ("".*"", encoding)]
CorpusReader.__init__(
self, root, find_corpus_fileids(root, self._FILE_RE), encoding=encoding
)
self._utterances = [
name[:-4] for name in find_corpus_fileids(root, self._UTTERANCE_RE)
]
""""""A list of the utterance identifiers for all utterances in
this corpus.""""""
self._speakerinfo = None
self._root = root
self.speakers = sorted({u.split(""/"")[0] for u in self._utterances})
",[],0,[],/corpus/reader/timit.py___init__
4518,/home/amandapotts/git/nltk/nltk/corpus/reader/timit.py_fileids,"def fileids(self, filetype=None):
""""""
Return a list of file identifiers for the files that make up
this corpus.
:param filetype: If specified, then ``filetype`` indicates that
only the files that have the given type should be
returned.  Accepted values are: ``txt``, ``wrd``, ``phn``,
``wav``, or ``metadata``,
""""""
if filetype is None:
return CorpusReader.fileids(self)
elif filetype in (""txt"", ""wrd"", ""phn"", ""wav""):
return [f""{u}.{filetype}"" for u in self._utterances]
elif filetype == ""metadata"":
return [""timitdic.txt"", ""spkrinfo.txt""]
else:
raise ValueError(""Bad value for filetype: %r"" % filetype)
",[],0,[],/corpus/reader/timit.py_fileids
4519,/home/amandapotts/git/nltk/nltk/corpus/reader/timit.py_utteranceids,"def utteranceids(
self, dialect=None, sex=None, spkrid=None, sent_type=None, sentid=None
",[],0,[],/corpus/reader/timit.py_utteranceids
4520,/home/amandapotts/git/nltk/nltk/corpus/reader/timit.py_transcription_dict,"def transcription_dict(self):
""""""
:return: A dictionary giving the 'standard' transcription for
each word.
""""""
_transcriptions = {}
with self.open(""timitdic.txt"") as fp:
for line in fp:
if not line.strip() or line[0] == ""
continue
m = re.match(r""\s*(\S+)\s+/(.*)/\s*$"", line)
if not m:
raise ValueError(""Bad line: %r"" % line)
_transcriptions[m.group(1)] = m.group(2).split()
return _transcriptions
",[],0,[],/corpus/reader/timit.py_transcription_dict
4521,/home/amandapotts/git/nltk/nltk/corpus/reader/timit.py_spkrid,"def spkrid(self, utterance):
return utterance.split(""/"")[0]
",[],0,[],/corpus/reader/timit.py_spkrid
4522,/home/amandapotts/git/nltk/nltk/corpus/reader/timit.py_sentid,"def sentid(self, utterance):
return utterance.split(""/"")[1]
",[],0,[],/corpus/reader/timit.py_sentid
4523,/home/amandapotts/git/nltk/nltk/corpus/reader/timit.py_utterance,"def utterance(self, spkrid, sentid):
return f""{spkrid}/{sentid}""
",[],0,[],/corpus/reader/timit.py_utterance
4524,/home/amandapotts/git/nltk/nltk/corpus/reader/timit.py_spkrutteranceids,"def spkrutteranceids(self, speaker):
""""""
:return: A list of all utterances associated with a given
speaker.
""""""
return [
utterance
for utterance in self._utterances
if utterance.startswith(speaker + ""/"")
]
",[],0,[],/corpus/reader/timit.py_spkrutteranceids
4525,/home/amandapotts/git/nltk/nltk/corpus/reader/timit.py_spkrinfo,"def spkrinfo(self, speaker):
""""""
:return: A dictionary mapping .. something.
""""""
if speaker in self._utterances:
speaker = self.spkrid(speaker)
if self._speakerinfo is None:
self._speakerinfo = {}
with self.open(""spkrinfo.txt"") as fp:
for line in fp:
if not line.strip() or line[0] == ""
continue
rec = line.strip().split(None, 9)
key = f""dr{rec[2]}-{rec[1].lower()}{rec[0].lower()}""
self._speakerinfo[key] = SpeakerInfo(*rec)
return self._speakerinfo[speaker]
",[],0,[],/corpus/reader/timit.py_spkrinfo
4526,/home/amandapotts/git/nltk/nltk/corpus/reader/timit.py_phones,"def phones(self, utterances=None):
results = []
for fileid in self._utterance_fileids(utterances, "".phn""):
with self.open(fileid) as fp:
for line in fp:
if line.strip():
results.append(line.split()[-1])
return results
",[],0,[],/corpus/reader/timit.py_phones
4527,/home/amandapotts/git/nltk/nltk/corpus/reader/timit.py_phone_times,"def phone_times(self, utterances=None):
""""""
offset is represented as a number of 16kHz samples!
""""""
results = []
for fileid in self._utterance_fileids(utterances, "".phn""):
with self.open(fileid) as fp:
for line in fp:
if line.strip():
results.append(
(
line.split()[2],
int(line.split()[0]),
int(line.split()[1]),
)
)
return results
",[],0,[],/corpus/reader/timit.py_phone_times
4528,/home/amandapotts/git/nltk/nltk/corpus/reader/timit.py_words,"def words(self, utterances=None):
results = []
for fileid in self._utterance_fileids(utterances, "".wrd""):
with self.open(fileid) as fp:
for line in fp:
if line.strip():
results.append(line.split()[-1])
return results
",[],0,[],/corpus/reader/timit.py_words
4529,/home/amandapotts/git/nltk/nltk/corpus/reader/timit.py_word_times,"def word_times(self, utterances=None):
results = []
for fileid in self._utterance_fileids(utterances, "".wrd""):
with self.open(fileid) as fp:
for line in fp:
if line.strip():
results.append(
(
line.split()[2],
int(line.split()[0]),
int(line.split()[1]),
)
)
return results
",[],0,[],/corpus/reader/timit.py_word_times
4530,/home/amandapotts/git/nltk/nltk/corpus/reader/timit.py_sents,"def sents(self, utterances=None):
results = []
for fileid in self._utterance_fileids(utterances, "".wrd""):
with self.open(fileid) as fp:
results.append([line.split()[-1] for line in fp if line.strip()])
return results
",[],0,[],/corpus/reader/timit.py_sents
4531,/home/amandapotts/git/nltk/nltk/corpus/reader/timit.py_sent_times,"def sent_times(self, utterances=None):
return [
(
line.split(None, 2)[-1].strip(),
int(line.split()[0]),
int(line.split()[1]),
)
for fileid in self._utterance_fileids(utterances, "".txt"")
for line in self.open(fileid)
if line.strip()
]
",[],0,[],/corpus/reader/timit.py_sent_times
4532,/home/amandapotts/git/nltk/nltk/corpus/reader/timit.py_phone_trees,"def phone_trees(self, utterances=None):
if utterances is None:
utterances = self._utterances
if isinstance(utterances, str):
utterances = [utterances]
trees = []
for utterance in utterances:
word_times = self.word_times(utterance)
phone_times = self.phone_times(utterance)
sent_times = self.sent_times(utterance)
while sent_times:
(sent, sent_start, sent_end) = sent_times.pop(0)
trees.append(Tree(""S"", []))
while (
word_times and phone_times and phone_times[0][2] <= word_times[0][1]
):
trees[-1].append(phone_times.pop(0)[0])
while word_times and word_times[0][2] <= sent_end:
(word, word_start, word_end) = word_times.pop(0)
trees[-1].append(Tree(word, []))
while phone_times and phone_times[0][2] <= word_end:
trees[-1][-1].append(phone_times.pop(0)[0])
while phone_times and phone_times[0][2] <= sent_end:
trees[-1].append(phone_times.pop(0)[0])
return trees
",[],0,[],/corpus/reader/timit.py_phone_trees
4533,/home/amandapotts/git/nltk/nltk/corpus/reader/timit.py_wav,"def wav(self, utterance, start=0, end=None):
wave = import_from_stdlib(""wave"")
w = wave.open(self.open(utterance + "".wav""), ""rb"")
if end is None:
end = w.getnframes()
w.readframes(start)
frames = w.readframes(end - start)
tf = tempfile.TemporaryFile()
out = wave.open(tf, ""w"")
out.setparams(w.getparams())
out.writeframes(frames)
out.close()
tf.seek(0)
return tf.read()
",[],0,[],/corpus/reader/timit.py_wav
4534,/home/amandapotts/git/nltk/nltk/corpus/reader/timit.py_audiodata,"def audiodata(self, utterance, start=0, end=None):
assert end is None or end > start
headersize = 44
with self.open(utterance + "".wav"") as fp:
if end is None:
data = fp.read()
else:
data = fp.read(headersize + end * 2)
return data[headersize + start * 2 :]
",[],0,[],/corpus/reader/timit.py_audiodata
4535,/home/amandapotts/git/nltk/nltk/corpus/reader/timit.py__utterance_fileids,"def _utterance_fileids(self, utterances, extension):
if utterances is None:
utterances = self._utterances
if isinstance(utterances, str):
utterances = [utterances]
return [f""{u}{extension}"" for u in utterances]
",[],0,[],/corpus/reader/timit.py__utterance_fileids
4536,/home/amandapotts/git/nltk/nltk/corpus/reader/timit.py_play,"def play(self, utterance, start=0, end=None):
""""""
Play the given audio sample.
:param utterance: The utterance id of the sample to play
""""""
try:
import ossaudiodev
try:
dsp = ossaudiodev.open(""w"")
dsp.setfmt(ossaudiodev.AFMT_S16_LE)
dsp.channels(1)
dsp.speed(16000)
dsp.write(self.audiodata(utterance, start, end))
dsp.close()
except OSError as e:
print(
(
""can't acquire the audio device
""activate your audio device.""
),
file=sys.stderr,
)
print(""system error message:"", str(e), file=sys.stderr)
return
except ImportError:
pass
try:
import pygame.mixer
import StringIO
pygame.mixer.init(16000)
f = StringIO.StringIO(self.wav(utterance, start, end))
pygame.mixer.Sound(f).play()
while pygame.mixer.get_busy():
time.sleep(0.01)
return
except ImportError:
pass
print(
(""you must install pygame or ossaudiodev "" ""for audio playback.""),
file=sys.stderr,
)
",[],0,[],/corpus/reader/timit.py_play
4537,/home/amandapotts/git/nltk/nltk/corpus/reader/timit.py___init__,"def __init__(
self, id, sex, dr, use, recdate, birthdate, ht, race, edu, comments=None
",[],0,[],/corpus/reader/timit.py___init__
4538,/home/amandapotts/git/nltk/nltk/corpus/reader/timit.py___repr__,"def __repr__(self):
attribs = ""id sex dr use recdate birthdate ht race edu comments""
args = [f""{attr}={getattr(self, attr)!r}"" for attr in attribs.split()]
return ""SpeakerInfo(%s)"" % ("", "".join(args))
",[],0,[],/corpus/reader/timit.py___repr__
4539,/home/amandapotts/git/nltk/nltk/corpus/reader/timit.py_read_timit_block,"def read_timit_block(stream):
""""""
Block reader for timit tagged sentences, which are preceded by a sentence
number that will be ignored.
""""""
line = stream.readline()
if not line:
return []
n, sent = line.split("" "", 1)
return [sent]
",[],0,[],/corpus/reader/timit.py_read_timit_block
4540,/home/amandapotts/git/nltk/nltk/corpus/reader/reviews.py___init__,"def __init__(self, title=None, review_lines=None):
""""""
:param title: the title of the review.
:param review_lines: the list of the ReviewLines that belong to the Review.
""""""
self.title = title
if review_lines is None:
self.review_lines = []
else:
self.review_lines = review_lines
",[],0,[],/corpus/reader/reviews.py___init__
4541,/home/amandapotts/git/nltk/nltk/corpus/reader/reviews.py_add_line,"def add_line(self, review_line):
""""""
Add a line (ReviewLine) to the review.
:param review_line: a ReviewLine instance that belongs to the Review.
""""""
assert isinstance(review_line, ReviewLine)
self.review_lines.append(review_line)
",[],0,[],/corpus/reader/reviews.py_add_line
4542,/home/amandapotts/git/nltk/nltk/corpus/reader/reviews.py_features,"def features(self):
""""""
Return a list of features in the review. Each feature is a tuple made of
the specific item feature and the opinion strength about that feature.
:return: all features of the review as a list of tuples (feat, score).
:rtype: list(tuple)
""""""
features = []
for review_line in self.review_lines:
features.extend(review_line.features)
return features
",[],0,[],/corpus/reader/reviews.py_features
4543,/home/amandapotts/git/nltk/nltk/corpus/reader/reviews.py_sents,"def sents(self):
""""""
Return all tokenized sentences in the review.
:return: all sentences of the review as lists of tokens.
:rtype: list(list(str))
""""""
return [review_line.sent for review_line in self.review_lines]
",[],0,[],/corpus/reader/reviews.py_sents
4544,/home/amandapotts/git/nltk/nltk/corpus/reader/reviews.py___repr__,"def __repr__(self):
return 'Review(title=""{}"", review_lines={})'.format(
self.title, self.review_lines
)
",[],0,[],/corpus/reader/reviews.py___repr__
4545,/home/amandapotts/git/nltk/nltk/corpus/reader/reviews.py___init__,"def __init__(self, sent, features=None, notes=None):
self.sent = sent
if features is None:
self.features = []
else:
self.features = features
if notes is None:
self.notes = []
else:
self.notes = notes
",[],0,[],/corpus/reader/reviews.py___init__
4546,/home/amandapotts/git/nltk/nltk/corpus/reader/reviews.py___repr__,"def __repr__(self):
return ""ReviewLine(features={}, notes={}, sent={})"".format(
self.features, self.notes, self.sent
)
",[],0,[],/corpus/reader/reviews.py___repr__
4547,/home/amandapotts/git/nltk/nltk/corpus/reader/reviews.py___init__,"def __init__(
self, root, fileids, word_tokenizer=WordPunctTokenizer(), encoding=""utf8""
",[],0,[],/corpus/reader/reviews.py___init__
4548,/home/amandapotts/git/nltk/nltk/corpus/reader/reviews.py_features,"def features(self, fileids=None):
""""""
Return a list of features. Each feature is a tuple made of the specific
item feature and the opinion strength about that feature.
:param fileids: a list or regexp specifying the ids of the files whose
features have to be returned.
:return: all features for the item(s) in the given file(s).
:rtype: list(tuple)
""""""
if fileids is None:
fileids = self._fileids
elif isinstance(fileids, str):
fileids = [fileids]
return concat(
[
self.CorpusView(fileid, self._read_features, encoding=enc)
for (fileid, enc) in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/reviews.py_features
4549,/home/amandapotts/git/nltk/nltk/corpus/reader/reviews.py_reviews,"def reviews(self, fileids=None):
""""""
Return all the reviews as a list of Review objects. If `fileids` is
specified, return all the reviews from each of the specified files.
:param fileids: a list or regexp specifying the ids of the files whose
reviews have to be returned.
:return: the given file(s) as a list of reviews.
""""""
if fileids is None:
fileids = self._fileids
return concat(
[
self.CorpusView(fileid, self._read_review_block, encoding=enc)
for (fileid, enc) in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/reviews.py_reviews
4550,/home/amandapotts/git/nltk/nltk/corpus/reader/reviews.py_sents,"def sents(self, fileids=None):
""""""
Return all sentences in the corpus or in the specified files.
:param fileids: a list or regexp specifying the ids of the files whose
sentences have to be returned.
:return: the given file(s) as a list of sentences, each encoded as a
list of word strings.
:rtype: list(list(str))
""""""
return concat(
[
self.CorpusView(path, self._read_sent_block, encoding=enc)
for (path, enc, fileid) in self.abspaths(fileids, True, True)
]
)
",[],0,[],/corpus/reader/reviews.py_sents
4551,/home/amandapotts/git/nltk/nltk/corpus/reader/reviews.py_words,"def words(self, fileids=None):
""""""
Return all words and punctuation symbols in the corpus or in the specified
files.
:param fileids: a list or regexp specifying the ids of the files whose
words have to be returned.
:return: the given file(s) as a list of words and punctuation symbols.
:rtype: list(str)
""""""
return concat(
[
self.CorpusView(path, self._read_word_block, encoding=enc)
for (path, enc, fileid) in self.abspaths(fileids, True, True)
]
)
",[],0,[],/corpus/reader/reviews.py_words
4552,/home/amandapotts/git/nltk/nltk/corpus/reader/reviews.py__read_features,"def _read_features(self, stream):
features = []
for i in range(20):
line = stream.readline()
if not line:
return features
features.extend(re.findall(FEATURES, line))
return features
",[],0,[],/corpus/reader/reviews.py__read_features
4553,/home/amandapotts/git/nltk/nltk/corpus/reader/reviews.py__read_review_block,"def _read_review_block(self, stream):
while True:
line = stream.readline()
if not line:
return []  # end of file.
title_match = re.match(TITLE, line)
if title_match:
review = Review(
title=title_match.group(1).strip()
)  # We create a new review
break
while True:
oldpos = stream.tell()
line = stream.readline()
if not line:
return [review]
if re.match(TITLE, line):
stream.seek(oldpos)
return [review]
feats = re.findall(FEATURES, line)
notes = re.findall(NOTES, line)
sent = re.findall(SENT, line)
if sent:
sent = self._word_tokenizer.tokenize(sent[0])
review_line = ReviewLine(sent=sent, features=feats, notes=notes)
review.add_line(review_line)
",[],0,[],/corpus/reader/reviews.py__read_review_block
4554,/home/amandapotts/git/nltk/nltk/corpus/reader/reviews.py__read_sent_block,"def _read_sent_block(self, stream):
sents = []
for review in self._read_review_block(stream):
sents.extend([sent for sent in review.sents()])
return sents
",[],0,[],/corpus/reader/reviews.py__read_sent_block
4555,/home/amandapotts/git/nltk/nltk/corpus/reader/reviews.py__read_word_block,"def _read_word_block(self, stream):
words = []
for i in range(20):  # Read 20 lines at a time.
line = stream.readline()
sent = re.findall(SENT, line)
if sent:
words.extend(self._word_tokenizer.tokenize(sent[0]))
return words
",[],0,[],/corpus/reader/reviews.py__read_word_block
4556,/home/amandapotts/git/nltk/nltk/corpus/reader/aligned.py___init__,"def __init__(
self,
root,
fileids,
sep=""/"",
word_tokenizer=WhitespaceTokenizer(),
sent_tokenizer=RegexpTokenizer(""\n"", gaps=True),
alignedsent_block_reader=read_alignedsent_block,
encoding=""latin1"",
",[],0,[],/corpus/reader/aligned.py___init__
4557,/home/amandapotts/git/nltk/nltk/corpus/reader/aligned.py_words,"def words(self, fileids=None):
""""""
:return: the given file(s) as a list of words
and punctuation symbols.
:rtype: list(str)
""""""
return concat(
[
AlignedSentCorpusView(
fileid,
enc,
False,
False,
self._word_tokenizer,
self._sent_tokenizer,
self._alignedsent_block_reader,
)
for (fileid, enc) in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/aligned.py_words
4558,/home/amandapotts/git/nltk/nltk/corpus/reader/aligned.py_sents,"def sents(self, fileids=None):
""""""
:return: the given file(s) as a list of
sentences or utterances, each encoded as a list of word
strings.
:rtype: list(list(str))
""""""
return concat(
[
AlignedSentCorpusView(
fileid,
enc,
False,
True,
self._word_tokenizer,
self._sent_tokenizer,
self._alignedsent_block_reader,
)
for (fileid, enc) in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/aligned.py_sents
4559,/home/amandapotts/git/nltk/nltk/corpus/reader/aligned.py_aligned_sents,"def aligned_sents(self, fileids=None):
""""""
:return: the given file(s) as a list of AlignedSent objects.
:rtype: list(AlignedSent)
""""""
return concat(
[
AlignedSentCorpusView(
fileid,
enc,
True,
True,
self._word_tokenizer,
self._sent_tokenizer,
self._alignedsent_block_reader,
)
for (fileid, enc) in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/aligned.py_aligned_sents
4560,/home/amandapotts/git/nltk/nltk/corpus/reader/aligned.py___init__,"def __init__(
self,
corpus_file,
encoding,
aligned,
group_by_sent,
word_tokenizer,
sent_tokenizer,
alignedsent_block_reader,
",[],0,[],/corpus/reader/aligned.py___init__
4561,/home/amandapotts/git/nltk/nltk/corpus/reader/aligned.py_read_block,"def read_block(self, stream):
block = [
self._word_tokenizer.tokenize(sent_str)
for alignedsent_str in self._alignedsent_block_reader(stream)
for sent_str in self._sent_tokenizer.tokenize(alignedsent_str)
]
if self._aligned:
block[2] = Alignment.fromstring(
"" "".join(block[2])
)  # kludge
block = [AlignedSent(*block)]
elif self._group_by_sent:
block = [block[0]]
else:
block = block[0]
return block
",[],0,[],/corpus/reader/aligned.py_read_block
4562,/home/amandapotts/git/nltk/nltk/corpus/reader/wordlist.py_words,"def words(self, fileids=None, ignore_lines_startswith=""\n""):
return [
line
for line in line_tokenize(self.raw(fileids))
if not line.startswith(ignore_lines_startswith)
]
",[],0,[],/corpus/reader/wordlist.py_words
4563,/home/amandapotts/git/nltk/nltk/corpus/reader/wordlist.py_entries,"def entries(self, fileids=None):
""""""
:return: a tuple of words for the specified fileids.
""""""
if not fileids:
fileids = self.fileids()
wordlists = [self.words(f) for f in fileids]
return list(zip(*wordlists))
",[],0,[],/corpus/reader/wordlist.py_entries
4564,/home/amandapotts/git/nltk/nltk/corpus/reader/wordlist.py_words,"def words(self, lang=None, fileids=None, ignore_lines_startswith=""#""):
""""""
This module returns a list of nonbreaking prefixes for the specified
language(s).
>>> from nltk.corpus import nonbreaking_prefixes as nbp
>>> nbp.words('en')[:10] == [u'A', u'B', u'C', u'D', u'E', u'F', u'G', u'H', u'I', u'J']
True
>>> nbp.words('ta')[:5] == [u'\u0b85', u'\u0b86', u'\u0b87', u'\u0b88', u'\u0b89']
True
:return: a list words for the specified language(s).
""""""
if lang in self.available_langs:
lang = self.available_langs[lang]
fileids = [""nonbreaking_prefix."" + lang]
return [
line
for line in line_tokenize(self.raw(fileids))
if not line.startswith(ignore_lines_startswith)
]
",[],0,[],/corpus/reader/wordlist.py_words
4565,/home/amandapotts/git/nltk/nltk/corpus/reader/wordlist.py_chars,"def chars(self, category=None, fileids=None):
""""""
This module returns a list of characters from  the Perl Unicode Properties.
They are very useful when porting Perl tokenizers to Python.
>>> from nltk.corpus import perluniprops as pup
>>> pup.chars('Open_Punctuation')[:5] == [u'(', u'[', u'{', u'\u0f3a', u'\u0f3c']
True
>>> pup.chars('Currency_Symbol')[:5] == [u'$', u'\xa2', u'\xa3', u'\xa4', u'\xa5']
True
>>> pup.available_categories
['Close_Punctuation', 'Currency_Symbol', 'IsAlnum', 'IsAlpha', 'IsLower', 'IsN', 'IsSc', 'IsSo', 'IsUpper', 'Line_Separator', 'Number', 'Open_Punctuation', 'Punctuation', 'Separator', 'Symbol']
:return: a list of characters given the specific unicode character category
""""""
if category in self.available_categories:
fileids = [category + "".txt""]
return list(self.raw(fileids).strip())
",[],0,[],/corpus/reader/wordlist.py_chars
4566,/home/amandapotts/git/nltk/nltk/corpus/reader/wordlist.py_entries,"def entries(self, fileids=mwa_ppdb_xxxl_file):
""""""
:return: a tuple of synonym word pairs.
""""""
return [tuple(line.split(""\t"")) for line in line_tokenize(self.raw(fileids))]
",[],0,[],/corpus/reader/wordlist.py_entries
4567,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_hypernyms,"def hypernyms(self):
return self._related(""@"")
",[],0,[],/corpus/reader/wordnet.py_hypernyms
4568,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py__hypernyms,"def _hypernyms(self):
return self._related(""@"")
",[],0,[],/corpus/reader/wordnet.py__hypernyms
4569,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_instance_hypernyms,"def instance_hypernyms(self):
return self._related(""@i"")
",[],0,[],/corpus/reader/wordnet.py_instance_hypernyms
4570,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py__instance_hypernyms,"def _instance_hypernyms(self):
return self._related(""@i"")
",[],0,[],/corpus/reader/wordnet.py__instance_hypernyms
4571,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_hyponyms,"def hyponyms(self):
return self._related(""~"")
",[],0,[],/corpus/reader/wordnet.py_hyponyms
4572,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_instance_hyponyms,"def instance_hyponyms(self):
return self._related(""~i"")
",[],0,[],/corpus/reader/wordnet.py_instance_hyponyms
4573,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_member_holonyms,"def member_holonyms(self):
return self._related(""#m"")
",[],0,[],/corpus/reader/wordnet.py_member_holonyms
4574,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_substance_holonyms,"def substance_holonyms(self):
return self._related(""#s"")
",[],0,[],/corpus/reader/wordnet.py_substance_holonyms
4575,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_part_holonyms,"def part_holonyms(self):
return self._related(""#p"")
",[],0,[],/corpus/reader/wordnet.py_part_holonyms
4576,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_member_meronyms,"def member_meronyms(self):
return self._related(""%m"")
",[],0,[],/corpus/reader/wordnet.py_member_meronyms
4577,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_substance_meronyms,"def substance_meronyms(self):
return self._related(""%s"")
",[],0,[],/corpus/reader/wordnet.py_substance_meronyms
4578,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_part_meronyms,"def part_meronyms(self):
return self._related(""%p"")
",[],0,[],/corpus/reader/wordnet.py_part_meronyms
4579,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_topic_domains,"def topic_domains(self):
return self._related(""
",[],0,[],/corpus/reader/wordnet.py_topic_domains
4580,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_in_topic_domains,"def in_topic_domains(self):
return self._related(""-c"")
",[],0,[],/corpus/reader/wordnet.py_in_topic_domains
4581,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_region_domains,"def region_domains(self):
return self._related(""
",[],0,[],/corpus/reader/wordnet.py_region_domains
4582,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_in_region_domains,"def in_region_domains(self):
return self._related(""-r"")
",[],0,[],/corpus/reader/wordnet.py_in_region_domains
4583,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_usage_domains,"def usage_domains(self):
return self._related(""
",[],0,[],/corpus/reader/wordnet.py_usage_domains
4584,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_in_usage_domains,"def in_usage_domains(self):
return self._related(""-u"")
",[],0,[],/corpus/reader/wordnet.py_in_usage_domains
4585,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_attributes,"def attributes(self):
return self._related(""="")
",[],0,[],/corpus/reader/wordnet.py_attributes
4586,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_entailments,"def entailments(self):
return self._related(""*"")
",[],0,[],/corpus/reader/wordnet.py_entailments
4587,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_causes,"def causes(self):
return self._related("">"")
",[],0,[],/corpus/reader/wordnet.py_causes
4588,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_also_sees,"def also_sees(self):
return self._related(""^"")
",[],0,[],/corpus/reader/wordnet.py_also_sees
4589,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_verb_groups,"def verb_groups(self):
return self._related(""$"")
",[],0,[],/corpus/reader/wordnet.py_verb_groups
4590,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_similar_tos,"def similar_tos(self):
return self._related(""&"")
",[],0,[],/corpus/reader/wordnet.py_similar_tos
4591,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py___hash__,"def __hash__(self):
return hash(self._name)
",[],0,[],/corpus/reader/wordnet.py___hash__
4592,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py___eq__,"def __eq__(self, other):
return self._name == other._name
",[],0,[],/corpus/reader/wordnet.py___eq__
4593,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py___ne__,"def __ne__(self, other):
return self._name != other._name
",[],0,[],/corpus/reader/wordnet.py___ne__
4594,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py___lt__,"def __lt__(self, other):
return self._name < other._name
",[],0,[],/corpus/reader/wordnet.py___lt__
4595,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py___init__,"def __init__(
self,
wordnet_corpus_reader,
synset,
name,
lexname_index,
lex_id,
syntactic_marker,
",[],0,[],/corpus/reader/wordnet.py___init__
4596,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_name,"def name(self):
return self._name
",[],0,[],/corpus/reader/wordnet.py_name
4597,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_syntactic_marker,"def syntactic_marker(self):
return self._syntactic_marker
",[],0,[],/corpus/reader/wordnet.py_syntactic_marker
4598,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_synset,"def synset(self):
return self._synset
",[],0,[],/corpus/reader/wordnet.py_synset
4599,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_frame_strings,"def frame_strings(self):
return self._frame_strings
",[],0,[],/corpus/reader/wordnet.py_frame_strings
4600,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_frame_ids,"def frame_ids(self):
return self._frame_ids
",[],0,[],/corpus/reader/wordnet.py_frame_ids
4601,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_lang,"def lang(self):
return self._lang
",[],0,[],/corpus/reader/wordnet.py_lang
4602,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_key,"def key(self):
return self._key
",[],0,[],/corpus/reader/wordnet.py_key
4603,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py___repr__,"def __repr__(self):
tup = type(self).__name__, self._synset._name, self._name
return ""%s('%s.%s')"" % tup
",[],0,[],/corpus/reader/wordnet.py___repr__
4604,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py__related,"def _related(self, relation_symbol):
get_synset = self._wordnet_corpus_reader.synset_from_pos_and_offset
if (self._name, relation_symbol) not in self._synset._lemma_pointers:
return []
return [
get_synset(pos, offset)._lemmas[lemma_index]
for pos, offset, lemma_index in self._synset._lemma_pointers[
self._name, relation_symbol
]
]
",[],0,[],/corpus/reader/wordnet.py__related
4605,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_count,"def count(self):
""""""Return the frequency count for this Lemma""""""
return self._wordnet_corpus_reader.lemma_count(self)
",[],0,[],/corpus/reader/wordnet.py_count
4606,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_antonyms,"def antonyms(self):
return self._related(""!"")
",[],0,[],/corpus/reader/wordnet.py_antonyms
4607,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_derivationally_related_forms,"def derivationally_related_forms(self):
return self._related(""+"")
",[],0,[],/corpus/reader/wordnet.py_derivationally_related_forms
4608,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_pertainyms,"def pertainyms(self):
return self._related(""\\"")
",[],0,[],/corpus/reader/wordnet.py_pertainyms
4609,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py___init__,"def __init__(self, wordnet_corpus_reader):
self._wordnet_corpus_reader = wordnet_corpus_reader
self._pos = None
self._offset = None
self._name = None
self._frame_ids = []
self._lemmas = []
self._lemma_names = []
self._definition = None
self._examples = []
self._lexname = None  # lexicographer name
self._all_hypernyms = None
self._pointers = defaultdict(set)
self._lemma_pointers = defaultdict(list)
",[],0,[],/corpus/reader/wordnet.py___init__
4610,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_pos,"def pos(self):
return self._pos
",[],0,[],/corpus/reader/wordnet.py_pos
4611,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_offset,"def offset(self):
return self._offset
",[],0,[],/corpus/reader/wordnet.py_offset
4612,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_name,"def name(self):
return self._name
",[],0,[],/corpus/reader/wordnet.py_name
4613,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_frame_ids,"def frame_ids(self):
return self._frame_ids
",[],0,[],/corpus/reader/wordnet.py_frame_ids
4614,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py__doc,"def _doc(self, doc_type, default, lang=""eng""):
""""""Helper method for Synset.definition and Synset.examples""""""
corpus = self._wordnet_corpus_reader
if lang not in corpus.langs():
return None
elif lang == ""eng"":
return default
else:
corpus._load_lang_data(lang)
of = corpus.ss2of(self)
i = corpus.lg_attrs.index(doc_type)
if of in corpus._lang_data[lang][i]:
return corpus._lang_data[lang][i][of]
else:
return None
",[],0,[],/corpus/reader/wordnet.py__doc
4615,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_definition,"def definition(self, lang=""eng""):
""""""Return definition in specified language""""""
return self._doc(""def"", self._definition, lang=lang)
",[],0,[],/corpus/reader/wordnet.py_definition
4616,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_examples,"def examples(self, lang=""eng""):
""""""Return examples in specified language""""""
return self._doc(""exe"", self._examples, lang=lang)
",[],0,[],/corpus/reader/wordnet.py_examples
4617,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_lexname,"def lexname(self):
return self._lexname
",[],0,[],/corpus/reader/wordnet.py_lexname
4618,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py__needs_root,"def _needs_root(self):
if self._pos == NOUN and self._wordnet_corpus_reader.get_version() != ""1.6"":
return False
else:
return True
",[],0,[],/corpus/reader/wordnet.py__needs_root
4619,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_lemma_names,"def lemma_names(self, lang=""eng""):
""""""Return all the lemma_names associated with the synset""""""
if lang == ""eng"":
return self._lemma_names
else:
reader = self._wordnet_corpus_reader
reader._load_lang_data(lang)
i = reader.ss2of(self)
if i in reader._lang_data[lang][0]:
return reader._lang_data[lang][0][i]
else:
return []
",[],0,[],/corpus/reader/wordnet.py_lemma_names
4620,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_lemmas,"def lemmas(self, lang=""eng""):
""""""Return all the lemma objects associated with the synset""""""
if lang == ""eng"":
return self._lemmas
elif self._name:
self._wordnet_corpus_reader._load_lang_data(lang)
lemmark = []
lemmy = self.lemma_names(lang)
for lem in lemmy:
temp = Lemma(
self._wordnet_corpus_reader,
self,
lem,
self._wordnet_corpus_reader._lexnames.index(self.lexname()),
0,
None,
)
temp._lang = lang
lemmark.append(temp)
return lemmark
",[],0,[],/corpus/reader/wordnet.py_lemmas
4621,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_root_hypernyms,"def root_hypernyms(self):
""""""Get the topmost hypernyms of this synset in WordNet.""""""
result = []
seen = set()
todo = [self]
while todo:
next_synset = todo.pop()
if next_synset not in seen:
seen.add(next_synset)
next_hypernyms = (
next_synset.hypernyms() + next_synset.instance_hypernyms()
)
if not next_hypernyms:
result.append(next_synset)
else:
todo.extend(next_hypernyms)
return result
",[],0,[],/corpus/reader/wordnet.py_root_hypernyms
4622,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_max_depth,"def max_depth(self):
""""""
:return: The length of the longest hypernym path from this
synset to the root.
""""""
if ""_max_depth"" not in self.__dict__:
hypernyms = self.hypernyms() + self.instance_hypernyms()
if not hypernyms:
self._max_depth = 0
else:
self._max_depth = 1 + max(h.max_depth() for h in hypernyms)
return self._max_depth
",[],0,[],/corpus/reader/wordnet.py_max_depth
4623,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_min_depth,"def min_depth(self):
""""""
:return: The length of the shortest hypernym path from this
synset to the root.
""""""
if ""_min_depth"" not in self.__dict__:
hypernyms = self.hypernyms() + self.instance_hypernyms()
if not hypernyms:
self._min_depth = 0
else:
self._min_depth = 1 + min(h.min_depth() for h in hypernyms)
return self._min_depth
",[],0,[],/corpus/reader/wordnet.py_min_depth
4624,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_hypernym_paths,"def hypernym_paths(self):
""""""
Get the path(s) from this synset to the root, where each path is a
list of the synset nodes traversed on the way to the root.
:return: A list of lists, where each list gives the node sequence
connecting the initial ``Synset`` node and a root node.
""""""
paths = []
hypernyms = self.hypernyms() + self.instance_hypernyms()
if len(hypernyms) == 0:
paths = [[self]]
for hypernym in hypernyms:
for ancestor_list in hypernym.hypernym_paths():
ancestor_list.append(self)
paths.append(ancestor_list)
return paths
",[],0,[],/corpus/reader/wordnet.py_hypernym_paths
4625,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_common_hypernyms,"def common_hypernyms(self, other):
""""""
Find all synsets that are hypernyms of this synset and the
other synset.
:type other: Synset
:param other: other input synset.
:return: The synsets that are hypernyms of both synsets.
""""""
if not self._all_hypernyms:
self._all_hypernyms = {
self_synset
for self_synsets in self._iter_hypernym_lists()
for self_synset in self_synsets
}
if not other._all_hypernyms:
other._all_hypernyms = {
other_synset
for other_synsets in other._iter_hypernym_lists()
for other_synset in other_synsets
}
return list(self._all_hypernyms.intersection(other._all_hypernyms))
",[],0,[],/corpus/reader/wordnet.py_common_hypernyms
4626,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_hypernym_distances,"def hypernym_distances(self, distance=0, simulate_root=False):
""""""
Get the path(s) from this synset to the root, counting the distance
of each node from the initial node on the way. A set of
(synset, distance) tuples is returned.
:type distance: int
:param distance: the distance (number of edges) from this hypernym to
the original hypernym ``Synset`` on which this method was called.
:return: A set of ``(Synset, int)`` tuples where each ``Synset`` is
a hypernym of the first ``Synset``.
""""""
distances = {(self, distance)}
for hypernym in self._hypernyms() + self._instance_hypernyms():
distances |= hypernym.hypernym_distances(distance + 1, simulate_root=False)
if simulate_root:
fake_synset = Synset(None)
fake_synset._name = ""*ROOT*""
fake_synset_distance = max(distances, key=itemgetter(1))[1]
distances.add((fake_synset, fake_synset_distance + 1))
return distances
",[],0,[],/corpus/reader/wordnet.py_hypernym_distances
4627,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py__shortest_hypernym_paths,"def _shortest_hypernym_paths(self, simulate_root):
if self._name == ""*ROOT*"":
return {self: 0}
queue = deque([(self, 0)])
path = {}
while queue:
s, depth = queue.popleft()
if s in path:
continue
path[s] = depth
depth += 1
queue.extend((hyp, depth) for hyp in s._hypernyms())
queue.extend((hyp, depth) for hyp in s._instance_hypernyms())
if simulate_root:
fake_synset = Synset(None)
fake_synset._name = ""*ROOT*""
path[fake_synset] = max(path.values()) + 1
return path
",[],0,[],/corpus/reader/wordnet.py__shortest_hypernym_paths
4628,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_shortest_path_distance,"def shortest_path_distance(self, other, simulate_root=False):
""""""
Returns the distance of the shortest path linking the two synsets (if
one exists). For each synset, all the ancestor nodes and their
distances are recorded and compared. The ancestor node common to both
synsets that can be reached with the minimum number of traversals is
used. If no ancestor nodes are common, None is returned. If a node is
compared with itself 0 is returned.
:type other: Synset
:param other: The Synset to which the shortest path will be found.
:return: The number of edges in the shortest path connecting the two
nodes, or None if no path exists.
""""""
if self == other:
return 0
dist_dict1 = self._shortest_hypernym_paths(simulate_root)
dist_dict2 = other._shortest_hypernym_paths(simulate_root)
inf = float(""inf"")
path_distance = inf
for synset, d1 in dist_dict1.items():
d2 = dist_dict2.get(synset, inf)
path_distance = min(path_distance, d1 + d2)
return None if math.isinf(path_distance) else path_distance
",[],0,[],/corpus/reader/wordnet.py_shortest_path_distance
4629,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_path_similarity,"def path_similarity(self, other, verbose=False, simulate_root=True):
""""""
Path Distance Similarity:
Return a score denoting how similar two word senses are, based on the
shortest path that connects the senses in the is-a (hypernym/hypnoym)
taxonomy. The score is in the range 0 to 1, except in those cases where
a path cannot be found (will only be true for verbs as there are many
distinct verb taxonomies), in which case None is returned. A score of
1 represents identity i.e. comparing a sense with itself will return 1.
:type other: Synset
:param other: The ``Synset`` that this ``Synset`` is being compared to.
:type simulate_root: bool
:param simulate_root: The various verb taxonomies do not
share a single root which disallows this metric from working for
synsets that are not connected. This flag (True by default)
creates a fake root that connects all the taxonomies. Set it
to false to disable this behavior. For the noun taxonomy,
there is usually a default root except for WordNet version 1.6.
If you are using wordnet 1.6, a fake root will be added for nouns
as well.
:return: A score denoting the similarity of the two ``Synset`` objects,
normally between 0 and 1. None is returned if no connecting path
could be found. 1 is returned if a ``Synset`` is compared with
itself.
""""""
distance = self.shortest_path_distance(
other,
simulate_root=simulate_root and (self._needs_root() or other._needs_root()),
)
if distance is None or distance < 0:
return None
return 1.0 / (distance + 1)
",[],0,[],/corpus/reader/wordnet.py_path_similarity
4630,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_lch_similarity,"def lch_similarity(self, other, verbose=False, simulate_root=True):
""""""
Leacock Chodorow Similarity:
Return a score denoting how similar two word senses are, based on the
shortest path that connects the senses (as above) and the maximum depth
of the taxonomy in which the senses occur. The relationship is given as
-log(p/2d) where p is the shortest path length and d is the taxonomy
depth.
:type  other: Synset
:param other: The ``Synset`` that this ``Synset`` is being compared to.
:type simulate_root: bool
:param simulate_root: The various verb taxonomies do not
share a single root which disallows this metric from working for
synsets that are not connected. This flag (True by default)
creates a fake root that connects all the taxonomies. Set it
to false to disable this behavior. For the noun taxonomy,
there is usually a default root except for WordNet version 1.6.
If you are using wordnet 1.6, a fake root will be added for nouns
as well.
:return: A score denoting the similarity of the two ``Synset`` objects,
normally greater than 0. None is returned if no connecting path
could be found. If a ``Synset`` is compared with itself, the
maximum score is returned, which varies depending on the taxonomy
depth.
""""""
if self._pos != other._pos:
raise WordNetError(
""Computing the lch similarity requires ""
""%s and %s to have the same part of speech."" % (self, other)
)
need_root = self._needs_root()
if self._pos not in self._wordnet_corpus_reader._max_depth:
self._wordnet_corpus_reader._compute_max_depth(self._pos, need_root)
depth = self._wordnet_corpus_reader._max_depth[self._pos]
distance = self.shortest_path_distance(
other, simulate_root=simulate_root and need_root
)
if distance is None or distance < 0 or depth == 0:
return None
return -math.log((distance + 1) / (2.0 * depth))
",[],0,[],/corpus/reader/wordnet.py_lch_similarity
4631,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_wup_similarity,"def wup_similarity(self, other, verbose=False, simulate_root=True):
""""""
Wu-Palmer Similarity:
Return a score denoting how similar two word senses are, based on the
depth of the two senses in the taxonomy and that of their Least Common
Subsumer (most specific ancestor node). Previously, the scores computed
by this implementation did _not_ always agree with those given by
Pedersen's Perl implementation of WordNet Similarity. However, with
the addition of the simulate_root flag (see below), the score for
verbs now almost always agree but not always for nouns.
The LCS does not necessarily feature in the shortest path connecting
the two senses, as it is by definition the common ancestor deepest in
the taxonomy, not closest to the two senses. Typically, however, it
will so feature. Where multiple candidates for the LCS exist, that
whose shortest path to the root node is the longest will be selected.
Where the LCS has multiple paths to the root, the longer path is used
for the purposes of the calculation.
:type  other: Synset
:param other: The ``Synset`` that this ``Synset`` is being compared to.
:type simulate_root: bool
:param simulate_root: The various verb taxonomies do not
share a single root which disallows this metric from working for
synsets that are not connected. This flag (True by default)
creates a fake root that connects all the taxonomies. Set it
to false to disable this behavior. For the noun taxonomy,
there is usually a default root except for WordNet version 1.6.
If you are using wordnet 1.6, a fake root will be added for nouns
as well.
:return: A float score denoting the similarity of the two ``Synset``
objects, normally greater than zero. If no connecting path between
the two senses can be found, None is returned.
""""""
need_root = self._needs_root() or other._needs_root()
subsumers = self.lowest_common_hypernyms(
other, simulate_root=simulate_root and need_root, use_min_depth=True
)
if len(subsumers) == 0:
return None
subsumer = self if self in subsumers else subsumers[0]
depth = subsumer.max_depth() + 1
len1 = self.shortest_path_distance(
subsumer, simulate_root=simulate_root and need_root
)
len2 = other.shortest_path_distance(
subsumer, simulate_root=simulate_root and need_root
)
if len1 is None or len2 is None:
return None
len1 += depth
len2 += depth
return (2.0 * depth) / (len1 + len2)
",[],0,[],/corpus/reader/wordnet.py_wup_similarity
4632,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_res_similarity,"def res_similarity(self, other, ic, verbose=False):
""""""
Resnik Similarity:
Return a score denoting how similar two word senses are, based on the
Information Content (IC) of the Least Common Subsumer (most specific
ancestor node).
:type  other: Synset
:param other: The ``Synset`` that this ``Synset`` is being compared to.
:type ic: dict
:param ic: an information content object (as returned by
``nltk.corpus.wordnet_ic.ic()``).
:return: A float score denoting the similarity of the two ``Synset``
objects. Synsets whose LCS is the root node of the taxonomy will
have a score of 0 (e.g. N['dog'][0] and N['table'][0]).
""""""
ic1, ic2, lcs_ic = _lcs_ic(self, other, ic)
return lcs_ic
",[],0,[],/corpus/reader/wordnet.py_res_similarity
4633,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_jcn_similarity,"def jcn_similarity(self, other, ic, verbose=False):
""""""
Jiang-Conrath Similarity:
Return a score denoting how similar two word senses are, based on the
Information Content (IC) of the Least Common Subsumer (most specific
ancestor node) and that of the two input Synsets. The relationship is
given by the equation 1 / (IC(s1) + IC(s2) - 2 * IC(lcs)).
:type  other: Synset
:param other: The ``Synset`` that this ``Synset`` is being compared to.
:type  ic: dict
:param ic: an information content object (as returned by
``nltk.corpus.wordnet_ic.ic()``).
:return: A float score denoting the similarity of the two ``Synset``
objects.
""""""
if self == other:
return _INF
ic1, ic2, lcs_ic = _lcs_ic(self, other, ic)
if ic1 == 0 or ic2 == 0:
return 0
ic_difference = ic1 + ic2 - 2 * lcs_ic
if ic_difference == 0:
return _INF
return 1 / ic_difference
",[],0,[],/corpus/reader/wordnet.py_jcn_similarity
4634,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_lin_similarity,"def lin_similarity(self, other, ic, verbose=False):
""""""
Lin Similarity:
Return a score denoting how similar two word senses are, based on the
Information Content (IC) of the Least Common Subsumer (most specific
ancestor node) and that of the two input Synsets. The relationship is
given by the equation 2 * IC(lcs) / (IC(s1) + IC(s2)).
:type other: Synset
:param other: The ``Synset`` that this ``Synset`` is being compared to.
:type ic: dict
:param ic: an information content object (as returned by
``nltk.corpus.wordnet_ic.ic()``).
:return: A float score denoting the similarity of the two ``Synset``
objects, in the range 0 to 1.
""""""
ic1, ic2, lcs_ic = _lcs_ic(self, other, ic)
return (2.0 * lcs_ic) / (ic1 + ic2)
",[],0,[],/corpus/reader/wordnet.py_lin_similarity
4635,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py__iter_hypernym_lists,"def _iter_hypernym_lists(self):
""""""
:return: An iterator over ``Synset`` objects that are either proper
hypernyms or instance of hypernyms of the synset.
""""""
todo = [self]
seen = set()
while todo:
for synset in todo:
seen.add(synset)
yield todo
todo = [
hypernym
for synset in todo
for hypernym in (synset.hypernyms() + synset.instance_hypernyms())
if hypernym not in seen
]
",[],0,[],/corpus/reader/wordnet.py__iter_hypernym_lists
4636,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py___repr__,"def __repr__(self):
return f""{type(self).__name__}('{self._name}')""
",[],0,[],/corpus/reader/wordnet.py___repr__
4637,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py__related,"def _related(self, relation_symbol, sort=True):
get_synset = self._wordnet_corpus_reader.synset_from_pos_and_offset
if relation_symbol not in self._pointers:
return []
pointer_tuples = self._pointers[relation_symbol]
r = [get_synset(pos, offset) for pos, offset in pointer_tuples]
if sort:
r.sort()
return r
",[],0,[],/corpus/reader/wordnet.py__related
4638,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py___init__,"def __init__(self, root, omw_reader):
""""""
Construct a new wordnet corpus reader, with the given root
directory.
""""""
super().__init__(root, self._FILES, encoding=self._ENCODING)
self._lemma_pos_offset_map = defaultdict(dict)
self._synset_offset_cache = defaultdict(dict)
self._max_depth = defaultdict(dict)
self._omw_reader = omw_reader
self._exomw_reader = None
self.provenances = defaultdict(str)
self.provenances[""eng""] = """"
if self._omw_reader is None:
warnings.warn(
""The multilingual functions are not available with this Wordnet version""
)
self.omw_langs = set()
self._lang_data = defaultdict(list)
self._data_file_map = {}
self._exception_map = {}
self._lexnames = []
self._key_count_file = None
self._key_synset_file = None
with self.open(""lexnames"") as fp:
for i, line in enumerate(fp):
index, lexname, _ = line.split()
assert int(index) == i
self._lexnames.append(lexname)
self._load_lemma_pos_offset_map()
self._load_exception_map()
self.nomap = {}
self.splits = {}
self.merges = {}
self.map30 = self.map_wn()
self.lg_attrs = [""lemma"", ""of"", ""def"", ""exe""]
",[],0,[],/corpus/reader/wordnet.py___init__
4639,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_index_sense,"def index_sense(self, version=None):
""""""Read sense key to synset id mapping from index.sense file in corpus directory""""""
fn = ""index.sense""
if version:
from nltk.corpus import CorpusReader, LazyCorpusLoader
ixreader = LazyCorpusLoader(version, CorpusReader, r"".*/"" + fn)
else:
ixreader = self
with ixreader.open(fn) as fp:
sensekey_map = {}
for line in fp:
fields = line.strip().split()
sensekey = fields[0]
pos = self._pos_names[int(sensekey.split(""%"")[1].split("":"")[0])]
sensekey_map[sensekey] = f""{fields[1]}-{pos}""
return sensekey_map
",[],0,[],/corpus/reader/wordnet.py_index_sense
4640,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_map_to_many,"def map_to_many(self, version=""wordnet""):
sensekey_map1 = self.index_sense(version)
sensekey_map2 = self.index_sense()
synset_to_many = {}
for synsetid in set(sensekey_map1.values()):
synset_to_many[synsetid] = []
for sensekey in set(sensekey_map1.keys()).intersection(
set(sensekey_map2.keys())
):
source = sensekey_map1[sensekey]
target = sensekey_map2[sensekey]
synset_to_many[source].append(target)
return synset_to_many
",[],0,[],/corpus/reader/wordnet.py_map_to_many
4641,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_map_to_one,"def map_to_one(self, version=""wordnet""):
self.nomap[version] = set()
self.splits[version] = {}
synset_to_many = self.map_to_many(version)
synset_to_one = {}
for source in synset_to_many:
candidates_bag = synset_to_many[source]
if candidates_bag:
candidates_set = set(candidates_bag)
if len(candidates_set) == 1:
target = candidates_bag[0]
else:
counts = []
for candidate in candidates_set:
counts.append((candidates_bag.count(candidate), candidate))
self.splits[version][source] = counts
target = max(counts)[1]
synset_to_one[source] = target
if source[-1] == ""s"":
synset_to_one[f""{source[:-1]}a""] = target
else:
self.nomap[version].add(source)
return synset_to_one
",[],0,[],/corpus/reader/wordnet.py_map_to_one
4642,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_map_wn,"def map_wn(self, version=""wordnet""):
""""""Mapping from Wordnet 'version' to currently loaded Wordnet version""""""
if self.get_version() == version:
return None
else:
return self.map_to_one(version)
",[],0,[],/corpus/reader/wordnet.py_map_wn
4643,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_split_synsets,"def split_synsets(self, version=""wordnet""):
if version not in self.splits:
_mymap = self.map_to_one(version)
return self.splits[version]
",[],0,[],/corpus/reader/wordnet.py_split_synsets
4644,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_merged_synsets,"def merged_synsets(self, version=""wordnet""):
if version not in self.merges:
merge = defaultdict(set)
for source, targets in self.map_to_many(version).items():
for target in targets:
merge[target].add(source)
self.merges[version] = {
trg: src for trg, src in merge.items() if len(src) > 1
}
return self.merges[version]
",[],0,[],/corpus/reader/wordnet.py_merged_synsets
4645,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_of2ss,"def of2ss(self, of):
""""""take an id and return the synsets""""""
return self.synset_from_pos_and_offset(of[-1], int(of[:8]))
",[],0,[],/corpus/reader/wordnet.py_of2ss
4646,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_ss2of,"def ss2of(self, ss):
""""""return the ID of the synset""""""
if ss:
return f""{ss.offset():08d}-{ss.pos()}""
",[],0,[],/corpus/reader/wordnet.py_ss2of
4647,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py__load_lang_data,"def _load_lang_data(self, lang):
""""""load the wordnet data of the requested language from the file to
the cache, _lang_data""""""
if lang in self._lang_data:
return
if self._omw_reader and not self.omw_langs:
self.add_omw()
if lang not in self.langs():
raise WordNetError(""Language is not supported."")
if self._exomw_reader and lang not in self.omw_langs:
reader = self._exomw_reader
else:
reader = self._omw_reader
prov = self.provenances[lang]
if prov in [""cldr"", ""wikt""]:
prov2 = prov
else:
prov2 = ""data""
with reader.open(f""{prov}/wn-{prov2}-{lang.split('_')[0]}.tab"") as fp:
self.custom_lemmas(fp, lang)
self.disable_custom_lemmas(lang)
",[],0,[],/corpus/reader/wordnet.py__load_lang_data
4648,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_add_provs,"def add_provs(self, reader):
""""""Add languages from Multilingual Wordnet to the provenance dictionary""""""
fileids = reader.fileids()
for fileid in fileids:
prov, langfile = os.path.split(fileid)
file_name, file_extension = os.path.splitext(langfile)
if file_extension == "".tab"":
lang = file_name.split(""-"")[-1]
if lang in self.provenances or prov in [""cldr"", ""wikt""]:
lang = f""{lang}_{prov}""
self.provenances[lang] = prov
",[],0,[],/corpus/reader/wordnet.py_add_provs
4649,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_add_omw,"def add_omw(self):
self.add_provs(self._omw_reader)
self.omw_langs = set(self.provenances.keys())
",[],0,[],/corpus/reader/wordnet.py_add_omw
4650,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_add_exomw,"def add_exomw(self):
""""""
Add languages from Extended OMW
>>> import nltk
>>> from nltk.corpus import wordnet as wn
>>> wn.add_exomw()
>>> print(wn.synset('intrinsically.r.01').lemmas(lang=""eng_wikt""))
[Lemma('intrinsically.r.01.per_se'), Lemma('intrinsically.r.01.as_such')]
""""""
from nltk.corpus import extended_omw
self.add_omw()
self._exomw_reader = extended_omw
self.add_provs(self._exomw_reader)
",[],0,[],/corpus/reader/wordnet.py_add_exomw
4651,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_langs,"def langs(self):
""""""return a list of languages supported by Multilingual Wordnet""""""
return list(self.provenances.keys())
",[],0,[],/corpus/reader/wordnet.py_langs
4652,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py__load_lemma_pos_offset_map,"def _load_lemma_pos_offset_map(self):
for suffix in self._FILEMAP.values():
with self.open(""index.%s"" % suffix) as fp:
for i, line in enumerate(fp):
if line.startswith("" ""):
continue
_iter = iter(line.split())
",[],0,[],/corpus/reader/wordnet.py__load_lemma_pos_offset_map
4653,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py__next_token,"def _next_token():
return next(_iter)
",[],0,[],/corpus/reader/wordnet.py__next_token
4654,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py__load_exception_map,"def _load_exception_map(self):
for pos, suffix in self._FILEMAP.items():
self._exception_map[pos] = {}
with self.open(""%s.exc"" % suffix) as fp:
for line in fp:
terms = line.split()
self._exception_map[pos][terms[0]] = terms[1:]
self._exception_map[ADJ_SAT] = self._exception_map[ADJ]
",[],0,[],/corpus/reader/wordnet.py__load_exception_map
4655,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py__compute_max_depth,"def _compute_max_depth(self, pos, simulate_root):
""""""
Compute the max depth for the given part of speech.  This is
used by the lch similarity metric.
""""""
depth = 0
for ii in self.all_synsets(pos):
try:
depth = max(depth, ii.max_depth())
except RuntimeError:
print(ii)
if simulate_root:
depth += 1
self._max_depth[pos] = depth
",[],0,[],/corpus/reader/wordnet.py__compute_max_depth
4656,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_get_version,"def get_version(self):
fh = self._data_file(ADJ)
fh.seek(0)
for line in fh:
match = re.search(r""Word[nN]et (\d+|\d+\.\d+) Copyright"", line)
if match is not None:
version = match.group(1)
fh.seek(0)
return version
",[],0,[],/corpus/reader/wordnet.py_get_version
4657,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_lemma,"def lemma(self, name, lang=""eng""):
""""""Return lemma object that matches the name""""""
separator = SENSENUM_RE.search(name).end()
synset_name, lemma_name = name[: separator - 1], name[separator:]
synset = self.synset(synset_name)
for lemma in synset.lemmas(lang):
if lemma._name == lemma_name:
return lemma
raise WordNetError(f""No lemma {lemma_name!r} in {synset_name!r}"")
",[],0,[],/corpus/reader/wordnet.py_lemma
4658,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_lemma_from_key,"def lemma_from_key(self, key):
key = key.lower()
lemma_name, lex_sense = key.split(""%"")
pos_number, lexname_index, lex_id, _, _ = lex_sense.split("":"")
pos = self._pos_names[int(pos_number)]
if self._key_synset_file is None:
self._key_synset_file = self.open(""index.sense"")
synset_line = _binary_search_file(self._key_synset_file, key)
if not synset_line:
raise WordNetError(""No synset found for key %r"" % key)
offset = int(synset_line.split()[1])
synset = self.synset_from_pos_and_offset(pos, offset)
for lemma in synset._lemmas:
if lemma._key == key:
return lemma
raise WordNetError(""No lemma found for for key %r"" % key)
",[],0,[],/corpus/reader/wordnet.py_lemma_from_key
4659,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_synset,"def synset(self, name):
lemma, pos, synset_index_str = name.lower().rsplit(""."", 2)
synset_index = int(synset_index_str) - 1
try:
offset = self._lemma_pos_offset_map[lemma][pos][synset_index]
except KeyError as e:
raise WordNetError(f""No lemma {lemma!r} with part of speech {pos!r}"") from e
except IndexError as e:
n_senses = len(self._lemma_pos_offset_map[lemma][pos])
raise WordNetError(
f""Lemma {lemma!r} with part of speech {pos!r} only ""
f""has {n_senses} {'sense' if n_senses == 1 else 'senses'}""
) from e
synset = self.synset_from_pos_and_offset(pos, offset)
if pos == ""s"" and synset._pos == ""a"":
message = (
""Adjective satellite requested but only plain ""
""adjective found for lemma %r""
)
raise WordNetError(message % lemma)
assert synset._pos == pos or (pos == ""a"" and synset._pos == ""s"")
return synset
",[],0,[],/corpus/reader/wordnet.py_synset
4660,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py__data_file,"def _data_file(self, pos):
""""""
Return an open file pointer for the data file for the given
part of speech.
""""""
if pos == ADJ_SAT:
pos = ADJ
if self._data_file_map.get(pos) is None:
fileid = ""data.%s"" % self._FILEMAP[pos]
self._data_file_map[pos] = self.open(fileid)
return self._data_file_map[pos]
",[],0,[],/corpus/reader/wordnet.py__data_file
4661,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_synset_from_pos_and_offset,"def synset_from_pos_and_offset(self, pos, offset):
""""""
- pos: The synset's part of speech, matching one of the module level
attributes ADJ, ADJ_SAT, ADV, NOUN or VERB ('a', 's', 'r', 'n', or 'v').
- offset: The byte offset of this synset in the WordNet dict file
for this pos.
>>> from nltk.corpus import wordnet as wn
>>> print(wn.synset_from_pos_and_offset('n', 1740))
Synset('entity.n.01')
""""""
if offset in self._synset_offset_cache[pos]:
return self._synset_offset_cache[pos][offset]
data_file = self._data_file(pos)
data_file.seek(offset)
data_file_line = data_file.readline()
line_offset = data_file_line[:8]
if (
line_offset.isalnum()
and line_offset == f""{'0'*(8-len(str(offset)))}{str(offset)}""
):
synset = self._synset_from_pos_and_line(pos, data_file_line)
assert synset._offset == offset
self._synset_offset_cache[pos][offset] = synset
else:
synset = None
warnings.warn(f""No WordNet synset found for pos={pos} at offset={offset}."")
data_file.seek(0)
return synset
",[],0,[],/corpus/reader/wordnet.py_synset_from_pos_and_offset
4662,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py__synset_from_pos_and_offset,"def _synset_from_pos_and_offset(self, *args, **kwargs):
""""""
Hack to help people like the readers of
https://stackoverflow.com/a/27145655/1709587
who were using this function before it was officially a public method
""""""
return self.synset_from_pos_and_offset(*args, **kwargs)
",[],0,[],/corpus/reader/wordnet.py__synset_from_pos_and_offset
4663,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py__synset_from_pos_and_line,"def _synset_from_pos_and_line(self, pos, data_file_line):
synset = Synset(self)
try:
columns_str, gloss = data_file_line.strip().split(""|"")
definition = re.sub(r""[\""].*?[\""]"", """", gloss).strip()
examples = re.findall(r'""([^""]*)""', gloss)
for example in examples:
synset._examples.append(example)
synset._definition = definition.strip(""
_iter = iter(columns_str.split())
",[],0,[],/corpus/reader/wordnet.py__synset_from_pos_and_line
4664,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py__next_token,"def _next_token():
return next(_iter)
",[],0,[],/corpus/reader/wordnet.py__next_token
4665,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_synset_from_sense_key,"def synset_from_sense_key(self, sense_key):
""""""
Retrieves synset based on a given sense_key. Sense keys can be
obtained from lemma.key()
From https://wordnet.princeton.edu/documentation/senseidx5wn:
A sense_key is represented as::
lemma % lex_sense (e.g. 'dog%1:18:01::')
where lex_sense is encoded as::
ss_type:lex_filenum:lex_id:head_word:head_id
:lemma:       ASCII text of word/collocation, in lower case
:ss_type:     synset type for the sense (1 digit int)
The synset type is encoded as follows::
1    NOUN
2    VERB
3    ADJECTIVE
4    ADVERB
5    ADJECTIVE SATELLITE
:lex_filenum: name of lexicographer file containing the synset for the sense (2 digit int)
:lex_id:      when paired with lemma, uniquely identifies a sense in the lexicographer file (2 digit int)
:head_word:   lemma of the first word in satellite's head synset
Only used if sense is in an adjective satellite synset
:head_id:     uniquely identifies sense in a lexicographer file when paired with head_word
Only used if head_word is present (2 digit int)
>>> import nltk
>>> from nltk.corpus import wordnet as wn
>>> print(wn.synset_from_sense_key(""drive%1:04:03::""))
Synset('drive.n.06')
>>> print(wn.synset_from_sense_key(""driving%1:04:03::""))
Synset('drive.n.06')
""""""
return self.lemma_from_key(sense_key).synset()
",[],0,[],/corpus/reader/wordnet.py_synset_from_sense_key
4666,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_synsets,"def synsets(self, lemma, pos=None, lang=""eng"", check_exceptions=True):
""""""Load all synsets with a given lemma and part of speech tag.
If no pos is specified, all synsets for all parts of speech
will be loaded.
If lang is specified, all the synsets associated with the lemma name
of that language will be returned.
""""""
lemma = lemma.lower()
if lang == ""eng"":
get_synset = self.synset_from_pos_and_offset
index = self._lemma_pos_offset_map
if pos is None:
pos = POS_LIST
return [
get_synset(p, offset)
for p in pos
for form in self._morphy(lemma, p, check_exceptions)
for offset in index[form].get(p, [])
]
else:
self._load_lang_data(lang)
synset_list = []
if lemma in self._lang_data[lang][1]:
for l in self._lang_data[lang][1][lemma]:
if pos is not None and l[-1] != pos:
continue
synset_list.append(self.of2ss(l))
return synset_list
",[],0,[],/corpus/reader/wordnet.py_synsets
4667,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_lemmas,"def lemmas(self, lemma, pos=None, lang=""eng""):
""""""Return all Lemma objects with a name matching the specified lemma
name and part of speech tag. Matches any part of speech tag if none is
specified.""""""
lemma = lemma.lower()
if lang == ""eng"":
return [
lemma_obj
for synset in self.synsets(lemma, pos)
for lemma_obj in synset.lemmas()
if lemma_obj.name().lower() == lemma
]
else:
self._load_lang_data(lang)
lemmas = []
syn = self.synsets(lemma, lang=lang)
for s in syn:
if pos is not None and s.pos() != pos:
continue
for lemma_obj in s.lemmas(lang=lang):
if lemma_obj.name().lower() == lemma:
lemmas.append(lemma_obj)
return lemmas
",[],0,[],/corpus/reader/wordnet.py_lemmas
4668,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_all_lemma_names,"def all_lemma_names(self, pos=None, lang=""eng""):
""""""Return all lemma names for all synsets for the given
part of speech tag and language or languages. If pos is
not specified, all synsets for all parts of speech will
be used.""""""
if lang == ""eng"":
if pos is None:
return iter(self._lemma_pos_offset_map)
else:
return (
lemma
for lemma in self._lemma_pos_offset_map
if pos in self._lemma_pos_offset_map[lemma]
)
else:
self._load_lang_data(lang)
lemma = []
for i in self._lang_data[lang][0]:
if pos is not None and i[-1] != pos:
continue
lemma.extend(self._lang_data[lang][0][i])
lemma = iter(set(lemma))
return lemma
",[],0,[],/corpus/reader/wordnet.py_all_lemma_names
4669,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_all_omw_synsets,"def all_omw_synsets(self, pos=None, lang=None):
if lang not in self.langs():
return None
self._load_lang_data(lang)
for of in self._lang_data[lang][0]:
if not pos or of[-1] == pos:
ss = self.of2ss(of)
if ss:
yield ss
",[],0,[],/corpus/reader/wordnet.py_all_omw_synsets
4670,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_all_synsets,"def all_synsets(self, pos=None, lang=""eng""):
""""""Iterate over all synsets with a given part of speech tag.
If no pos is specified, all synsets for all parts of speech
will be loaded.
""""""
if lang == ""eng"":
return self.all_eng_synsets(pos=pos)
else:
return self.all_omw_synsets(pos=pos, lang=lang)
",[],0,[],/corpus/reader/wordnet.py_all_synsets
4671,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_all_eng_synsets,"def all_eng_synsets(self, pos=None):
if pos is None:
pos_tags = self._FILEMAP.keys()
else:
pos_tags = [pos]
cache = self._synset_offset_cache
from_pos_and_line = self._synset_from_pos_and_line
for pos_tag in pos_tags:
if pos_tag == ADJ_SAT:
pos_file = ADJ
else:
pos_file = pos_tag
fileid = ""data.%s"" % self._FILEMAP[pos_file]
data_file = self.open(fileid)
try:
offset = data_file.tell()
line = data_file.readline()
while line:
if not line[0].isspace():
if offset in cache[pos_tag]:
synset = cache[pos_tag][offset]
else:
synset = from_pos_and_line(pos_tag, line)
cache[pos_tag][offset] = synset
if pos_tag == ADJ_SAT and synset._pos == ADJ_SAT:
yield synset
elif pos_tag != ADJ_SAT:
yield synset
offset = data_file.tell()
line = data_file.readline()
except:
data_file.close()
raise
else:
data_file.close()
",[],0,[],/corpus/reader/wordnet.py_all_eng_synsets
4672,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_words,"def words(self, lang=""eng""):
""""""return lemmas of the given language as list of words""""""
return self.all_lemma_names(lang=lang)
",[],0,[],/corpus/reader/wordnet.py_words
4673,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_synonyms,"def synonyms(self, word, lang=""eng""):
""""""return nested list with the synonyms of the different senses of word in the given language""""""
return [
sorted(list(set(ss.lemma_names(lang=lang)) - {word}))
for ss in self.synsets(word, lang=lang)
]
",[],0,[],/corpus/reader/wordnet.py_synonyms
4674,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_doc,"def doc(self, file=""README"", lang=""eng""):
""""""Return the contents of readme, license or citation file
use lang=lang to get the file for an individual language""""""
if lang == ""eng"":
reader = self
else:
reader = self._omw_reader
if lang in self.langs():
file = f""{os.path.join(self.provenances[lang],file)}""
try:
with reader.open(file) as fp:
return fp.read()
except:
if lang in self._lang_data:
return f""Cannot determine {file} for {lang}""
else:
return f""Language {lang} is not supported.""
",[],0,[],/corpus/reader/wordnet.py_doc
4675,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_license,"def license(self, lang=""eng""):
""""""Return the contents of LICENSE (for omw)
use lang=lang to get the license for an individual language""""""
return self.doc(file=""LICENSE"", lang=lang)
",[],0,[],/corpus/reader/wordnet.py_license
4676,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_readme,"def readme(self, lang=""eng""):
""""""Return the contents of README (for omw)
use lang=lang to get the readme for an individual language""""""
return self.doc(file=""README"", lang=lang)
",[],0,[],/corpus/reader/wordnet.py_readme
4677,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_citation,"def citation(self, lang=""eng""):
""""""Return the contents of citation.bib file (for omw)
use lang=lang to get the citation for an individual language""""""
return self.doc(file=""citation.bib"", lang=lang)
",[],0,[],/corpus/reader/wordnet.py_citation
4678,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_lemma_count,"def lemma_count(self, lemma):
""""""Return the frequency count for this Lemma""""""
if lemma._lang != ""eng"":
return 0
if self._key_count_file is None:
self._key_count_file = self.open(""cntlist.rev"")
line = _binary_search_file(self._key_count_file, lemma._key)
if line:
return int(line.rsplit("" "", 1)[-1])
else:
return 0
",[],0,[],/corpus/reader/wordnet.py_lemma_count
4679,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_path_similarity,"def path_similarity(self, synset1, synset2, verbose=False, simulate_root=True):
return synset1.path_similarity(synset2, verbose, simulate_root)
",[],0,[],/corpus/reader/wordnet.py_path_similarity
4680,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_lch_similarity,"def lch_similarity(self, synset1, synset2, verbose=False, simulate_root=True):
return synset1.lch_similarity(synset2, verbose, simulate_root)
",[],0,[],/corpus/reader/wordnet.py_lch_similarity
4681,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_wup_similarity,"def wup_similarity(self, synset1, synset2, verbose=False, simulate_root=True):
return synset1.wup_similarity(synset2, verbose, simulate_root)
",[],0,[],/corpus/reader/wordnet.py_wup_similarity
4682,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_res_similarity,"def res_similarity(self, synset1, synset2, ic, verbose=False):
return synset1.res_similarity(synset2, ic, verbose)
",[],0,[],/corpus/reader/wordnet.py_res_similarity
4683,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_jcn_similarity,"def jcn_similarity(self, synset1, synset2, ic, verbose=False):
return synset1.jcn_similarity(synset2, ic, verbose)
",[],0,[],/corpus/reader/wordnet.py_jcn_similarity
4684,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_lin_similarity,"def lin_similarity(self, synset1, synset2, ic, verbose=False):
return synset1.lin_similarity(synset2, ic, verbose)
",[],0,[],/corpus/reader/wordnet.py_lin_similarity
4685,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_morphy,"def morphy(self, form, pos=None, check_exceptions=True):
""""""
Find a possible base form for the given form, with the given
part of speech, by checking WordNet's list of exceptional
forms, and by recursively stripping affixes for this part of
speech until a form in WordNet is found.
>>> from nltk.corpus import wordnet as wn
>>> print(wn.morphy('dogs'))
dog
>>> print(wn.morphy('churches'))
church
>>> print(wn.morphy('aardwolves'))
aardwolf
>>> print(wn.morphy('abaci'))
abacus
>>> wn.morphy('hardrock', wn.ADV)
>>> print(wn.morphy('book', wn.NOUN))
book
>>> wn.morphy('book', wn.ADJ)
""""""
if pos is None:
morphy = self._morphy
analyses = chain(a for p in POS_LIST for a in morphy(form, p))
else:
analyses = self._morphy(form, pos, check_exceptions)
first = list(islice(analyses, 1))
if len(first) == 1:
return first[0]
else:
return None
",[],0,[],/corpus/reader/wordnet.py_morphy
4686,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py__morphy,"def _morphy(self, form, pos, check_exceptions=True):
exceptions = self._exception_map[pos]
substitutions = self.MORPHOLOGICAL_SUBSTITUTIONS[pos]
",[],0,[],/corpus/reader/wordnet.py__morphy
4687,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_apply_rules,"def apply_rules(forms):
return [
form[: -len(old)] + new
for form in forms
for old, new in substitutions
if form.endswith(old)
]
",[],0,[],/corpus/reader/wordnet.py_apply_rules
4688,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_filter_forms,"def filter_forms(forms):
result = []
seen = set()
for form in forms:
if form in self._lemma_pos_offset_map:
if pos in self._lemma_pos_offset_map[form]:
if form not in seen:
result.append(form)
seen.add(form)
return result
",[],0,[],/corpus/reader/wordnet.py_filter_forms
4689,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_ic,"def ic(self, corpus, weight_senses_equally=False, smoothing=1.0):
""""""
Creates an information content lookup dictionary from a corpus.
:type corpus: CorpusReader
:param corpus: The corpus from which we create an information
content dictionary.
:type weight_senses_equally: bool
:param weight_senses_equally: If this is True, gives all
possible senses equal weight rather than dividing by the
number of possible senses.  (If a word has 3 synses, each
sense gets 0.3333 per appearance when this is False, 1.0 when
it is true.)
:param smoothing: How much do we smooth synset counts (default is 1.0)
:type smoothing: float
:return: An information content dictionary
""""""
counts = FreqDist()
for ww in corpus.words():
counts[ww] += 1
ic = {}
for pp in POS_LIST:
ic[pp] = defaultdict(float)
if smoothing > 0.0:
for pp in POS_LIST:
ic[pp][0] = smoothing
for ss in self.all_synsets():
pos = ss._pos
if pos == ADJ_SAT:
pos = ADJ
ic[pos][ss._offset] = smoothing
for ww in counts:
possible_synsets = self.synsets(ww)
if len(possible_synsets) == 0:
continue
weight = float(counts[ww])
if not weight_senses_equally:
weight /= float(len(possible_synsets))
for ss in possible_synsets:
pos = ss._pos
if pos == ADJ_SAT:
pos = ADJ
for level in ss._iter_hypernym_lists():
for hh in level:
ic[pos][hh._offset] += weight
ic[pos][0] += weight
return ic
",[],0,[],/corpus/reader/wordnet.py_ic
4690,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_custom_lemmas,"def custom_lemmas(self, tab_file, lang):
""""""
Reads a custom tab file containing mappings of lemmas in the given
language to Princeton WordNet 3.0 synset offsets, allowing NLTK's
WordNet functions to then be used with that language.
See the ""Tab files"" section at https://omwn.org/omw1.html for
documentation on the Multilingual WordNet tab file format.
:param tab_file: Tab file as a file or file-like object
:type: lang str
:param: lang ISO 639-3 code of the language of the tab file
""""""
lg = lang.split(""_"")[0]
if len(lg) != 3:
raise ValueError(""lang should be a (3 character) ISO 639-3 code"")
self._lang_data[lang] = [
defaultdict(list),
defaultdict(list),
defaultdict(list),
defaultdict(list),
]
for line in tab_file.readlines():
if isinstance(line, bytes):
line = line.decode(""utf-8"")
if not line.startswith(""#""):
triple = line.strip().split(""\t"")
if len(triple) < 3:
continue
offset_pos, label = triple[:2]
val = triple[-1]
if self.map30:
if offset_pos in self.map30:
offset_pos = self.map30[offset_pos]
else:
if (
offset_pos not in self.nomap[""wordnet""]
and offset_pos.replace(""a"", ""s"")
not in self.nomap[""wordnet""]
):
warnings.warn(
f""{lang}: invalid offset {offset_pos} in '{line}'""
)
continue
elif offset_pos[-1] == ""a"":
wnss = self.of2ss(offset_pos)
if wnss and wnss.pos() == ""s"":  # Wordnet pos is ""s""
offset_pos = self.ss2of(wnss)
pair = label.split("":"")
attr = pair[-1]
if len(pair) == 1 or pair[0] == lg:
if attr == ""lemma"":
val = val.strip().replace("" "", ""_"")
lang_offsets = self._lang_data[lang][1][val.lower()]
if offset_pos not in lang_offsets:
lang_offsets.append(offset_pos)
if attr in self.lg_attrs:
lang_lemmas = self._lang_data[lang][self.lg_attrs.index(attr)][
offset_pos
]
if val not in lang_lemmas:
lang_lemmas.append(val)
",[],0,[],/corpus/reader/wordnet.py_custom_lemmas
4691,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_disable_custom_lemmas,"def disable_custom_lemmas(self, lang):
""""""prevent synsets from being mistakenly added""""""
for n in range(len(self.lg_attrs)):
self._lang_data[lang][n].default_factory = None
",[],0,[],/corpus/reader/wordnet.py_disable_custom_lemmas
4692,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_add_lemma,"def add_lemma(lem):
ss = lem.synset()
synsets.add(ss)
edges.add((lem, ss))
",[],0,[],/corpus/reader/wordnet.py_add_lemma
4693,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py___init__,"def __init__(self, root, fileids):
CorpusReader.__init__(self, root, fileids, encoding=""utf8"")
",[],0,[],/corpus/reader/wordnet.py___init__
4694,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_ic,"def ic(self, icfile):
""""""
Load an information content file from the wordnet_ic corpus
and return a dictionary.  This dictionary has just two keys,
NOUN and VERB, whose values are dictionaries that map from
synsets to information content values.
:type icfile: str
:param icfile: The name of the wordnet_ic file (e.g. ""ic-brown.dat"")
:return: An information content dictionary
""""""
ic = {}
ic[NOUN] = defaultdict(float)
ic[VERB] = defaultdict(float)
with self.open(icfile) as fp:
for num, line in enumerate(fp):
if num == 0:  # skip the header
continue
fields = line.split()
offset = int(fields[0][:-1])
value = float(fields[1])
pos = _get_pos(fields[0])
if len(fields) == 3 and fields[2] == ""ROOT"":
ic[pos][0] += value
if value != 0:
ic[pos][offset] = value
return ic
",[],0,[],/corpus/reader/wordnet.py_ic
4695,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_path_similarity,"def path_similarity(synset1, synset2, verbose=False, simulate_root=True):
return synset1.path_similarity(
synset2, verbose=verbose, simulate_root=simulate_root
)
",[],0,[],/corpus/reader/wordnet.py_path_similarity
4696,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_lch_similarity,"def lch_similarity(synset1, synset2, verbose=False, simulate_root=True):
return synset1.lch_similarity(synset2, verbose=verbose, simulate_root=simulate_root)
",[],0,[],/corpus/reader/wordnet.py_lch_similarity
4697,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_wup_similarity,"def wup_similarity(synset1, synset2, verbose=False, simulate_root=True):
return synset1.wup_similarity(synset2, verbose=verbose, simulate_root=simulate_root)
",[],0,[],/corpus/reader/wordnet.py_wup_similarity
4698,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_res_similarity,"def res_similarity(synset1, synset2, ic, verbose=False):
return synset1.res_similarity(synset2, ic, verbose=verbose)
",[],0,[],/corpus/reader/wordnet.py_res_similarity
4699,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_jcn_similarity,"def jcn_similarity(synset1, synset2, ic, verbose=False):
return synset1.jcn_similarity(synset2, ic, verbose=verbose)
",[],0,[],/corpus/reader/wordnet.py_jcn_similarity
4700,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_lin_similarity,"def lin_similarity(synset1, synset2, ic, verbose=False):
return synset1.lin_similarity(synset2, ic, verbose=verbose)
",[],0,[],/corpus/reader/wordnet.py_lin_similarity
4701,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py__lcs_ic,"def _lcs_ic(synset1, synset2, ic, verbose=False):
""""""
Get the information content of the least common subsumer that has
the highest information content value.  If two nodes have no
explicit common subsumer, assume that they share an artificial
root node that is the hypernym of all explicit roots.
:type synset1: Synset
:param synset1: First input synset.
:type synset2: Synset
:param synset2: Second input synset.  Must be the same part of
speech as the first synset.
:type  ic: dict
:param ic: an information content object (as returned by ``load_ic()``).
:return: The information content of the two synsets and their most
informative subsumer
""""""
if synset1._pos != synset2._pos:
raise WordNetError(
""Computing the least common subsumer requires ""
""%s and %s to have the same part of speech."" % (synset1, synset2)
)
ic1 = information_content(synset1, ic)
ic2 = information_content(synset2, ic)
subsumers = synset1.common_hypernyms(synset2)
if len(subsumers) == 0:
subsumer_ic = 0
else:
subsumer_ic = max(information_content(s, ic) for s in subsumers)
if verbose:
print(""> LCS Subsumer by content:"", subsumer_ic)
return ic1, ic2, subsumer_ic
",[],0,[],/corpus/reader/wordnet.py__lcs_ic
4702,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py_information_content,"def information_content(synset, ic):
pos = synset._pos
if pos == ADJ_SAT:
pos = ADJ
try:
icpos = ic[pos]
except KeyError as e:
msg = ""Information content file has no entries for part-of-speech: %s""
raise WordNetError(msg % pos) from e
counts = icpos[synset._offset]
if counts == 0:
return _INF
else:
return -math.log(counts / icpos[0])
",[],0,[],/corpus/reader/wordnet.py_information_content
4703,/home/amandapotts/git/nltk/nltk/corpus/reader/wordnet.py__get_pos,"def _get_pos(field):
if field[-1] == ""n"":
return NOUN
elif field[-1] == ""v"":
return VERB
else:
msg = (
""Unidentified part of speech in WordNet Information Content file ""
""for field %s"" % field
)
raise ValueError(msg)
",[],0,[],/corpus/reader/wordnet.py__get_pos
4704,/home/amandapotts/git/nltk/nltk/corpus/reader/panlex_lite.py___init__,"def __init__(self, root):
self._c = sqlite3.connect(os.path.join(root, ""db.sqlite"")).cursor()
self._uid_lv = {}
self._lv_uid = {}
for row in self._c.execute(""SELECT uid, lv FROM lv""):
self._uid_lv[row[0]] = row[1]
self._lv_uid[row[1]] = row[0]
",[],0,[],/corpus/reader/panlex_lite.py___init__
4705,/home/amandapotts/git/nltk/nltk/corpus/reader/panlex_lite.py_language_varieties,"def language_varieties(self, lc=None):
""""""
Return a list of PanLex language varieties.
:param lc: ISO 639 alpha-3 code. If specified, filters returned varieties
by this code. If unspecified, all varieties are returned.
:return: the specified language varieties as a list of tuples. The first
element is the language variety's seven-character uniform identifier,
and the second element is its default name.
:rtype: list(tuple)
""""""
if lc is None:
return self._c.execute(""SELECT uid, tt FROM lv ORDER BY uid"").fetchall()
else:
return self._c.execute(
""SELECT uid, tt FROM lv WHERE lc = ? ORDER BY uid"", (lc,)
).fetchall()
",[],0,[],/corpus/reader/panlex_lite.py_language_varieties
4706,/home/amandapotts/git/nltk/nltk/corpus/reader/panlex_lite.py_meanings,"def meanings(self, expr_uid, expr_tt):
""""""
Return a list of meanings for an expression.
:param expr_uid: the expression's language variety, as a seven-character
uniform identifier.
:param expr_tt: the expression's text.
:return: a list of Meaning objects.
:rtype: list(Meaning)
""""""
expr_lv = self._uid_lv[expr_uid]
mn_info = {}
for i in self._c.execute(self.MEANING_Q, (expr_tt, expr_lv)):
mn = i[0]
uid = self._lv_uid[i[5]]
if not mn in mn_info:
mn_info[mn] = {
""uq"": i[1],
""ap"": i[2],
""ui"": i[3],
""ex"": {expr_uid: [expr_tt]},
}
if not uid in mn_info[mn][""ex""]:
mn_info[mn][""ex""][uid] = []
mn_info[mn][""ex""][uid].append(i[4])
return [Meaning(mn, mn_info[mn]) for mn in mn_info]
",[],0,[],/corpus/reader/panlex_lite.py_meanings
4707,/home/amandapotts/git/nltk/nltk/corpus/reader/panlex_lite.py_translations,"def translations(self, from_uid, from_tt, to_uid):
""""""
Return a list of translations for an expression into a single language
variety.
:param from_uid: the source expression's language variety, as a
seven-character uniform identifier.
:param from_tt: the source expression's text.
:param to_uid: the target language variety, as a seven-character
uniform identifier.
:return: a list of translation tuples. The first element is the expression
text and the second element is the translation quality.
:rtype: list(tuple)
""""""
from_lv = self._uid_lv[from_uid]
to_lv = self._uid_lv[to_uid]
return self._c.execute(self.TRANSLATION_Q, (from_lv, from_tt, to_lv)).fetchall()
",[],0,[],/corpus/reader/panlex_lite.py_translations
4708,/home/amandapotts/git/nltk/nltk/corpus/reader/panlex_lite.py___init__,"def __init__(self, mn, attr):
super().__init__(**attr)
self[""mn""] = mn
",[],0,[],/corpus/reader/panlex_lite.py___init__
4709,/home/amandapotts/git/nltk/nltk/corpus/reader/panlex_lite.py_id,"def id(self):
""""""
:return: the meaning's id.
:rtype: int
""""""
return self[""mn""]
",[],0,[],/corpus/reader/panlex_lite.py_id
4710,/home/amandapotts/git/nltk/nltk/corpus/reader/panlex_lite.py_quality,"def quality(self):
""""""
:return: the meaning's source's quality (0=worst, 9=best).
:rtype: int
""""""
return self[""uq""]
",[],0,[],/corpus/reader/panlex_lite.py_quality
4711,/home/amandapotts/git/nltk/nltk/corpus/reader/panlex_lite.py_source,"def source(self):
""""""
:return: the meaning's source id.
:rtype: int
""""""
return self[""ap""]
",[],0,[],/corpus/reader/panlex_lite.py_source
4712,/home/amandapotts/git/nltk/nltk/corpus/reader/panlex_lite.py_source_group,"def source_group(self):
""""""
:return: the meaning's source group id.
:rtype: int
""""""
return self[""ui""]
",[],0,[],/corpus/reader/panlex_lite.py_source_group
4713,/home/amandapotts/git/nltk/nltk/corpus/reader/panlex_lite.py_expressions,"def expressions(self):
""""""
:return: the meaning's expressions as a dictionary whose keys are language
variety uniform identifiers and whose values are lists of expression
texts.
:rtype: dict
""""""
return self[""ex""]
",[],0,[],/corpus/reader/panlex_lite.py_expressions
4714,/home/amandapotts/git/nltk/nltk/corpus/reader/xmldocs.py___init__,"def __init__(self, root, fileids, wrap_etree=False):
self._wrap_etree = wrap_etree
CorpusReader.__init__(self, root, fileids)
",[],0,[],/corpus/reader/xmldocs.py___init__
4715,/home/amandapotts/git/nltk/nltk/corpus/reader/xmldocs.py_xml,"def xml(self, fileid=None):
if fileid is None and len(self._fileids) == 1:
fileid = self._fileids[0]
if not isinstance(fileid, str):
raise TypeError(""Expected a single file identifier string"")
with self.abspath(fileid).open() as fp:
elt = ElementTree.parse(fp).getroot()
if self._wrap_etree:
elt = ElementWrapper(elt)
return elt
",[],0,[],/corpus/reader/xmldocs.py_xml
4716,/home/amandapotts/git/nltk/nltk/corpus/reader/xmldocs.py_words,"def words(self, fileid=None):
""""""
Returns all of the words and punctuation symbols in the specified file
that were in text nodes -- ie, tags are ignored. Like the xml() method,
fileid can only specify one file.
:return: the given file's text nodes as a list of words and punctuation symbols
:rtype: list(str)
""""""
elt = self.xml(fileid)
encoding = self.encoding(fileid)
word_tokenizer = WordPunctTokenizer()
try:
iterator = elt.getiterator()
except:
iterator = elt.iter()
out = []
for node in iterator:
text = node.text
if text is not None:
if isinstance(text, bytes):
text = text.decode(encoding)
toks = word_tokenizer.tokenize(text)
out.extend(toks)
return out
",[],0,[],/corpus/reader/xmldocs.py_words
4717,/home/amandapotts/git/nltk/nltk/corpus/reader/xmldocs.py___init__,"def __init__(self, fileid, tagspec, elt_handler=None):
""""""
Create a new corpus view based on a specified XML file.
Note that the ``XMLCorpusView`` constructor does not take an
``encoding`` argument, because the unicode encoding is
specified by the XML files themselves.
:type tagspec: str
:param tagspec: A tag specification, indicating what XML
elements should be included in the view.  Each non-nested
element that matches this specification corresponds to one
item in the view.
:param elt_handler: A function used to transform each element
to a value for the view.  If no handler is specified, then
``self.handle_elt()`` is called, which returns the element
as an ElementTree object.  The signature of elt_handler is::
elt_handler(elt, tagspec) -> value
""""""
if elt_handler:
self.handle_elt = elt_handler
self._tagspec = re.compile(tagspec + r""\Z"")
""""""The tag specification for this corpus view.""""""
self._tag_context = {0: ()}
""""""A dictionary mapping from file positions (as returned by
``stream.seek()`` to XML contexts.  An XML context is a
tuple of XML tag names, indicating which tags have not yet
been closed.""""""
encoding = self._detect_encoding(fileid)
StreamBackedCorpusView.__init__(self, fileid, encoding=encoding)
",[],0,[],/corpus/reader/xmldocs.py___init__
4718,/home/amandapotts/git/nltk/nltk/corpus/reader/xmldocs.py__detect_encoding,"def _detect_encoding(self, fileid):
if isinstance(fileid, PathPointer):
try:
infile = fileid.open()
s = infile.readline()
finally:
infile.close()
else:
with open(fileid, ""rb"") as infile:
s = infile.readline()
if s.startswith(codecs.BOM_UTF16_BE):
return ""utf-16-be""
if s.startswith(codecs.BOM_UTF16_LE):
return ""utf-16-le""
if s.startswith(codecs.BOM_UTF32_BE):
return ""utf-32-be""
if s.startswith(codecs.BOM_UTF32_LE):
return ""utf-32-le""
if s.startswith(codecs.BOM_UTF8):
return ""utf-8""
m = re.match(rb'\s*<\?xml\b.*\bencoding=""([^""]+)""', s)
if m:
return m.group(1).decode()
m = re.match(rb""\s*<\?xml\b.*\bencoding='([^']+)'"", s)
if m:
return m.group(1).decode()
return ""utf-8""
",[],0,[],/corpus/reader/xmldocs.py__detect_encoding
4719,/home/amandapotts/git/nltk/nltk/corpus/reader/xmldocs.py_handle_elt,"def handle_elt(self, elt, context):
""""""
Convert an element into an appropriate value for inclusion in
the view.  Unless overridden by a subclass or by the
``elt_handler`` constructor argument, this method simply
returns ``elt``.
:return: The view value corresponding to ``elt``.
:type elt: ElementTree
:param elt: The element that should be converted.
:type context: str
:param context: A string composed of element tags separated by
forward slashes, indicating the XML context of the given
element.  For example, the string ``'foo/bar/baz'``
indicates that the element is a ``baz`` element whose
parent is a ``bar`` element and whose grandparent is a
top-level ``foo`` element.
""""""
return elt
",[],0,[],/corpus/reader/xmldocs.py_handle_elt
4720,/home/amandapotts/git/nltk/nltk/corpus/reader/xmldocs.py__read_xml_fragment,"def _read_xml_fragment(self, stream):
""""""
Read a string from the given stream that does not contain any
un-closed tags.  In particular, this function first reads a
block from the stream of size ``self._BLOCK_SIZE``.  It then
checks if that block contains an un-closed tag.  If it does,
then this function either backtracks to the last '<', or reads
another block.
""""""
fragment = """"
if isinstance(stream, SeekableUnicodeStreamReader):
startpos = stream.tell()
while True:
xml_block = stream.read(self._BLOCK_SIZE)
fragment += xml_block
if self._VALID_XML_RE.match(fragment):
return fragment
if re.search(""[<>]"", fragment).group(0) == "">"":
pos = stream.tell() - (
len(fragment) - re.search(""[<>]"", fragment).end()
)
raise ValueError('Unexpected "">"" near char %s' % pos)
if not xml_block:
raise ValueError(""Unexpected end of file: tag not closed"")
last_open_bracket = fragment.rfind(""<"")
if last_open_bracket > 0:
if self._VALID_XML_RE.match(fragment[:last_open_bracket]):
if isinstance(stream, SeekableUnicodeStreamReader):
stream.seek(startpos)
stream.char_seek_forward(last_open_bracket)
else:
stream.seek(-(len(fragment) - last_open_bracket), 1)
return fragment[:last_open_bracket]
",[],0,[],/corpus/reader/xmldocs.py__read_xml_fragment
4721,/home/amandapotts/git/nltk/nltk/corpus/reader/xmldocs.py_read_block,"def read_block(self, stream, tagspec=None, elt_handler=None):
""""""
Read from ``stream`` until we find at least one element that
matches ``tagspec``, and return the result of applying
``elt_handler`` to each element found.
""""""
if tagspec is None:
tagspec = self._tagspec
if elt_handler is None:
elt_handler = self.handle_elt
context = list(self._tag_context.get(stream.tell()))
assert context is not None  # check this -- could it ever happen?
elts = []
elt_start = None  # where does the elt start
elt_depth = None  # what context depth
elt_text = """"
while elts == [] or elt_start is not None:
if isinstance(stream, SeekableUnicodeStreamReader):
startpos = stream.tell()
xml_fragment = self._read_xml_fragment(stream)
if not xml_fragment:
if elt_start is None:
break
else:
raise ValueError(""Unexpected end of file"")
for piece in self._XML_PIECE.finditer(xml_fragment):
if self._DEBUG:
print(""{:>25} {}"".format(""/"".join(context)[-20:], piece.group()))
if piece.group(""START_TAG""):
name = self._XML_TAG_NAME.match(piece.group()).group(1)
context.append(name)
if elt_start is None:
if re.match(tagspec, ""/"".join(context)):
elt_start = piece.start()
elt_depth = len(context)
elif piece.group(""END_TAG""):
name = self._XML_TAG_NAME.match(piece.group()).group(1)
if not context:
raise ValueError(""Unmatched tag </%s>"" % name)
if name != context[-1]:
raise ValueError(f""Unmatched tag <{context[-1]}>...</{name}>"")
if elt_start is not None and elt_depth == len(context):
elt_text += xml_fragment[elt_start : piece.end()]
elts.append((elt_text, ""/"".join(context)))
elt_start = elt_depth = None
elt_text = """"
context.pop()
elif piece.group(""EMPTY_ELT_TAG""):
name = self._XML_TAG_NAME.match(piece.group()).group(1)
if elt_start is None:
if re.match(tagspec, ""/"".join(context) + ""/"" + name):
elts.append((piece.group(), ""/"".join(context) + ""/"" + name))
if elt_start is not None:
if elts == []:
elt_text += xml_fragment[elt_start:]
elt_start = 0
else:
if self._DEBUG:
print("" "" * 36 + ""(backtrack)"")
if isinstance(stream, SeekableUnicodeStreamReader):
stream.seek(startpos)
stream.char_seek_forward(elt_start)
else:
stream.seek(-(len(xml_fragment) - elt_start), 1)
context = context[: elt_depth - 1]
elt_start = elt_depth = None
elt_text = """"
pos = stream.tell()
if pos in self._tag_context:
assert tuple(context) == self._tag_context[pos]
else:
self._tag_context[pos] = tuple(context)
return [
elt_handler(
ElementTree.fromstring(elt.encode(""ascii"", ""xmlcharrefreplace"")),
context,
)
for (elt, context) in elts
]
",[],0,[],/corpus/reader/xmldocs.py_read_block
4722,/home/amandapotts/git/nltk/nltk/corpus/reader/childes.py___init__,"def __init__(self, root, fileids, lazy=True):
XMLCorpusReader.__init__(self, root, fileids)
self._lazy = lazy
",[],0,[],/corpus/reader/childes.py___init__
4723,/home/amandapotts/git/nltk/nltk/corpus/reader/childes.py_words,"def words(
self,
fileids=None,
speaker=""ALL"",
stem=False,
relation=False,
strip_space=True,
replace=False,
",[],0,[],/corpus/reader/childes.py_words
4724,/home/amandapotts/git/nltk/nltk/corpus/reader/childes.py_tagged_words,"def tagged_words(
self,
fileids=None,
speaker=""ALL"",
stem=False,
relation=False,
strip_space=True,
replace=False,
",[],0,[],/corpus/reader/childes.py_tagged_words
4725,/home/amandapotts/git/nltk/nltk/corpus/reader/childes.py_sents,"def sents(
self,
fileids=None,
speaker=""ALL"",
stem=False,
relation=None,
strip_space=True,
replace=False,
",[],0,[],/corpus/reader/childes.py_sents
4726,/home/amandapotts/git/nltk/nltk/corpus/reader/childes.py_tagged_sents,"def tagged_sents(
self,
fileids=None,
speaker=""ALL"",
stem=False,
relation=None,
strip_space=True,
replace=False,
",[],0,[],/corpus/reader/childes.py_tagged_sents
4727,/home/amandapotts/git/nltk/nltk/corpus/reader/childes.py_corpus,"def corpus(self, fileids=None):
""""""
:return: the given file(s) as a dict of ``(corpus_property_key, value)``
:rtype: list(dict)
""""""
if not self._lazy:
return [self._get_corpus(fileid) for fileid in self.abspaths(fileids)]
return LazyMap(self._get_corpus, self.abspaths(fileids))
",[],0,[],/corpus/reader/childes.py_corpus
4728,/home/amandapotts/git/nltk/nltk/corpus/reader/childes.py__get_corpus,"def _get_corpus(self, fileid):
results = dict()
xmldoc = ElementTree.parse(fileid).getroot()
for key, value in xmldoc.items():
results[key] = value
return results
",[],0,[],/corpus/reader/childes.py__get_corpus
4729,/home/amandapotts/git/nltk/nltk/corpus/reader/childes.py_participants,"def participants(self, fileids=None):
""""""
:return: the given file(s) as a dict of
``(participant_property_key, value)``
:rtype: list(dict)
""""""
if not self._lazy:
return [self._get_participants(fileid) for fileid in self.abspaths(fileids)]
return LazyMap(self._get_participants, self.abspaths(fileids))
",[],0,[],/corpus/reader/childes.py_participants
4730,/home/amandapotts/git/nltk/nltk/corpus/reader/childes.py__get_participants,"def _get_participants(self, fileid):
",[],0,[],/corpus/reader/childes.py__get_participants
4731,/home/amandapotts/git/nltk/nltk/corpus/reader/childes.py_dictOfDicts,"def dictOfDicts():
return defaultdict(dictOfDicts)
",[],0,[],/corpus/reader/childes.py_dictOfDicts
4732,/home/amandapotts/git/nltk/nltk/corpus/reader/childes.py_age,"def age(self, fileids=None, speaker=""CHI"", month=False):
""""""
:return: the given file(s) as string or int
:rtype: list or int
:param month: If true, return months instead of year-month-date
""""""
if not self._lazy:
return [
self._get_age(fileid, speaker, month)
for fileid in self.abspaths(fileids)
]
",[],0,[],/corpus/reader/childes.py_age
4733,/home/amandapotts/git/nltk/nltk/corpus/reader/childes.py__get_age,"def _get_age(self, fileid, speaker, month):
xmldoc = ElementTree.parse(fileid).getroot()
for pat in xmldoc.findall(f"".//{{{NS}}}Participants/{{{NS}}}participant""):
try:
if pat.get(""id"") == speaker:
age = pat.get(""age"")
if month:
age = self.convert_age(age)
return age
except (TypeError, AttributeError) as e:
return None
",[],0,[],/corpus/reader/childes.py__get_age
4734,/home/amandapotts/git/nltk/nltk/corpus/reader/childes.py_convert_age,"def convert_age(self, age_year):
""Caclculate age in months from a string in CHILDES format""
m = re.match(r""P(\d+)Y(\d+)M?(\d?\d?)D?"", age_year)
age_month = int(m.group(1)) * 12 + int(m.group(2))
try:
if int(m.group(3)) > 15:
age_month += 1
except ValueError as e:
pass
return age_month
",[],0,[],/corpus/reader/childes.py_convert_age
4735,/home/amandapotts/git/nltk/nltk/corpus/reader/childes.py_MLU,"def MLU(self, fileids=None, speaker=""CHI""):
""""""
:return: the given file(s) as a floating number
:rtype: list(float)
""""""
if not self._lazy:
return [
self._getMLU(fileid, speaker=speaker)
for fileid in self.abspaths(fileids)
]
",[],0,[],/corpus/reader/childes.py_MLU
4736,/home/amandapotts/git/nltk/nltk/corpus/reader/childes.py__getMLU,"def _getMLU(self, fileid, speaker):
sents = self._get_words(
fileid,
speaker=speaker,
sent=True,
stem=True,
relation=False,
pos=True,
strip_space=True,
replace=True,
)
results = []
lastSent = []
numFillers = 0
sentDiscount = 0
for sent in sents:
posList = [pos for (word, pos) in sent]
if any(pos == ""unk"" for pos in posList):
continue
elif sent == []:
continue
elif sent == lastSent:
continue
else:
results.append([word for (word, pos) in sent])
if len({""co"", None}.intersection(posList)) > 0:
numFillers += posList.count(""co"")
numFillers += posList.count(None)
sentDiscount += 1
lastSent = sent
try:
thisWordList = flatten(results)
numWords = (
len(flatten([word.split(""-"") for word in thisWordList])) - numFillers
)
numSents = len(results) - sentDiscount
mlu = numWords / numSents
except ZeroDivisionError:
mlu = 0
return mlu
",[],0,[],/corpus/reader/childes.py__getMLU
4737,/home/amandapotts/git/nltk/nltk/corpus/reader/childes.py_webview_file,"def webview_file(self, fileid, urlbase=None):
""""""Map a corpus file to its web version on the CHILDES website,
and open it in a web browser.
The complete URL to be used is:
childes.childes_url_base + urlbase + fileid.replace('.xml', '.cha')
If no urlbase is passed, we try to calculate it.  This
requires that the childes corpus was set up to mirror the
folder hierarchy under childes.psy.cmu.edu/data-xml/, e.g.:
nltk_data/corpora/childes/Eng-USA/Cornell/??? or
nltk_data/corpora/childes/Romance/Spanish/Aguirre/???
The function first looks (as a special case) if ""Eng-USA"" is
on the path consisting of <corpus root>+fileid
""childes"", possibly followed by ""data-xml"", appears. If neither
one is found, we use the unmodified fileid and hope for the best.
If this is not right, specify urlbase explicitly, e.g., if the
corpus root points to the Cornell folder, urlbase='Eng-USA/Cornell'.
""""""
import webbrowser
if urlbase:
path = urlbase + ""/"" + fileid
else:
full = self.root + ""/"" + fileid
full = re.sub(r""\\"", ""/"", full)
if ""/childes/"" in full.lower():
path = re.findall(r""(?i)/childes(?:/data-xml)?/(.*)\.xml"", full)[0]
elif ""eng-usa"" in full.lower():
path = ""Eng-USA/"" + re.findall(r""/(?i)Eng-USA/(.*)\.xml"", full)[0]
else:
path = fileid
if path.endswith("".xml""):
path = path[:-4]
if not path.endswith("".cha""):
path = path + "".cha""
url = self.childes_url_base + path
webbrowser.open_new_tab(url)
print(""Opening in browser:"", url)
",[],0,[],/corpus/reader/childes.py_webview_file
4738,/home/amandapotts/git/nltk/nltk/corpus/reader/childes.py_demo,"def demo(corpus_root=None):
""""""
The CHILDES corpus should be manually downloaded and saved
to ``[NLTK_Data_Dir]/corpora/childes/``
""""""
if not corpus_root:
from nltk.data import find
corpus_root = find(""corpora/childes/data-xml/Eng-USA/"")
try:
childes = CHILDESCorpusReader(corpus_root, "".*.xml"")
for file in childes.fileids()[:5]:
corpus = """"
corpus_id = """"
for key, value in childes.corpus(file)[0].items():
if key == ""Corpus"":
corpus = value
if key == ""Id"":
corpus_id = value
print(""Reading"", corpus, corpus_id, "" ....."")
print(""words:"", childes.words(file)[:7], ""..."")
print(
""words with replaced words:"",
childes.words(file, replace=True)[:7],
"" ..."",
)
print(""words with pos tags:"", childes.tagged_words(file)[:7], "" ..."")
print(""words (only MOT):"", childes.words(file, speaker=""MOT"")[:7], ""..."")
print(""words (only CHI):"", childes.words(file, speaker=""CHI"")[:7], ""..."")
print(""stemmed words:"", childes.words(file, stem=True)[:7], "" ..."")
print(
""words with relations and pos-tag:"",
childes.words(file, relation=True)[:5],
"" ..."",
)
print(""sentence:"", childes.sents(file)[:2], "" ..."")
for participant, values in childes.participants(file)[0].items():
for key, value in values.items():
print(""\tparticipant"", participant, key, "":"", value)
print(""num of sent:"", len(childes.sents(file)))
print(""num of morphemes:"", len(childes.words(file, stem=True)))
print(""age:"", childes.age(file))
print(""age in month:"", childes.age(file, month=True))
print(""MLU:"", childes.MLU(file))
print()
except LookupError as e:
print(
""""""The CHILDES corpus, or the parts you need, should be manually
downloaded from https://childes.talkbank.org/data-xml/ and saved at
[NLTK_Data_Dir]/corpora/childes/
Alternately, you can call the demo with the path to a portion of the CHILDES corpus, e.g.:
demo('/path/to/childes/data-xml/Eng-USA/"")
""""""
)
",[],0,[],/corpus/reader/childes.py_demo
4739,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py_comma_separated_string_args,"def comma_separated_string_args(func):
""""""
A decorator that allows a function to be called with
a single string of comma-separated values which become
individual function arguments.
""""""
@wraps(func)
",[],0,[],/corpus/reader/markdown.py_comma_separated_string_args
4740,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py_wrapper,"def wrapper(*args, **kwargs):
_args = list()
for arg in args:
if isinstance(arg, str):
_args.append({part.strip() for part in arg.split("","")})
elif isinstance(arg, list):
_args.append(set(arg))
else:
_args.append(arg)
for name, value in kwargs.items():
if isinstance(value, str):
kwargs[name] = {part.strip() for part in value.split("","")}
return func(*_args, **kwargs)
",[],0,[],/corpus/reader/markdown.py_wrapper
4741,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py_read_parse_blankline_block,"def read_parse_blankline_block(stream, parser):
block = read_blankline_block(stream)
if block:
return [parser.render(block[0])]
return block
",[],0,[],/corpus/reader/markdown.py_read_parse_blankline_block
4742,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py___init__,"def __init__(self, content):
self.content = content
self.truncate_at = 16
",[],0,[],/corpus/reader/markdown.py___init__
4743,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py___repr__,"def __repr__(self):
return f""{self.__class__.__name__}(content={repr(str(self))})""
",[],0,[],/corpus/reader/markdown.py___repr__
4744,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py___str__,"def __str__(self):
return (
f""{self.content[:self.truncate_at]}""
f""{'...' if len(self.content) > self.truncate_at else ''}""
)
",[],0,[],/corpus/reader/markdown.py___str__
4745,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py_raw,"def raw(self):
return self.content
",[],0,[],/corpus/reader/markdown.py_raw
4746,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py_words,"def words(self):
return word_tokenize(self.content)
",[],0,[],/corpus/reader/markdown.py_words
4747,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py_sents,"def sents(self):
return [word_tokenize(sent) for sent in sent_tokenize(self.content)]
",[],0,[],/corpus/reader/markdown.py_sents
4748,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py_paras,"def paras(self):
return [
[word_tokenize(sent) for sent in sent_tokenize(para)]
for para in blankline_tokenize(self.content)
]
",[],0,[],/corpus/reader/markdown.py_paras
4749,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py___init__,"def __init__(self, language, *args):
self.language = language
super().__init__(*args)
",[],0,[],/corpus/reader/markdown.py___init__
4750,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py_sents,"def sents(self):
return [word_tokenize(line) for line in self.content.splitlines()]
",[],0,[],/corpus/reader/markdown.py_sents
4751,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py_lines,"def lines(self):
return self.content.splitlines()
",[],0,[],/corpus/reader/markdown.py_lines
4752,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py_paras,"def paras(self):
return [
[word_tokenize(line) for line in para.splitlines()]
for para in blankline_tokenize(self.content)
]
",[],0,[],/corpus/reader/markdown.py_paras
4753,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py___init__,"def __init__(self, heading, level, *args):
self.heading = heading
self.level = level
super().__init__(*args)
",[],0,[],/corpus/reader/markdown.py___init__
4754,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py___init__,"def __init__(self, *args, parser=None, **kwargs):
from markdown_it import MarkdownIt
from mdit_plain.renderer import RendererPlain
from mdit_py_plugins.front_matter import front_matter_plugin
self.parser = parser
if self.parser is None:
self.parser = MarkdownIt(""commonmark"", renderer_cls=RendererPlain)
self.parser.use(front_matter_plugin)
kwargs.setdefault(
""para_block_reader"", partial(read_parse_blankline_block, parser=self.parser)
)
super().__init__(*args, **kwargs)
",[],0,[],/corpus/reader/markdown.py___init__
4755,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py__read_word_block,"def _read_word_block(self, stream):
words = list()
for para in self._para_block_reader(stream):
words.extend(self._word_tokenizer.tokenize(para))
return words
",[],0,[],/corpus/reader/markdown.py__read_word_block
4756,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py___init__,"def __init__(self, *args, cat_field=""tags"", **kwargs):
""""""
Initialize the corpus reader. Categorization arguments
(``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to
the ``CategorizedCorpusReader`` constructor.  The remaining arguments
are passed to the ``MarkdownCorpusReader`` constructor.
""""""
cat_args = [""cat_pattern"", ""cat_map"", ""cat_file""]
if not any(arg in kwargs for arg in cat_args):
kwargs[""cat_map""] = dict()
CategorizedCorpusReader.__init__(self, kwargs)
MarkdownCorpusReader.__init__(self, *args, **kwargs)
if self._map is not None and not self._map:
for file_id in self._fileids:
metadata = self.metadata(file_id)
if metadata:
self._map[file_id] = metadata[0].get(cat_field, [])
",[],0,[],/corpus/reader/markdown.py___init__
4757,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py_categories,"def categories(self, fileids=None):
return super().categories(fileids)
",[],0,[],/corpus/reader/markdown.py_categories
4758,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py_fileids,"def fileids(self, categories=None):
if categories is None:
return self._fileids
return super().fileids(categories)
",[],0,[],/corpus/reader/markdown.py_fileids
4759,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py_raw,"def raw(self, fileids=None, categories=None):
return super().raw(self._resolve(fileids, categories))
",[],0,[],/corpus/reader/markdown.py_raw
4760,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py_words,"def words(self, fileids=None, categories=None):
return super().words(self._resolve(fileids, categories))
",[],0,[],/corpus/reader/markdown.py_words
4761,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py_sents,"def sents(self, fileids=None, categories=None):
return super().sents(self._resolve(fileids, categories))
",[],0,[],/corpus/reader/markdown.py_sents
4762,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py_paras,"def paras(self, fileids=None, categories=None):
return super().paras(self._resolve(fileids, categories))
",[],0,[],/corpus/reader/markdown.py_paras
4763,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py_concatenated_view,"def concatenated_view(self, reader, fileids, categories):
return concat(
[
self.CorpusView(path, reader, encoding=enc)
for (path, enc) in self.abspaths(
self._resolve(fileids, categories), include_encoding=True
)
]
)
",[],0,[],/corpus/reader/markdown.py_concatenated_view
4764,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py_metadata_reader,"def metadata_reader(self, stream):
from yaml import safe_load
return [
safe_load(t.content)
for t in self.parser.parse(stream.read())
if t.type == ""front_matter""
]
",[],0,[],/corpus/reader/markdown.py_metadata_reader
4765,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py_metadata,"def metadata(self, fileids=None, categories=None):
return self.concatenated_view(self.metadata_reader, fileids, categories)
",[],0,[],/corpus/reader/markdown.py_metadata
4766,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py_blockquotes,"def blockquotes(self, fileids=None, categories=None):
return self.concatenated_view(self.blockquote_reader, fileids, categories)
",[],0,[],/corpus/reader/markdown.py_blockquotes
4767,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py_code_block_reader,"def code_block_reader(self, stream):
return [
CodeBlock(
t.info,
t.content,
)
for t in self.parser.parse(stream.read())
if t.level == 0 and t.type in (""fence"", ""code_block"")
]
",[],0,[],/corpus/reader/markdown.py_code_block_reader
4768,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py_code_blocks,"def code_blocks(self, fileids=None, categories=None):
return self.concatenated_view(self.code_block_reader, fileids, categories)
",[],0,[],/corpus/reader/markdown.py_code_blocks
4769,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py_images,"def images(self, fileids=None, categories=None):
return self.concatenated_view(self.image_reader, fileids, categories)
",[],0,[],/corpus/reader/markdown.py_images
4770,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py_links,"def links(self, fileids=None, categories=None):
return self.concatenated_view(self.link_reader, fileids, categories)
",[],0,[],/corpus/reader/markdown.py_links
4771,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py_lists,"def lists(self, fileids=None, categories=None):
return self.concatenated_view(self.list_reader, fileids, categories)
",[],0,[],/corpus/reader/markdown.py_lists
4772,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py_section_reader,"def section_reader(self, stream):
section_blocks, block = list(), list()
for t in self.parser.parse(stream.read()):
if t.level == 0 and t.type == ""heading_open"":
if not block:
block.append(t)
else:
section_blocks.append(block)
block = [t]
elif block:
block.append(t)
if block:
section_blocks.append(block)
return [
MarkdownSection(
block[1].content,
block[0].markup.count(""#""),
self.parser.renderer.render(block, self.parser.options, env=None),
)
for block in section_blocks
]
",[],0,[],/corpus/reader/markdown.py_section_reader
4773,/home/amandapotts/git/nltk/nltk/corpus/reader/markdown.py_sections,"def sections(self, fileids=None, categories=None):
return self.concatenated_view(self.section_reader, fileids, categories)
",[],0,[],/corpus/reader/markdown.py_sections
4774,/home/amandapotts/git/nltk/nltk/corpus/reader/sentiwordnet.py___init__,"def __init__(self, root, fileids, encoding=""utf-8""):
""""""
Construct a new SentiWordNet Corpus Reader, using data from
the specified file.
""""""
super().__init__(root, fileids, encoding=encoding)
if len(self._fileids) != 1:
raise ValueError(""Exactly one file must be specified"")
self._db = {}
self._parse_src_file()
",[],0,[],/corpus/reader/sentiwordnet.py___init__
4775,/home/amandapotts/git/nltk/nltk/corpus/reader/sentiwordnet.py_senti_synset,"def senti_synset(self, *vals):
from nltk.corpus import wordnet as wn
if tuple(vals) in self._db:
pos_score, neg_score = self._db[tuple(vals)]
pos, offset = vals
if pos == ""s"":
pos = ""a""
synset = wn.synset_from_pos_and_offset(pos, offset)
return SentiSynset(pos_score, neg_score, synset)
else:
synset = wn.synset(vals[0])
pos = synset.pos()
if pos == ""s"":
pos = ""a""
offset = synset.offset()
if (pos, offset) in self._db:
pos_score, neg_score = self._db[(pos, offset)]
return SentiSynset(pos_score, neg_score, synset)
else:
return None
",[],0,[],/corpus/reader/sentiwordnet.py_senti_synset
4776,/home/amandapotts/git/nltk/nltk/corpus/reader/sentiwordnet.py_all_senti_synsets,"def all_senti_synsets(self):
from nltk.corpus import wordnet as wn
for key, fields in self._db.items():
pos, offset = key
pos_score, neg_score = fields
synset = wn.synset_from_pos_and_offset(pos, offset)
yield SentiSynset(pos_score, neg_score, synset)
",[],0,[],/corpus/reader/sentiwordnet.py_all_senti_synsets
4777,/home/amandapotts/git/nltk/nltk/corpus/reader/sentiwordnet.py___init__,"def __init__(self, pos_score, neg_score, synset):
self._pos_score = pos_score
self._neg_score = neg_score
self._obj_score = 1.0 - (self._pos_score + self._neg_score)
self.synset = synset
",[],0,[],/corpus/reader/sentiwordnet.py___init__
4778,/home/amandapotts/git/nltk/nltk/corpus/reader/sentiwordnet.py_pos_score,"def pos_score(self):
return self._pos_score
",[],0,[],/corpus/reader/sentiwordnet.py_pos_score
4779,/home/amandapotts/git/nltk/nltk/corpus/reader/sentiwordnet.py_neg_score,"def neg_score(self):
return self._neg_score
",[],0,[],/corpus/reader/sentiwordnet.py_neg_score
4780,/home/amandapotts/git/nltk/nltk/corpus/reader/sentiwordnet.py_obj_score,"def obj_score(self):
return self._obj_score
",[],0,[],/corpus/reader/sentiwordnet.py_obj_score
4781,/home/amandapotts/git/nltk/nltk/corpus/reader/sentiwordnet.py___str__,"def __str__(self):
""""""Prints just the Pos/Neg scores for now.""""""
s = ""<""
s += self.synset.name() + "": ""
s += ""PosScore=%s "" % self._pos_score
s += ""NegScore=%s"" % self._neg_score
s += "">""
return s
",[],0,[],/corpus/reader/sentiwordnet.py___str__
4782,/home/amandapotts/git/nltk/nltk/corpus/reader/sentiwordnet.py___repr__,"def __repr__(self):
return ""Senti"" + repr(self.synset)
",[],0,[],/corpus/reader/sentiwordnet.py___repr__
4783,/home/amandapotts/git/nltk/nltk/corpus/reader/categorized_sents.py___init__,"def __init__(
self,
root,
fileids,
word_tokenizer=WhitespaceTokenizer(),
sent_tokenizer=None,
encoding=""utf8"",
",[],0,[],/corpus/reader/categorized_sents.py___init__
4784,/home/amandapotts/git/nltk/nltk/corpus/reader/categorized_sents.py_sents,"def sents(self, fileids=None, categories=None):
""""""
Return all sentences in the corpus or in the specified file(s).
:param fileids: a list or regexp specifying the ids of the files whose
sentences have to be returned.
:param categories: a list specifying the categories whose sentences have
to be returned.
:return: the given file(s) as a list of sentences.
Each sentence is tokenized using the specified word_tokenizer.
:rtype: list(list(str))
""""""
fileids = self._resolve(fileids, categories)
if fileids is None:
fileids = self._fileids
elif isinstance(fileids, str):
fileids = [fileids]
return concat(
[
self.CorpusView(path, self._read_sent_block, encoding=enc)
for (path, enc, fileid) in self.abspaths(fileids, True, True)
]
)
",[],0,[],/corpus/reader/categorized_sents.py_sents
4785,/home/amandapotts/git/nltk/nltk/corpus/reader/categorized_sents.py_words,"def words(self, fileids=None, categories=None):
""""""
Return all words and punctuation symbols in the corpus or in the specified
file(s).
:param fileids: a list or regexp specifying the ids of the files whose
words have to be returned.
:param categories: a list specifying the categories whose words have to
be returned.
:return: the given file(s) as a list of words and punctuation symbols.
:rtype: list(str)
""""""
fileids = self._resolve(fileids, categories)
if fileids is None:
fileids = self._fileids
elif isinstance(fileids, str):
fileids = [fileids]
return concat(
[
self.CorpusView(path, self._read_word_block, encoding=enc)
for (path, enc, fileid) in self.abspaths(fileids, True, True)
]
)
",[],0,[],/corpus/reader/categorized_sents.py_words
4786,/home/amandapotts/git/nltk/nltk/corpus/reader/categorized_sents.py__read_sent_block,"def _read_sent_block(self, stream):
sents = []
for i in range(20):  # Read 20 lines at a time.
line = stream.readline()
if not line:
continue
if self._sent_tokenizer:
sents.extend(
[
self._word_tokenizer.tokenize(sent)
for sent in self._sent_tokenizer.tokenize(line)
]
)
else:
sents.append(self._word_tokenizer.tokenize(line))
return sents
",[],0,[],/corpus/reader/categorized_sents.py__read_sent_block
4787,/home/amandapotts/git/nltk/nltk/corpus/reader/categorized_sents.py__read_word_block,"def _read_word_block(self, stream):
words = []
for sent in self._read_sent_block(stream):
words.extend(sent)
return words
",[],0,[],/corpus/reader/categorized_sents.py__read_word_block
4788,/home/amandapotts/git/nltk/nltk/corpus/reader/panlex_swadesh.py___init__,"def __init__(self, *args, **kwargs):
super().__init__(*args, **kwargs)
self.swadesh_size = re.match(r""swadesh([0-9].*)\/"", self.fileids()[0]).group(1)
self._languages = {lang.panlex_uid: lang for lang in self.get_languages()}
self._macro_langauges = self.get_macrolanguages()
",[],0,[],/corpus/reader/panlex_swadesh.py___init__
4789,/home/amandapotts/git/nltk/nltk/corpus/reader/panlex_swadesh.py_license,"def license(self):
return ""CC0 1.0 Universal""
",[],0,[],/corpus/reader/panlex_swadesh.py_license
4790,/home/amandapotts/git/nltk/nltk/corpus/reader/panlex_swadesh.py_language_codes,"def language_codes(self):
return self._languages.keys()
",[],0,[],/corpus/reader/panlex_swadesh.py_language_codes
4791,/home/amandapotts/git/nltk/nltk/corpus/reader/panlex_swadesh.py_get_languages,"def get_languages(self):
for line in self.raw(f""langs{self.swadesh_size}.txt"").split(""\n""):
if not line.strip():  # Skip empty lines.
continue
yield PanlexLanguage(*line.strip().split(""\t""))
",[],0,[],/corpus/reader/panlex_swadesh.py_get_languages
4792,/home/amandapotts/git/nltk/nltk/corpus/reader/panlex_swadesh.py_get_macrolanguages,"def get_macrolanguages(self):
macro_langauges = defaultdict(list)
for lang in self._languages.values():
macro_langauges[lang.iso639].append(lang.panlex_uid)
return macro_langauges
",[],0,[],/corpus/reader/panlex_swadesh.py_get_macrolanguages
4793,/home/amandapotts/git/nltk/nltk/corpus/reader/panlex_swadesh.py_words_by_lang,"def words_by_lang(self, lang_code):
""""""
:return: a list of list(str)
""""""
fileid = f""swadesh{self.swadesh_size}/{lang_code}.txt""
return [concept.split(""\t"") for concept in self.words(fileid)]
",[],0,[],/corpus/reader/panlex_swadesh.py_words_by_lang
4794,/home/amandapotts/git/nltk/nltk/corpus/reader/panlex_swadesh.py_words_by_iso639,"def words_by_iso639(self, iso63_code):
""""""
:return: a list of list(str)
""""""
fileids = [
f""swadesh{self.swadesh_size}/{lang_code}.txt""
for lang_code in self._macro_langauges[iso63_code]
]
return [
concept.split(""\t"") for fileid in fileids for concept in self.words(fileid)
]
",[],0,[],/corpus/reader/panlex_swadesh.py_words_by_iso639
4795,/home/amandapotts/git/nltk/nltk/corpus/reader/panlex_swadesh.py_entries,"def entries(self, fileids=None):
""""""
:return: a tuple of words for the specified fileids.
""""""
if not fileids:
fileids = self.fileids()
wordlists = [self.words(f) for f in fileids]
return list(zip(*wordlists))
",[],0,[],/corpus/reader/panlex_swadesh.py_entries
4796,/home/amandapotts/git/nltk/nltk/corpus/reader/string_category.py___init__,"def __init__(self, root, fileids, delimiter="" "", encoding=""utf8""):
""""""
:param root: The root directory for this corpus.
:param fileids: A list or regexp specifying the fileids in this corpus.
:param delimiter: Field delimiter
""""""
CorpusReader.__init__(self, root, fileids, encoding)
self._delimiter = delimiter
",[],0,[],/corpus/reader/string_category.py___init__
4797,/home/amandapotts/git/nltk/nltk/corpus/reader/string_category.py_tuples,"def tuples(self, fileids=None):
if fileids is None:
fileids = self._fileids
elif isinstance(fileids, str):
fileids = [fileids]
return concat(
[
StreamBackedCorpusView(fileid, self._read_tuple_block, encoding=enc)
for (fileid, enc) in self.abspaths(fileids, True)
]
)
",[],0,[],/corpus/reader/string_category.py_tuples
4798,/home/amandapotts/git/nltk/nltk/corpus/reader/string_category.py__read_tuple_block,"def _read_tuple_block(self, stream):
line = stream.readline().strip()
if line:
return [tuple(line.split(self._delimiter, 1))]
else:
return []
",[],0,[],/corpus/reader/string_category.py__read_tuple_block
4799,/home/amandapotts/git/nltk/nltk/corpus/reader/bnc.py___init__,"def __init__(self, root, fileids, lazy=True):
XMLCorpusReader.__init__(self, root, fileids)
self._lazy = lazy
",[],0,[],/corpus/reader/bnc.py___init__
4800,/home/amandapotts/git/nltk/nltk/corpus/reader/bnc.py_words,"def words(self, fileids=None, strip_space=True, stem=False):
""""""
:return: the given file(s) as a list of words
and punctuation symbols.
:rtype: list(str)
:param strip_space: If true, then strip trailing spaces from
word tokens.  Otherwise, leave the spaces on the tokens.
:param stem: If true, then use word stems instead of word strings.
""""""
return self._views(fileids, False, None, strip_space, stem)
",[],0,[],/corpus/reader/bnc.py_words
4801,/home/amandapotts/git/nltk/nltk/corpus/reader/bnc.py_tagged_words,"def tagged_words(self, fileids=None, c5=False, strip_space=True, stem=False):
""""""
:return: the given file(s) as a list of tagged
words and punctuation symbols, encoded as tuples
``(word,tag)``.
:rtype: list(tuple(str,str))
:param c5: If true, then the tags used will be the more detailed
c5 tags.  Otherwise, the simplified tags will be used.
:param strip_space: If true, then strip trailing spaces from
word tokens.  Otherwise, leave the spaces on the tokens.
:param stem: If true, then use word stems instead of word strings.
""""""
tag = ""c5"" if c5 else ""pos""
return self._views(fileids, False, tag, strip_space, stem)
",[],0,[],/corpus/reader/bnc.py_tagged_words
4802,/home/amandapotts/git/nltk/nltk/corpus/reader/bnc.py_sents,"def sents(self, fileids=None, strip_space=True, stem=False):
""""""
:return: the given file(s) as a list of
sentences or utterances, each encoded as a list of word
strings.
:rtype: list(list(str))
:param strip_space: If true, then strip trailing spaces from
word tokens.  Otherwise, leave the spaces on the tokens.
:param stem: If true, then use word stems instead of word strings.
""""""
return self._views(fileids, True, None, strip_space, stem)
",[],0,[],/corpus/reader/bnc.py_sents
4803,/home/amandapotts/git/nltk/nltk/corpus/reader/bnc.py_tagged_sents,"def tagged_sents(self, fileids=None, c5=False, strip_space=True, stem=False):
""""""
:return: the given file(s) as a list of
sentences, each encoded as a list of ``(word,tag)`` tuples.
:rtype: list(list(tuple(str,str)))
:param c5: If true, then the tags used will be the more detailed
c5 tags.  Otherwise, the simplified tags will be used.
:param strip_space: If true, then strip trailing spaces from
word tokens.  Otherwise, leave the spaces on the tokens.
:param stem: If true, then use word stems instead of word strings.
""""""
tag = ""c5"" if c5 else ""pos""
return self._views(
fileids, sent=True, tag=tag, strip_space=strip_space, stem=stem
)
",[],0,[],/corpus/reader/bnc.py_tagged_sents
4804,/home/amandapotts/git/nltk/nltk/corpus/reader/bnc.py__views,"def _views(self, fileids=None, sent=False, tag=False, strip_space=True, stem=False):
""""""A helper function that instantiates BNCWordViews or the list of words/sentences.""""""
f = BNCWordView if self._lazy else self._words
return concat(
[
f(fileid, sent, tag, strip_space, stem)
for fileid in self.abspaths(fileids)
]
)
",[],0,[],/corpus/reader/bnc.py__views
4805,/home/amandapotts/git/nltk/nltk/corpus/reader/bnc.py__words,"def _words(self, fileid, bracket_sent, tag, strip_space, stem):
""""""
Helper used to implement the view methods -- returns a list of
words or a list of sentences, optionally tagged.
:param fileid: The name of the underlying file.
:param bracket_sent: If true, include sentence bracketing.
:param tag: The name of the tagset to use, or None for no tags.
:param strip_space: If true, strip spaces from word tokens.
:param stem: If true, then substitute stems for words.
""""""
result = []
xmldoc = ElementTree.parse(fileid).getroot()
for xmlsent in xmldoc.findall("".//s""):
sent = []
for xmlword in _all_xmlwords_in(xmlsent):
word = xmlword.text
if not word:
word = """"  # fixes issue 337?
if strip_space or stem:
word = word.strip()
if stem:
word = xmlword.get(""hw"", word)
if tag == ""c5"":
word = (word, xmlword.get(""c5""))
elif tag == ""pos"":
word = (word, xmlword.get(""pos"", xmlword.get(""c5"")))
sent.append(word)
if bracket_sent:
result.append(BNCSentence(xmlsent.attrib[""n""], sent))
else:
result.extend(sent)
assert None not in result
return result
",[],0,[],/corpus/reader/bnc.py__words
4806,/home/amandapotts/git/nltk/nltk/corpus/reader/bnc.py__all_xmlwords_in,"def _all_xmlwords_in(elt, result=None):
if result is None:
result = []
for child in elt:
if child.tag in (""c"", ""w""):
result.append(child)
else:
_all_xmlwords_in(child, result)
return result
",[],0,[],/corpus/reader/bnc.py__all_xmlwords_in
4807,/home/amandapotts/git/nltk/nltk/corpus/reader/bnc.py___init__,"def __init__(self, num, items):
self.num = num
list.__init__(self, items)
",[],0,[],/corpus/reader/bnc.py___init__
4808,/home/amandapotts/git/nltk/nltk/corpus/reader/bnc.py___init__,"def __init__(self, fileid, sent, tag, strip_space, stem):
""""""
:param fileid: The name of the underlying file.
:param sent: If true, include sentence bracketing.
:param tag: The name of the tagset to use, or None for no tags.
:param strip_space: If true, strip spaces from word tokens.
:param stem: If true, then substitute stems for words.
""""""
if sent:
tagspec = "".*/s""
else:
tagspec = "".*/s/(.*/)?(c|w)""
self._sent = sent
self._tag = tag
self._strip_space = strip_space
self._stem = stem
self.title = None  #: Title of the document.
self.author = None  #: Author of the document.
self.editor = None  #: Editor
self.resps = None  #: Statement of responsibility
XMLCorpusView.__init__(self, fileid, tagspec)
self._open()
self.read_block(self._stream, "".*/teiHeader$"", self.handle_header)
self.close()
self._tag_context = {0: ()}
",[],0,[],/corpus/reader/bnc.py___init__
4809,/home/amandapotts/git/nltk/nltk/corpus/reader/bnc.py_handle_header,"def handle_header(self, elt, context):
titles = elt.findall(""titleStmt/title"")
if titles:
self.title = ""\n"".join(title.text.strip() for title in titles)
authors = elt.findall(""titleStmt/author"")
if authors:
self.author = ""\n"".join(author.text.strip() for author in authors)
editors = elt.findall(""titleStmt/editor"")
if editors:
self.editor = ""\n"".join(editor.text.strip() for editor in editors)
resps = elt.findall(""titleStmt/respStmt"")
if resps:
self.resps = ""\n\n"".join(
""\n"".join(resp_elt.text.strip() for resp_elt in resp) for resp in resps
)
",[],0,[],/corpus/reader/bnc.py_handle_header
4810,/home/amandapotts/git/nltk/nltk/corpus/reader/bnc.py_handle_elt,"def handle_elt(self, elt, context):
if self._sent:
return self.handle_sent(elt)
else:
return self.handle_word(elt)
",[],0,[],/corpus/reader/bnc.py_handle_elt
4811,/home/amandapotts/git/nltk/nltk/corpus/reader/bnc.py_handle_word,"def handle_word(self, elt):
word = elt.text
if not word:
word = """"  # fixes issue 337?
if self._strip_space or self._stem:
word = word.strip()
if self._stem:
word = elt.get(""hw"", word)
if self._tag == ""c5"":
word = (word, elt.get(""c5""))
elif self._tag == ""pos"":
word = (word, elt.get(""pos"", elt.get(""c5"")))
return word
",[],0,[],/corpus/reader/bnc.py_handle_word
4812,/home/amandapotts/git/nltk/nltk/corpus/reader/bnc.py_handle_sent,"def handle_sent(self, elt):
sent = []
for child in elt:
if child.tag in (""mw"", ""hi"", ""corr"", ""trunc""):
sent += [self.handle_word(w) for w in child]
elif child.tag in (""w"", ""c""):
sent.append(self.handle_word(child))
elif child.tag not in self.tags_to_ignore:
raise ValueError(""Unexpected element %s"" % child.tag)
return BNCSentence(elt.attrib[""n""], sent)
",[],0,[],/corpus/reader/bnc.py_handle_sent
4813,/home/amandapotts/git/nltk/nltk/tbl/template.py_applicable_rules,"def applicable_rules(self, tokens, i, correctTag):
""""""
Return a list of the transformational rules that would correct
the ``i``-th subtoken's tag in the given token.  In particular,
return a list of zero or more rules that would change
``tokens[i][1]`` to ``correctTag``, if applied to ``token[i]``.
If the ``i``-th token already has the correct tag (i.e., if
``tagged_tokens[i][1] == correctTag``), then
``applicable_rules()`` should return the empty list.
:param tokens: The tagged tokens being tagged.
:type tokens: list(tuple)
:param i: The index of the token whose tag should be corrected.
:type i: int
:param correctTag: The correct tag for the ``i``-th token.
:type correctTag: any
:rtype: list(BrillRule)
""""""
",[],0,[],/tbl/template.py_applicable_rules
4814,/home/amandapotts/git/nltk/nltk/tbl/template.py_get_neighborhood,"def get_neighborhood(self, token, index):
""""""
Returns the set of indices *i* such that
``applicable_rules(token, i, ...)`` depends on the value of
the *index*th token of *token*.
This method is used by the ""fast"" Brill tagger trainer.
:param token: The tokens being tagged.
:type token: list(tuple)
:param index: The index whose neighborhood should be returned.
:type index: int
:rtype: set
""""""
",[],0,[],/tbl/template.py_get_neighborhood
4815,/home/amandapotts/git/nltk/nltk/tbl/template.py___init__,"def __init__(self, *features):
""""""
Construct a Template for generating Rules.
Takes a list of Features. A C{Feature} is a combination
of a specific property and its relative positions and should be
a subclass of L{nltk.tbl.feature.Feature}.
An alternative calling convention (kept for backwards compatibility,
but less expressive as it only permits one feature type) is
Template(Feature, (start1, end1), (start2, end2), ...)
In new code, that would be better written
Template(Feature(start1, end1), Feature(start2, end2), ...)
For instance, importing some features
>>> from nltk.tbl.template import Template
>>> from nltk.tag.brill import Word, Pos
Create some features
>>> wfeat1, wfeat2, pfeat = (Word([-1]), Word([1,2]), Pos([-2,-1]))
Create a single-feature template
>>> Template(wfeat1)
Template(Word([-1]))
Or a two-feature one
>>> Template(wfeat1, wfeat2)
Template(Word([-1]),Word([1, 2]))
Or a three-feature one with two different feature types
>>> Template(wfeat1, wfeat2, pfeat)
Template(Word([-1]),Word([1, 2]),Pos([-2, -1]))
deprecated api: Feature subclass, followed by list of (start,end) pairs
(permits only a single Feature)
>>> Template(Word, (-2,-1), (0,0))
Template(Word([-2, -1]),Word([0]))
Incorrect specification raises TypeError
>>> Template(Word, (-2,-1), Pos, (0,0))
Traceback (most recent call last):
File ""<stdin>"", line 1, in <module>
File ""nltk/tag/tbl/template.py"", line 143, in __init__
raise TypeError(
TypeError: expected either Feature1(args), Feature2(args), ... or Feature, (start1, end1), (start2, end2), ...
:type features: list of Features
:param features: the features to build this Template on
""""""
if all(isinstance(f, Feature) for f in features):
self._features = features
elif issubclass(features[0], Feature) and all(
isinstance(a, tuple) for a in features[1:]
):
self._features = [features[0](*tp) for tp in features[1:]]
else:
raise TypeError(
""expected either Feature1(args), Feature2(args), ... or Feature, (start1, end1), (start2, end2), ...""
)
self.id = f""{len(self.ALLTEMPLATES):03d}""
self.ALLTEMPLATES.append(self)
",[],0,[],/tbl/template.py___init__
4816,/home/amandapotts/git/nltk/nltk/tbl/template.py___repr__,"def __repr__(self):
return ""{}({})"".format(
self.__class__.__name__,
"","".join([str(f) for f in self._features]),
)
",[],0,[],/tbl/template.py___repr__
4817,/home/amandapotts/git/nltk/nltk/tbl/template.py_applicable_rules,"def applicable_rules(self, tokens, index, correct_tag):
if tokens[index][1] == correct_tag:
return []
applicable_conditions = self._applicable_conditions(tokens, index)
xs = list(it.product(*applicable_conditions))
return [Rule(self.id, tokens[index][1], correct_tag, tuple(x)) for x in xs]
",[],0,[],/tbl/template.py_applicable_rules
4818,/home/amandapotts/git/nltk/nltk/tbl/template.py__applicable_conditions,"def _applicable_conditions(self, tokens, index):
""""""
:returns: A set of all conditions for rules
that are applicable to C{tokens[index]}.
""""""
conditions = []
for feature in self._features:
conditions.append([])
for pos in feature.positions:
if not (0 <= index + pos < len(tokens)):
continue
value = feature.extract_property(tokens, index + pos)
conditions[-1].append((feature, value))
return conditions
",[],0,[],/tbl/template.py__applicable_conditions
4819,/home/amandapotts/git/nltk/nltk/tbl/template.py_get_neighborhood,"def get_neighborhood(self, tokens, index):
neighborhood = {index}  # set literal for python 2.7+
allpositions = [0] + [p for feat in self._features for p in feat.positions]
start, end = min(allpositions), max(allpositions)
s = max(0, index + (-end))
e = min(index + (-start) + 1, len(tokens))
for i in range(s, e):
neighborhood.add(i)
return neighborhood
",[],0,[],/tbl/template.py_get_neighborhood
4820,/home/amandapotts/git/nltk/nltk/tbl/template.py_expand,"def expand(cls, featurelists, combinations=None, skipintersecting=True):
""""""
Factory method to mass generate Templates from a list L of lists of  Features.
The feature lists may have been specified
manually, or generated from Feature.expand(). For instance,
>>> from nltk.tbl.template import Template
>>> from nltk.tag.brill import Word, Pos
>>> (wd_0, wd_01) = (Word([0]), Word([0,1]))
>>> (pos_m2, pos_m33) = (Pos([-2]), Pos([3-2,-1,0,1,2,3]))
>>> list(Template.expand([[wd_0], [pos_m2]]))
[Template(Word([0])), Template(Pos([-2])), Template(Pos([-2]),Word([0]))]
>>> list(Template.expand([[wd_0, wd_01], [pos_m2]]))
[Template(Word([0])), Template(Word([0, 1])), Template(Pos([-2])), Template(Pos([-2]),Word([0])), Template(Pos([-2]),Word([0, 1]))]
>>> wordtpls = Word.expand([-2,-1,0,1], [1,2], excludezero=False)
>>> len(wordtpls)
7
>>> postpls = Pos.expand([-3,-2,-1,0,1,2], [1,2,3], excludezero=True)
>>> len(postpls)
9
>>> templates = list(Template.expand([wordtpls, wordtpls, postpls, postpls]))
>>> len(templates)
713
will return a list of eight templates
Template(Word([0])),
Template(Word([0, 1])),
Template(Pos([-2])),
Template(Pos([-1])),
Template(Pos([-2]),Word([0])),
Template(Pos([-1]),Word([0])),
Template(Pos([-2]),Word([0, 1])),
Template(Pos([-1]),Word([0, 1]))]
WARNING: this method makes it very easy to fill all your memory when training
generated templates on any real-world corpus
:param featurelists: lists of Features, whose Cartesian product will return a set of Templates
:type featurelists: list of (list of Features)
:param combinations: given n featurelists: if combinations=k, all generated Templates will have
k features
:type combinations: None, int, or (int, int)
:param skipintersecting: if True, do not output intersecting Templates (non-disjoint positions for some feature)
:type skipintersecting: bool
:returns: generator of Templates
""""""
",[],0,[],/tbl/template.py_expand
4821,/home/amandapotts/git/nltk/nltk/tbl/template.py_nonempty_powerset,"def nonempty_powerset(xs):  # xs is a list
k = combinations  # for brevity
combrange = (
(1, len(xs) + 1)
if k is None
else (k, k + 1)  # n over 1 .. n over n (all non-empty combinations)
if isinstance(k, int)
else (k[0], k[1] + 1)  # n over k (only
)  # n over k1, n over k1+1... n over k2
return it.chain.from_iterable(
it.combinations(xs, r) for r in range(*combrange)
)
",[],0,[],/tbl/template.py_nonempty_powerset
4822,/home/amandapotts/git/nltk/nltk/tbl/template.py__cleartemplates,"def _cleartemplates(cls):
cls.ALLTEMPLATES = []
",[],0,[],/tbl/template.py__cleartemplates
4823,/home/amandapotts/git/nltk/nltk/tbl/template.py__poptemplate,"def _poptemplate(cls):
return cls.ALLTEMPLATES.pop() if cls.ALLTEMPLATES else None
",[],0,[],/tbl/template.py__poptemplate
4824,/home/amandapotts/git/nltk/nltk/tbl/feature.py___init__,"def __init__(self, positions, end=None):
""""""
Construct a Feature which may apply at C{positions}.
>>> # For instance, importing some concrete subclasses (Feature is abstract)
>>> from nltk.tag.brill import Word, Pos
>>> # Feature Word, applying at one of [-2, -1]
>>> Word([-2,-1])
Word([-2, -1])
>>> # Positions need not be contiguous
>>> Word([-2,-1, 1])
Word([-2, -1, 1])
>>> # Contiguous ranges can alternatively be specified giving the
>>> # two endpoints (inclusive)
>>> Pos(-3, -1)
Pos([-3, -2, -1])
>>> # In two-arg form, start <= end is enforced
>>> Pos(2, 1)
Traceback (most recent call last):
File ""<stdin>"", line 1, in <module>
File ""nltk/tbl/template.py"", line 306, in __init__
raise TypeError
ValueError: illegal interval specification: (start=2, end=1)
:type positions: list of int
:param positions: the positions at which this features should apply
:raises ValueError: illegal position specifications
An alternative calling convention, for contiguous positions only,
is Feature(start, end):
:type start: int
:param start: start of range where this feature should apply
:type end: int
:param end: end of range (NOTE: inclusive!) where this feature should apply
""""""
self.positions = None  # to avoid warnings
if end is None:
self.positions = tuple(sorted({int(i) for i in positions}))
else:  # positions was actually not a list, but only the start index
try:
if positions > end:
raise TypeError
self.positions = tuple(range(positions, end + 1))
except TypeError as e:
raise ValueError(
""illegal interval specification: (start={}, end={})"".format(
positions, end
)
) from e
self.PROPERTY_NAME = self.__class__.PROPERTY_NAME or self.__class__.__name__
",[],0,[],/tbl/feature.py___init__
4825,/home/amandapotts/git/nltk/nltk/tbl/feature.py_encode_json_obj,"def encode_json_obj(self):
return self.positions
",[],0,[],/tbl/feature.py_encode_json_obj
4826,/home/amandapotts/git/nltk/nltk/tbl/feature.py_decode_json_obj,"def decode_json_obj(cls, obj):
positions = obj
return cls(positions)
",[],0,[],/tbl/feature.py_decode_json_obj
4827,/home/amandapotts/git/nltk/nltk/tbl/feature.py___repr__,"def __repr__(self):
return f""{self.__class__.__name__}({list(self.positions)!r})""
",[],0,[],/tbl/feature.py___repr__
4828,/home/amandapotts/git/nltk/nltk/tbl/feature.py_expand,"def expand(cls, starts, winlens, excludezero=False):
""""""
Return a list of features, one for each start point in starts
and for each window length in winlen. If excludezero is True,
no Features containing 0 in its positions will be generated
(many tbl trainers have a special representation for the
target feature at [0])
For instance, importing a concrete subclass (Feature is abstract)
>>> from nltk.tag.brill import Word
First argument gives the possible start positions, second the
possible window lengths
>>> Word.expand([-3,-2,-1], [1])
[Word([-3]), Word([-2]), Word([-1])]
>>> Word.expand([-2,-1], [1])
[Word([-2]), Word([-1])]
>>> Word.expand([-3,-2,-1], [1,2])
[Word([-3]), Word([-2]), Word([-1]), Word([-3, -2]), Word([-2, -1])]
>>> Word.expand([-2,-1], [1])
[Word([-2]), Word([-1])]
A third optional argument excludes all Features whose positions contain zero
>>> Word.expand([-2,-1,0], [1,2], excludezero=False)
[Word([-2]), Word([-1]), Word([0]), Word([-2, -1]), Word([-1, 0])]
>>> Word.expand([-2,-1,0], [1,2], excludezero=True)
[Word([-2]), Word([-1]), Word([-2, -1])]
All window lengths must be positive
>>> Word.expand([-2,-1], [0])
Traceback (most recent call last):
File ""<stdin>"", line 1, in <module>
File ""nltk/tag/tbl/template.py"", line 371, in expand
:param starts: where to start looking for Feature
ValueError: non-positive window length in [0]
:param starts: where to start looking for Feature
:type starts: list of ints
:param winlens: window lengths where to look for Feature
:type starts: list of ints
:param excludezero: do not output any Feature with 0 in any of its positions.
:type excludezero: bool
:returns: list of Features
:raises ValueError: for non-positive window lengths
""""""
if not all(x > 0 for x in winlens):
raise ValueError(f""non-positive window length in {winlens}"")
xs = (starts[i : i + w] for w in winlens for i in range(len(starts) - w + 1))
return [cls(x) for x in xs if not (excludezero and 0 in x)]
",[],0,[],/tbl/feature.py_expand
4829,/home/amandapotts/git/nltk/nltk/tbl/feature.py_issuperset,"def issuperset(self, other):
""""""
Return True if this Feature always returns True when other does
More precisely, return True if this feature refers to the same property as other
and this Feature looks at all positions that other does (and possibly
other positions in addition).
>>> from nltk.tag.brill import Word, Pos
>>> Word([-3,-2,-1]).issuperset(Word([-3,-2]))
True
>>> Word([-3,-2,-1]).issuperset(Word([-3,-2, 0]))
False
>>> Word([-3,-2,-1]).issuperset(Pos([-3,-2]))
False
:param other: feature with which to compare
:type other: (subclass of) Feature
:return: True if this feature is superset, otherwise False
:rtype: bool
""""""
return self.__class__ is other.__class__ and set(self.positions) >= set(
other.positions
)
",[],0,[],/tbl/feature.py_issuperset
4830,/home/amandapotts/git/nltk/nltk/tbl/feature.py_intersects,"def intersects(self, other):
""""""
Return True if the positions of this Feature intersects with those of other
More precisely, return True if this feature refers to the same property as other
and there is some overlap in the positions they look at.
>>> from nltk.tag.brill import Word, Pos
>>> Word([-3,-2,-1]).intersects(Word([-3,-2]))
True
>>> Word([-3,-2,-1]).intersects(Word([-3,-2, 0]))
True
>>> Word([-3,-2,-1]).intersects(Word([0]))
False
>>> Word([-3,-2,-1]).intersects(Pos([-3,-2]))
False
:param other: feature with which to compare
:type other: (subclass of) Feature
:return: True if feature classes agree and there is some overlap in the positions they look at
:rtype: bool
""""""
return bool(
self.__class__ is other.__class__
and set(self.positions) & set(other.positions)
)
",[],0,[],/tbl/feature.py_intersects
4831,/home/amandapotts/git/nltk/nltk/tbl/feature.py___eq__,"def __eq__(self, other):
return self.__class__ is other.__class__ and self.positions == other.positions
",[],0,[],/tbl/feature.py___eq__
4832,/home/amandapotts/git/nltk/nltk/tbl/feature.py___lt__,"def __lt__(self, other):
return (
self.__class__.__name__ < other.__class__.__name__
or
self.positions < other.positions
)
",[],0,[],/tbl/feature.py___lt__
4833,/home/amandapotts/git/nltk/nltk/tbl/feature.py___ne__,"def __ne__(self, other):
return not (self == other)
",[],0,[],/tbl/feature.py___ne__
4834,/home/amandapotts/git/nltk/nltk/tbl/feature.py___gt__,"def __gt__(self, other):
return other < self
",[],0,[],/tbl/feature.py___gt__
4835,/home/amandapotts/git/nltk/nltk/tbl/feature.py___ge__,"def __ge__(self, other):
return not self < other
",[],0,[],/tbl/feature.py___ge__
4836,/home/amandapotts/git/nltk/nltk/tbl/feature.py___le__,"def __le__(self, other):
return self < other or self == other
",[],0,[],/tbl/feature.py___le__
4837,/home/amandapotts/git/nltk/nltk/tbl/feature.py_extract_property,"def extract_property(tokens, index):
""""""
Any subclass of Feature must define static method extract_property(tokens, index)
:param tokens: the sequence of tokens
:type tokens: list of tokens
:param index: the current index
:type index: int
:return: feature value
:rtype: any (but usually scalar)
""""""
",[],0,[],/tbl/feature.py_extract_property
4838,/home/amandapotts/git/nltk/nltk/tbl/erroranalysis.py_error_list,"def error_list(train_sents, test_sents):
""""""
Returns a list of human-readable strings indicating the errors in the
given tagging of the corpus.
:param train_sents: The correct tagging of the corpus
:type train_sents: list(tuple)
:param test_sents: The tagged corpus
:type test_sents: list(tuple)
""""""
hdr = (""%25s | %s | %s\n"" + ""-"" * 26 + ""+"" + ""-"" * 24 + ""+"" + ""-"" * 26) % (
""left context"",
""word/test->gold"".center(22),
""right context"",
)
errors = [hdr]
for train_sent, test_sent in zip(train_sents, test_sents):
for wordnum, (word, train_pos) in enumerate(train_sent):
test_pos = test_sent[wordnum][1]
if train_pos != test_pos:
left = "" "".join(""%s/%s"" % w for w in train_sent[:wordnum])
right = "" "".join(""%s/%s"" % w for w in train_sent[wordnum + 1 :])
mid = f""{word}/{test_pos}->{train_pos}""
errors.append(f""{left[-25:]:>25} | {mid.center(22)} | {right[:25]}"")
return errors
",[],0,[],/tbl/erroranalysis.py_error_list
4839,/home/amandapotts/git/nltk/nltk/tbl/rule.py___init__,"def __init__(self, original_tag, replacement_tag):
self.original_tag = original_tag
""""""The tag which this TagRule may cause to be replaced.""""""
self.replacement_tag = replacement_tag
""""""The tag with which this TagRule may replace another tag.""""""
",[],0,[],/tbl/rule.py___init__
4840,/home/amandapotts/git/nltk/nltk/tbl/rule.py_apply,"def apply(self, tokens, positions=None):
""""""
Apply this rule at every position in positions where it
applies to the given sentence.  I.e., for each position p
in *positions*, if *tokens[p]* is tagged with this rule's
original tag, and satisfies this rule's condition, then set
its tag to be this rule's replacement tag.
:param tokens: The tagged sentence
:type tokens: list(tuple(str, str))
:type positions: list(int)
:param positions: The positions where the transformation is to
be tried.  If not specified, try it at all positions.
:return: The indices of tokens whose tags were changed by this
rule.
:rtype: int
""""""
if positions is None:
positions = list(range(len(tokens)))
change = [i for i in positions if self.applies(tokens, i)]
for i in change:
tokens[i] = (tokens[i][0], self.replacement_tag)
return change
",[],0,[],/tbl/rule.py_apply
4841,/home/amandapotts/git/nltk/nltk/tbl/rule.py_applies,"def applies(self, tokens, index):
""""""
:return: True if the rule would change the tag of
``tokens[index]``, False otherwise
:rtype: bool
:param tokens: A tagged sentence
:type tokens: list(str)
:param index: The index to check
:type index: int
""""""
",[],0,[],/tbl/rule.py_applies
4842,/home/amandapotts/git/nltk/nltk/tbl/rule.py___eq__,"def __eq__(self, other):
raise TypeError(""Rules must implement __eq__()"")
",[],0,[],/tbl/rule.py___eq__
4843,/home/amandapotts/git/nltk/nltk/tbl/rule.py___ne__,"def __ne__(self, other):
raise TypeError(""Rules must implement __ne__()"")
",[],0,[],/tbl/rule.py___ne__
4844,/home/amandapotts/git/nltk/nltk/tbl/rule.py___hash__,"def __hash__(self):
raise TypeError(""Rules must implement __hash__()"")
",[],0,[],/tbl/rule.py___hash__
4845,/home/amandapotts/git/nltk/nltk/tbl/rule.py___init__,"def __init__(self, templateid, original_tag, replacement_tag, conditions):
""""""
Construct a new Rule that changes a token's tag from
C{original_tag} to C{replacement_tag} if all of the properties
specified in C{conditions} hold.
:param templateid: the template id (a zero-padded string, '001' etc,
so it will sort nicely)
:type templateid: string
:param conditions: A list of Feature(positions),
each of which specifies that the property (computed by
Feature.extract_property()) of at least one
token in M{n} + p in positions is C{value}.
:type conditions: C{iterable} of C{Feature}
""""""
TagRule.__init__(self, original_tag, replacement_tag)
self._conditions = conditions
self.templateid = templateid
",[],0,[],/tbl/rule.py___init__
4846,/home/amandapotts/git/nltk/nltk/tbl/rule.py_encode_json_obj,"def encode_json_obj(self):
return {
""templateid"": self.templateid,
""original"": self.original_tag,
""replacement"": self.replacement_tag,
""conditions"": self._conditions,
}
",[],0,[],/tbl/rule.py_encode_json_obj
4847,/home/amandapotts/git/nltk/nltk/tbl/rule.py_decode_json_obj,"def decode_json_obj(cls, obj):
return cls(
obj[""templateid""],
obj[""original""],
obj[""replacement""],
tuple(tuple(feat) for feat in obj[""conditions""]),
)
",[],0,[],/tbl/rule.py_decode_json_obj
4848,/home/amandapotts/git/nltk/nltk/tbl/rule.py_applies,"def applies(self, tokens, index):
if tokens[index][1] != self.original_tag:
return False
for feature, val in self._conditions:
for pos in feature.positions:
if not (0 <= index + pos < len(tokens)):
continue
if feature.extract_property(tokens, index + pos) == val:
break
else:
return False
return True
",[],0,[],/tbl/rule.py_applies
4849,/home/amandapotts/git/nltk/nltk/tbl/rule.py___eq__,"def __eq__(self, other):
return self is other or (
other is not None
and other.__class__ == self.__class__
and self.original_tag == other.original_tag
and self.replacement_tag == other.replacement_tag
and self._conditions == other._conditions
)
",[],0,[],/tbl/rule.py___eq__
4850,/home/amandapotts/git/nltk/nltk/tbl/rule.py___ne__,"def __ne__(self, other):
return not (self == other)
",[],0,[],/tbl/rule.py___ne__
4851,/home/amandapotts/git/nltk/nltk/tbl/rule.py___hash__,"def __hash__(self):
try:
return self.__hash
except AttributeError:
self.__hash = hash(repr(self))
return self.__hash
",[],0,[],/tbl/rule.py___hash__
4852,/home/amandapotts/git/nltk/nltk/tbl/rule.py___repr__,"def __repr__(self):
try:
return self.__repr
except AttributeError:
self.__repr = ""{}('{}', {}, {}, [{}])"".format(
self.__class__.__name__,
self.templateid,
repr(self.original_tag),
repr(self.replacement_tag),
"", "".join(f""({f},{repr(v)})"" for (f, v) in self._conditions),
)
return self.__repr
",[],0,[],/tbl/rule.py___repr__
4853,/home/amandapotts/git/nltk/nltk/tbl/rule.py___str__,"def __str__(self):
",[],0,[],/tbl/rule.py___str__
4854,/home/amandapotts/git/nltk/nltk/tbl/rule.py__condition_to_logic,"def _condition_to_logic(feature, value):
""""""
Return a compact, predicate-logic styled string representation
of the given condition.
""""""
return ""{}:{}@[{}]"".format(
feature.PROPERTY_NAME,
value,
"","".join(str(w) for w in feature.positions),
)
",[],0,[],/tbl/rule.py__condition_to_logic
4855,/home/amandapotts/git/nltk/nltk/tbl/rule.py_format,"def format(self, fmt):
""""""
Return a string representation of this rule.
>>> from nltk.tbl.rule import Rule
>>> from nltk.tag.brill import Pos
>>> r = Rule(""23"", ""VB"", ""NN"", [(Pos([-2,-1]), 'DT')])
r.format(""str"") == str(r)
True
>>> r.format(""str"")
'VB->NN if Pos:DT@[-2,-1]'
r.format(""repr"") == repr(r)
True
>>> r.format(""repr"")
""Rule('23', 'VB', 'NN', [(Pos([-2, -1]),'DT')])""
>>> r.format(""verbose"")
'VB -> NN if the Pos of words i-2...i-1 is ""DT""'
>>> r.format(""not_found"")
Traceback (most recent call last):
File ""<stdin>"", line 1, in <module>
File ""nltk/tbl/rule.py"", line 256, in format
raise ValueError(""unknown rule format spec: {0}"".format(fmt))
ValueError: unknown rule format spec: not_found
>>>
:param fmt: format specification
:type fmt: str
:return: string representation
:rtype: str
""""""
if fmt == ""str"":
return self.__str__()
elif fmt == ""repr"":
return self.__repr__()
elif fmt == ""verbose"":
return self._verbose_format()
else:
raise ValueError(f""unknown rule format spec: {fmt}"")
",[],0,[],/tbl/rule.py_format
4856,/home/amandapotts/git/nltk/nltk/tbl/rule.py__verbose_format,"def _verbose_format(self):
""""""
Return a wordy, human-readable string representation
of the given rule.
Not sure how useful this is.
""""""
",[],0,[],/tbl/rule.py__verbose_format
4857,/home/amandapotts/git/nltk/nltk/tbl/rule.py_condition_to_str,"def condition_to_str(feature, value):
return 'the {} of {} is ""{}""'.format(
feature.PROPERTY_NAME,
range_to_str(feature.positions),
value,
)
",[],0,[],/tbl/rule.py_condition_to_str
4858,/home/amandapotts/git/nltk/nltk/tbl/rule.py_range_to_str,"def range_to_str(positions):
if len(positions) == 1:
p = positions[0]
if p == 0:
return ""this word""
if p == -1:
return ""the preceding word""
elif p == 1:
return ""the following word""
elif p < 0:
return ""word i-%d"" % -p
elif p > 0:
return ""word i+%d"" % p
else:
mx = max(positions)
mn = min(positions)
if mx - mn == len(positions) - 1:
return ""words i%+d...i%+d"" % (mn, mx)
else:
return ""words {{{}}}"".format(
"","".join(""i%+d"" % d for d in positions)
)
",[],0,[],/tbl/rule.py_range_to_str
4859,/home/amandapotts/git/nltk/nltk/tbl/demo.py_demo,"def demo():
""""""
Run a demo with defaults. See source comments for details,
or docstrings of any of the more specific demo_* functions.
""""""
postag()
",[],0,[],/tbl/demo.py_demo
4860,/home/amandapotts/git/nltk/nltk/tbl/demo.py_demo_repr_rule_format,"def demo_repr_rule_format():
""""""
Exemplify repr(Rule) (see also str(Rule) and Rule.format(""verbose""))
""""""
postag(ruleformat=""repr"")
",[],0,[],/tbl/demo.py_demo_repr_rule_format
4861,/home/amandapotts/git/nltk/nltk/tbl/demo.py_demo_str_rule_format,"def demo_str_rule_format():
""""""
Exemplify repr(Rule) (see also str(Rule) and Rule.format(""verbose""))
""""""
postag(ruleformat=""str"")
",[],0,[],/tbl/demo.py_demo_str_rule_format
4862,/home/amandapotts/git/nltk/nltk/tbl/demo.py_demo_verbose_rule_format,"def demo_verbose_rule_format():
""""""
Exemplify Rule.format(""verbose"")
""""""
postag(ruleformat=""verbose"")
",[],0,[],/tbl/demo.py_demo_verbose_rule_format
4863,/home/amandapotts/git/nltk/nltk/tbl/demo.py_demo_multiposition_feature,"def demo_multiposition_feature():
""""""
The feature/s of a template takes a list of positions
relative to the current word where the feature should be
looked for, conceptually joined by logical OR. For instance,
Pos([-1, 1]), given a value V, will hold whenever V is found
one step to the left and/or one step to the right.
For contiguous ranges, a 2-arg form giving inclusive end
points can also be used: Pos(-3, -1) is the same as the arg
below.
""""""
postag(templates=[Template(Pos([-3, -2, -1]))])
",[],0,[],/tbl/demo.py_demo_multiposition_feature
4864,/home/amandapotts/git/nltk/nltk/tbl/demo.py_demo_multifeature_template,"def demo_multifeature_template():
""""""
Templates can have more than a single feature.
""""""
postag(templates=[Template(Word([0]), Pos([-2, -1]))])
",[],0,[],/tbl/demo.py_demo_multifeature_template
4865,/home/amandapotts/git/nltk/nltk/tbl/demo.py_demo_template_statistics,"def demo_template_statistics():
""""""
Show aggregate statistics per template. Little used templates are
candidates for deletion, much used templates may possibly be refined.
Deleting unused templates is mostly about saving time and/or space:
training is basically O(T) in the number of templates T
(also in terms of memory usage, which often will be the limiting factor).
""""""
postag(incremental_stats=True, template_stats=True)
",[],0,[],/tbl/demo.py_demo_template_statistics
4866,/home/amandapotts/git/nltk/nltk/tbl/demo.py_demo_generated_templates,"def demo_generated_templates():
""""""
Template.expand and Feature.expand are class methods facilitating
generating large amounts of templates. See their documentation for
details.
Note: training with 500 templates can easily fill all available
even on relatively small corpora
""""""
wordtpls = Word.expand([-1, 0, 1], [1, 2], excludezero=False)
tagtpls = Pos.expand([-2, -1, 0, 1], [1, 2], excludezero=True)
templates = list(Template.expand([wordtpls, tagtpls], combinations=(1, 3)))
print(
""Generated {} templates for transformation-based learning"".format(
len(templates)
)
)
postag(templates=templates, incremental_stats=True, template_stats=True)
",[],0,[],/tbl/demo.py_demo_generated_templates
4867,/home/amandapotts/git/nltk/nltk/tbl/demo.py_demo_learning_curve,"def demo_learning_curve():
""""""
Plot a learning curve -- the contribution on tagging accuracy of
the individual rules.
Note: requires matplotlib
""""""
postag(
incremental_stats=True,
separate_baseline_data=True,
learning_curve_output=""learningcurve.png"",
)
",[],0,[],/tbl/demo.py_demo_learning_curve
4868,/home/amandapotts/git/nltk/nltk/tbl/demo.py_demo_error_analysis,"def demo_error_analysis():
""""""
Writes a file with context for each erroneous word after tagging testing data
""""""
postag(error_output=""errors.txt"")
",[],0,[],/tbl/demo.py_demo_error_analysis
4869,/home/amandapotts/git/nltk/nltk/tbl/demo.py_demo_serialize_tagger,"def demo_serialize_tagger():
""""""
Serializes the learned tagger to a file in pickle format
and validates the process.
""""""
postag(serialize_output=""tagger.pcl"")
",[],0,[],/tbl/demo.py_demo_serialize_tagger
4870,/home/amandapotts/git/nltk/nltk/tbl/demo.py_demo_high_accuracy_rules,"def demo_high_accuracy_rules():
""""""
Discard rules with low accuracy. This may hurt performance a bit,
but will often produce rules which are more interesting read to a human.
""""""
postag(num_sents=3000, min_acc=0.96, min_score=10)
",[],0,[],/tbl/demo.py_demo_high_accuracy_rules
4871,/home/amandapotts/git/nltk/nltk/tbl/demo.py_postag,"def postag(
templates=None,
tagged_data=None,
num_sents=1000,
max_rules=300,
min_score=3,
min_acc=None,
train=0.8,
trace=3,
randomize=False,
ruleformat=""str"",
incremental_stats=False,
template_stats=False,
error_output=None,
serialize_output=None,
learning_curve_output=None,
learning_curve_take=300,
baseline_backoff_tagger=None,
separate_baseline_data=False,
cache_baseline_tagger=None,
",[],0,[],/tbl/demo.py_postag
4872,/home/amandapotts/git/nltk/nltk/tbl/demo.py__demo_prepare_data,"def _demo_prepare_data(
tagged_data, train, num_sents, randomize, separate_baseline_data
",[],0,[],/tbl/demo.py__demo_prepare_data
4873,/home/amandapotts/git/nltk/nltk/tbl/demo.py__demo_plot,"def _demo_plot(learning_curve_output, teststats, trainstats=None, take=None):
testcurve = [teststats[""initialerrors""]]
for rulescore in teststats[""rulescores""]:
testcurve.append(testcurve[-1] - rulescore)
testcurve = [1 - x / teststats[""tokencount""] for x in testcurve[:take]]
traincurve = [trainstats[""initialerrors""]]
for rulescore in trainstats[""rulescores""]:
traincurve.append(traincurve[-1] - rulescore)
traincurve = [1 - x / trainstats[""tokencount""] for x in traincurve[:take]]
import matplotlib.pyplot as plt
r = list(range(len(testcurve)))
plt.plot(r, testcurve, r, traincurve)
plt.axis([None, None, None, 1.0])
plt.savefig(learning_curve_output)
",[],0,[],/tbl/demo.py__demo_plot
4874,/home/amandapotts/git/nltk/nltk/tbl/demo.py_corpus_size,"def corpus_size(seqs):
return (len(seqs), sum(len(x) for x in seqs))
",[],0,[],/tbl/demo.py_corpus_size
4875,/home/amandapotts/git/nltk/nltk/tree/prettyprinter.py___init__,"def __init__(self, tree, sentence=None, highlight=()):
if sentence is None:
leaves = tree.leaves()
if (
leaves
and all(len(a) > 0 for a in tree.subtrees())
and all(isinstance(a, int) for a in leaves)
):
sentence = [str(a) for a in leaves]
else:
tree = tree.copy(True)
sentence = []
for a in tree.subtrees():
if len(a) == 0:
a.append(len(sentence))
sentence.append(None)
elif any(not isinstance(b, Tree) for b in a):
for n, b in enumerate(a):
if not isinstance(b, Tree):
a[n] = len(sentence)
if type(b) == tuple:
b = ""/"".join(b)
sentence.append(""%s"" % b)
self.nodes, self.coords, self.edges, self.highlight = self.nodecoords(
tree, sentence, highlight
)
",[],0,[],/tree/prettyprinter.py___init__
4876,/home/amandapotts/git/nltk/nltk/tree/prettyprinter.py___str__,"def __str__(self):
return self.text()
",[],0,[],/tree/prettyprinter.py___str__
4877,/home/amandapotts/git/nltk/nltk/tree/prettyprinter.py___repr__,"def __repr__(self):
return ""<TreePrettyPrinter with %d nodes>"" % len(self.nodes)
",[],0,[],/tree/prettyprinter.py___repr__
4878,/home/amandapotts/git/nltk/nltk/tree/prettyprinter.py_nodecoords,"def nodecoords(tree, sentence, highlight):
""""""
Produce coordinates of nodes on a grid.
Objective:
- Produce coordinates for a non-overlapping placement of nodes and
horizontal lines.
- Order edges so that crossing edges cross a minimal number of previous
horizontal lines (never vertical lines).
Approach:
- bottom up level order traversal (start at terminals)
- at each level, identify nodes which cannot be on the same row
- identify nodes which cannot be in the same column
- place nodes into a grid at (row, column)
- order child-parent edges with crossing edges last
Coordinates are (row, column)
the root node is on row 0. Coordinates do not consider the size of a
node (which depends on font, &c), so the width of a column of the grid
should be automatically determined by the element with the greatest
width in that column. Alternatively, the integer coordinates could be
converted to coordinates in which the distances between adjacent nodes
are non-uniform.
Produces tuple (nodes, coords, edges, highlighted) where:
- nodes[id]: Tree object for the node with this integer id
- coords[id]: (n, m) coordinate where to draw node with id in the grid
- edges[id]: parent id of node with this id (ordered dictionary)
- highlighted: set of ids that should be highlighted
""""""
",[],0,[],/tree/prettyprinter.py_nodecoords
4879,/home/amandapotts/git/nltk/nltk/tree/prettyprinter.py_findcell,"def findcell(m, matrix, startoflevel, children):
""""""
Find vacant row, column index for node ``m``.
Iterate over current rows for this level (try lowest first)
and look for cell between first and last child of this node,
add new row to level if no free row available.
""""""
candidates = [a for _, a in children[m]]
minidx, maxidx = min(candidates), max(candidates)
leaves = tree[m].leaves()
center = scale * sum(leaves) // len(leaves)  # center of gravity
if minidx < maxidx and not minidx < center < maxidx:
center = sum(candidates) // len(candidates)
if max(candidates) - min(candidates) > 2 * scale:
center -= center % scale  # round to unscaled coordinate
if minidx < maxidx and not minidx < center < maxidx:
center += scale
if ids[m] == 0:
startoflevel = len(matrix)
for rowidx in range(startoflevel, len(matrix) + 1):
if rowidx == len(matrix):  # need to add a new row
matrix.append(
[
vertline if a not in (corner, None) else None
for a in matrix[-1]
]
)
row = matrix[rowidx]
if len(children[m]) == 1:  # place unaries directly above child
return rowidx, next(iter(children[m]))[1]
elif all(
a is None or a == vertline
for a in row[min(candidates) : max(candidates) + 1]
):
for n in range(scale):
i = j = center + n
while j > minidx or i < maxidx:
if i < maxidx and (
matrix[rowidx][i] is None or i in candidates
):
return rowidx, i
elif j > minidx and (
matrix[rowidx][j] is None or j in candidates
):
return rowidx, j
i += scale
j -= scale
raise ValueError(
""could not find a free cell for:\n%s\n%s""
""min=%d
)
",[],0,[],/tree/prettyprinter.py_findcell
4880,/home/amandapotts/git/nltk/nltk/tree/prettyprinter.py_text,"def text(
self,
nodedist=1,
unicodelines=False,
html=False,
ansi=False,
nodecolor=""blue"",
leafcolor=""red"",
funccolor=""green"",
abbreviate=None,
maxwidth=16,
",[],0,[],/tree/prettyprinter.py_text
4881,/home/amandapotts/git/nltk/nltk/tree/prettyprinter.py_svg,"def svg(self, nodecolor=""blue"", leafcolor=""red"", funccolor=""green""):
""""""
:return: SVG representation of a tree.
""""""
fontsize = 12
hscale = 40
vscale = 25
hstart = vstart = 20
width = max(col for _, col in self.coords.values())
height = max(row for row, _ in self.coords.values())
result = [
'<svg version=""1.1"" xmlns=""http://www.w3.org/2000/svg"" '
'width=""%dem"" height=""%dem"" viewBox=""%d %d %d %d"">'
% (
width * 3,
height * 2.5,
-hstart,
-vstart,
width * hscale + 3 * hstart,
height * vscale + 3 * vstart,
)
]
children = defaultdict(set)
for n in self.nodes:
if n:
children[self.edges[n]].add(n)
for node in self.nodes:
if not children[node]:
continue
y, x = self.coords[node]
x *= hscale
y *= vscale
x += hstart
y += vstart + fontsize // 2
childx = [self.coords[c][1] for c in children[node]]
xmin = hstart + hscale * min(childx)
xmax = hstart + hscale * max(childx)
result.append(
'\t<polyline style=""stroke:black
'points=""%g,%g %g,%g"" />' % (xmin, y, xmax, y)
)
result.append(
'\t<polyline style=""stroke:black
'points=""%g,%g %g,%g"" />' % (x, y, x, y - fontsize // 3)
)
for child, parent in self.edges.items():
y, _ = self.coords[parent]
y *= vscale
y += vstart + fontsize // 2
childy, childx = self.coords[child]
childx *= hscale
childy *= vscale
childx += hstart
childy += vstart - fontsize
result += [
'\t<polyline style=""stroke:white
' points=""%g,%g %g,%g"" />' % (childx, childy, childx, y + 5),
'\t<polyline style=""stroke:black
' points=""%g,%g %g,%g"" />' % (childx, childy, childx, y),
]
for n, (row, column) in self.coords.items():
node = self.nodes[n]
x = column * hscale + hstart
y = row * vscale + vstart
if n in self.highlight:
color = nodecolor if isinstance(node, Tree) else leafcolor
if isinstance(node, Tree) and node.label().startswith(""-""):
color = funccolor
else:
color = ""black""
result += [
'\t<text style=""text-anchor: middle
'font-size: %dpx
% (
color,
fontsize,
x,
y,
escape(
node.label() if isinstance(node, Tree) else node, quote=False
),
)
]
result += [""</svg>""]
return ""\n"".join(result)
",[],0,[],/tree/prettyprinter.py_svg
4882,/home/amandapotts/git/nltk/nltk/tree/prettyprinter.py_test,"def test():
""""""Do some tree drawing tests.""""""
",[],0,[],/tree/prettyprinter.py_test
4883,/home/amandapotts/git/nltk/nltk/tree/prettyprinter.py_print_tree,"def print_tree(n, tree, sentence=None, ansi=True, **xargs):
print()
print('{}: ""{}""'.format(n, "" "".join(sentence or tree.leaves())))
print(tree)
print()
drawtree = TreePrettyPrinter(tree, sentence)
try:
print(drawtree.text(unicodelines=ansi, ansi=ansi, **xargs))
except (UnicodeDecodeError, UnicodeEncodeError):
print(drawtree.text(unicodelines=False, ansi=False, **xargs))
",[],0,[],/tree/prettyprinter.py_print_tree
4884,/home/amandapotts/git/nltk/nltk/tree/parented.py___init__,"def __init__(self, node, children=None):
super().__init__(node, children)
if children is not None:
for i, child in enumerate(self):
if isinstance(child, Tree):
self._setparent(child, i, dry_run=True)
for i, child in enumerate(self):
if isinstance(child, Tree):
self._setparent(child, i)
",[],0,[],/tree/parented.py___init__
4885,/home/amandapotts/git/nltk/nltk/tree/parented.py__setparent,"def _setparent(self, child, index, dry_run=False):
""""""
Update the parent pointer of ``child`` to point to ``self``.  This
method is only called if the type of ``child`` is ``Tree``
i.e., it is not called when adding a leaf to a tree.  This method
is always called before the child is actually added to the
child list of ``self``.
:type child: Tree
:type index: int
:param index: The index of ``child`` in ``self``.
:raise TypeError: If ``child`` is a tree with an impropriate
type.  Typically, if ``child`` is a tree, then its type needs
to match the type of ``self``.  This prevents mixing of
different tree types (single-parented, multi-parented, and
non-parented).
:param dry_run: If true, the don't actually set the child's
parent pointer
raise an exception if one is found.
""""""
",[],0,[],/tree/parented.py__setparent
4886,/home/amandapotts/git/nltk/nltk/tree/parented.py__delparent,"def _delparent(self, child, index):
""""""
Update the parent pointer of ``child`` to not point to self.  This
method is only called if the type of ``child`` is ``Tree``
is not called when removing a leaf from a tree.  This method
is always called before the child is actually removed from the
child list of ``self``.
:type child: Tree
:type index: int
:param index: The index of ``child`` in ``self``.
""""""
",[],0,[],/tree/parented.py__delparent
4887,/home/amandapotts/git/nltk/nltk/tree/parented.py___delitem__,"def __delitem__(self, index):
if isinstance(index, slice):
start, stop, step = slice_bounds(self, index, allow_step=True)
for i in range(start, stop, step):
if isinstance(self[i], Tree):
self._delparent(self[i], i)
super().__delitem__(index)
elif isinstance(index, int):
if index < 0:
index += len(self)
if index < 0:
raise IndexError(""index out of range"")
if isinstance(self[index], Tree):
self._delparent(self[index], index)
super().__delitem__(index)
elif isinstance(index, (list, tuple)):
if len(index) == 0:
raise IndexError(""The tree position () may not be deleted."")
elif len(index) == 1:
del self[index[0]]
else:
del self[index[0]][index[1:]]
else:
raise TypeError(
""%s indices must be integers, not %s""
% (type(self).__name__, type(index).__name__)
)
",[],0,[],/tree/parented.py___delitem__
4888,/home/amandapotts/git/nltk/nltk/tree/parented.py___setitem__,"def __setitem__(self, index, value):
if isinstance(index, slice):
start, stop, step = slice_bounds(self, index, allow_step=True)
if not isinstance(value, (list, tuple)):
value = list(value)
for i, child in enumerate(value):
if isinstance(child, Tree):
self._setparent(child, start + i * step, dry_run=True)
for i in range(start, stop, step):
if isinstance(self[i], Tree):
self._delparent(self[i], i)
for i, child in enumerate(value):
if isinstance(child, Tree):
self._setparent(child, start + i * step)
super().__setitem__(index, value)
elif isinstance(index, int):
if index < 0:
index += len(self)
if index < 0:
raise IndexError(""index out of range"")
if value is self[index]:
return
if isinstance(value, Tree):
self._setparent(value, index)
if isinstance(self[index], Tree):
self._delparent(self[index], index)
super().__setitem__(index, value)
elif isinstance(index, (list, tuple)):
if len(index) == 0:
raise IndexError(""The tree position () may not be assigned to."")
elif len(index) == 1:
self[index[0]] = value
else:
self[index[0]][index[1:]] = value
else:
raise TypeError(
""%s indices must be integers, not %s""
% (type(self).__name__, type(index).__name__)
)
",[],0,[],/tree/parented.py___setitem__
4889,/home/amandapotts/git/nltk/nltk/tree/parented.py_append,"def append(self, child):
if isinstance(child, Tree):
self._setparent(child, len(self))
super().append(child)
",[],0,[],/tree/parented.py_append
4890,/home/amandapotts/git/nltk/nltk/tree/parented.py_extend,"def extend(self, children):
for child in children:
if isinstance(child, Tree):
self._setparent(child, len(self))
super().append(child)
",[],0,[],/tree/parented.py_extend
4891,/home/amandapotts/git/nltk/nltk/tree/parented.py_insert,"def insert(self, index, child):
if index < 0:
index += len(self)
if index < 0:
index = 0
if isinstance(child, Tree):
self._setparent(child, index)
super().insert(index, child)
",[],0,[],/tree/parented.py_insert
4892,/home/amandapotts/git/nltk/nltk/tree/parented.py_pop,"def pop(self, index=-1):
if index < 0:
index += len(self)
if index < 0:
raise IndexError(""index out of range"")
if isinstance(self[index], Tree):
self._delparent(self[index], index)
return super().pop(index)
",[],0,[],/tree/parented.py_pop
4893,/home/amandapotts/git/nltk/nltk/tree/parented.py_remove,"def remove(self, child):
index = self.index(child)
if isinstance(self[index], Tree):
self._delparent(self[index], index)
super().remove(child)
",[],0,[],/tree/parented.py_remove
4894,/home/amandapotts/git/nltk/nltk/tree/parented.py___getslice__,"def __getslice__(self, start, stop):
return self.__getitem__(slice(max(0, start), max(0, stop)))
",[],0,[],/tree/parented.py___getslice__
4895,/home/amandapotts/git/nltk/nltk/tree/parented.py___delslice__,"def __delslice__(self, start, stop):
return self.__delitem__(slice(max(0, start), max(0, stop)))
",[],0,[],/tree/parented.py___delslice__
4896,/home/amandapotts/git/nltk/nltk/tree/parented.py___setslice__,"def __setslice__(self, start, stop, value):
return self.__setitem__(slice(max(0, start), max(0, stop)), value)
",[],0,[],/tree/parented.py___setslice__
4897,/home/amandapotts/git/nltk/nltk/tree/parented.py___getnewargs__,"def __getnewargs__(self):
""""""Method used by the pickle module when un-pickling.
This method provides the arguments passed to ``__new__``
upon un-pickling. Without this method, ParentedTree instances
cannot be pickled and unpickled in Python 3.7+ onwards.
:return: Tuple of arguments for ``__new__``, i.e. the label
and the children of this node.
:rtype: Tuple[Any, List[AbstractParentedTree]]
""""""
return (self._label, list(self))
",[],0,[],/tree/parented.py___getnewargs__
4898,/home/amandapotts/git/nltk/nltk/tree/parented.py___init__,"def __init__(self, node, children=None):
self._parent = None
""""""The parent of this Tree, or None if it has no parent.""""""
super().__init__(node, children)
if children is None:
for i, child in enumerate(self):
if isinstance(child, Tree):
child._parent = None
self._setparent(child, i)
",[],0,[],/tree/parented.py___init__
4899,/home/amandapotts/git/nltk/nltk/tree/parented.py__frozen_class,"def _frozen_class(self):
from nltk.tree.immutable import ImmutableParentedTree
return ImmutableParentedTree
",[],0,[],/tree/parented.py__frozen_class
4900,/home/amandapotts/git/nltk/nltk/tree/parented.py_copy,"def copy(self, deep=False):
if not deep:
warnings.warn(
f""{self.__class__.__name__} objects do not support shallow copies. Defaulting to a deep copy.""
)
return super().copy(deep=True)
",[],0,[],/tree/parented.py_copy
4901,/home/amandapotts/git/nltk/nltk/tree/parented.py_parent,"def parent(self):
""""""The parent of this tree, or None if it has no parent.""""""
return self._parent
",[],0,[],/tree/parented.py_parent
4902,/home/amandapotts/git/nltk/nltk/tree/parented.py_parent_index,"def parent_index(self):
""""""
The index of this tree in its parent.  I.e.,
``ptree.parent()[ptree.parent_index()] is ptree``.  Note that
``ptree.parent_index()`` is not necessarily equal to
``ptree.parent.index(ptree)``, since the ``index()`` method
returns the first child that is equal to its argument.
""""""
if self._parent is None:
return None
for i, child in enumerate(self._parent):
if child is self:
return i
assert False, ""expected to find self in self._parent!""
",[],0,[],/tree/parented.py_parent_index
4903,/home/amandapotts/git/nltk/nltk/tree/parented.py_left_sibling,"def left_sibling(self):
""""""The left sibling of this tree, or None if it has none.""""""
parent_index = self.parent_index()
if self._parent and parent_index > 0:
return self._parent[parent_index - 1]
return None  # no left sibling
",[],0,[],/tree/parented.py_left_sibling
4904,/home/amandapotts/git/nltk/nltk/tree/parented.py_right_sibling,"def right_sibling(self):
""""""The right sibling of this tree, or None if it has none.""""""
parent_index = self.parent_index()
if self._parent and parent_index < (len(self._parent) - 1):
return self._parent[parent_index + 1]
return None  # no right sibling
",[],0,[],/tree/parented.py_right_sibling
4905,/home/amandapotts/git/nltk/nltk/tree/parented.py_root,"def root(self):
""""""
The root of this tree.  I.e., the unique ancestor of this tree
whose parent is None.  If ``ptree.parent()`` is None, then
``ptree`` is its own root.
""""""
root = self
while root.parent() is not None:
root = root.parent()
return root
",[],0,[],/tree/parented.py_root
4906,/home/amandapotts/git/nltk/nltk/tree/parented.py_treeposition,"def treeposition(self):
""""""
The tree position of this tree, relative to the root of the
tree.  I.e., ``ptree.root[ptree.treeposition] is ptree``.
""""""
if self.parent() is None:
return ()
else:
return self.parent().treeposition() + (self.parent_index(),)
",[],0,[],/tree/parented.py_treeposition
4907,/home/amandapotts/git/nltk/nltk/tree/parented.py__delparent,"def _delparent(self, child, index):
assert isinstance(child, ParentedTree)
assert self[index] is child
assert child._parent is self
child._parent = None
",[],0,[],/tree/parented.py__delparent
4908,/home/amandapotts/git/nltk/nltk/tree/parented.py__setparent,"def _setparent(self, child, index, dry_run=False):
if not isinstance(child, ParentedTree):
raise TypeError(""Can not insert a non-ParentedTree into a ParentedTree"")
if hasattr(child, ""_parent"") and child._parent is not None:
raise ValueError(""Can not insert a subtree that already has a parent."")
if not dry_run:
child._parent = self
",[],0,[],/tree/parented.py__setparent
4909,/home/amandapotts/git/nltk/nltk/tree/parented.py___init__,"def __init__(self, node, children=None):
self._parents = []
""""""A list of this tree's parents.  This list should not
contain duplicates, even if a parent contains this tree
multiple times.""""""
super().__init__(node, children)
if children is None:
for i, child in enumerate(self):
if isinstance(child, Tree):
child._parents = []
self._setparent(child, i)
",[],0,[],/tree/parented.py___init__
4910,/home/amandapotts/git/nltk/nltk/tree/parented.py__frozen_class,"def _frozen_class(self):
from nltk.tree.immutable import ImmutableMultiParentedTree
return ImmutableMultiParentedTree
",[],0,[],/tree/parented.py__frozen_class
4911,/home/amandapotts/git/nltk/nltk/tree/parented.py_parents,"def parents(self):
""""""
The set of parents of this tree.  If this tree has no parents,
then ``parents`` is the empty set.  To check if a tree is used
as multiple children of the same parent, use the
``parent_indices()`` method.
:type: list(MultiParentedTree)
""""""
return list(self._parents)
",[],0,[],/tree/parented.py_parents
4912,/home/amandapotts/git/nltk/nltk/tree/parented.py_left_siblings,"def left_siblings(self):
""""""
A list of all left siblings of this tree, in any of its parent
trees.  A tree may be its own left sibling if it is used as
multiple contiguous children of the same parent.  A tree may
appear multiple times in this list if it is the left sibling
of this tree with respect to multiple parents.
:type: list(MultiParentedTree)
""""""
return [
parent[index - 1]
for (parent, index) in self._get_parent_indices()
if index > 0
]
",[],0,[],/tree/parented.py_left_siblings
4913,/home/amandapotts/git/nltk/nltk/tree/parented.py_right_siblings,"def right_siblings(self):
""""""
A list of all right siblings of this tree, in any of its parent
trees.  A tree may be its own right sibling if it is used as
multiple contiguous children of the same parent.  A tree may
appear multiple times in this list if it is the right sibling
of this tree with respect to multiple parents.
:type: list(MultiParentedTree)
""""""
return [
parent[index + 1]
for (parent, index) in self._get_parent_indices()
if index < (len(parent) - 1)
]
",[],0,[],/tree/parented.py_right_siblings
4914,/home/amandapotts/git/nltk/nltk/tree/parented.py__get_parent_indices,"def _get_parent_indices(self):
return [
(parent, index)
for parent in self._parents
for index, child in enumerate(parent)
if child is self
]
",[],0,[],/tree/parented.py__get_parent_indices
4915,/home/amandapotts/git/nltk/nltk/tree/parented.py_roots,"def roots(self):
""""""
The set of all roots of this tree.  This set is formed by
tracing all possible parent paths until trees with no parents
are found.
:type: list(MultiParentedTree)
""""""
return list(self._get_roots_helper({}).values())
",[],0,[],/tree/parented.py_roots
4916,/home/amandapotts/git/nltk/nltk/tree/parented.py__get_roots_helper,"def _get_roots_helper(self, result):
if self._parents:
for parent in self._parents:
parent._get_roots_helper(result)
else:
result[id(self)] = self
return result
",[],0,[],/tree/parented.py__get_roots_helper
4917,/home/amandapotts/git/nltk/nltk/tree/parented.py_parent_indices,"def parent_indices(self, parent):
""""""
Return a list of the indices where this tree occurs as a child
of ``parent``.  If this child does not occur as a child of
``parent``, then the empty list is returned.  The following is
always true::
for parent_index in ptree.parent_indices(parent):
parent[parent_index] is ptree
""""""
if parent not in self._parents:
return []
else:
return [index for (index, child) in enumerate(parent) if child is self]
",[],0,[],/tree/parented.py_parent_indices
4918,/home/amandapotts/git/nltk/nltk/tree/parented.py_treepositions,"def treepositions(self, root):
""""""
Return a list of all tree positions that can be used to reach
this multi-parented tree starting from ``root``.  I.e., the
following is always true::
for treepos in ptree.treepositions(root):
root[treepos] is ptree
""""""
if self is root:
return [()]
else:
return [
treepos + (index,)
for parent in self._parents
for treepos in parent.treepositions(root)
for (index, child) in enumerate(parent)
if child is self
]
",[],0,[],/tree/parented.py_treepositions
4919,/home/amandapotts/git/nltk/nltk/tree/parented.py__delparent,"def _delparent(self, child, index):
assert isinstance(child, MultiParentedTree)
assert self[index] is child
assert len([p for p in child._parents if p is self]) == 1
for i, c in enumerate(self):
if c is child and i != index:
break
else:
child._parents.remove(self)
",[],0,[],/tree/parented.py__delparent
4920,/home/amandapotts/git/nltk/nltk/tree/parented.py__setparent,"def _setparent(self, child, index, dry_run=False):
if not isinstance(child, MultiParentedTree):
raise TypeError(
""Can not insert a non-MultiParentedTree into a MultiParentedTree""
)
if not dry_run:
for parent in child._parents:
if parent is self:
break
else:
child._parents.append(self)
",[],0,[],/tree/parented.py__setparent
4921,/home/amandapotts/git/nltk/nltk/tree/immutable.py___init__,"def __init__(self, node, children=None):
super().__init__(node, children)
try:
self._hash = hash((self._label, tuple(self)))
except (TypeError, ValueError) as e:
raise ValueError(
""%s: node value and children "" ""must be immutable"" % type(self).__name__
) from e
",[],0,[],/tree/immutable.py___init__
4922,/home/amandapotts/git/nltk/nltk/tree/immutable.py___setitem__,"def __setitem__(self, index, value):
raise ValueError(""%s may not be modified"" % type(self).__name__)
",[],0,[],/tree/immutable.py___setitem__
4923,/home/amandapotts/git/nltk/nltk/tree/immutable.py___setslice__,"def __setslice__(self, i, j, value):
raise ValueError(""%s may not be modified"" % type(self).__name__)
",[],0,[],/tree/immutable.py___setslice__
4924,/home/amandapotts/git/nltk/nltk/tree/immutable.py___delitem__,"def __delitem__(self, index):
raise ValueError(""%s may not be modified"" % type(self).__name__)
",[],0,[],/tree/immutable.py___delitem__
4925,/home/amandapotts/git/nltk/nltk/tree/immutable.py___delslice__,"def __delslice__(self, i, j):
raise ValueError(""%s may not be modified"" % type(self).__name__)
",[],0,[],/tree/immutable.py___delslice__
4926,/home/amandapotts/git/nltk/nltk/tree/immutable.py___iadd__,"def __iadd__(self, other):
raise ValueError(""%s may not be modified"" % type(self).__name__)
",[],0,[],/tree/immutable.py___iadd__
4927,/home/amandapotts/git/nltk/nltk/tree/immutable.py___imul__,"def __imul__(self, other):
raise ValueError(""%s may not be modified"" % type(self).__name__)
",[],0,[],/tree/immutable.py___imul__
4928,/home/amandapotts/git/nltk/nltk/tree/immutable.py_append,"def append(self, v):
raise ValueError(""%s may not be modified"" % type(self).__name__)
",[],0,[],/tree/immutable.py_append
4929,/home/amandapotts/git/nltk/nltk/tree/immutable.py_extend,"def extend(self, v):
raise ValueError(""%s may not be modified"" % type(self).__name__)
",[],0,[],/tree/immutable.py_extend
4930,/home/amandapotts/git/nltk/nltk/tree/immutable.py_pop,"def pop(self, v=None):
raise ValueError(""%s may not be modified"" % type(self).__name__)
",[],0,[],/tree/immutable.py_pop
4931,/home/amandapotts/git/nltk/nltk/tree/immutable.py_remove,"def remove(self, v):
raise ValueError(""%s may not be modified"" % type(self).__name__)
",[],0,[],/tree/immutable.py_remove
4932,/home/amandapotts/git/nltk/nltk/tree/immutable.py_reverse,"def reverse(self):
raise ValueError(""%s may not be modified"" % type(self).__name__)
",[],0,[],/tree/immutable.py_reverse
4933,/home/amandapotts/git/nltk/nltk/tree/immutable.py_sort,"def sort(self):
raise ValueError(""%s may not be modified"" % type(self).__name__)
",[],0,[],/tree/immutable.py_sort
4934,/home/amandapotts/git/nltk/nltk/tree/immutable.py___hash__,"def __hash__(self):
return self._hash
",[],0,[],/tree/immutable.py___hash__
4935,/home/amandapotts/git/nltk/nltk/tree/immutable.py_set_label,"def set_label(self, value):
""""""
Set the node label.  This will only succeed the first time the
node label is set, which should occur in ImmutableTree.__init__().
""""""
if hasattr(self, ""_label""):
raise ValueError(""%s may not be modified"" % type(self).__name__)
self._label = value
",[],0,[],/tree/immutable.py_set_label
4936,/home/amandapotts/git/nltk/nltk/tree/immutable.py___init__,"def __init__(self, node, children=None, **prob_kwargs):
ImmutableTree.__init__(self, node, children)
ProbabilisticMixIn.__init__(self, **prob_kwargs)
self._hash = hash((self._label, tuple(self), self.prob()))
",[],0,[],/tree/immutable.py___init__
4937,/home/amandapotts/git/nltk/nltk/tree/immutable.py__frozen_class,"def _frozen_class(self):
return ImmutableProbabilisticTree
",[],0,[],/tree/immutable.py__frozen_class
4938,/home/amandapotts/git/nltk/nltk/tree/immutable.py___repr__,"def __repr__(self):
return f""{Tree.__repr__(self)} [{self.prob()}]""
",[],0,[],/tree/immutable.py___repr__
4939,/home/amandapotts/git/nltk/nltk/tree/immutable.py___str__,"def __str__(self):
return f""{self.pformat(margin=60)} [{self.prob()}]""
",[],0,[],/tree/immutable.py___str__
4940,/home/amandapotts/git/nltk/nltk/tree/immutable.py_copy,"def copy(self, deep=False):
if not deep:
return type(self)(self._label, self, prob=self.prob())
else:
return type(self).convert(self)
",[],0,[],/tree/immutable.py_copy
4941,/home/amandapotts/git/nltk/nltk/tree/immutable.py_convert,"def convert(cls, val):
if isinstance(val, Tree):
children = [cls.convert(child) for child in val]
if isinstance(val, ProbabilisticMixIn):
return cls(val._label, children, prob=val.prob())
else:
return cls(val._label, children, prob=1.0)
else:
return val
",[],0,[],/tree/immutable.py_convert
4942,/home/amandapotts/git/nltk/nltk/tree/tree.py___init__,"def __init__(self, node, children=None):
if children is None:
raise TypeError(
""%s: Expected a node value and child list "" % type(self).__name__
)
elif isinstance(children, str):
raise TypeError(
""%s() argument 2 should be a list, not a ""
""string"" % type(self).__name__
)
else:
list.__init__(self, children)
self._label = node
",[],0,[],/tree/tree.py___init__
4943,/home/amandapotts/git/nltk/nltk/tree/tree.py___eq__,"def __eq__(self, other):
return self.__class__ is other.__class__ and (self._label, list(self)) == (
other._label,
list(other),
)
",[],0,[],/tree/tree.py___eq__
4944,/home/amandapotts/git/nltk/nltk/tree/tree.py___lt__,"def __lt__(self, other):
if not isinstance(other, Tree):
return self.__class__.__name__ < other.__class__.__name__
elif self.__class__ is other.__class__:
return (self._label, list(self)) < (other._label, list(other))
else:
return self.__class__.__name__ < other.__class__.__name__
",[],0,[],/tree/tree.py___lt__
4945,/home/amandapotts/git/nltk/nltk/tree/tree.py___mul__,"def __mul__(self, v):
raise TypeError(""Tree does not support multiplication"")
",[],0,[],/tree/tree.py___mul__
4946,/home/amandapotts/git/nltk/nltk/tree/tree.py___rmul__,"def __rmul__(self, v):
raise TypeError(""Tree does not support multiplication"")
",[],0,[],/tree/tree.py___rmul__
4947,/home/amandapotts/git/nltk/nltk/tree/tree.py___add__,"def __add__(self, v):
raise TypeError(""Tree does not support addition"")
",[],0,[],/tree/tree.py___add__
4948,/home/amandapotts/git/nltk/nltk/tree/tree.py___radd__,"def __radd__(self, v):
raise TypeError(""Tree does not support addition"")
",[],0,[],/tree/tree.py___radd__
4949,/home/amandapotts/git/nltk/nltk/tree/tree.py___getitem__,"def __getitem__(self, index):
if isinstance(index, (int, slice)):
return list.__getitem__(self, index)
elif isinstance(index, (list, tuple)):
if len(index) == 0:
return self
elif len(index) == 1:
return self[index[0]]
else:
return self[index[0]][index[1:]]
else:
raise TypeError(
""%s indices must be integers, not %s""
% (type(self).__name__, type(index).__name__)
)
",[],0,[],/tree/tree.py___getitem__
4950,/home/amandapotts/git/nltk/nltk/tree/tree.py___setitem__,"def __setitem__(self, index, value):
if isinstance(index, (int, slice)):
return list.__setitem__(self, index, value)
elif isinstance(index, (list, tuple)):
if len(index) == 0:
raise IndexError(""The tree position () may not be "" ""assigned to."")
elif len(index) == 1:
self[index[0]] = value
else:
self[index[0]][index[1:]] = value
else:
raise TypeError(
""%s indices must be integers, not %s""
% (type(self).__name__, type(index).__name__)
)
",[],0,[],/tree/tree.py___setitem__
4951,/home/amandapotts/git/nltk/nltk/tree/tree.py___delitem__,"def __delitem__(self, index):
if isinstance(index, (int, slice)):
return list.__delitem__(self, index)
elif isinstance(index, (list, tuple)):
if len(index) == 0:
raise IndexError(""The tree position () may not be deleted."")
elif len(index) == 1:
del self[index[0]]
else:
del self[index[0]][index[1:]]
else:
raise TypeError(
""%s indices must be integers, not %s""
% (type(self).__name__, type(index).__name__)
)
",[],0,[],/tree/tree.py___delitem__
4952,/home/amandapotts/git/nltk/nltk/tree/tree.py__get_node,"def _get_node(self):
""""""Outdated method to access the node value
",[],0,[],/tree/tree.py__get_node
4953,/home/amandapotts/git/nltk/nltk/tree/tree.py__set_node,"def _set_node(self, value):
""""""Outdated method to set the node value
",[],0,[],/tree/tree.py__set_node
4954,/home/amandapotts/git/nltk/nltk/tree/tree.py_label,"def label(self):
""""""
Return the node label of the tree.
>>> t = Tree.fromstring('(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))')
>>> t.label()
'S'
:return: the node label (typically a string)
:rtype: any
""""""
return self._label
",[],0,[],/tree/tree.py_label
4955,/home/amandapotts/git/nltk/nltk/tree/tree.py_set_label,"def set_label(self, label):
""""""
Set the node label of the tree.
>>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.set_label(""T"")
>>> print(t)
(T (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))
:param label: the node label (typically a string)
:type label: any
""""""
self._label = label
",[],0,[],/tree/tree.py_set_label
4956,/home/amandapotts/git/nltk/nltk/tree/tree.py_leaves,"def leaves(self):
""""""
Return the leaves of the tree.
>>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.leaves()
['the', 'dog', 'chased', 'the', 'cat']
:return: a list containing this tree's leaves.
The order reflects the order of the
leaves in the tree's hierarchical structure.
:rtype: list
""""""
leaves = []
for child in self:
if isinstance(child, Tree):
leaves.extend(child.leaves())
else:
leaves.append(child)
return leaves
",[],0,[],/tree/tree.py_leaves
4957,/home/amandapotts/git/nltk/nltk/tree/tree.py_flatten,"def flatten(self):
""""""
Return a flat version of the tree, with all non-root non-terminals removed.
>>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> print(t.flatten())
(S the dog chased the cat)
:return: a tree consisting of this tree's root connected directly to
its leaves, omitting all intervening non-terminal nodes.
:rtype: Tree
""""""
return Tree(self.label(), self.leaves())
",[],0,[],/tree/tree.py_flatten
4958,/home/amandapotts/git/nltk/nltk/tree/tree.py_height,"def height(self):
""""""
Return the height of the tree.
>>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.height()
5
>>> print(t[0,0])
(D the)
>>> t[0,0].height()
2
:return: The height of this tree.  The height of a tree
containing no children is 1
containing only leaves is 2
tree is one plus the maximum of its children's
heights.
:rtype: int
""""""
max_child_height = 0
for child in self:
if isinstance(child, Tree):
max_child_height = max(max_child_height, child.height())
else:
max_child_height = max(max_child_height, 1)
return 1 + max_child_height
",[],0,[],/tree/tree.py_height
4959,/home/amandapotts/git/nltk/nltk/tree/tree.py_treepositions,"def treepositions(self, order=""preorder""):
""""""
>>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.treepositions() # doctest: +ELLIPSIS
[(), (0,), (0, 0), (0, 0, 0), (0, 1), (0, 1, 0), (1,), (1, 0), (1, 0, 0), ...]
>>> for pos in t.treepositions('leaves'):
...     t[pos] = t[pos][::-1].upper()
>>> print(t)
(S (NP (D EHT) (N GOD)) (VP (V DESAHC) (NP (D EHT) (N TAC))))
:param order: One of: ``preorder``, ``postorder``, ``bothorder``,
``leaves``.
""""""
positions = []
if order in (""preorder"", ""bothorder""):
positions.append(())
for i, child in enumerate(self):
if isinstance(child, Tree):
childpos = child.treepositions(order)
positions.extend((i,) + p for p in childpos)
else:
positions.append((i,))
if order in (""postorder"", ""bothorder""):
positions.append(())
return positions
",[],0,[],/tree/tree.py_treepositions
4960,/home/amandapotts/git/nltk/nltk/tree/tree.py_productions,"def productions(self):
""""""
Generate the productions that correspond to the non-terminal nodes of the tree.
For each subtree of the form (P: C1 C2 ... Cn) this produces a production of the
form P -> C1 C2 ... Cn.
>>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.productions() # doctest: +NORMALIZE_WHITESPACE
[S -> NP VP, NP -> D N, D -> 'the', N -> 'dog', VP -> V NP, V -> 'chased',
NP -> D N, D -> 'the', N -> 'cat']
:rtype: list(Production)
""""""
if not isinstance(self._label, str):
raise TypeError(
""Productions can only be generated from trees having node labels that are strings""
)
prods = [Production(Nonterminal(self._label), _child_names(self))]
for child in self:
if isinstance(child, Tree):
prods += child.productions()
return prods
",[],0,[],/tree/tree.py_productions
4961,/home/amandapotts/git/nltk/nltk/tree/tree.py_pos,"def pos(self):
""""""
Return a sequence of pos-tagged words extracted from the tree.
>>> t = Tree.fromstring(""(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))"")
>>> t.pos()
[('the', 'D'), ('dog', 'N'), ('chased', 'V'), ('the', 'D'), ('cat', 'N')]
:return: a list of tuples containing leaves and pre-terminals (part-of-speech tags).
The order reflects the order of the leaves in the tree's hierarchical structure.
:rtype: list(tuple)
""""""
pos = []
for child in self:
if isinstance(child, Tree):
pos.extend(child.pos())
else:
pos.append((child, self._label))
return pos
",[],0,[],/tree/tree.py_pos
4962,/home/amandapotts/git/nltk/nltk/tree/tree.py_leaf_treeposition,"def leaf_treeposition(self, index):
""""""
:return: The tree position of the ``index``-th leaf in this
tree.  I.e., if ``tp=self.leaf_treeposition(i)``, then
``self[tp]==self.leaves()[i]``.
:raise IndexError: If this tree contains fewer than ``index+1``
leaves, or if ``index<0``.
""""""
if index < 0:
raise IndexError(""index must be non-negative"")
stack = [(self, ())]
while stack:
value, treepos = stack.pop()
if not isinstance(value, Tree):
if index == 0:
return treepos
else:
index -= 1
else:
for i in range(len(value) - 1, -1, -1):
stack.append((value[i], treepos + (i,)))
raise IndexError(""index must be less than or equal to len(self)"")
",[],0,[],/tree/tree.py_leaf_treeposition
4963,/home/amandapotts/git/nltk/nltk/tree/tree.py_treeposition_spanning_leaves,"def treeposition_spanning_leaves(self, start, end):
""""""
:return: The tree position of the lowest descendant of this
tree that dominates ``self.leaves()[start:end]``.
:raise ValueError: if ``end <= start``
""""""
if end <= start:
raise ValueError(""end must be greater than start"")
start_treepos = self.leaf_treeposition(start)
end_treepos = self.leaf_treeposition(end - 1)
for i in range(len(start_treepos)):
if i == len(end_treepos) or start_treepos[i] != end_treepos[i]:
return start_treepos[:i]
return start_treepos
",[],0,[],/tree/tree.py_treeposition_spanning_leaves
4964,/home/amandapotts/git/nltk/nltk/tree/tree.py_chomsky_normal_form,"def chomsky_normal_form(
self,
factor=""right"",
horzMarkov=None,
vertMarkov=0,
childChar=""|"",
parentChar=""^"",
",[],0,[],/tree/tree.py_chomsky_normal_form
4965,/home/amandapotts/git/nltk/nltk/tree/tree.py_un_chomsky_normal_form,"def un_chomsky_normal_form(
self, expandUnary=True, childChar=""|"", parentChar=""^"", unaryChar=""+""
",[],0,[],/tree/tree.py_un_chomsky_normal_form
4966,/home/amandapotts/git/nltk/nltk/tree/tree.py_collapse_unary,"def collapse_unary(self, collapsePOS=False, collapseRoot=False, joinChar=""+""):
""""""
Collapse subtrees with a single child (ie. unary productions)
into a new non-terminal (Tree node) joined by 'joinChar'.
This is useful when working with algorithms that do not allow
unary productions, and completely removing the unary productions
would require loss of useful information.  The Tree is modified
directly (since it is passed by reference) and no value is returned.
:param collapsePOS: 'False' (default) will not collapse the parent of leaf nodes (ie.
Part-of-Speech tags) since they are always unary productions
:type  collapsePOS: bool
:param collapseRoot: 'False' (default) will not modify the root production
if it is unary.  For the Penn WSJ treebank corpus, this corresponds
to the TOP -> productions.
:type collapseRoot: bool
:param joinChar: A string used to connect collapsed node values (default = ""+"")
:type  joinChar: str
""""""
from nltk.tree.transforms import collapse_unary
collapse_unary(self, collapsePOS, collapseRoot, joinChar)
",[],0,[],/tree/tree.py_collapse_unary
4967,/home/amandapotts/git/nltk/nltk/tree/tree.py_convert,"def convert(cls, tree):
""""""
Convert a tree between different subtypes of Tree.  ``cls`` determines
which class will be used to encode the new tree.
:type tree: Tree
:param tree: The tree that should be converted.
:return: The new Tree.
""""""
if isinstance(tree, Tree):
children = [cls.convert(child) for child in tree]
return cls(tree._label, children)
else:
return tree
",[],0,[],/tree/tree.py_convert
4968,/home/amandapotts/git/nltk/nltk/tree/tree.py___copy__,"def __copy__(self):
return self.copy()
",[],0,[],/tree/tree.py___copy__
4969,/home/amandapotts/git/nltk/nltk/tree/tree.py___deepcopy__,"def __deepcopy__(self, memo):
return self.copy(deep=True)
",[],0,[],/tree/tree.py___deepcopy__
4970,/home/amandapotts/git/nltk/nltk/tree/tree.py_copy,"def copy(self, deep=False):
if not deep:
return type(self)(self._label, self)
else:
return type(self).convert(self)
",[],0,[],/tree/tree.py_copy
4971,/home/amandapotts/git/nltk/nltk/tree/tree.py__frozen_class,"def _frozen_class(self):
from nltk.tree.immutable import ImmutableTree
return ImmutableTree
",[],0,[],/tree/tree.py__frozen_class
4972,/home/amandapotts/git/nltk/nltk/tree/tree.py_freeze,"def freeze(self, leaf_freezer=None):
frozen_class = self._frozen_class()
if leaf_freezer is None:
newcopy = frozen_class.convert(self)
else:
newcopy = self.copy(deep=True)
for pos in newcopy.treepositions(""leaves""):
newcopy[pos] = leaf_freezer(newcopy[pos])
newcopy = frozen_class.convert(newcopy)
hash(newcopy)  # Make sure the leaves are hashable.
return newcopy
",[],0,[],/tree/tree.py_freeze
4973,/home/amandapotts/git/nltk/nltk/tree/tree.py_fromstring,"def fromstring(
cls,
s,
brackets=""()"",
read_node=None,
read_leaf=None,
node_pattern=None,
leaf_pattern=None,
remove_empty_top_bracketing=False,
",[],0,[],/tree/tree.py_fromstring
4974,/home/amandapotts/git/nltk/nltk/tree/tree.py__parse_error,"def _parse_error(cls, s, match, expecting):
""""""
Display a friendly error message when parsing a tree string fails.
:param s: The string we're parsing.
:param match: regexp match of the problem token.
:param expecting: what we expected to see instead.
""""""
if match == ""end-of-string"":
pos, token = len(s), ""end-of-string""
else:
pos, token = match.start(), match.group()
msg = ""%s.read(): expected %r but got %r\n%sat index %d."" % (
cls.__name__,
expecting,
token,
"" "" * 12,
pos,
)
s = s.replace(""\n"", "" "").replace(""\t"", "" "")
offset = pos
if len(s) > pos + 10:
s = s[: pos + 10] + ""...""
if pos > 10:
s = ""..."" + s[pos - 10 :]
offset = 13
msg += '\n{}""{}""\n{}^'.format("" "" * 16, s, "" "" * (17 + offset))
raise ValueError(msg)
",[],0,[],/tree/tree.py__parse_error
4975,/home/amandapotts/git/nltk/nltk/tree/tree.py_fromlist,"def fromlist(cls, l):
""""""
:type l: list
:param l: a tree represented as nested lists
:return: A tree corresponding to the list representation ``l``.
:rtype: Tree
Convert nested lists to a NLTK Tree
""""""
if type(l) == list and len(l) > 0:
label = repr(l[0])
if len(l) > 1:
return Tree(label, [cls.fromlist(child) for child in l[1:]])
else:
return label
",[],0,[],/tree/tree.py_fromlist
4976,/home/amandapotts/git/nltk/nltk/tree/tree.py_draw,"def draw(self):
""""""
Open a new window containing a graphical diagram of this tree.
""""""
from nltk.draw.tree import draw_trees
draw_trees(self)
",[],0,[],/tree/tree.py_draw
4977,/home/amandapotts/git/nltk/nltk/tree/tree.py_pretty_print,"def pretty_print(self, sentence=None, highlight=(), stream=None, **kwargs):
""""""
Pretty-print this tree as ASCII or Unicode art.
For explanation of the arguments, see the documentation for
`nltk.tree.prettyprinter.TreePrettyPrinter`.
""""""
from nltk.tree.prettyprinter import TreePrettyPrinter
print(TreePrettyPrinter(self, sentence, highlight).text(**kwargs), file=stream)
",[],0,[],/tree/tree.py_pretty_print
4978,/home/amandapotts/git/nltk/nltk/tree/tree.py___repr__,"def __repr__(self):
childstr = "", "".join(repr(c) for c in self)
return ""{}({}, [{}])"".format(
type(self).__name__,
repr(self._label),
childstr,
)
",[],0,[],/tree/tree.py___repr__
4979,/home/amandapotts/git/nltk/nltk/tree/tree.py__repr_svg_,"def _repr_svg_(self):
from svgling import draw_tree
return draw_tree(self)._repr_svg_()
",[],0,[],/tree/tree.py__repr_svg_
4980,/home/amandapotts/git/nltk/nltk/tree/tree.py___str__,"def __str__(self):
return self.pformat()
",[],0,[],/tree/tree.py___str__
4981,/home/amandapotts/git/nltk/nltk/tree/tree.py_pprint,"def pprint(self, **kwargs):
""""""
Print a string representation of this Tree to 'stream'
""""""
if ""stream"" in kwargs:
stream = kwargs[""stream""]
del kwargs[""stream""]
else:
stream = None
print(self.pformat(**kwargs), file=stream)
",[],0,[],/tree/tree.py_pprint
4982,/home/amandapotts/git/nltk/nltk/tree/tree.py_pformat,"def pformat(self, margin=70, indent=0, nodesep="""", parens=""()"", quotes=False):
""""""
:return: A pretty-printed string representation of this tree.
:rtype: str
:param margin: The right margin at which to do line-wrapping.
:type margin: int
:param indent: The indentation level at which printing
begins.  This number is used to decide how far to indent
subsequent lines.
:type indent: int
:param nodesep: A string that is used to separate the node
from the children.  E.g., the default value ``':'`` gives
trees like ``(S: (NP: I) (VP: (V: saw) (NP: it)))``.
""""""
s = self._pformat_flat(nodesep, parens, quotes)
if len(s) + indent < margin:
return s
if isinstance(self._label, str):
s = f""{parens[0]}{self._label}{nodesep}""
else:
s = f""{parens[0]}{repr(self._label)}{nodesep}""
for child in self:
if isinstance(child, Tree):
s += (
""\n""
+ "" "" * (indent + 2)
+ child.pformat(margin, indent + 2, nodesep, parens, quotes)
)
elif isinstance(child, tuple):
s += ""\n"" + "" "" * (indent + 2) + ""/"".join(child)
elif isinstance(child, str) and not quotes:
s += ""\n"" + "" "" * (indent + 2) + ""%s"" % child
else:
s += ""\n"" + "" "" * (indent + 2) + repr(child)
return s + parens[1]
",[],0,[],/tree/tree.py_pformat
4983,/home/amandapotts/git/nltk/nltk/tree/tree.py_pformat_latex_qtree,"def pformat_latex_qtree(self):
r""""""
Returns a representation of the tree compatible with the
LaTeX qtree package. This consists of the string ``\Tree``
followed by the tree represented in bracketed notation.
For example, the following result was generated from a parse tree of
the sentence ``The announcement astounded us``::
\Tree [.I'' [.N'' [.D The ] [.N' [.N announcement ] ] ]
[.I' [.V'' [.V' [.V astounded ] [.N'' [.N' [.N us ] ] ] ] ] ] ]
See https://www.ling.upenn.edu/advice/latex.html for the LaTeX
style file for the qtree package.
:return: A latex qtree representation of this tree.
:rtype: str
""""""
reserved_chars = re.compile(r""([#\$%&~_\{\}])"")
pformat = self.pformat(indent=6, nodesep="""", parens=(""[."", "" ]""))
return r""\Tree "" + re.sub(reserved_chars, r""\\\1"", pformat)
",[],0,[],/tree/tree.py_pformat_latex_qtree
4984,/home/amandapotts/git/nltk/nltk/tree/tree.py__pformat_flat,"def _pformat_flat(self, nodesep, parens, quotes):
childstrs = []
for child in self:
if isinstance(child, Tree):
childstrs.append(child._pformat_flat(nodesep, parens, quotes))
elif isinstance(child, tuple):
childstrs.append(""/"".join(child))
elif isinstance(child, str) and not quotes:
childstrs.append(""%s"" % child)
else:
childstrs.append(repr(child))
if isinstance(self._label, str):
return ""{}{}{} {}{}"".format(
parens[0],
self._label,
nodesep,
"" "".join(childstrs),
parens[1],
)
else:
return ""{}{}{} {}{}"".format(
parens[0],
repr(self._label),
nodesep,
"" "".join(childstrs),
parens[1],
)
",[],0,[],/tree/tree.py__pformat_flat
4985,/home/amandapotts/git/nltk/nltk/tree/tree.py__child_names,"def _child_names(tree):
names = []
for child in tree:
if isinstance(child, Tree):
names.append(Nonterminal(child._label))
else:
names.append(child)
return names
",[],0,[],/tree/tree.py__child_names
4986,/home/amandapotts/git/nltk/nltk/tree/tree.py_demo,"def demo():
""""""
A demonstration showing how Trees and Trees can be
used.  This demonstration creates a Tree, and loads a
Tree from the Treebank corpus,
and shows the results of calling several of their methods.
""""""
from nltk import ProbabilisticTree, Tree
s = ""(S (NP (DT the) (NN cat)) (VP (VBD ate) (NP (DT a) (NN cookie))))""
t = Tree.fromstring(s)
print(""Convert bracketed string into tree:"")
print(t)
print(t.__repr__())
print(""Display tree properties:"")
print(t.label())  # tree's constituent type
print(t[0])  # tree's first child
print(t[1])  # tree's second child
print(t.height())
print(t.leaves())
print(t[1])
print(t[1, 1])
print(t[1, 1, 0])
the_cat = t[0]
the_cat.insert(1, Tree.fromstring(""(JJ big)""))
print(""Tree modification:"")
print(t)
t[1, 1, 1] = Tree.fromstring(""(NN cake)"")
print(t)
print()
print(""Collapse unary:"")
t.collapse_unary()
print(t)
print(""Chomsky normal form:"")
t.chomsky_normal_form()
print(t)
print()
pt = ProbabilisticTree(""x"", [""y"", ""z""], prob=0.5)
print(""Probabilistic Tree:"")
print(pt)
print()
t = Tree.fromstring(t.pformat())
print(""Convert tree to bracketed string and back again:"")
print(t)
print()
print(""LaTeX output:"")
print(t.pformat_latex_qtree())
print()
print(""Production output:"")
print(t.productions())
print()
t.set_label((""test"", 3))
print(t)
",[],0,[],/tree/tree.py_demo
4987,/home/amandapotts/git/nltk/nltk/tree/probabilistic.py___init__,"def __init__(self, node, children=None, **prob_kwargs):
Tree.__init__(self, node, children)
ProbabilisticMixIn.__init__(self, **prob_kwargs)
",[],0,[],/tree/probabilistic.py___init__
4988,/home/amandapotts/git/nltk/nltk/tree/probabilistic.py__frozen_class,"def _frozen_class(self):
return ImmutableProbabilisticTree
",[],0,[],/tree/probabilistic.py__frozen_class
4989,/home/amandapotts/git/nltk/nltk/tree/probabilistic.py___repr__,"def __repr__(self):
return f""{Tree.__repr__(self)} (p={self.prob()!r})""
",[],0,[],/tree/probabilistic.py___repr__
4990,/home/amandapotts/git/nltk/nltk/tree/probabilistic.py___str__,"def __str__(self):
return f""{self.pformat(margin=60)} (p={self.prob():.6g})""
",[],0,[],/tree/probabilistic.py___str__
4991,/home/amandapotts/git/nltk/nltk/tree/probabilistic.py_copy,"def copy(self, deep=False):
if not deep:
return type(self)(self._label, self, prob=self.prob())
else:
return type(self).convert(self)
",[],0,[],/tree/probabilistic.py_copy
4992,/home/amandapotts/git/nltk/nltk/tree/probabilistic.py_convert,"def convert(cls, val):
if isinstance(val, Tree):
children = [cls.convert(child) for child in val]
if isinstance(val, ProbabilisticMixIn):
return cls(val._label, children, prob=val.prob())
else:
return cls(val._label, children, prob=1.0)
else:
return val
",[],0,[],/tree/probabilistic.py_convert
4993,/home/amandapotts/git/nltk/nltk/tree/probabilistic.py___eq__,"def __eq__(self, other):
return self.__class__ is other.__class__ and (
self._label,
list(self),
self.prob(),
) == (other._label, list(other), other.prob())
",[],0,[],/tree/probabilistic.py___eq__
4994,/home/amandapotts/git/nltk/nltk/tree/probabilistic.py___lt__,"def __lt__(self, other):
if not isinstance(other, Tree):
raise_unorderable_types(""<"", self, other)
if self.__class__ is other.__class__:
return (self._label, list(self), self.prob()) < (
other._label,
list(other),
other.prob(),
)
else:
return self.__class__.__name__ < other.__class__.__name__
",[],0,[],/tree/probabilistic.py___lt__
4995,/home/amandapotts/git/nltk/nltk/tree/transforms.py_chomsky_normal_form,"def chomsky_normal_form(
tree, factor=""right"", horzMarkov=None, vertMarkov=0, childChar=""|"", parentChar=""^""
",[],0,[],/tree/transforms.py_chomsky_normal_form
4996,/home/amandapotts/git/nltk/nltk/tree/transforms.py_un_chomsky_normal_form,"def un_chomsky_normal_form(
tree, expandUnary=True, childChar=""|"", parentChar=""^"", unaryChar=""+""
",[],0,[],/tree/transforms.py_un_chomsky_normal_form
4997,/home/amandapotts/git/nltk/nltk/tree/transforms.py_collapse_unary,"def collapse_unary(tree, collapsePOS=False, collapseRoot=False, joinChar=""+""):
""""""
Collapse subtrees with a single child (ie. unary productions)
into a new non-terminal (Tree node) joined by 'joinChar'.
This is useful when working with algorithms that do not allow
unary productions, and completely removing the unary productions
would require loss of useful information.  The Tree is modified
directly (since it is passed by reference) and no value is returned.
:param tree: The Tree to be collapsed
:type  tree: Tree
:param collapsePOS: 'False' (default) will not collapse the parent of leaf nodes (ie.
Part-of-Speech tags) since they are always unary productions
:type  collapsePOS: bool
:param collapseRoot: 'False' (default) will not modify the root production
if it is unary.  For the Penn WSJ treebank corpus, this corresponds
to the TOP -> productions.
:type collapseRoot: bool
:param joinChar: A string used to connect collapsed node values (default = ""+"")
:type  joinChar: str
""""""
if collapseRoot == False and isinstance(tree, Tree) and len(tree) == 1:
nodeList = [tree[0]]
else:
nodeList = [tree]
while nodeList != []:
node = nodeList.pop()
if isinstance(node, Tree):
if (
len(node) == 1
and isinstance(node[0], Tree)
and (collapsePOS == True or isinstance(node[0, 0], Tree))
):
node.set_label(node.label() + joinChar + node[0].label())
node[0:] = [child for child in node[0]]
nodeList.append(node)
else:
for child in node:
nodeList.append(child)
",[],0,[],/tree/transforms.py_collapse_unary
4998,/home/amandapotts/git/nltk/nltk/tree/transforms.py_demo,"def demo():
""""""
A demonstration showing how each tree transform can be used.
""""""
from copy import deepcopy
from nltk.draw.tree import draw_trees
from nltk.tree.tree import Tree
sentence = """"""(TOP
(S
(S
(VP
(VBN Turned)
(ADVP (RB loose))
(PP
(IN in)
(NP
(NP (NNP Shane) (NNP Longman) (POS 's))
(NN trading)
(NN room)))))
(, ,)
(NP (DT the) (NN yuppie) (NNS dealers))
(VP (AUX do) (NP (NP (RB little)) (ADJP (RB right))))
(. .)))""""""
t = Tree.fromstring(sentence, remove_empty_top_bracketing=True)
collapsedTree = deepcopy(t)
collapse_unary(collapsedTree)
cnfTree = deepcopy(collapsedTree)
chomsky_normal_form(cnfTree)
parentTree = deepcopy(collapsedTree)
chomsky_normal_form(parentTree, horzMarkov=2, vertMarkov=1)
original = deepcopy(parentTree)
un_chomsky_normal_form(original)
sentence2 = original.pprint()
print(sentence)
print(sentence2)
print(""Sentences the same? "", sentence == sentence2)
draw_trees(t, collapsedTree, cnfTree, parentTree, original)
",[],0,[],/tree/transforms.py_demo
4999,/home/amandapotts/git/nltk/nltk/tree/parsing.py_bracket_parse,"def bracket_parse(s):
""""""
Use Tree.read(s, remove_empty_top_bracketing=True) instead.
""""""
raise NameError(""Use Tree.read(s, remove_empty_top_bracketing=True) instead."")
",[],0,[],/tree/parsing.py_bracket_parse
5000,/home/amandapotts/git/nltk/nltk/tree/parsing.py_sinica_parse,"def sinica_parse(s):
""""""
Parse a Sinica Treebank string and return a tree.  Trees are represented as nested brackettings,
as shown in the following example (X represents a Chinese character):
S(goal:NP(Head:Nep:XX)|theme:NP(Head:Nhaa:X)|quantity:Dab:X|Head:VL2:X)#0(PERIODCATEGORY)
:return: A tree corresponding to the string representation.
:rtype: Tree
:param s: The string to be converted
:type s: str
""""""
tokens = re.split(r""([()| ])"", s)
for i in range(len(tokens)):
if tokens[i] == ""("":
tokens[i - 1], tokens[i] = (
tokens[i],
tokens[i - 1],
)  # pull nonterminal inside parens
elif "":"" in tokens[i]:
fields = tokens[i].split("":"")
if len(fields) == 2:  # non-terminal
tokens[i] = fields[1]
else:
tokens[i] = ""("" + fields[-2] + "" "" + fields[-1] + "")""
elif tokens[i] == ""|"":
tokens[i] = """"
treebank_string = "" "".join(tokens)
return Tree.fromstring(treebank_string, remove_empty_top_bracketing=True)
",[],0,[],/tree/parsing.py_sinica_parse
5001,/home/amandapotts/git/nltk/nltk/tokenize/stanford_segmenter.py___init__,"def __init__(
self,
path_to_jar=None,
path_to_slf4j=None,
java_class=None,
path_to_model=None,
path_to_dict=None,
path_to_sihan_corpora_dict=None,
sihan_post_processing=""false"",
keep_whitespaces=""false"",
encoding=""UTF-8"",
options=None,
verbose=False,
java_options=""-mx2g"",
",[],0,[],/tokenize/stanford_segmenter.py___init__
5002,/home/amandapotts/git/nltk/nltk/tokenize/stanford_segmenter.py_default_config,"def default_config(self, lang):
""""""
Attempt to initialize Stanford Word Segmenter for the specified language
using the STANFORD_SEGMENTER and STANFORD_MODELS environment variables
""""""
search_path = ()
if os.environ.get(""STANFORD_SEGMENTER""):
search_path = {os.path.join(os.environ.get(""STANFORD_SEGMENTER""), ""data"")}
self._dict = None
self._sihan_corpora_dict = None
self._sihan_post_processing = ""false""
if lang == ""ar"":
self._java_class = (
""edu.stanford.nlp.international.arabic.process.ArabicSegmenter""
)
model = ""arabic-segmenter-atb+bn+arztrain.ser.gz""
elif lang == ""zh"":
self._java_class = ""edu.stanford.nlp.ie.crf.CRFClassifier""
model = ""pku.gz""
self._sihan_post_processing = ""true""
path_to_dict = ""dict-chris6.ser.gz""
try:
self._dict = find_file(
path_to_dict,
searchpath=search_path,
url=_stanford_url,
verbose=False,
env_vars=(""STANFORD_MODELS"",),
)
except LookupError as e:
raise LookupError(
""Could not find '%s' (tried using env. ""
""variables STANFORD_MODELS and <STANFORD_SEGMENTER>/data/)""
% path_to_dict
) from e
sihan_dir = ""./data/""
try:
path_to_sihan_dir = find_dir(
sihan_dir,
url=_stanford_url,
verbose=False,
env_vars=(""STANFORD_SEGMENTER"",),
)
self._sihan_corpora_dict = os.path.join(path_to_sihan_dir, sihan_dir)
except LookupError as e:
raise LookupError(
""Could not find '%s' (tried using the ""
""STANFORD_SEGMENTER environment variable)"" % sihan_dir
) from e
else:
raise LookupError(f""Unsupported language {lang}"")
try:
self._model = find_file(
model,
searchpath=search_path,
url=_stanford_url,
verbose=False,
env_vars=(""STANFORD_MODELS"", ""STANFORD_SEGMENTER""),
)
except LookupError as e:
raise LookupError(
""Could not find '%s' (tried using env. ""
""variables STANFORD_MODELS and <STANFORD_SEGMENTER>/data/)"" % model
) from e
",[],0,[],/tokenize/stanford_segmenter.py_default_config
5003,/home/amandapotts/git/nltk/nltk/tokenize/stanford_segmenter.py_tokenize,"def tokenize(self, s):
super().tokenize(s)
",[],0,[],/tokenize/stanford_segmenter.py_tokenize
5004,/home/amandapotts/git/nltk/nltk/tokenize/stanford_segmenter.py_segment_file,"def segment_file(self, input_file_path):
"""""" """"""
cmd = [
self._java_class,
""-loadClassifier"",
self._model,
""-keepAllWhitespaces"",
self._keep_whitespaces,
""-textFile"",
input_file_path,
]
if self._sihan_corpora_dict is not None:
cmd.extend(
[
""-serDictionary"",
self._dict,
""-sighanCorporaDict"",
self._sihan_corpora_dict,
""-sighanPostProcessing"",
self._sihan_post_processing,
]
)
stdout = self._execute(cmd)
return stdout
",[],0,[],/tokenize/stanford_segmenter.py_segment_file
5005,/home/amandapotts/git/nltk/nltk/tokenize/stanford_segmenter.py_segment,"def segment(self, tokens):
return self.segment_sents([tokens])
",[],0,[],/tokenize/stanford_segmenter.py_segment
5006,/home/amandapotts/git/nltk/nltk/tokenize/stanford_segmenter.py_segment_sents,"def segment_sents(self, sentences):
"""""" """"""
encoding = self._encoding
_input_fh, self._input_file_path = tempfile.mkstemp(text=True)
_input_fh = os.fdopen(_input_fh, ""wb"")
_input = ""\n"".join("" "".join(x) for x in sentences)
if isinstance(_input, str) and encoding:
_input = _input.encode(encoding)
_input_fh.write(_input)
_input_fh.close()
cmd = [
self._java_class,
""-loadClassifier"",
self._model,
""-keepAllWhitespaces"",
self._keep_whitespaces,
""-textFile"",
self._input_file_path,
]
if self._sihan_corpora_dict is not None:
cmd.extend(
[
""-serDictionary"",
self._dict,
""-sighanCorporaDict"",
self._sihan_corpora_dict,
""-sighanPostProcessing"",
self._sihan_post_processing,
]
)
stdout = self._execute(cmd)
os.unlink(self._input_file_path)
return stdout
",[],0,[],/tokenize/stanford_segmenter.py_segment_sents
5007,/home/amandapotts/git/nltk/nltk/tokenize/stanford_segmenter.py__execute,"def _execute(self, cmd, verbose=False):
encoding = self._encoding
cmd.extend([""-inputEncoding"", encoding])
_options_cmd = self._options_cmd
if _options_cmd:
cmd.extend([""-options"", self._options_cmd])
default_options = "" "".join(_java_options)
config_java(options=self.java_options, verbose=verbose)
stdout, _stderr = java(
cmd, classpath=self._stanford_jar, stdout=PIPE, stderr=PIPE
)
stdout = stdout.decode(encoding)
config_java(options=default_options, verbose=False)
return stdout
",[],0,[],/tokenize/stanford_segmenter.py__execute
5008,/home/amandapotts/git/nltk/nltk/tokenize/repp.py___init__,"def __init__(self, repp_dir, encoding=""utf8""):
self.repp_dir = self.find_repptokenizer(repp_dir)
self.working_dir = tempfile.gettempdir()
self.encoding = encoding
",[],0,[],/tokenize/repp.py___init__
5009,/home/amandapotts/git/nltk/nltk/tokenize/repp.py_tokenize,"def tokenize(self, sentence):
""""""
Use Repp to tokenize a single sentence.
:param sentence: A single sentence string.
:type sentence: str
:return: A tuple of tokens.
:rtype: tuple(str)
""""""
return next(self.tokenize_sents([sentence]))
",[],0,[],/tokenize/repp.py_tokenize
5010,/home/amandapotts/git/nltk/nltk/tokenize/repp.py_tokenize_sents,"def tokenize_sents(self, sentences, keep_token_positions=False):
""""""
Tokenize multiple sentences using Repp.
:param sentences: A list of sentence strings.
:type sentences: list(str)
:return: A list of tuples of tokens
:rtype: iter(tuple(str))
""""""
with tempfile.NamedTemporaryFile(
prefix=""repp_input."", dir=self.working_dir, mode=""w"", delete=False
) as input_file:
for sent in sentences:
input_file.write(str(sent) + ""\n"")
input_file.close()
cmd = self.generate_repp_command(input_file.name)
repp_output = self._execute(cmd).decode(self.encoding).strip()
for tokenized_sent in self.parse_repp_outputs(repp_output):
if not keep_token_positions:
tokenized_sent, starts, ends = zip(*tokenized_sent)
yield tokenized_sent
",[],0,[],/tokenize/repp.py_tokenize_sents
5011,/home/amandapotts/git/nltk/nltk/tokenize/repp.py_generate_repp_command,"def generate_repp_command(self, inputfilename):
""""""
This module generates the REPP command to be used at the terminal.
:param inputfilename: path to the input file
:type inputfilename: str
""""""
cmd = [self.repp_dir + ""/src/repp""]
cmd += [""-c"", self.repp_dir + ""/erg/repp.set""]
cmd += [""--format"", ""triple""]
cmd += [inputfilename]
return cmd
",[],0,[],/tokenize/repp.py_generate_repp_command
5012,/home/amandapotts/git/nltk/nltk/tokenize/repp.py__execute,"def _execute(cmd):
p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
stdout, stderr = p.communicate()
return stdout
",[],0,[],/tokenize/repp.py__execute
5013,/home/amandapotts/git/nltk/nltk/tokenize/repp.py_parse_repp_outputs,"def parse_repp_outputs(repp_output):
""""""
This module parses the tri-tuple format that REPP outputs using the
""--format triple"" option and returns an generator with tuple of string
tokens.
:param repp_output:
:type repp_output: type
:return: an iterable of the tokenized sentences as tuples of strings
:rtype: iter(tuple)
""""""
line_regex = re.compile(r""^\((\d+), (\d+), (.+)\)$"", re.MULTILINE)
for section in repp_output.split(""\n\n""):
words_with_positions = [
(token, int(start), int(end))
for start, end, token in line_regex.findall(section)
]
words = tuple(t[2] for t in words_with_positions)
yield words_with_positions
",[],0,[],/tokenize/repp.py_parse_repp_outputs
5014,/home/amandapotts/git/nltk/nltk/tokenize/repp.py_find_repptokenizer,"def find_repptokenizer(self, repp_dirname):
""""""
A module to find REPP tokenizer binary and its *repp.set* config file.
""""""
if os.path.exists(repp_dirname):  # If a full path is given.
_repp_dir = repp_dirname
else:  # Try to find path to REPP directory in environment variables.
_repp_dir = find_dir(repp_dirname, env_vars=(""REPP_TOKENIZER"",))
assert os.path.exists(_repp_dir + ""/src/repp"")
assert os.path.exists(_repp_dir + ""/erg/repp.set"")
return _repp_dir
",[],0,[],/tokenize/repp.py_find_repptokenizer
5015,/home/amandapotts/git/nltk/nltk/tokenize/casual.py__str_to_unicode,"def _str_to_unicode(text, encoding=None, errors=""strict""):
if encoding is None:
encoding = ""utf-8""
if isinstance(text, bytes):
return text.decode(encoding, errors)
return text
",[],0,[],/tokenize/casual.py__str_to_unicode
5016,/home/amandapotts/git/nltk/nltk/tokenize/casual.py__replace_html_entities,"def _replace_html_entities(text, keep=(), remove_illegal=True, encoding=""utf-8""):
""""""
Remove entities from text by converting them to their
corresponding unicode character.
:param text: a unicode string or a byte string encoded in the given
`encoding` (which defaults to 'utf-8').
:param list keep:  list of entity names which should not be replaced.\
This supports both numeric entities (``&#nnnn
and named entities (such as ``&nbsp
:param bool remove_illegal: If `True`, entities that can't be converted are\
removed. Otherwise, entities that can't be converted are kept ""as
is"".
:returns: A unicode string with the entities removed.
See https://github.com/scrapy/w3lib/blob/master/w3lib/html.py
>>> from nltk.tokenize.casual import _replace_html_entities
>>> _replace_html_entities(b'Price: &pound
'Price: \\xa3100'
>>> print(_replace_html_entities(b'Price: &pound
Price: £100
>>>
""""""
",[],0,[],/tokenize/casual.py__replace_html_entities
5017,/home/amandapotts/git/nltk/nltk/tokenize/casual.py__convert_entity,"def _convert_entity(match):
entity_body = match.group(3)
if match.group(1):
try:
if match.group(2):
number = int(entity_body, 16)
else:
number = int(entity_body, 10)
if 0x80 <= number <= 0x9F:
return bytes((number,)).decode(""cp1252"")
except ValueError:
number = None
else:
if entity_body in keep:
return match.group(0)
number = html.entities.name2codepoint.get(entity_body)
if number is not None:
try:
return chr(number)
except (ValueError, OverflowError):
pass
return """" if remove_illegal else match.group(0)
",[],0,[],/tokenize/casual.py__convert_entity
5018,/home/amandapotts/git/nltk/nltk/tokenize/casual.py___init__,"def __init__(
self,
preserve_case=True,
reduce_len=False,
strip_handles=False,
match_phone_numbers=True,
",[],0,[],/tokenize/casual.py___init__
5019,/home/amandapotts/git/nltk/nltk/tokenize/casual.py_WORD_RE,"def WORD_RE(self) -> ""regex.Pattern"":
""""""Core TweetTokenizer regex""""""
if not type(self)._WORD_RE:
type(self)._WORD_RE = regex.compile(
f""({'|'.join(REGEXPS)})"",
regex.VERBOSE | regex.I | regex.UNICODE,
)
return type(self)._WORD_RE
",[],0,[],/tokenize/casual.py_WORD_RE
5020,/home/amandapotts/git/nltk/nltk/tokenize/casual.py_PHONE_WORD_RE,"def PHONE_WORD_RE(self) -> ""regex.Pattern"":
""""""Secondary core TweetTokenizer regex""""""
if not type(self)._PHONE_WORD_RE:
type(self)._PHONE_WORD_RE = regex.compile(
f""({'|'.join(REGEXPS_PHONE)})"",
regex.VERBOSE | regex.I | regex.UNICODE,
)
return type(self)._PHONE_WORD_RE
",[],0,[],/tokenize/casual.py_PHONE_WORD_RE
5021,/home/amandapotts/git/nltk/nltk/tokenize/casual.py_reduce_lengthening,"def reduce_lengthening(text):
""""""
Replace repeated character sequences of length 3 or greater with sequences
of length 3.
""""""
pattern = regex.compile(r""(.)\1{2,}"")
return pattern.sub(r""\1\1\1"", text)
",[],0,[],/tokenize/casual.py_reduce_lengthening
5022,/home/amandapotts/git/nltk/nltk/tokenize/casual.py_remove_handles,"def remove_handles(text):
""""""
Remove Twitter username handles from text.
""""""
return HANDLES_RE.sub("" "", text)
",[],0,[],/tokenize/casual.py_remove_handles
5023,/home/amandapotts/git/nltk/nltk/tokenize/casual.py_casual_tokenize,"def casual_tokenize(
text,
preserve_case=True,
reduce_len=False,
strip_handles=False,
match_phone_numbers=True,
",[],0,[],/tokenize/casual.py_casual_tokenize
5024,/home/amandapotts/git/nltk/nltk/tokenize/__init__.py_sent_tokenize,"def sent_tokenize(text, language=""english""):
""""""
Return a sentence-tokenized copy of *text*,
using NLTK's recommended sentence tokenizer
(currently :class:`.PunktSentenceTokenizer`
for the specified language).
:param text: text to split into sentences
:param language: the model name in the Punkt corpus
""""""
tokenizer = load(f""tokenizers/punkt/{language}.pickle"")
return tokenizer.tokenize(text)
",[],0,[],/tokenize/__init__.py_sent_tokenize
5025,/home/amandapotts/git/nltk/nltk/tokenize/__init__.py_word_tokenize,"def word_tokenize(text, language=""english"", preserve_line=False):
""""""
Return a tokenized copy of *text*,
using NLTK's recommended word tokenizer
(currently an improved :class:`.TreebankWordTokenizer`
along with :class:`.PunktSentenceTokenizer`
for the specified language).
:param text: text to split into words
:type text: str
:param language: the model name in the Punkt corpus
:type language: str
:param preserve_line: A flag to decide whether to sentence tokenize the text or not.
:type preserve_line: bool
""""""
sentences = [text] if preserve_line else sent_tokenize(text, language)
return [
token for sent in sentences for token in _treebank_word_tokenizer.tokenize(sent)
]
",[],0,[],/tokenize/__init__.py_word_tokenize
5026,/home/amandapotts/git/nltk/nltk/tokenize/toktok.py_tokenize,"def tokenize(self, text, return_str=False):
text = str(text)  # Converts input string into unicode.
for regexp, substitution in self.TOKTOK_REGEXES:
text = regexp.sub(substitution, text)
text = str(text.strip())
return text if return_str else text.split()
",[],0,[],/tokenize/toktok.py_tokenize
5027,/home/amandapotts/git/nltk/nltk/tokenize/regexp.py___init__,"def __init__(
self,
pattern,
gaps=False,
discard_empty=True,
flags=re.UNICODE | re.MULTILINE | re.DOTALL,
",[],0,[],/tokenize/regexp.py___init__
5028,/home/amandapotts/git/nltk/nltk/tokenize/regexp.py__check_regexp,"def _check_regexp(self):
if self._regexp is None:
self._regexp = re.compile(self._pattern, self._flags)
",[],0,[],/tokenize/regexp.py__check_regexp
5029,/home/amandapotts/git/nltk/nltk/tokenize/regexp.py_tokenize,"def tokenize(self, text):
self._check_regexp()
if self._gaps:
if self._discard_empty:
return [tok for tok in self._regexp.split(text) if tok]
else:
return self._regexp.split(text)
else:
return self._regexp.findall(text)
",[],0,[],/tokenize/regexp.py_tokenize
5030,/home/amandapotts/git/nltk/nltk/tokenize/regexp.py_span_tokenize,"def span_tokenize(self, text):
self._check_regexp()
if self._gaps:
for left, right in regexp_span_tokenize(text, self._regexp):
if not (self._discard_empty and left == right):
yield left, right
else:
for m in re.finditer(self._regexp, text):
yield m.span()
",[],0,[],/tokenize/regexp.py_span_tokenize
5031,/home/amandapotts/git/nltk/nltk/tokenize/regexp.py___repr__,"def __repr__(self):
return ""{}(pattern={!r}, gaps={!r}, discard_empty={!r}, flags={!r})"".format(
self.__class__.__name__,
self._pattern,
self._gaps,
self._discard_empty,
self._flags,
)
",[],0,[],/tokenize/regexp.py___repr__
5032,/home/amandapotts/git/nltk/nltk/tokenize/regexp.py___init__,"def __init__(self):
RegexpTokenizer.__init__(self, r""\s+"", gaps=True)
",[],0,[],/tokenize/regexp.py___init__
5033,/home/amandapotts/git/nltk/nltk/tokenize/regexp.py___init__,"def __init__(self):
RegexpTokenizer.__init__(self, r""\s*\n\s*\n\s*"", gaps=True)
",[],0,[],/tokenize/regexp.py___init__
5034,/home/amandapotts/git/nltk/nltk/tokenize/regexp.py___init__,"def __init__(self):
RegexpTokenizer.__init__(self, r""\w+|[^\w\s]+"")
",[],0,[],/tokenize/regexp.py___init__
5035,/home/amandapotts/git/nltk/nltk/tokenize/regexp.py_regexp_tokenize,"def regexp_tokenize(
text,
pattern,
gaps=False,
discard_empty=True,
flags=re.UNICODE | re.MULTILINE | re.DOTALL,
",[],0,[],/tokenize/regexp.py_regexp_tokenize
5036,/home/amandapotts/git/nltk/nltk/tokenize/util.py_string_span_tokenize,"def string_span_tokenize(s, sep):
r""""""
Return the offsets of the tokens in *s*, as a sequence of ``(start, end)``
tuples, by splitting the string at each occurrence of *sep*.
>>> from nltk.tokenize.util import string_span_tokenize
>>> s = '''Good muffins cost $3.88\nin New York.  Please buy me
... two of them.\n\nThanks.'''
>>> list(string_span_tokenize(s, "" "")) # doctest: +NORMALIZE_WHITESPACE
[(0, 4), (5, 12), (13, 17), (18, 26), (27, 30), (31, 36), (37, 37),
(38, 44), (45, 48), (49, 55), (56, 58), (59, 73)]
:param s: the string to be tokenized
:type s: str
:param sep: the token separator
:type sep: str
:rtype: iter(tuple(int, int))
""""""
if len(sep) == 0:
raise ValueError(""Token delimiter must not be empty"")
left = 0
while True:
try:
right = s.index(sep, left)
if right != 0:
yield left, right
except ValueError:
if left != len(s):
yield left, len(s)
break
left = right + len(sep)
",[],0,[],/tokenize/util.py_string_span_tokenize
5037,/home/amandapotts/git/nltk/nltk/tokenize/util.py_regexp_span_tokenize,"def regexp_span_tokenize(s, regexp):
r""""""
Return the offsets of the tokens in *s*, as a sequence of ``(start, end)``
tuples, by splitting the string at each successive match of *regexp*.
>>> from nltk.tokenize.util import regexp_span_tokenize
>>> s = '''Good muffins cost $3.88\nin New York.  Please buy me
... two of them.\n\nThanks.'''
>>> list(regexp_span_tokenize(s, r'\s')) # doctest: +NORMALIZE_WHITESPACE
[(0, 4), (5, 12), (13, 17), (18, 23), (24, 26), (27, 30), (31, 36),
(38, 44), (45, 48), (49, 51), (52, 55), (56, 58), (59, 64), (66, 73)]
:param s: the string to be tokenized
:type s: str
:param regexp: regular expression that matches token separators (must not be empty)
:type regexp: str
:rtype: iter(tuple(int, int))
""""""
left = 0
for m in finditer(regexp, s):
right, next = m.span()
if right != left:
yield left, right
left = next
yield left, len(s)
",[],0,[],/tokenize/util.py_regexp_span_tokenize
5038,/home/amandapotts/git/nltk/nltk/tokenize/util.py_spans_to_relative,"def spans_to_relative(spans):
r""""""
Return a sequence of relative spans, given a sequence of spans.
>>> from nltk.tokenize import WhitespaceTokenizer
>>> from nltk.tokenize.util import spans_to_relative
>>> s = '''Good muffins cost $3.88\nin New York.  Please buy me
... two of them.\n\nThanks.'''
>>> list(spans_to_relative(WhitespaceTokenizer().span_tokenize(s))) # doctest: +NORMALIZE_WHITESPACE
[(0, 4), (1, 7), (1, 4), (1, 5), (1, 2), (1, 3), (1, 5), (2, 6),
(1, 3), (1, 2), (1, 3), (1, 2), (1, 5), (2, 7)]
:param spans: a sequence of (start, end) offsets of the tokens
:type spans: iter(tuple(int, int))
:rtype: iter(tuple(int, int))
""""""
prev = 0
for left, right in spans:
yield left - prev, right - left
prev = right
",[],0,[],/tokenize/util.py_spans_to_relative
5039,/home/amandapotts/git/nltk/nltk/tokenize/util.py_is_cjk,"def is_cjk(character):
""""""
Python port of Moses' code to check for CJK character.
>>> CJKChars().ranges
[(4352, 4607), (11904, 42191), (43072, 43135), (44032, 55215), (63744, 64255), (65072, 65103), (65381, 65500), (131072, 196607)]
>>> is_cjk(u'\u33fe')
True
>>> is_cjk(u'\uFE5F')
False
:param character: The character that needs to be checked.
:type character: char
:return: bool
""""""
return any(
[
start <= ord(character) <= end
for start, end in [
(4352, 4607),
(11904, 42191),
(43072, 43135),
(44032, 55215),
(63744, 64255),
(65072, 65103),
(65381, 65500),
(131072, 196607),
]
]
)
",[],0,[],/tokenize/util.py_is_cjk
5040,/home/amandapotts/git/nltk/nltk/tokenize/util.py_xml_escape,"def xml_escape(text):
""""""
This function transforms the input text into an ""escaped"" version suitable
for well-formed XML formatting.
Note that the default xml.sax.saxutils.escape() function don't escape
some characters that Moses does so we have to manually add them to the
entities dictionary.
>>> input_str = ''')| & < > ' "" ] ['''
>>> expected_output =  ''')| &amp
>>> escape(input_str) == expected_output
True
>>> xml_escape(input_str)
')&#124
:param text: The text that needs to be escaped.
:type text: str
:rtype: str
""""""
return escape(
text,
entities={
r""'"": r""&apos
r'""': r""&quot
r""|"": r""&#124
r""["": r""&#91
r""]"": r""&#93
},
)
",[],0,[],/tokenize/util.py_xml_escape
5041,/home/amandapotts/git/nltk/nltk/tokenize/util.py_xml_unescape,"def xml_unescape(text):
""""""
This function transforms the ""escaped"" version suitable
for well-formed XML formatting into humanly-readable string.
Note that the default xml.sax.saxutils.unescape() function don't unescape
some characters that Moses does so we have to manually add them to the
entities dictionary.
>>> from xml.sax.saxutils import unescape
>>> s = ')&#124
>>> expected = ''')| & < > \' "" ] ['''
>>> xml_unescape(s) == expected
True
:param text: The text that needs to be unescaped.
:type text: str
:rtype: str
""""""
return unescape(
text,
entities={
r""&apos
r""&quot
r""&#124
r""&#91
r""&#93
},
)
",[],0,[],/tokenize/util.py_xml_unescape
5042,/home/amandapotts/git/nltk/nltk/tokenize/util.py_align_tokens,"def align_tokens(tokens, sentence):
""""""
This module attempt to find the offsets of the tokens in *s*, as a sequence
of ``(start, end)`` tuples, given the tokens and also the source string.
>>> from nltk.tokenize import TreebankWordTokenizer
>>> from nltk.tokenize.util import align_tokens
>>> s = str(""The plane, bound for St Petersburg, crashed in Egypt's ""
... ""Sinai desert just 23 minutes after take-off from Sharm el-Sheikh ""
... ""on Saturday."")
>>> tokens = TreebankWordTokenizer().tokenize(s)
>>> expected = [(0, 3), (4, 9), (9, 10), (11, 16), (17, 20), (21, 23),
... (24, 34), (34, 35), (36, 43), (44, 46), (47, 52), (52, 54),
... (55, 60), (61, 67), (68, 72), (73, 75), (76, 83), (84, 89),
... (90, 98), (99, 103), (104, 109), (110, 119), (120, 122),
... (123, 131), (131, 132)]
>>> output = list(align_tokens(tokens, s))
>>> len(tokens) == len(expected) == len(output)  # Check that length of tokens and tuples are the same.
True
>>> expected == list(align_tokens(tokens, s))  # Check that the output is as expected.
True
>>> tokens == [s[start:end] for start, end in output]  # Check that the slices of the string corresponds to the tokens.
True
:param tokens: The list of strings that are the result of tokenization
:type tokens: list(str)
:param sentence: The original string
:type sentence: str
:rtype: list(tuple(int,int))
""""""
point = 0
offsets = []
for token in tokens:
try:
start = sentence.index(token, point)
except ValueError as e:
raise ValueError(f'substring ""{token}"" not found in ""{sentence}""') from e
point = start + len(token)
offsets.append((start, point))
return offsets
",[],0,[],/tokenize/util.py_align_tokens
5043,/home/amandapotts/git/nltk/nltk/tokenize/texttiling.py___init__,"def __init__(
self,
w=20,
k=10,
similarity_method=BLOCK_COMPARISON,
stopwords=None,
smoothing_method=DEFAULT_SMOOTHING,
smoothing_width=2,
smoothing_rounds=1,
cutoff_policy=HC,
demo_mode=False,
",[],0,[],/tokenize/texttiling.py___init__
5044,/home/amandapotts/git/nltk/nltk/tokenize/texttiling.py_tokenize,"def tokenize(self, text):
""""""Return a tokenized copy of *text*, where each ""token"" represents
a separate topic.""""""
lowercase_text = text.lower()
paragraph_breaks = self._mark_paragraph_breaks(text)
text_length = len(lowercase_text)
nopunct_text = """".join(
c for c in lowercase_text if re.match(r""[a-z\-' \n\t]"", c)
)
nopunct_par_breaks = self._mark_paragraph_breaks(nopunct_text)
tokseqs = self._divide_to_tokensequences(nopunct_text)
for ts in tokseqs:
ts.wrdindex_list = [
wi for wi in ts.wrdindex_list if wi[0] not in self.stopwords
]
token_table = self._create_token_table(tokseqs, nopunct_par_breaks)
if self.similarity_method == BLOCK_COMPARISON:
gap_scores = self._block_comparison(tokseqs, token_table)
elif self.similarity_method == VOCABULARY_INTRODUCTION:
raise NotImplementedError(""Vocabulary introduction not implemented"")
else:
raise ValueError(
f""Similarity method {self.similarity_method} not recognized""
)
if self.smoothing_method == DEFAULT_SMOOTHING:
smooth_scores = self._smooth_scores(gap_scores)
else:
raise ValueError(f""Smoothing method {self.smoothing_method} not recognized"")
depth_scores = self._depth_scores(smooth_scores)
segment_boundaries = self._identify_boundaries(depth_scores)
normalized_boundaries = self._normalize_boundaries(
text, segment_boundaries, paragraph_breaks
)
segmented_text = []
prevb = 0
for b in normalized_boundaries:
if b == 0:
continue
segmented_text.append(text[prevb:b])
prevb = b
if prevb < text_length:  # append any text that may be remaining
segmented_text.append(text[prevb:])
if not segmented_text:
segmented_text = [text]
if self.demo_mode:
return gap_scores, smooth_scores, depth_scores, segment_boundaries
return segmented_text
",[],0,[],/tokenize/texttiling.py_tokenize
5045,/home/amandapotts/git/nltk/nltk/tokenize/texttiling.py__block_comparison,"def _block_comparison(self, tokseqs, token_table):
""""""Implements the block comparison method""""""
",[],0,[],/tokenize/texttiling.py__block_comparison
5046,/home/amandapotts/git/nltk/nltk/tokenize/texttiling.py__smooth_scores,"def _smooth_scores(self, gap_scores):
""Wraps the smooth function from the SciPy Cookbook""
return list(
smooth(numpy.array(gap_scores[:]), window_len=self.smoothing_width + 1)
)
",[],0,[],/tokenize/texttiling.py__smooth_scores
5047,/home/amandapotts/git/nltk/nltk/tokenize/texttiling.py__mark_paragraph_breaks,"def _mark_paragraph_breaks(self, text):
""""""Identifies indented text or line breaks as the beginning of
paragraphs""""""
MIN_PARAGRAPH = 100
pattern = re.compile(""[ \t\r\f\v]*\n[ \t\r\f\v]*\n[ \t\r\f\v]*"")
matches = pattern.finditer(text)
last_break = 0
pbreaks = [0]
for pb in matches:
if pb.start() - last_break < MIN_PARAGRAPH:
continue
else:
pbreaks.append(pb.start())
last_break = pb.start()
return pbreaks
",[],0,[],/tokenize/texttiling.py__mark_paragraph_breaks
5048,/home/amandapotts/git/nltk/nltk/tokenize/texttiling.py__divide_to_tokensequences,"def _divide_to_tokensequences(self, text):
""Divides the text into pseudosentences of fixed size""
w = self.w
wrdindex_list = []
matches = re.finditer(r""\w+"", text)
for match in matches:
wrdindex_list.append((match.group(), match.start()))
return [
TokenSequence(i / w, wrdindex_list[i : i + w])
for i in range(0, len(wrdindex_list), w)
]
",[],0,[],/tokenize/texttiling.py__divide_to_tokensequences
5049,/home/amandapotts/git/nltk/nltk/tokenize/texttiling.py__create_token_table,"def _create_token_table(self, token_sequences, par_breaks):
""Creates a table of TokenTableFields""
token_table = {}
current_par = 0
current_tok_seq = 0
pb_iter = par_breaks.__iter__()
current_par_break = next(pb_iter)
if current_par_break == 0:
try:
current_par_break = next(pb_iter)  # skip break at 0
except StopIteration as e:
raise ValueError(
""No paragraph breaks were found(text too short perhaps?)""
) from e
for ts in token_sequences:
for word, index in ts.wrdindex_list:
try:
while index > current_par_break:
current_par_break = next(pb_iter)
current_par += 1
except StopIteration:
pass
if word in token_table:
token_table[word].total_count += 1
if token_table[word].last_par != current_par:
token_table[word].last_par = current_par
token_table[word].par_count += 1
if token_table[word].last_tok_seq != current_tok_seq:
token_table[word].last_tok_seq = current_tok_seq
token_table[word].ts_occurences.append([current_tok_seq, 1])
else:
token_table[word].ts_occurences[-1][1] += 1
else:  # new word
token_table[word] = TokenTableField(
first_pos=index,
ts_occurences=[[current_tok_seq, 1]],
total_count=1,
par_count=1,
last_par=current_par,
last_tok_seq=current_tok_seq,
)
current_tok_seq += 1
return token_table
",[],0,[],/tokenize/texttiling.py__create_token_table
5050,/home/amandapotts/git/nltk/nltk/tokenize/texttiling.py__depth_scores,"def _depth_scores(self, scores):
""""""Calculates the depth of each gap, i.e. the average difference
between the left and right peaks and the gap's score""""""
depth_scores = [0 for x in scores]
clip = min(max(len(scores) // 10, 2), 5)
index = clip
for gapscore in scores[clip:-clip]:
lpeak = gapscore
for score in scores[index::-1]:
if score >= lpeak:
lpeak = score
else:
break
rpeak = gapscore
for score in scores[index:]:
if score >= rpeak:
rpeak = score
else:
break
depth_scores[index] = lpeak + rpeak - 2 * gapscore
index += 1
return depth_scores
",[],0,[],/tokenize/texttiling.py__depth_scores
5051,/home/amandapotts/git/nltk/nltk/tokenize/texttiling.py__normalize_boundaries,"def _normalize_boundaries(self, text, boundaries, paragraph_breaks):
""""""Normalize the boundaries identified to the original text's
paragraph breaks""""""
norm_boundaries = []
char_count, word_count, gaps_seen = 0, 0, 0
seen_word = False
for char in text:
char_count += 1
if char in "" \t\n"" and seen_word:
seen_word = False
word_count += 1
if char not in "" \t\n"" and not seen_word:
seen_word = True
if gaps_seen < len(boundaries) and word_count > (
max(gaps_seen * self.w, self.w)
):
if boundaries[gaps_seen] == 1:
best_fit = len(text)
for br in paragraph_breaks:
if best_fit > abs(br - char_count):
best_fit = abs(br - char_count)
bestbr = br
else:
break
if bestbr not in norm_boundaries:  # avoid duplicates
norm_boundaries.append(bestbr)
gaps_seen += 1
return norm_boundaries
",[],0,[],/tokenize/texttiling.py__normalize_boundaries
5052,/home/amandapotts/git/nltk/nltk/tokenize/texttiling.py___init__,"def __init__(
self,
first_pos,
ts_occurences,
total_count=1,
par_count=1,
last_par=0,
last_tok_seq=None,
",[],0,[],/tokenize/texttiling.py___init__
5053,/home/amandapotts/git/nltk/nltk/tokenize/texttiling.py___init__,"def __init__(self, index, wrdindex_list, original_length=None):
original_length = original_length or len(wrdindex_list)
self.__dict__.update(locals())
del self.__dict__[""self""]
",[],0,[],/tokenize/texttiling.py___init__
5054,/home/amandapotts/git/nltk/nltk/tokenize/texttiling.py_smooth,"def smooth(x, window_len=11, window=""flat""):
""""""smooth the data using a window with requested size.
This method is based on the convolution of a scaled window with the signal.
The signal is prepared by introducing reflected copies of the signal
(with the window size) in both ends so that transient parts are minimized
in the beginning and end part of the output signal.
:param x: the input signal
:param window_len: the dimension of the smoothing window
:param window: the type of window from 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'
flat window will produce a moving average smoothing.
:return: the smoothed signal
example::
t=linspace(-2,2,0.1)
x=sin(t)+randn(len(t))*0.1
y=smooth(x)
:see also: numpy.hanning, numpy.hamming, numpy.bartlett, numpy.blackman, numpy.convolve,
scipy.signal.lfilter
TODO: the window parameter could be the window itself if an array instead of a string
""""""
if x.ndim != 1:
raise ValueError(""smooth only accepts 1 dimension arrays."")
if x.size < window_len:
raise ValueError(""Input vector needs to be bigger than window size."")
if window_len < 3:
return x
if window not in [""flat"", ""hanning"", ""hamming"", ""bartlett"", ""blackman""]:
raise ValueError(
""Window is on of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'""
)
s = numpy.r_[2 * x[0] - x[window_len:1:-1], x, 2 * x[-1] - x[-1:-window_len:-1]]
if window == ""flat"":  # moving average
w = numpy.ones(window_len, ""d"")
else:
w = eval(""numpy."" + window + ""(window_len)"")
y = numpy.convolve(w / w.sum(), s, mode=""same"")
return y[window_len - 1 : -window_len + 1]
",[],0,[],/tokenize/texttiling.py_smooth
5055,/home/amandapotts/git/nltk/nltk/tokenize/texttiling.py_demo,"def demo(text=None):
from matplotlib import pylab
from nltk.corpus import brown
tt = TextTilingTokenizer(demo_mode=True)
if text is None:
text = brown.raw()[:10000]
s, ss, d, b = tt.tokenize(text)
pylab.xlabel(""Sentence Gap index"")
pylab.ylabel(""Gap Scores"")
pylab.plot(range(len(s)), s, label=""Gap Scores"")
pylab.plot(range(len(ss)), ss, label=""Smoothed Gap scores"")
pylab.plot(range(len(d)), d, label=""Depth scores"")
pylab.stem(range(len(b)), b)
pylab.legend()
pylab.show()
",[],0,[],/tokenize/texttiling.py_demo
5056,/home/amandapotts/git/nltk/nltk/tokenize/legality_principle.py___init__,"def __init__(
self, tokenized_source_text, vowels=""aeiouy"", legal_frequency_threshold=0.001
",[],0,[],/tokenize/legality_principle.py___init__
5057,/home/amandapotts/git/nltk/nltk/tokenize/legality_principle.py_find_legal_onsets,"def find_legal_onsets(self, words):
""""""
Gathers all onsets and then return only those above the frequency threshold
:param words: List of words in a language
:type words: list(str)
:return: Set of legal onsets
:rtype: set(str)
""""""
onsets = [self.onset(word) for word in words]
legal_onsets = [
k
for k, v in Counter(onsets).items()
if (v / len(onsets)) > self.legal_frequency_threshold
]
return set(legal_onsets)
",[],0,[],/tokenize/legality_principle.py_find_legal_onsets
5058,/home/amandapotts/git/nltk/nltk/tokenize/legality_principle.py_onset,"def onset(self, word):
""""""
Returns consonant cluster of word, i.e. all characters until the first vowel.
:param word: Single word or token
:type word: str
:return: String of characters of onset
:rtype: str
""""""
onset = """"
for c in word.lower():
if c in self.vowels:
return onset
else:
onset += c
return onset
",[],0,[],/tokenize/legality_principle.py_onset
5059,/home/amandapotts/git/nltk/nltk/tokenize/legality_principle.py_tokenize,"def tokenize(self, token):
""""""
Apply the Legality Principle in combination with
Onset Maximization to return a list of syllables.
:param token: Single word or token
:type token: str
:return syllable_list: Single word or token broken up into syllables.
:rtype: list(str)
""""""
syllables = []
syllable, current_onset = """", """"
vowel, onset = False, False
for char in token[::-1]:
char_lower = char.lower()
if not vowel:
syllable += char
vowel = bool(char_lower in self.vowels)
else:
if char_lower + current_onset[::-1] in self.legal_onsets:
syllable += char
current_onset += char_lower
onset = True
elif char_lower in self.vowels and not onset:
syllable += char
current_onset += char_lower
else:
syllables.append(syllable)
syllable = char
current_onset = """"
vowel = bool(char_lower in self.vowels)
syllables.append(syllable)
syllables_ordered = [syllable[::-1] for syllable in syllables][::-1]
return syllables_ordered
",[],0,[],/tokenize/legality_principle.py_tokenize
5060,/home/amandapotts/git/nltk/nltk/tokenize/sexpr.py___init__,"def __init__(self, parens=""()"", strict=True):
if len(parens) != 2:
raise ValueError(""parens must contain exactly two strings"")
self._strict = strict
self._open_paren = parens[0]
self._close_paren = parens[1]
self._paren_regexp = re.compile(
f""{re.escape(parens[0])}|{re.escape(parens[1])}""
)
",[],0,[],/tokenize/sexpr.py___init__
5061,/home/amandapotts/git/nltk/nltk/tokenize/sexpr.py_tokenize,"def tokenize(self, text):
""""""
Return a list of s-expressions extracted from *text*.
For example:
>>> SExprTokenizer().tokenize('(a b (c d)) e f (g)')
['(a b (c d))', 'e', 'f', '(g)']
All parentheses are assumed to mark s-expressions.
(No special processing is done to exclude parentheses that occur
inside strings, or following backslash characters.)
If the given expression contains non-matching parentheses,
then the behavior of the tokenizer depends on the ``strict``
parameter to the constructor.  If ``strict`` is ``True``, then
raise a ``ValueError``.  If ``strict`` is ``False``, then any
unmatched close parentheses will be listed as their own
s-expression
parentheses will be listed as its own s-expression:
>>> SExprTokenizer(strict=False).tokenize('c) d) e (f (g')
['c', ')', 'd', ')', 'e', '(f (g']
:param text: the string to be tokenized
:type text: str or iter(str)
:rtype: iter(str)
""""""
result = []
pos = 0
depth = 0
for m in self._paren_regexp.finditer(text):
paren = m.group()
if depth == 0:
result += text[pos : m.start()].split()
pos = m.start()
if paren == self._open_paren:
depth += 1
if paren == self._close_paren:
if self._strict and depth == 0:
raise ValueError(""Un-matched close paren at char %d"" % m.start())
depth = max(0, depth - 1)
if depth == 0:
result.append(text[pos : m.end()])
pos = m.end()
if self._strict and depth > 0:
raise ValueError(""Un-matched open paren at char %d"" % pos)
if pos < len(text):
result.append(text[pos:])
return result
",[],0,[],/tokenize/sexpr.py_tokenize
5062,/home/amandapotts/git/nltk/nltk/tokenize/stanford.py___init__,"def __init__(
self,
path_to_jar=None,
encoding=""utf8"",
options=None,
verbose=False,
java_options=""-mx1000m"",
",[],0,[],/tokenize/stanford.py___init__
5063,/home/amandapotts/git/nltk/nltk/tokenize/stanford.py__parse_tokenized_output,"def _parse_tokenized_output(s):
return s.splitlines()
",[],0,[],/tokenize/stanford.py__parse_tokenized_output
5064,/home/amandapotts/git/nltk/nltk/tokenize/stanford.py_tokenize,"def tokenize(self, s):
""""""
Use stanford tokenizer's PTBTokenizer to tokenize multiple sentences.
""""""
cmd = [""edu.stanford.nlp.process.PTBTokenizer""]
return self._parse_tokenized_output(self._execute(cmd, s))
",[],0,[],/tokenize/stanford.py_tokenize
5065,/home/amandapotts/git/nltk/nltk/tokenize/stanford.py__execute,"def _execute(self, cmd, input_, verbose=False):
encoding = self._encoding
cmd.extend([""-charset"", encoding])
_options_cmd = self._options_cmd
if _options_cmd:
cmd.extend([""-options"", self._options_cmd])
default_options = "" "".join(_java_options)
config_java(options=self.java_options, verbose=verbose)
with tempfile.NamedTemporaryFile(mode=""wb"", delete=False) as input_file:
if isinstance(input_, str) and encoding:
input_ = input_.encode(encoding)
input_file.write(input_)
input_file.flush()
cmd.append(input_file.name)
stdout, stderr = java(
cmd, classpath=self._stanford_jar, stdout=PIPE, stderr=PIPE
)
stdout = stdout.decode(encoding)
os.unlink(input_file.name)
config_java(options=default_options, verbose=False)
return stdout
",[],0,[],/tokenize/stanford.py__execute
5066,/home/amandapotts/git/nltk/nltk/tokenize/api.py_tokenize,"def tokenize(self, s: str) -> List[str]:
""""""
Return a tokenized copy of *s*.
:rtype: List[str]
""""""
if overridden(self.tokenize_sents):
return self.tokenize_sents([s])[0]
",[],0,[],/tokenize/api.py_tokenize
5067,/home/amandapotts/git/nltk/nltk/tokenize/api.py_span_tokenize,"def span_tokenize(self, s: str) -> Iterator[Tuple[int, int]]:
""""""
Identify the tokens using integer offsets ``(start_i, end_i)``,
where ``s[start_i:end_i]`` is the corresponding token.
:rtype: Iterator[Tuple[int, int]]
""""""
raise NotImplementedError()
",[],0,[],/tokenize/api.py_span_tokenize
5068,/home/amandapotts/git/nltk/nltk/tokenize/api.py_tokenize_sents,"def tokenize_sents(self, strings: List[str]) -> List[List[str]]:
""""""
Apply ``self.tokenize()`` to each element of ``strings``.  I.e.:
return [self.tokenize(s) for s in strings]
:rtype: List[List[str]]
""""""
return [self.tokenize(s) for s in strings]
",[],0,[],/tokenize/api.py_tokenize_sents
5069,/home/amandapotts/git/nltk/nltk/tokenize/api.py_span_tokenize_sents,"def span_tokenize_sents(
self, strings: List[str]
",[],0,[],/tokenize/api.py_span_tokenize_sents
5070,/home/amandapotts/git/nltk/nltk/tokenize/api.py__string,"def _string(self):
raise NotImplementedError
",[],0,[],/tokenize/api.py__string
5071,/home/amandapotts/git/nltk/nltk/tokenize/api.py_tokenize,"def tokenize(self, s):
return s.split(self._string)
",[],0,[],/tokenize/api.py_tokenize
5072,/home/amandapotts/git/nltk/nltk/tokenize/api.py_span_tokenize,"def span_tokenize(self, s):
yield from string_span_tokenize(s, self._string)
",[],0,[],/tokenize/api.py_span_tokenize
5073,/home/amandapotts/git/nltk/nltk/tokenize/sonority_sequencing.py___init__,"def __init__(self, lang=""en"", sonority_hierarchy=False):
""""""
:param lang: Language parameter, default is English, 'en'
:type lang: str
:param sonority_hierarchy: Sonority hierarchy according to the
Sonority Sequencing Principle.
:type sonority_hierarchy: list(str)
""""""
if not sonority_hierarchy and lang == ""en"":
sonority_hierarchy = [
""aeiouy"",  # vowels.
""lmnrw"",  # nasals.
""zvsf"",  # fricatives.
""bcdgtkpqxhj"",  # stops.
]
self.vowels = sonority_hierarchy[0]
self.phoneme_map = {}
for i, level in enumerate(sonority_hierarchy):
for c in level:
sonority_level = len(sonority_hierarchy) - i
self.phoneme_map[c] = sonority_level
self.phoneme_map[c.upper()] = sonority_level
",[],0,[],/tokenize/sonority_sequencing.py___init__
5074,/home/amandapotts/git/nltk/nltk/tokenize/sonority_sequencing.py_assign_values,"def assign_values(self, token):
""""""
Assigns each phoneme its value from the sonority hierarchy.
Note: Sentence/text has to be tokenized first.
:param token: Single word or token
:type token: str
:return: List of tuples, first element is character/phoneme and
second is the soronity value.
:rtype: list(tuple(str, int))
""""""
syllables_values = []
for c in token:
try:
syllables_values.append((c, self.phoneme_map[c]))
except KeyError:
if c not in ""0123456789"" and c not in punctuation:
warnings.warn(
""Character not defined in sonority_hierarchy,""
"" assigning as vowel: '{}'"".format(c)
)
syllables_values.append((c, max(self.phoneme_map.values())))
if c not in self.vowels:
self.vowels += c
else:  # If it's a punctuation or numbers, assign -1.
syllables_values.append((c, -1))
return syllables_values
",[],0,[],/tokenize/sonority_sequencing.py_assign_values
5075,/home/amandapotts/git/nltk/nltk/tokenize/sonority_sequencing.py_validate_syllables,"def validate_syllables(self, syllable_list):
""""""
Ensures each syllable has at least one vowel.
If the following syllable doesn't have vowel, add it to the current one.
:param syllable_list: Single word or token broken up into syllables.
:type syllable_list: list(str)
:return: Single word or token broken up into syllables
(with added syllables if necessary)
:rtype: list(str)
""""""
valid_syllables = []
front = """"
vowel_pattern = re.compile(""|"".join(self.vowels))
for i, syllable in enumerate(syllable_list):
if syllable in punctuation:
valid_syllables.append(syllable)
continue
if not vowel_pattern.search(syllable):
if len(valid_syllables) == 0:
front += syllable
else:
valid_syllables = valid_syllables[:-1] + [
valid_syllables[-1] + syllable
]
else:
if len(valid_syllables) == 0:
valid_syllables.append(front + syllable)
else:
valid_syllables.append(syllable)
return valid_syllables
",[],0,[],/tokenize/sonority_sequencing.py_validate_syllables
5076,/home/amandapotts/git/nltk/nltk/tokenize/sonority_sequencing.py_tokenize,"def tokenize(self, token):
""""""
Apply the SSP to return a list of syllables.
Note: Sentence/text has to be tokenized first.
:param token: Single word or token
:type token: str
:return syllable_list: Single word or token broken up into syllables.
:rtype: list(str)
""""""
syllables_values = self.assign_values(token)
if sum(token.count(x) for x in self.vowels) <= 1:
return [token]
syllable_list = []
syllable = syllables_values[0][0]  # start syllable with first phoneme
for trigram in ngrams(syllables_values, n=3):
phonemes, values = zip(*trigram)
prev_value, focal_value, next_value = values
focal_phoneme = phonemes[1]
if focal_value == -1:  # If it's a punctuation, just break.
syllable_list.append(syllable)
syllable_list.append(focal_phoneme)
syllable = """"
elif prev_value >= focal_value == next_value:
syllable += focal_phoneme
syllable_list.append(syllable)
syllable = """"
elif prev_value > focal_value < next_value:
syllable_list.append(syllable)
syllable = """"
syllable += focal_phoneme
else:
syllable += focal_phoneme
syllable += syllables_values[-1][0]  # append last phoneme
syllable_list.append(syllable)
return self.validate_syllables(syllable_list)
",[],0,[],/tokenize/sonority_sequencing.py_tokenize
5077,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py___getstate__,"def __getstate__(self):
return 1
",[],0,[],/tokenize/punkt.py___getstate__
5078,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py___setstate__,"def __setstate__(self, state):
return 1
",[],0,[],/tokenize/punkt.py___setstate__
5079,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py__re_sent_end_chars,"def _re_sent_end_chars(self):
return ""[%s]"" % re.escape("""".join(self.sent_end_chars))
",[],0,[],/tokenize/punkt.py__re_sent_end_chars
5080,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py__re_non_word_chars,"def _re_non_word_chars(self):
return r""(?:[)\""
"""".join(set(self.sent_end_chars) - {"".""})
)
",[],0,[],/tokenize/punkt.py__re_non_word_chars
5081,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py__word_tokenizer_re,"def _word_tokenizer_re(self):
""""""Compiles and returns a regular expression for word tokenization""""""
try:
return self._re_word_tokenizer
except AttributeError:
self._re_word_tokenizer = re.compile(
self._word_tokenize_fmt
% {
""NonWord"": self._re_non_word_chars,
""MultiChar"": self._re_multi_char_punct,
""WordStart"": self._re_word_start,
},
re.UNICODE | re.VERBOSE,
)
return self._re_word_tokenizer
",[],0,[],/tokenize/punkt.py__word_tokenizer_re
5082,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py_word_tokenize,"def word_tokenize(self, s):
""""""Tokenize a string to split off punctuation other than periods""""""
return self._word_tokenizer_re().findall(s)
",[],0,[],/tokenize/punkt.py_word_tokenize
5083,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py_period_context_re,"def period_context_re(self):
""""""Compiles and returns a regular expression to find contexts
including possible sentence boundaries.""""""
try:
return self._re_period_context
except:
self._re_period_context = re.compile(
self._period_context_fmt
% {
""NonWord"": self._re_non_word_chars,
""SentEndChars"": self._re_sent_end_chars,
},
re.UNICODE | re.VERBOSE,
)
return self._re_period_context
",[],0,[],/tokenize/punkt.py_period_context_re
5084,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py__pair_iter,"def _pair_iter(iterator):
""""""
Yields pairs of tokens from the given iterator such that each input
token will appear as the first element in a yielded tuple. The last
pair will have None as its second element.
""""""
iterator = iter(iterator)
try:
prev = next(iterator)
except StopIteration:
return
for el in iterator:
yield (prev, el)
prev = el
yield (prev, None)
",[],0,[],/tokenize/punkt.py__pair_iter
5085,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py___init__,"def __init__(self):
self.abbrev_types = set()
""""""A set of word types for known abbreviations.""""""
self.collocations = set()
""""""A set of word type tuples for known common collocations
where the first word ends in a period.  E.g., ('S.', 'Bach')
is a common collocation in a text that discusses 'Johann
S. Bach'.  These count as negative evidence for sentence
boundaries.""""""
self.sent_starters = set()
""""""A set of word types for words that often appear at the
beginning of sentences.""""""
self.ortho_context = defaultdict(int)
""""""A dictionary mapping word types to the set of orthographic
contexts that word type appears in.  Contexts are represented
by adding orthographic context flags: ...""""""
",[],0,[],/tokenize/punkt.py___init__
5086,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py_clear_abbrevs,"def clear_abbrevs(self):
self.abbrev_types = set()
",[],0,[],/tokenize/punkt.py_clear_abbrevs
5087,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py_clear_collocations,"def clear_collocations(self):
self.collocations = set()
",[],0,[],/tokenize/punkt.py_clear_collocations
5088,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py_clear_sent_starters,"def clear_sent_starters(self):
self.sent_starters = set()
",[],0,[],/tokenize/punkt.py_clear_sent_starters
5089,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py_clear_ortho_context,"def clear_ortho_context(self):
self.ortho_context = defaultdict(int)
",[],0,[],/tokenize/punkt.py_clear_ortho_context
5090,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py_add_ortho_context,"def add_ortho_context(self, typ, flag):
self.ortho_context[typ] |= flag
",[],0,[],/tokenize/punkt.py_add_ortho_context
5091,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py__debug_ortho_context,"def _debug_ortho_context(self, typ):
context = self.ortho_context[typ]
if context & _ORTHO_BEG_UC:
yield ""BEG-UC""
if context & _ORTHO_MID_UC:
yield ""MID-UC""
if context & _ORTHO_UNK_UC:
yield ""UNK-UC""
if context & _ORTHO_BEG_LC:
yield ""BEG-LC""
if context & _ORTHO_MID_LC:
yield ""MID-LC""
if context & _ORTHO_UNK_LC:
yield ""UNK-LC""
",[],0,[],/tokenize/punkt.py__debug_ortho_context
5092,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py___init__,"def __init__(self, tok, **params):
self.tok = tok
self.type = self._get_type(tok)
self.period_final = tok.endswith(""."")
for prop in self._properties:
setattr(self, prop, None)
for k in params:
setattr(self, k, params[k])
",[],0,[],/tokenize/punkt.py___init__
5093,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py__get_type,"def _get_type(self, tok):
""""""Returns a case-normalized representation of the token.""""""
return self._RE_NUMERIC.sub(""##number##"", tok.lower())
",[],0,[],/tokenize/punkt.py__get_type
5094,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py_type_no_period,"def type_no_period(self):
""""""
The type with its final period removed if it has one.
""""""
if len(self.type) > 1 and self.type[-1] == ""."":
return self.type[:-1]
return self.type
",[],0,[],/tokenize/punkt.py_type_no_period
5095,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py_type_no_sentperiod,"def type_no_sentperiod(self):
""""""
The type with its final period removed if it is marked as a
sentence break.
""""""
if self.sentbreak:
return self.type_no_period
return self.type
",[],0,[],/tokenize/punkt.py_type_no_sentperiod
5096,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py_first_upper,"def first_upper(self):
""""""True if the token's first character is uppercase.""""""
return self.tok[0].isupper()
",[],0,[],/tokenize/punkt.py_first_upper
5097,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py_first_lower,"def first_lower(self):
""""""True if the token's first character is lowercase.""""""
return self.tok[0].islower()
",[],0,[],/tokenize/punkt.py_first_lower
5098,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py_first_case,"def first_case(self):
if self.first_lower:
return ""lower""
if self.first_upper:
return ""upper""
return ""none""
",[],0,[],/tokenize/punkt.py_first_case
5099,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py_is_ellipsis,"def is_ellipsis(self):
""""""True if the token text is that of an ellipsis.""""""
return self._RE_ELLIPSIS.match(self.tok)
",[],0,[],/tokenize/punkt.py_is_ellipsis
5100,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py_is_number,"def is_number(self):
""""""True if the token text is that of a number.""""""
return self.type.startswith(""##number##"")
",[],0,[],/tokenize/punkt.py_is_number
5101,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py_is_initial,"def is_initial(self):
""""""True if the token text is that of an initial.""""""
return self._RE_INITIAL.match(self.tok)
",[],0,[],/tokenize/punkt.py_is_initial
5102,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py_is_alpha,"def is_alpha(self):
""""""True if the token text is all alphabetic.""""""
return self._RE_ALPHA.match(self.tok)
",[],0,[],/tokenize/punkt.py_is_alpha
5103,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py_is_non_punct,"def is_non_punct(self):
""""""True if the token is either a number or is alphabetic.""""""
return _re_non_punct.search(self.type)
",[],0,[],/tokenize/punkt.py_is_non_punct
5104,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py___repr__,"def __repr__(self):
""""""
A string representation of the token that can reproduce it
with eval(), which lists all the token's non-default
annotations.
""""""
typestr = "" type=%s,"" % repr(self.type) if self.type != self.tok else """"
propvals = "", "".join(
f""{p}={repr(getattr(self, p))}""
for p in self._properties
if getattr(self, p)
)
return ""{}({},{} {})"".format(
self.__class__.__name__,
repr(self.tok),
typestr,
propvals,
)
",[],0,[],/tokenize/punkt.py___repr__
5105,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py___str__,"def __str__(self):
""""""
A string representation akin to that used by Kiss and Strunk.
""""""
res = self.tok
if self.abbr:
res += ""<A>""
if self.ellipsis:
res += ""<E>""
if self.sentbreak:
res += ""<S>""
return res
",[],0,[],/tokenize/punkt.py___str__
5106,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py___init__,"def __init__(self, lang_vars=None, token_cls=PunktToken, params=None):
if lang_vars is None:
lang_vars = PunktLanguageVars()
if params is None:
params = PunktParameters()
self._params = params
self._lang_vars = lang_vars
self._Token = token_cls
""""""The collection of parameters that determines the behavior
of the punkt tokenizer.""""""
",[],0,[],/tokenize/punkt.py___init__
5107,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py__tokenize_words,"def _tokenize_words(self, plaintext):
""""""
Divide the given text into tokens, using the punkt word
segmentation regular expression, and generate the resulting list
of tokens augmented as three-tuples with two boolean values for whether
the given token occurs at the start of a paragraph or a new line,
respectively.
""""""
parastart = False
for line in plaintext.split(""\n""):
if line.strip():
line_toks = iter(self._lang_vars.word_tokenize(line))
try:
tok = next(line_toks)
except StopIteration:
continue
yield self._Token(tok, parastart=parastart, linestart=True)
parastart = False
for tok in line_toks:
yield self._Token(tok)
else:
parastart = True
",[],0,[],/tokenize/punkt.py__tokenize_words
5108,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py__annotate_first_pass,"def _annotate_first_pass(
self, tokens: Iterator[PunktToken]
",[],0,[],/tokenize/punkt.py__annotate_first_pass
5109,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py__first_pass_annotation,"def _first_pass_annotation(self, aug_tok: PunktToken) -> None:
""""""
Performs type-based annotation on a single token.
""""""
tok = aug_tok.tok
if tok in self._lang_vars.sent_end_chars:
aug_tok.sentbreak = True
elif aug_tok.is_ellipsis:
aug_tok.ellipsis = True
elif aug_tok.period_final and not tok.endswith(""..""):
if (
tok[:-1].lower() in self._params.abbrev_types
or tok[:-1].lower().split(""-"")[-1] in self._params.abbrev_types
):
aug_tok.abbr = True
else:
aug_tok.sentbreak = True
return
",[],0,[],/tokenize/punkt.py__first_pass_annotation
5110,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py___init__,"def __init__(
self, train_text=None, verbose=False, lang_vars=None, token_cls=PunktToken
",[],0,[],/tokenize/punkt.py___init__
5111,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py_get_params,"def get_params(self):
""""""
Calculates and returns parameters for sentence boundary detection as
derived from training.""""""
if not self._finalized:
self.finalize_training()
return self._params
",[],0,[],/tokenize/punkt.py_get_params
5112,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py_train,"def train(self, text, verbose=False, finalize=True):
""""""
Collects training data from a given text. If finalize is True, it
will determine all the parameters for sentence boundary detection. If
not, this will be delayed until get_params() or finalize_training() is
called. If verbose is True, abbreviations found will be listed.
""""""
self._train_tokens(self._tokenize_words(text), verbose)
if finalize:
self.finalize_training(verbose)
",[],0,[],/tokenize/punkt.py_train
5113,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py_train_tokens,"def train_tokens(self, tokens, verbose=False, finalize=True):
""""""
Collects training data from a given list of tokens.
""""""
self._train_tokens((self._Token(t) for t in tokens), verbose)
if finalize:
self.finalize_training(verbose)
",[],0,[],/tokenize/punkt.py_train_tokens
5114,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py__train_tokens,"def _train_tokens(self, tokens, verbose):
self._finalized = False
tokens = list(tokens)
for aug_tok in tokens:
self._type_fdist[aug_tok.type] += 1
if aug_tok.period_final:
self._num_period_toks += 1
unique_types = self._unique_types(tokens)
for abbr, score, is_add in self._reclassify_abbrev_types(unique_types):
if score >= self.ABBREV:
if is_add:
self._params.abbrev_types.add(abbr)
if verbose:
print(f""  Abbreviation: [{score:6.4f}] {abbr}"")
else:
if not is_add:
self._params.abbrev_types.remove(abbr)
if verbose:
print(f""  Removed abbreviation: [{score:6.4f}] {abbr}"")
tokens = list(self._annotate_first_pass(tokens))
self._get_orthography_data(tokens)
self._sentbreak_count += self._get_sentbreak_count(tokens)
for aug_tok1, aug_tok2 in _pair_iter(tokens):
if not aug_tok1.period_final or not aug_tok2:
continue
if self._is_rare_abbrev_type(aug_tok1, aug_tok2):
self._params.abbrev_types.add(aug_tok1.type_no_period)
if verbose:
print(""  Rare Abbrev: %s"" % aug_tok1.type)
if self._is_potential_sent_starter(aug_tok2, aug_tok1):
self._sent_starter_fdist[aug_tok2.type] += 1
if self._is_potential_collocation(aug_tok1, aug_tok2):
self._collocation_fdist[
(aug_tok1.type_no_period, aug_tok2.type_no_sentperiod)
] += 1
",[],0,[],/tokenize/punkt.py__train_tokens
5115,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py__unique_types,"def _unique_types(self, tokens):
return {aug_tok.type for aug_tok in tokens}
",[],0,[],/tokenize/punkt.py__unique_types
5116,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py_finalize_training,"def finalize_training(self, verbose=False):
""""""
Uses data that has been gathered in training to determine likely
collocations and sentence starters.
""""""
self._params.clear_sent_starters()
for typ, log_likelihood in self._find_sent_starters():
self._params.sent_starters.add(typ)
if verbose:
print(f""  Sent Starter: [{log_likelihood:6.4f}] {typ!r}"")
self._params.clear_collocations()
for (typ1, typ2), log_likelihood in self._find_collocations():
self._params.collocations.add((typ1, typ2))
if verbose:
print(f""  Collocation: [{log_likelihood:6.4f}] {typ1!r}+{typ2!r}"")
self._finalized = True
",[],0,[],/tokenize/punkt.py_finalize_training
5117,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py_freq_threshold,"def freq_threshold(
self, ortho_thresh=2, type_thresh=2, colloc_thres=2, sentstart_thresh=2
",[],0,[],/tokenize/punkt.py_freq_threshold
5118,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py__freq_threshold,"def _freq_threshold(self, fdist, threshold):
""""""
Returns a FreqDist containing only data with counts below a given
threshold, as well as a mapping (None -> count_removed).
""""""
res = FreqDist()
num_removed = 0
for tok in fdist:
count = fdist[tok]
if count < threshold:
num_removed += 1
else:
res[tok] += count
res[None] += num_removed
return res
",[],0,[],/tokenize/punkt.py__freq_threshold
5119,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py__get_orthography_data,"def _get_orthography_data(self, tokens):
""""""
Collect information about whether each token type occurs
with different case patterns (i) overall, (ii) at
sentence-initial positions, and (iii) at sentence-internal
positions.
""""""
context = ""internal""
tokens = list(tokens)
for aug_tok in tokens:
if aug_tok.parastart and context != ""unknown"":
context = ""initial""
if aug_tok.linestart and context == ""internal"":
context = ""unknown""
typ = aug_tok.type_no_sentperiod
flag = _ORTHO_MAP.get((context, aug_tok.first_case), 0)
if flag:
self._params.add_ortho_context(typ, flag)
if aug_tok.sentbreak:
if not (aug_tok.is_number or aug_tok.is_initial):
context = ""initial""
else:
context = ""unknown""
elif aug_tok.ellipsis or aug_tok.abbr:
context = ""unknown""
else:
context = ""internal""
",[],0,[],/tokenize/punkt.py__get_orthography_data
5120,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py__reclassify_abbrev_types,"def _reclassify_abbrev_types(self, types):
""""""
(Re)classifies each given token if
- it is period-final and not a known abbreviation
- it is not period-final and is otherwise a known abbreviation
by checking whether its previous classification still holds according
to the heuristics of section 3.
Yields triples (abbr, score, is_add) where abbr is the type in question,
score is its log-likelihood with penalties applied, and is_add specifies
whether the present type is a candidate for inclusion or exclusion as an
abbreviation, such that:
- (is_add and score >= 0.3)    suggests a new abbreviation
- (not is_add and score < 0.3) suggests excluding an abbreviation.
""""""
for typ in types:
if not _re_non_punct.search(typ) or typ == ""##number##"":
continue
if typ.endswith("".""):
if typ in self._params.abbrev_types:
continue
typ = typ[:-1]
is_add = True
else:
if typ not in self._params.abbrev_types:
continue
is_add = False
num_periods = typ.count(""."") + 1
num_nonperiods = len(typ) - num_periods + 1
count_with_period = self._type_fdist[typ + "".""]
count_without_period = self._type_fdist[typ]
log_likelihood = self._dunning_log_likelihood(
count_with_period + count_without_period,
self._num_period_toks,
count_with_period,
self._type_fdist.N(),
)
f_length = math.exp(-num_nonperiods)
f_periods = num_periods
f_penalty = int(self.IGNORE_ABBREV_PENALTY) or math.pow(
num_nonperiods, -count_without_period
)
score = log_likelihood * f_length * f_periods * f_penalty
yield typ, score, is_add
",[],0,[],/tokenize/punkt.py__reclassify_abbrev_types
5121,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py_find_abbrev_types,"def find_abbrev_types(self):
""""""
Recalculates abbreviations given type frequencies, despite no prior
determination of abbreviations.
This fails to include abbreviations otherwise found as ""rare"".
""""""
self._params.clear_abbrevs()
tokens = (typ for typ in self._type_fdist if typ and typ.endswith("".""))
for abbr, score, _is_add in self._reclassify_abbrev_types(tokens):
if score >= self.ABBREV:
self._params.abbrev_types.add(abbr)
",[],0,[],/tokenize/punkt.py_find_abbrev_types
5122,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py__is_rare_abbrev_type,"def _is_rare_abbrev_type(self, cur_tok, next_tok):
""""""
A word type is counted as a rare abbreviation if...
- it's not already marked as an abbreviation
- it occurs fewer than ABBREV_BACKOFF times
- either it is followed by a sentence-internal punctuation
mark, *or* it is followed by a lower-case word that
sometimes appears with upper case, but never occurs with
lower case at the beginning of sentences.
""""""
if cur_tok.abbr or not cur_tok.sentbreak:
return False
typ = cur_tok.type_no_sentperiod
count = self._type_fdist[typ] + self._type_fdist[typ[:-1]]
if typ in self._params.abbrev_types or count >= self.ABBREV_BACKOFF:
return False
if next_tok.tok[:1] in self._lang_vars.internal_punctuation:
return True
if next_tok.first_lower:
typ2 = next_tok.type_no_sentperiod
typ2ortho_context = self._params.ortho_context[typ2]
if (typ2ortho_context & _ORTHO_BEG_UC) and not (
typ2ortho_context & _ORTHO_MID_UC
):
return True
",[],0,[],/tokenize/punkt.py__is_rare_abbrev_type
5123,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py__dunning_log_likelihood,"def _dunning_log_likelihood(count_a, count_b, count_ab, N):
""""""
A function that calculates the modified Dunning log-likelihood
ratio scores for abbreviation candidates.  The details of how
this works is available in the paper.
""""""
p1 = count_b / N
p2 = 0.99
null_hypo = count_ab * math.log(p1 + 1e-8) + (count_a - count_ab) * math.log(
1.0 - p1 + 1e-8
)
alt_hypo = count_ab * math.log(p2) + (count_a - count_ab) * math.log(1.0 - p2)
likelihood = null_hypo - alt_hypo
return -2.0 * likelihood
",[],0,[],/tokenize/punkt.py__dunning_log_likelihood
5124,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py__col_log_likelihood,"def _col_log_likelihood(count_a, count_b, count_ab, N):
""""""
A function that will just compute log-likelihood estimate, in
the original paper it's described in algorithm 6 and 7.
This *should* be the original Dunning log-likelihood values,
unlike the previous log_l function where it used modified
Dunning log-likelihood values
""""""
p = count_b / N
p1 = count_ab / count_a
try:
p2 = (count_b - count_ab) / (N - count_a)
except ZeroDivisionError:
p2 = 1
try:
summand1 = count_ab * math.log(p) + (count_a - count_ab) * math.log(1.0 - p)
except ValueError:
summand1 = 0
try:
summand2 = (count_b - count_ab) * math.log(p) + (
N - count_a - count_b + count_ab
) * math.log(1.0 - p)
except ValueError:
summand2 = 0
if count_a == count_ab or p1 <= 0 or p1 >= 1:
summand3 = 0
else:
summand3 = count_ab * math.log(p1) + (count_a - count_ab) * math.log(
1.0 - p1
)
if count_b == count_ab or p2 <= 0 or p2 >= 1:
summand4 = 0
else:
summand4 = (count_b - count_ab) * math.log(p2) + (
N - count_a - count_b + count_ab
) * math.log(1.0 - p2)
likelihood = summand1 + summand2 - summand3 - summand4
return -2.0 * likelihood
",[],0,[],/tokenize/punkt.py__col_log_likelihood
5125,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py__is_potential_collocation,"def _is_potential_collocation(self, aug_tok1, aug_tok2):
""""""
Returns True if the pair of tokens may form a collocation given
log-likelihood statistics.
""""""
return (
(
self.INCLUDE_ALL_COLLOCS
or (self.INCLUDE_ABBREV_COLLOCS and aug_tok1.abbr)
or (aug_tok1.sentbreak and (aug_tok1.is_number or aug_tok1.is_initial))
)
and aug_tok1.is_non_punct
and aug_tok2.is_non_punct
)
",[],0,[],/tokenize/punkt.py__is_potential_collocation
5126,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py__find_collocations,"def _find_collocations(self):
""""""
Generates likely collocations and their log-likelihood.
""""""
for types in self._collocation_fdist:
try:
typ1, typ2 = types
except TypeError:
continue
if typ2 in self._params.sent_starters:
continue
col_count = self._collocation_fdist[types]
typ1_count = self._type_fdist[typ1] + self._type_fdist[typ1 + "".""]
typ2_count = self._type_fdist[typ2] + self._type_fdist[typ2 + "".""]
if (
typ1_count > 1
and typ2_count > 1
and self.MIN_COLLOC_FREQ < col_count <= min(typ1_count, typ2_count)
):
log_likelihood = self._col_log_likelihood(
typ1_count, typ2_count, col_count, self._type_fdist.N()
)
if log_likelihood >= self.COLLOCATION and (
self._type_fdist.N() / typ1_count > typ2_count / col_count
):
yield (typ1, typ2), log_likelihood
",[],0,[],/tokenize/punkt.py__find_collocations
5127,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py__is_potential_sent_starter,"def _is_potential_sent_starter(self, cur_tok, prev_tok):
""""""
Returns True given a token and the token that precedes it if it
seems clear that the token is beginning a sentence.
""""""
return (
prev_tok.sentbreak
and not (prev_tok.is_number or prev_tok.is_initial)
and cur_tok.is_alpha
)
",[],0,[],/tokenize/punkt.py__is_potential_sent_starter
5128,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py__find_sent_starters,"def _find_sent_starters(self):
""""""
Uses collocation heuristics for each candidate token to
determine if it frequently starts sentences.
""""""
for typ in self._sent_starter_fdist:
if not typ:
continue
typ_at_break_count = self._sent_starter_fdist[typ]
typ_count = self._type_fdist[typ] + self._type_fdist[typ + "".""]
if typ_count < typ_at_break_count:
continue
log_likelihood = self._col_log_likelihood(
self._sentbreak_count,
typ_count,
typ_at_break_count,
self._type_fdist.N(),
)
if (
log_likelihood >= self.SENT_STARTER
and self._type_fdist.N() / self._sentbreak_count
> typ_count / typ_at_break_count
):
yield typ, log_likelihood
",[],0,[],/tokenize/punkt.py__find_sent_starters
5129,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py__get_sentbreak_count,"def _get_sentbreak_count(self, tokens):
""""""
Returns the number of sentence breaks marked in a given set of
augmented tokens.
""""""
return sum(1 for aug_tok in tokens if aug_tok.sentbreak)
",[],0,[],/tokenize/punkt.py__get_sentbreak_count
5130,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py___init__,"def __init__(
self, train_text=None, verbose=False, lang_vars=None, token_cls=PunktToken
",[],0,[],/tokenize/punkt.py___init__
5131,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py_train,"def train(self, train_text, verbose=False):
""""""
Derives parameters from a given training text, or uses the parameters
given. Repeated calls to this method destroy previous parameters. For
incremental training, instantiate a separate PunktTrainer instance.
""""""
if not isinstance(train_text, str):
return train_text
return PunktTrainer(
train_text, lang_vars=self._lang_vars, token_cls=self._Token
).get_params()
",[],0,[],/tokenize/punkt.py_train
5132,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py_tokenize,"def tokenize(self, text: str, realign_boundaries: bool = True) -> List[str]:
""""""
Given a text, returns a list of the sentences in that text.
""""""
return list(self.sentences_from_text(text, realign_boundaries))
",[],0,[],/tokenize/punkt.py_tokenize
5133,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py_debug_decisions,"def debug_decisions(self, text: str) -> Iterator[Dict[str, Any]]:
""""""
Classifies candidate periods as sentence breaks, yielding a dict for
each that may be used to understand why the decision was made.
See format_debug_decision() to help make this output readable.
""""""
for match, decision_text in self._match_potential_end_contexts(text):
tokens = self._tokenize_words(decision_text)
tokens = list(self._annotate_first_pass(tokens))
while tokens and not tokens[0].tok.endswith(self._lang_vars.sent_end_chars):
tokens.pop(0)
yield {
""period_index"": match.end() - 1,
""text"": decision_text,
""type1"": tokens[0].type,
""type2"": tokens[1].type,
""type1_in_abbrs"": bool(tokens[0].abbr),
""type1_is_initial"": bool(tokens[0].is_initial),
""type2_is_sent_starter"": tokens[1].type_no_sentperiod
in self._params.sent_starters,
""type2_ortho_heuristic"": self._ortho_heuristic(tokens[1]),
""type2_ortho_contexts"": set(
self._params._debug_ortho_context(tokens[1].type_no_sentperiod)
),
""collocation"": (
tokens[0].type_no_sentperiod,
tokens[1].type_no_sentperiod,
)
in self._params.collocations,
""reason"": self._second_pass_annotation(tokens[0], tokens[1])
or REASON_DEFAULT_DECISION,
""break_decision"": tokens[0].sentbreak,
}
",[],0,[],/tokenize/punkt.py_debug_decisions
5134,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py_span_tokenize,"def span_tokenize(
self, text: str, realign_boundaries: bool = True
",[],0,[],/tokenize/punkt.py_span_tokenize
5135,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py_sentences_from_text,"def sentences_from_text(
self, text: str, realign_boundaries: bool = True
",[],0,[],/tokenize/punkt.py_sentences_from_text
5136,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py__get_last_whitespace_index,"def _get_last_whitespace_index(self, text: str) -> int:
""""""
Given a text, find the index of the *last* occurrence of *any*
whitespace character, i.e. "" "", ""\n"", ""\t"", ""\r"", etc.
If none is found, return 0.
""""""
for i in range(len(text) - 1, -1, -1):
if text[i] in string.whitespace:
return i
return 0
",[],0,[],/tokenize/punkt.py__get_last_whitespace_index
5137,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py__match_potential_end_contexts,"def _match_potential_end_contexts(self, text: str) -> Iterator[Tuple[Match, str]]:
""""""
Given a text, find the matches of potential sentence breaks,
alongside the contexts surrounding these sentence breaks.
Since the fix for the ReDOS discovered in issue #2866, we no longer match
the word before a potential end of sentence token. Instead, we use a separate
regex for this. As a consequence, `finditer`'s desire to find non-overlapping
matches no longer aids us in finding the single longest match.
Where previously, we could use::
>>> pst = PunktSentenceTokenizer()
>>> text = ""Very bad acting!!! I promise.""
>>> list(pst._lang_vars.period_context_re().finditer(text)) # doctest: +SKIP
[<re.Match object
Now we have to find the word before (i.e. 'acting') separately, and `finditer`
returns::
>>> pst = PunktSentenceTokenizer()
>>> text = ""Very bad acting!!! I promise.""
>>> list(pst._lang_vars.period_context_re().finditer(text)) # doctest: +NORMALIZE_WHITESPACE
[<re.Match object
<re.Match object
<re.Match object
So, we need to find the word before the match from right to left, and then manually remove
the overlaps. That is what this method does::
>>> pst = PunktSentenceTokenizer()
>>> text = ""Very bad acting!!! I promise.""
>>> list(pst._match_potential_end_contexts(text))
[(<re.Match object
:param text: String of one or more sentences
:type text: str
:return: Generator of match-context tuples.
:rtype: Iterator[Tuple[Match, str]]
""""""
previous_slice = slice(0, 0)
previous_match = None
for match in self._lang_vars.period_context_re().finditer(text):
before_text = text[previous_slice.stop : match.start()]
index_after_last_space = self._get_last_whitespace_index(before_text)
if index_after_last_space:
index_after_last_space += previous_slice.stop + 1
else:
index_after_last_space = previous_slice.start
prev_word_slice = slice(index_after_last_space, match.start())
if previous_match and previous_slice.stop <= prev_word_slice.start:
yield (
previous_match,
text[previous_slice]
+ previous_match.group()
+ previous_match.group(""after_tok""),
)
previous_match = match
previous_slice = prev_word_slice
if previous_match:
yield (
previous_match,
text[previous_slice]
+ previous_match.group()
+ previous_match.group(""after_tok""),
)
",[],0,[],/tokenize/punkt.py__match_potential_end_contexts
5138,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py__slices_from_text,"def _slices_from_text(self, text: str) -> Iterator[slice]:
last_break = 0
for match, context in self._match_potential_end_contexts(text):
if self.text_contains_sentbreak(context):
yield slice(last_break, match.end())
if match.group(""next_tok""):
last_break = match.start(""next_tok"")
else:
last_break = match.end()
yield slice(last_break, len(text.rstrip()))
",[],0,[],/tokenize/punkt.py__slices_from_text
5139,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py__realign_boundaries,"def _realign_boundaries(
self, text: str, slices: Iterator[slice]
",[],0,[],/tokenize/punkt.py__realign_boundaries
5140,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py_text_contains_sentbreak,"def text_contains_sentbreak(self, text: str) -> bool:
""""""
Returns True if the given text includes a sentence break.
""""""
found = False  # used to ignore last token
for tok in self._annotate_tokens(self._tokenize_words(text)):
if found:
return True
if tok.sentbreak:
found = True
return False
",[],0,[],/tokenize/punkt.py_text_contains_sentbreak
5141,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py_sentences_from_text_legacy,"def sentences_from_text_legacy(self, text: str) -> Iterator[str]:
""""""
Given a text, generates the sentences in that text. Annotates all
tokens, rather than just those with possible sentence breaks. Should
produce the same results as ``sentences_from_text``.
""""""
tokens = self._annotate_tokens(self._tokenize_words(text))
return self._build_sentence_list(text, tokens)
",[],0,[],/tokenize/punkt.py_sentences_from_text_legacy
5142,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py_sentences_from_tokens,"def sentences_from_tokens(
self, tokens: Iterator[PunktToken]
",[],0,[],/tokenize/punkt.py_sentences_from_tokens
5143,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py__annotate_tokens,"def _annotate_tokens(self, tokens: Iterator[PunktToken]) -> Iterator[PunktToken]:
""""""
Given a set of tokens augmented with markers for line-start and
paragraph-start, returns an iterator through those tokens with full
annotation including predicted sentence breaks.
""""""
tokens = self._annotate_first_pass(tokens)
tokens = self._annotate_second_pass(tokens)
return tokens
",[],0,[],/tokenize/punkt.py__annotate_tokens
5144,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py__build_sentence_list,"def _build_sentence_list(
self, text: str, tokens: Iterator[PunktToken]
",[],0,[],/tokenize/punkt.py__build_sentence_list
5145,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py_dump,"def dump(self, tokens: Iterator[PunktToken]) -> None:
print(""writing to /tmp/punkt.new..."")
with open(""/tmp/punkt.new"", ""w"") as outfile:
for aug_tok in tokens:
if aug_tok.parastart:
outfile.write(""\n\n"")
elif aug_tok.linestart:
outfile.write(""\n"")
else:
outfile.write("" "")
outfile.write(str(aug_tok))
",[],0,[],/tokenize/punkt.py_dump
5146,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py__annotate_second_pass,"def _annotate_second_pass(
self, tokens: Iterator[PunktToken]
",[],0,[],/tokenize/punkt.py__annotate_second_pass
5147,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py__second_pass_annotation,"def _second_pass_annotation(
self, aug_tok1: PunktToken, aug_tok2: Optional[PunktToken]
",[],0,[],/tokenize/punkt.py__second_pass_annotation
5148,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py__ortho_heuristic,"def _ortho_heuristic(self, aug_tok: PunktToken) -> Union[bool, str]:
""""""
Decide whether the given token is the first token in a sentence.
""""""
if aug_tok.tok in self.PUNCTUATION:
return False
ortho_context = self._params.ortho_context[aug_tok.type_no_sentperiod]
if (
aug_tok.first_upper
and (ortho_context & _ORTHO_LC)
and not (ortho_context & _ORTHO_MID_UC)
):
return True
if aug_tok.first_lower and (
(ortho_context & _ORTHO_UC) or not (ortho_context & _ORTHO_BEG_LC)
):
return False
return ""unknown""
",[],0,[],/tokenize/punkt.py__ortho_heuristic
5149,/home/amandapotts/git/nltk/nltk/tokenize/punkt.py_format_debug_decision,"def format_debug_decision(d):
return DEBUG_DECISION_FMT.format(**d)
",[],0,[],/tokenize/punkt.py_format_debug_decision
5150,/home/amandapotts/git/nltk/nltk/tokenize/destructive.py_tokenize,"def tokenize(
self, text: str, convert_parentheses: bool = False, return_str: bool = False
",[],0,[],/tokenize/destructive.py_tokenize
5151,/home/amandapotts/git/nltk/nltk/tokenize/destructive.py_span_tokenize,"def span_tokenize(self, text: str) -> Iterator[Tuple[int, int]]:
r""""""
Returns the spans of the tokens in ``text``.
Uses the post-hoc nltk.tokens.align_tokens to return the offset spans.
>>> from nltk.tokenize import NLTKWordTokenizer
>>> s = '''Good muffins cost $3.88\nin New (York).  Please (buy) me\ntwo of them.\n(Thanks).'''
>>> expected = [(0, 4), (5, 12), (13, 17), (18, 19), (19, 23),
... (24, 26), (27, 30), (31, 32), (32, 36), (36, 37), (37, 38),
... (40, 46), (47, 48), (48, 51), (51, 52), (53, 55), (56, 59),
... (60, 62), (63, 68), (69, 70), (70, 76), (76, 77), (77, 78)]
>>> list(NLTKWordTokenizer().span_tokenize(s)) == expected
True
>>> expected = ['Good', 'muffins', 'cost', '$', '3.88', 'in',
... 'New', '(', 'York', ')', '.', 'Please', '(', 'buy', ')',
... 'me', 'two', 'of', 'them.', '(', 'Thanks', ')', '.']
>>> [s[start:end] for start, end in NLTKWordTokenizer().span_tokenize(s)] == expected
True
:param text: A string with a sentence or sentences.
:type text: str
:yield: Tuple[int, int]
""""""
raw_tokens = self.tokenize(text)
if ('""' in text) or (""''"" in text):
matched = [m.group() for m in re.finditer(r""``|'{2}|\"""", text)]
tokens = [
matched.pop(0) if tok in ['""', ""``"", ""''""] else tok
for tok in raw_tokens
]
else:
tokens = raw_tokens
yield from align_tokens(tokens, text)
",[],0,[],/tokenize/destructive.py_span_tokenize
5152,/home/amandapotts/git/nltk/nltk/tokenize/mwe.py___init__,"def __init__(self, mwes=None, separator=""_""):
""""""Initialize the multi-word tokenizer with a list of expressions and a
separator
:type mwes: list(list(str))
:param mwes: A sequence of multi-word expressions to be merged, where
each MWE is a sequence of strings.
:type separator: str
:param separator: String that should be inserted between words in a multi-word
expression token. (Default is '_')
""""""
if not mwes:
mwes = []
self._mwes = Trie(mwes)
self._separator = separator
",[],0,[],/tokenize/mwe.py___init__
5153,/home/amandapotts/git/nltk/nltk/tokenize/mwe.py_add_mwe,"def add_mwe(self, mwe):
""""""Add a multi-word expression to the lexicon (stored as a word trie)
We use ``util.Trie`` to represent the trie. Its form is a dict of dicts.
The key True marks the end of a valid MWE.
:param mwe: The multi-word expression we're adding into the word trie
:type mwe: tuple(str) or list(str)
:Example:
>>> tokenizer = MWETokenizer()
>>> tokenizer.add_mwe(('a', 'b'))
>>> tokenizer.add_mwe(('a', 'b', 'c'))
>>> tokenizer.add_mwe(('a', 'x'))
>>> expected = {'a': {'x': {True: None}, 'b': {True: None, 'c': {True: None}}}}
>>> tokenizer._mwes == expected
True
""""""
self._mwes.insert(mwe)
",[],0,[],/tokenize/mwe.py_add_mwe
5154,/home/amandapotts/git/nltk/nltk/tokenize/mwe.py_tokenize,"def tokenize(self, text):
""""""
:param text: A list containing tokenized text
:type text: list(str)
:return: A list of the tokenized text with multi-words merged together
:rtype: list(str)
:Example:
>>> tokenizer = MWETokenizer([('hors', ""d'oeuvre"")], separator='+')
>>> tokenizer.tokenize(""An hors d'oeuvre tonight, sir?"".split())
['An', ""hors+d'oeuvre"", 'tonight,', 'sir?']
""""""
i = 0
n = len(text)
result = []
while i < n:
if text[i] in self._mwes:
j = i
trie = self._mwes
last_match = -1
while j < n and text[j] in trie:  # and len(trie[text[j]]) > 0 :
trie = trie[text[j]]
j = j + 1
if Trie.LEAF in trie:
last_match = j
else:
if last_match > -1:
j = last_match
if Trie.LEAF in trie or last_match > -1:
result.append(self._separator.join(text[i:j]))
i = j
else:
result.append(text[i])
i += 1
else:
result.append(text[i])
i += 1
return result
",[],0,[],/tokenize/mwe.py_tokenize
5155,/home/amandapotts/git/nltk/nltk/tokenize/nist.py_lang_independent_sub,"def lang_independent_sub(self, text):
""""""Performs the language independent string substituitions.""""""
regexp, substitution = self.STRIP_SKIP
text = regexp.sub(substitution, text)
text = xml_unescape(text)
regexp, substitution = self.STRIP_EOL_HYPHEN
text = regexp.sub(substitution, text)
return text
",[],0,[],/tokenize/nist.py_lang_independent_sub
5156,/home/amandapotts/git/nltk/nltk/tokenize/nist.py_tokenize,"def tokenize(self, text, lowercase=False, western_lang=True, return_str=False):
text = str(text)
text = self.lang_independent_sub(text)
if western_lang:
text = "" "" + text + "" ""
if lowercase:
text = text.lower()
for regexp, substitution in self.LANG_DEPENDENT_REGEXES:
text = regexp.sub(substitution, text)
text = "" "".join(text.split())
text = str(text.strip())
return text if return_str else text.split()
",[],0,[],/tokenize/nist.py_tokenize
5157,/home/amandapotts/git/nltk/nltk/tokenize/nist.py_international_tokenize,"def international_tokenize(
self, text, lowercase=False, split_non_ascii=True, return_str=False
",[],0,[],/tokenize/nist.py_international_tokenize
5158,/home/amandapotts/git/nltk/nltk/tokenize/simple.py_tokenize,"def tokenize(self, s):
return list(s)
",[],0,[],/tokenize/simple.py_tokenize
5159,/home/amandapotts/git/nltk/nltk/tokenize/simple.py_span_tokenize,"def span_tokenize(self, s):
yield from enumerate(range(1, len(s) + 1))
",[],0,[],/tokenize/simple.py_span_tokenize
5160,/home/amandapotts/git/nltk/nltk/tokenize/simple.py___init__,"def __init__(self, blanklines=""discard""):
valid_blanklines = (""discard"", ""keep"", ""discard-eof"")
if blanklines not in valid_blanklines:
raise ValueError(
""Blank lines must be one of: %s"" % "" "".join(valid_blanklines)
)
self._blanklines = blanklines
",[],0,[],/tokenize/simple.py___init__
5161,/home/amandapotts/git/nltk/nltk/tokenize/simple.py_tokenize,"def tokenize(self, s):
lines = s.splitlines()
if self._blanklines == ""discard"":
lines = [l for l in lines if l.rstrip()]
elif self._blanklines == ""discard-eof"":
if lines and not lines[-1].strip():
lines.pop()
return lines
",[],0,[],/tokenize/simple.py_tokenize
5162,/home/amandapotts/git/nltk/nltk/tokenize/simple.py_span_tokenize,"def span_tokenize(self, s):
if self._blanklines == ""keep"":
yield from string_span_tokenize(s, r""\n"")
else:
yield from regexp_span_tokenize(s, r""\n(\s+\n)*"")
",[],0,[],/tokenize/simple.py_span_tokenize
5163,/home/amandapotts/git/nltk/nltk/tokenize/simple.py_line_tokenize,"def line_tokenize(text, blanklines=""discard""):
return LineTokenizer(blanklines).tokenize(text)
",[],0,[],/tokenize/simple.py_line_tokenize
5164,/home/amandapotts/git/nltk/nltk/tokenize/treebank.py_tokenize,"def tokenize(
self, text: str, convert_parentheses: bool = False, return_str: bool = False
",[],0,[],/tokenize/treebank.py_tokenize
5165,/home/amandapotts/git/nltk/nltk/tokenize/treebank.py_span_tokenize,"def span_tokenize(self, text: str) -> Iterator[Tuple[int, int]]:
r""""""
Returns the spans of the tokens in ``text``.
Uses the post-hoc nltk.tokens.align_tokens to return the offset spans.
>>> from nltk.tokenize import TreebankWordTokenizer
>>> s = '''Good muffins cost $3.88\nin New (York).  Please (buy) me\ntwo of them.\n(Thanks).'''
>>> expected = [(0, 4), (5, 12), (13, 17), (18, 19), (19, 23),
... (24, 26), (27, 30), (31, 32), (32, 36), (36, 37), (37, 38),
... (40, 46), (47, 48), (48, 51), (51, 52), (53, 55), (56, 59),
... (60, 62), (63, 68), (69, 70), (70, 76), (76, 77), (77, 78)]
>>> list(TreebankWordTokenizer().span_tokenize(s)) == expected
True
>>> expected = ['Good', 'muffins', 'cost', '$', '3.88', 'in',
... 'New', '(', 'York', ')', '.', 'Please', '(', 'buy', ')',
... 'me', 'two', 'of', 'them.', '(', 'Thanks', ')', '.']
>>> [s[start:end] for start, end in TreebankWordTokenizer().span_tokenize(s)] == expected
True
:param text: A string with a sentence or sentences.
:type text: str
:yield: Tuple[int, int]
""""""
raw_tokens = self.tokenize(text)
if ('""' in text) or (""''"" in text):
matched = [m.group() for m in re.finditer(r""``|'{2}|\"""", text)]
tokens = [
matched.pop(0) if tok in ['""', ""``"", ""''""] else tok
for tok in raw_tokens
]
else:
tokens = raw_tokens
yield from align_tokens(tokens, text)
",[],0,[],/tokenize/treebank.py_span_tokenize
5166,/home/amandapotts/git/nltk/nltk/tokenize/treebank.py_tokenize,"def tokenize(self, tokens: List[str], convert_parentheses: bool = False) -> str:
""""""
Treebank detokenizer, created by undoing the regexes from
the TreebankWordTokenizer.tokenize.
:param tokens: A list of strings, i.e. tokenized text.
:type tokens: List[str]
:param convert_parentheses: if True, replace PTB symbols with parentheses,
e.g. `-LRB-` to `(`. Defaults to False.
:type convert_parentheses: bool, optional
:return: str
""""""
text = "" "".join(tokens)
text = "" "" + text + "" ""
for regexp in self.CONTRACTIONS3:
text = regexp.sub(r""\1\2"", text)
for regexp in self.CONTRACTIONS2:
text = regexp.sub(r""\1\2"", text)
for regexp, substitution in self.ENDING_QUOTES:
text = regexp.sub(substitution, text)
text = text.strip()
regexp, substitution = self.DOUBLE_DASHES
text = regexp.sub(substitution, text)
if convert_parentheses:
for regexp, substitution in self.CONVERT_PARENTHESES:
text = regexp.sub(substitution, text)
for regexp, substitution in self.PARENS_BRACKETS:
text = regexp.sub(substitution, text)
for regexp, substitution in self.PUNCTUATION:
text = regexp.sub(substitution, text)
for regexp, substitution in self.STARTING_QUOTES:
text = regexp.sub(substitution, text)
return text.strip()
",[],0,[],/tokenize/treebank.py_tokenize
5167,/home/amandapotts/git/nltk/nltk/tokenize/treebank.py_detokenize,"def detokenize(self, tokens: List[str], convert_parentheses: bool = False) -> str:
""""""Duck-typing the abstract *tokenize()*.""""""
return self.tokenize(tokens, convert_parentheses)
",[],0,[],/tokenize/treebank.py_detokenize
5168,/home/amandapotts/git/nltk/nltk/cluster/kmeans.py___init__,"def __init__(
self,
num_means,
distance,
repeats=1,
conv_test=1e-6,
initial_means=None,
normalise=False,
svd_dimensions=None,
rng=None,
avoid_empty_clusters=False,
",[],0,[],/cluster/kmeans.py___init__
5169,/home/amandapotts/git/nltk/nltk/cluster/kmeans.py_cluster_vectorspace,"def cluster_vectorspace(self, vectors, trace=False):
if self._means and self._repeats > 1:
print(""Warning: means will be discarded for subsequent trials"")
meanss = []
for trial in range(self._repeats):
if trace:
print(""k-means trial"", trial)
if not self._means or trial > 1:
self._means = self._rng.sample(list(vectors), self._num_means)
self._cluster_vectorspace(vectors, trace)
meanss.append(self._means)
if len(meanss) > 1:
for means in meanss:
means.sort(key=sum)
min_difference = min_means = None
for i in range(len(meanss)):
d = 0
for j in range(len(meanss)):
if i != j:
d += self._sum_distances(meanss[i], meanss[j])
if min_difference is None or d < min_difference:
min_difference, min_means = d, meanss[i]
self._means = min_means
",[],0,[],/cluster/kmeans.py_cluster_vectorspace
5170,/home/amandapotts/git/nltk/nltk/cluster/kmeans.py__cluster_vectorspace,"def _cluster_vectorspace(self, vectors, trace=False):
if self._num_means < len(vectors):
converged = False
while not converged:
clusters = [[] for m in range(self._num_means)]
for vector in vectors:
index = self.classify_vectorspace(vector)
clusters[index].append(vector)
if trace:
print(""iteration"")
new_means = list(map(self._centroid, clusters, self._means))
difference = self._sum_distances(self._means, new_means)
if difference < self._max_difference:
converged = True
self._means = new_means
",[],0,[],/cluster/kmeans.py__cluster_vectorspace
5171,/home/amandapotts/git/nltk/nltk/cluster/kmeans.py_classify_vectorspace,"def classify_vectorspace(self, vector):
best_distance = best_index = None
for index in range(len(self._means)):
mean = self._means[index]
dist = self._distance(vector, mean)
if best_distance is None or dist < best_distance:
best_index, best_distance = index, dist
return best_index
",[],0,[],/cluster/kmeans.py_classify_vectorspace
5172,/home/amandapotts/git/nltk/nltk/cluster/kmeans.py_num_clusters,"def num_clusters(self):
if self._means:
return len(self._means)
else:
return self._num_means
",[],0,[],/cluster/kmeans.py_num_clusters
5173,/home/amandapotts/git/nltk/nltk/cluster/kmeans.py_means,"def means(self):
""""""
The means used for clustering.
""""""
return self._means
",[],0,[],/cluster/kmeans.py_means
5174,/home/amandapotts/git/nltk/nltk/cluster/kmeans.py__sum_distances,"def _sum_distances(self, vectors1, vectors2):
difference = 0.0
for u, v in zip(vectors1, vectors2):
difference += self._distance(u, v)
return difference
",[],0,[],/cluster/kmeans.py__sum_distances
5175,/home/amandapotts/git/nltk/nltk/cluster/kmeans.py__centroid,"def _centroid(self, cluster, mean):
if self._avoid_empty_clusters:
centroid = copy.copy(mean)
for vector in cluster:
centroid += vector
return centroid / (1 + len(cluster))
else:
if not len(cluster):
sys.stderr.write(""Error: no centroid defined for empty cluster.\n"")
sys.stderr.write(
""Try setting argument 'avoid_empty_clusters' to True\n""
)
assert False
centroid = copy.copy(cluster[0])
for vector in cluster[1:]:
centroid += vector
return centroid / len(cluster)
",[],0,[],/cluster/kmeans.py__centroid
5176,/home/amandapotts/git/nltk/nltk/cluster/kmeans.py___repr__,"def __repr__(self):
return ""<KMeansClusterer means=%s repeats=%d>"" % (self._means, self._repeats)
",[],0,[],/cluster/kmeans.py___repr__
5177,/home/amandapotts/git/nltk/nltk/cluster/kmeans.py_demo,"def demo():
from nltk.cluster import KMeansClusterer, euclidean_distance
vectors = [numpy.array(f) for f in [[2, 1], [1, 3], [4, 7], [6, 7]]]
means = [[4, 3], [5, 5]]
clusterer = KMeansClusterer(2, euclidean_distance, initial_means=means)
clusters = clusterer.cluster(vectors, True, trace=True)
print(""Clustered:"", vectors)
print(""As:"", clusters)
print(""Means:"", clusterer.means())
print()
vectors = [numpy.array(f) for f in [[3, 3], [1, 2], [4, 2], [4, 0], [2, 3], [3, 1]]]
clusterer = KMeansClusterer(2, euclidean_distance, repeats=10)
clusters = clusterer.cluster(vectors, True)
print(""Clustered:"", vectors)
print(""As:"", clusters)
print(""Means:"", clusterer.means())
print()
vector = numpy.array([3, 3])
print(""classify(%s):"" % vector, end="" "")
print(clusterer.classify(vector))
print()
",[],0,[],/cluster/kmeans.py_demo
5178,/home/amandapotts/git/nltk/nltk/cluster/gaac.py___init__,"def __init__(self, num_clusters=1, normalise=True, svd_dimensions=None):
VectorSpaceClusterer.__init__(self, normalise, svd_dimensions)
self._num_clusters = num_clusters
self._dendrogram = None
self._groups_values = None
",[],0,[],/cluster/gaac.py___init__
5179,/home/amandapotts/git/nltk/nltk/cluster/gaac.py_cluster,"def cluster(self, vectors, assign_clusters=False, trace=False):
self._dendrogram = Dendrogram(
[numpy.array(vector, numpy.float64) for vector in vectors]
)
return VectorSpaceClusterer.cluster(self, vectors, assign_clusters, trace)
",[],0,[],/cluster/gaac.py_cluster
5180,/home/amandapotts/git/nltk/nltk/cluster/gaac.py_cluster_vectorspace,"def cluster_vectorspace(self, vectors, trace=False):
N = len(vectors)
cluster_len = [1] * N
cluster_count = N
index_map = numpy.arange(N)
dims = (N, N)
dist = numpy.ones(dims, dtype=float) * numpy.inf
for i in range(N):
for j in range(i + 1, N):
dist[i, j] = cosine_distance(vectors[i], vectors[j])
while cluster_count > max(self._num_clusters, 1):
i, j = numpy.unravel_index(dist.argmin(), dims)
if trace:
print(""merging %d and %d"" % (i, j))
self._merge_similarities(dist, cluster_len, i, j)
dist[:, j] = numpy.inf
dist[j, :] = numpy.inf
cluster_len[i] = cluster_len[i] + cluster_len[j]
self._dendrogram.merge(index_map[i], index_map[j])
cluster_count -= 1
index_map[j + 1 :] -= 1
index_map[j] = N
self.update_clusters(self._num_clusters)
",[],0,[],/cluster/gaac.py_cluster_vectorspace
5181,/home/amandapotts/git/nltk/nltk/cluster/gaac.py__merge_similarities,"def _merge_similarities(self, dist, cluster_len, i, j):
i_weight = cluster_len[i]
j_weight = cluster_len[j]
weight_sum = i_weight + j_weight
dist[:i, i] = dist[:i, i] * i_weight + dist[:i, j] * j_weight
dist[:i, i] /= weight_sum
dist[i, i + 1 : j] = (
dist[i, i + 1 : j] * i_weight + dist[i + 1 : j, j] * j_weight
)
dist[i, j + 1 :] = dist[i, j + 1 :] * i_weight + dist[j, j + 1 :] * j_weight
dist[i, i + 1 :] /= weight_sum
",[],0,[],/cluster/gaac.py__merge_similarities
5182,/home/amandapotts/git/nltk/nltk/cluster/gaac.py_update_clusters,"def update_clusters(self, num_clusters):
clusters = self._dendrogram.groups(num_clusters)
self._centroids = []
for cluster in clusters:
assert len(cluster) > 0
if self._should_normalise:
centroid = self._normalise(cluster[0])
else:
centroid = numpy.array(cluster[0])
for vector in cluster[1:]:
if self._should_normalise:
centroid += self._normalise(vector)
else:
centroid += vector
centroid /= len(cluster)
self._centroids.append(centroid)
self._num_clusters = len(self._centroids)
",[],0,[],/cluster/gaac.py_update_clusters
5183,/home/amandapotts/git/nltk/nltk/cluster/gaac.py_classify_vectorspace,"def classify_vectorspace(self, vector):
best = None
for i in range(self._num_clusters):
centroid = self._centroids[i]
dist = cosine_distance(vector, centroid)
if not best or dist < best[0]:
best = (dist, i)
return best[1]
",[],0,[],/cluster/gaac.py_classify_vectorspace
5184,/home/amandapotts/git/nltk/nltk/cluster/gaac.py_dendrogram,"def dendrogram(self):
""""""
:return: The dendrogram representing the current clustering
:rtype:  Dendrogram
""""""
return self._dendrogram
",[],0,[],/cluster/gaac.py_dendrogram
5185,/home/amandapotts/git/nltk/nltk/cluster/gaac.py_num_clusters,"def num_clusters(self):
return self._num_clusters
",[],0,[],/cluster/gaac.py_num_clusters
5186,/home/amandapotts/git/nltk/nltk/cluster/gaac.py___repr__,"def __repr__(self):
return ""<GroupAverageAgglomerative Clusterer n=%d>"" % self._num_clusters
",[],0,[],/cluster/gaac.py___repr__
5187,/home/amandapotts/git/nltk/nltk/cluster/gaac.py_demo,"def demo():
""""""
Non-interactive demonstration of the clusterers with simple 2-D data.
""""""
from nltk.cluster import GAAClusterer
vectors = [numpy.array(f) for f in [[3, 3], [1, 2], [4, 2], [4, 0], [2, 3], [3, 1]]]
clusterer = GAAClusterer(4)
clusters = clusterer.cluster(vectors, True)
print(""Clusterer:"", clusterer)
print(""Clustered:"", vectors)
print(""As:"", clusters)
print()
clusterer.dendrogram().show()
vector = numpy.array([3, 3])
print(""classify(%s):"" % vector, end="" "")
print(clusterer.classify(vector))
print()
",[],0,[],/cluster/gaac.py_demo
5188,/home/amandapotts/git/nltk/nltk/cluster/util.py___init__,"def __init__(self, normalise=False, svd_dimensions=None):
""""""
:param normalise:       should vectors be normalised to length 1
:type normalise:        boolean
:param svd_dimensions:  number of dimensions to use in reducing vector
dimensionsionality with SVD
:type svd_dimensions:   int
""""""
self._Tt = None
self._should_normalise = normalise
self._svd_dimensions = svd_dimensions
",[],0,[],/cluster/util.py___init__
5189,/home/amandapotts/git/nltk/nltk/cluster/util.py_cluster,"def cluster(self, vectors, assign_clusters=False, trace=False):
assert len(vectors) > 0
if self._should_normalise:
vectors = list(map(self._normalise, vectors))
if self._svd_dimensions and self._svd_dimensions < len(vectors[0]):
[u, d, vt] = numpy.linalg.svd(numpy.transpose(numpy.array(vectors)))
S = d[: self._svd_dimensions] * numpy.identity(
self._svd_dimensions, numpy.float64
)
T = u[:, : self._svd_dimensions]
Dt = vt[: self._svd_dimensions, :]
vectors = numpy.transpose(numpy.dot(S, Dt))
self._Tt = numpy.transpose(T)
self.cluster_vectorspace(vectors, trace)
if assign_clusters:
return [self.classify(vector) for vector in vectors]
",[],0,[],/cluster/util.py_cluster
5190,/home/amandapotts/git/nltk/nltk/cluster/util.py_cluster_vectorspace,"def cluster_vectorspace(self, vectors, trace):
""""""
Finds the clusters using the given set of vectors.
""""""
",[],0,[],/cluster/util.py_cluster_vectorspace
5191,/home/amandapotts/git/nltk/nltk/cluster/util.py_classify,"def classify(self, vector):
if self._should_normalise:
vector = self._normalise(vector)
if self._Tt is not None:
vector = numpy.dot(self._Tt, vector)
cluster = self.classify_vectorspace(vector)
return self.cluster_name(cluster)
",[],0,[],/cluster/util.py_classify
5192,/home/amandapotts/git/nltk/nltk/cluster/util.py_classify_vectorspace,"def classify_vectorspace(self, vector):
""""""
Returns the index of the appropriate cluster for the vector.
""""""
",[],0,[],/cluster/util.py_classify_vectorspace
5193,/home/amandapotts/git/nltk/nltk/cluster/util.py_likelihood,"def likelihood(self, vector, label):
if self._should_normalise:
vector = self._normalise(vector)
if self._Tt is not None:
vector = numpy.dot(self._Tt, vector)
return self.likelihood_vectorspace(vector, label)
",[],0,[],/cluster/util.py_likelihood
5194,/home/amandapotts/git/nltk/nltk/cluster/util.py_likelihood_vectorspace,"def likelihood_vectorspace(self, vector, cluster):
""""""
Returns the likelihood of the vector belonging to the cluster.
""""""
predicted = self.classify_vectorspace(vector)
return 1.0 if cluster == predicted else 0.0
",[],0,[],/cluster/util.py_likelihood_vectorspace
5195,/home/amandapotts/git/nltk/nltk/cluster/util.py_vector,"def vector(self, vector):
""""""
Returns the vector after normalisation and dimensionality reduction
""""""
if self._should_normalise:
vector = self._normalise(vector)
if self._Tt is not None:
vector = numpy.dot(self._Tt, vector)
return vector
",[],0,[],/cluster/util.py_vector
5196,/home/amandapotts/git/nltk/nltk/cluster/util.py__normalise,"def _normalise(self, vector):
""""""
Normalises the vector to unit length.
""""""
return vector / sqrt(numpy.dot(vector, vector))
",[],0,[],/cluster/util.py__normalise
5197,/home/amandapotts/git/nltk/nltk/cluster/util.py_euclidean_distance,"def euclidean_distance(u, v):
""""""
Returns the euclidean distance between vectors u and v. This is equivalent
to the length of the vector (u - v).
""""""
diff = u - v
return sqrt(numpy.dot(diff, diff))
",[],0,[],/cluster/util.py_euclidean_distance
5198,/home/amandapotts/git/nltk/nltk/cluster/util.py_cosine_distance,"def cosine_distance(u, v):
""""""
Returns 1 minus the cosine of the angle between vectors v and u. This is
equal to ``1 - (u.v / |u||v|)``.
""""""
return 1 - (numpy.dot(u, v) / (sqrt(numpy.dot(u, u)) * sqrt(numpy.dot(v, v))))
",[],0,[],/cluster/util.py_cosine_distance
5199,/home/amandapotts/git/nltk/nltk/cluster/util.py___init__,"def __init__(self, value, *children):
self._value = value
self._children = children
",[],0,[],/cluster/util.py___init__
5200,/home/amandapotts/git/nltk/nltk/cluster/util.py_leaves,"def leaves(self, values=True):
if self._children:
leaves = []
for child in self._children:
leaves.extend(child.leaves(values))
return leaves
elif values:
return [self._value]
else:
return [self]
",[],0,[],/cluster/util.py_leaves
5201,/home/amandapotts/git/nltk/nltk/cluster/util.py_groups,"def groups(self, n):
queue = [(self._value, self)]
while len(queue) < n:
priority, node = queue.pop()
if not node._children:
queue.push((priority, node))
break
for child in node._children:
if child._children:
queue.append((child._value, child))
else:
queue.append((0, child))
queue.sort()
groups = []
for priority, node in queue:
groups.append(node.leaves())
return groups
",[],0,[],/cluster/util.py_groups
5202,/home/amandapotts/git/nltk/nltk/cluster/util.py___lt__,"def __lt__(self, comparator):
return cosine_distance(self._value, comparator._value) < 0
",[],0,[],/cluster/util.py___lt__
5203,/home/amandapotts/git/nltk/nltk/cluster/util.py___init__,"def __init__(self, items=[]):
""""""
:param  items: the items at the leaves of the dendrogram
:type   items: sequence of (any)
""""""
self._items = [_DendrogramNode(item) for item in items]
self._original_items = copy.copy(self._items)
self._merge = 1
",[],0,[],/cluster/util.py___init__
5204,/home/amandapotts/git/nltk/nltk/cluster/util.py_merge,"def merge(self, *indices):
""""""
Merges nodes at given indices in the dendrogram. The nodes will be
combined which then replaces the first node specified. All other nodes
involved in the merge will be removed.
:param  indices: indices of the items to merge (at least two)
:type   indices: seq of int
""""""
assert len(indices) >= 2
node = _DendrogramNode(self._merge, *(self._items[i] for i in indices))
self._merge += 1
self._items[indices[0]] = node
for i in indices[1:]:
del self._items[i]
",[],0,[],/cluster/util.py_merge
5205,/home/amandapotts/git/nltk/nltk/cluster/util.py_groups,"def groups(self, n):
""""""
Finds the n-groups of items (leaves) reachable from a cut at depth n.
:param  n: number of groups
:type   n: int
""""""
if len(self._items) > 1:
root = _DendrogramNode(self._merge, *self._items)
else:
root = self._items[0]
return root.groups(n)
",[],0,[],/cluster/util.py_groups
5206,/home/amandapotts/git/nltk/nltk/cluster/util.py_show,"def show(self, leaf_labels=[]):
""""""
Print the dendrogram in ASCII art to standard out.
:param leaf_labels: an optional list of strings to use for labeling the
leaves
:type leaf_labels: list
""""""
JOIN, HLINK, VLINK = ""+"", ""-"", ""|""
if len(self._items) > 1:
root = _DendrogramNode(self._merge, *self._items)
else:
root = self._items[0]
leaves = self._original_items
if leaf_labels:
last_row = leaf_labels
else:
last_row = [""%s"" % leaf._value for leaf in leaves]
width = max(map(len, last_row)) + 1
lhalf = width // 2
rhalf = int(width - lhalf - 1)
",[],0,[],/cluster/util.py_show
5207,/home/amandapotts/git/nltk/nltk/cluster/util.py_format,"def format(centre, left="" "", right="" ""):
return f""{lhalf * left}{centre}{right * rhalf}""
",[],0,[],/cluster/util.py_format
5208,/home/amandapotts/git/nltk/nltk/cluster/util.py___repr__,"def __repr__(self):
if len(self._items) > 1:
root = _DendrogramNode(self._merge, *self._items)
else:
root = self._items[0]
leaves = root.leaves(False)
return ""<Dendrogram with %d leaves>"" % len(leaves)
",[],0,[],/cluster/util.py___repr__
5209,/home/amandapotts/git/nltk/nltk/cluster/api.py_cluster,"def cluster(self, vectors, assign_clusters=False):
""""""
Assigns the vectors to clusters, learning the clustering parameters
from the data. Returns a cluster identifier for each vector.
""""""
",[],0,[],/cluster/api.py_cluster
5210,/home/amandapotts/git/nltk/nltk/cluster/api.py_classify,"def classify(self, token):
""""""
Classifies the token into a cluster, setting the token's CLUSTER
parameter to that cluster identifier.
""""""
",[],0,[],/cluster/api.py_classify
5211,/home/amandapotts/git/nltk/nltk/cluster/api.py_likelihood,"def likelihood(self, vector, label):
""""""
Returns the likelihood (a float) of the token having the
corresponding cluster.
""""""
if self.classify(vector) == label:
return 1.0
else:
return 0.0
",[],0,[],/cluster/api.py_likelihood
5212,/home/amandapotts/git/nltk/nltk/cluster/api.py_classification_probdist,"def classification_probdist(self, vector):
""""""
Classifies the token into a cluster, returning
a probability distribution over the cluster identifiers.
""""""
likelihoods = {}
sum = 0.0
for cluster in self.cluster_names():
likelihoods[cluster] = self.likelihood(vector, cluster)
sum += likelihoods[cluster]
for cluster in self.cluster_names():
likelihoods[cluster] /= sum
return DictionaryProbDist(likelihoods)
",[],0,[],/cluster/api.py_classification_probdist
5213,/home/amandapotts/git/nltk/nltk/cluster/api.py_num_clusters,"def num_clusters(self):
""""""
Returns the number of clusters.
""""""
",[],0,[],/cluster/api.py_num_clusters
5214,/home/amandapotts/git/nltk/nltk/cluster/api.py_cluster_names,"def cluster_names(self):
""""""
Returns the names of the clusters.
:rtype: list
""""""
return list(range(self.num_clusters()))
",[],0,[],/cluster/api.py_cluster_names
5215,/home/amandapotts/git/nltk/nltk/cluster/api.py_cluster_name,"def cluster_name(self, index):
""""""
Returns the names of the cluster at index.
""""""
return index
",[],0,[],/cluster/api.py_cluster_name
5216,/home/amandapotts/git/nltk/nltk/cluster/em.py___init__,"def __init__(
self,
initial_means,
priors=None,
covariance_matrices=None,
conv_threshold=1e-6,
bias=0.1,
normalise=False,
svd_dimensions=None,
",[],0,[],/cluster/em.py___init__
5217,/home/amandapotts/git/nltk/nltk/cluster/em.py_num_clusters,"def num_clusters(self):
return self._num_clusters
",[],0,[],/cluster/em.py_num_clusters
5218,/home/amandapotts/git/nltk/nltk/cluster/em.py_cluster_vectorspace,"def cluster_vectorspace(self, vectors, trace=False):
assert len(vectors) > 0
dimensions = len(vectors[0])
means = self._means
priors = self._priors
if not priors:
priors = self._priors = (
numpy.ones(self._num_clusters, numpy.float64) / self._num_clusters
)
covariances = self._covariance_matrices
if not covariances:
covariances = self._covariance_matrices = [
numpy.identity(dimensions, numpy.float64)
for i in range(self._num_clusters)
]
lastl = self._loglikelihood(vectors, priors, means, covariances)
converged = False
while not converged:
if trace:
print(""iteration
h = numpy.zeros((len(vectors), self._num_clusters), numpy.float64)
for i in range(len(vectors)):
for j in range(self._num_clusters):
h[i, j] = priors[j] * self._gaussian(
means[j], covariances[j], vectors[i]
)
h[i, :] /= sum(h[i, :])
for j in range(self._num_clusters):
covariance_before = covariances[j]
new_covariance = numpy.zeros((dimensions, dimensions), numpy.float64)
new_mean = numpy.zeros(dimensions, numpy.float64)
sum_hj = 0.0
for i in range(len(vectors)):
delta = vectors[i] - means[j]
new_covariance += h[i, j] * numpy.multiply.outer(delta, delta)
sum_hj += h[i, j]
new_mean += h[i, j] * vectors[i]
covariances[j] = new_covariance / sum_hj
means[j] = new_mean / sum_hj
priors[j] = sum_hj / len(vectors)
covariances[j] += self._bias * numpy.identity(dimensions, numpy.float64)
l = self._loglikelihood(vectors, priors, means, covariances)
if abs(lastl - l) < self._conv_threshold:
converged = True
lastl = l
",[],0,[],/cluster/em.py_cluster_vectorspace
5219,/home/amandapotts/git/nltk/nltk/cluster/em.py_classify_vectorspace,"def classify_vectorspace(self, vector):
best = None
for j in range(self._num_clusters):
p = self._priors[j] * self._gaussian(
self._means[j], self._covariance_matrices[j], vector
)
if not best or p > best[0]:
best = (p, j)
return best[1]
",[],0,[],/cluster/em.py_classify_vectorspace
5220,/home/amandapotts/git/nltk/nltk/cluster/em.py_likelihood_vectorspace,"def likelihood_vectorspace(self, vector, cluster):
cid = self.cluster_names().index(cluster)
return self._priors[cluster] * self._gaussian(
self._means[cluster], self._covariance_matrices[cluster], vector
)
",[],0,[],/cluster/em.py_likelihood_vectorspace
5221,/home/amandapotts/git/nltk/nltk/cluster/em.py__gaussian,"def _gaussian(self, mean, cvm, x):
m = len(mean)
assert cvm.shape == (m, m), ""bad sized covariance matrix, %s"" % str(cvm.shape)
try:
det = numpy.linalg.det(cvm)
inv = numpy.linalg.inv(cvm)
a = det**-0.5 * (2 * numpy.pi) ** (-m / 2.0)
dx = x - mean
print(dx, inv)
b = -0.5 * numpy.dot(numpy.dot(dx, inv), dx)
return a * numpy.exp(b)
except OverflowError:
return 0
",[],0,[],/cluster/em.py__gaussian
5222,/home/amandapotts/git/nltk/nltk/cluster/em.py__loglikelihood,"def _loglikelihood(self, vectors, priors, means, covariances):
llh = 0.0
for vector in vectors:
p = 0
for j in range(len(priors)):
p += priors[j] * self._gaussian(means[j], covariances[j], vector)
llh += numpy.log(p)
return llh
",[],0,[],/cluster/em.py__loglikelihood
5223,/home/amandapotts/git/nltk/nltk/cluster/em.py___repr__,"def __repr__(self):
return ""<EMClusterer means=%s>"" % list(self._means)
",[],0,[],/cluster/em.py___repr__
5224,/home/amandapotts/git/nltk/nltk/cluster/em.py_demo,"def demo():
""""""
Non-interactive demonstration of the clusterers with simple 2-D data.
""""""
from nltk import cluster
vectors = [numpy.array(f) for f in [[0.5, 0.5], [1.5, 0.5], [1, 3]]]
means = [[4, 2], [4, 2.01]]
clusterer = cluster.EMClusterer(means, bias=0.1)
clusters = clusterer.cluster(vectors, True, trace=True)
print(""Clustered:"", vectors)
print(""As:       "", clusters)
print()
for c in range(2):
print(""Cluster:"", c)
print(""Prior:  "", clusterer._priors[c])
print(""Mean:   "", clusterer._means[c])
print(""Covar:  "", clusterer._covariance_matrices[c])
print()
vector = numpy.array([2, 2])
print(""classify(%s):"" % vector, end="" "")
print(clusterer.classify(vector))
vector = numpy.array([2, 2])
print(""classification_probdist(%s):"" % vector)
pdist = clusterer.classification_probdist(vector)
for sample in pdist.samples():
print(f""{sample} => {pdist.prob(sample) * 100:.0f}%"")
",[],0,[],/cluster/em.py_demo
5225,/home/amandapotts/git/nltk/nltk/twitter/common.py_extract_fields,"def extract_fields(tweet, fields):
""""""
Extract field values from a full tweet and return them as a list
:param json tweet: The tweet in JSON format
:param list fields: The fields to be extracted from the tweet
:rtype: list(str)
""""""
out = []
for field in fields:
try:
_add_field_to_out(tweet, field, out)
except TypeError as e:
raise RuntimeError(
""Fatal error when extracting fields. Cannot find field "", field
) from e
return out
",[],0,[],/twitter/common.py_extract_fields
5226,/home/amandapotts/git/nltk/nltk/twitter/common.py__add_field_to_out,"def _add_field_to_out(json, field, out):
if _is_composed_key(field):
key, value = _get_key_value_composed(field)
_add_field_to_out(json[key], value, out)
else:
out += [json[field]]
",[],0,[],/twitter/common.py__add_field_to_out
5227,/home/amandapotts/git/nltk/nltk/twitter/common.py__is_composed_key,"def _is_composed_key(field):
return HIER_SEPARATOR in field
",[],0,[],/twitter/common.py__is_composed_key
5228,/home/amandapotts/git/nltk/nltk/twitter/common.py__get_key_value_composed,"def _get_key_value_composed(field):
out = field.split(HIER_SEPARATOR)
key = out[0]
value = HIER_SEPARATOR.join(out[1:])
return key, value
",[],0,[],/twitter/common.py__get_key_value_composed
5229,/home/amandapotts/git/nltk/nltk/twitter/common.py__get_entity_recursive,"def _get_entity_recursive(json, entity):
if not json:
return None
elif isinstance(json, dict):
for key, value in json.items():
if key == entity:
return value
if key == ""entities"" or key == ""extended_entities"":
candidate = _get_entity_recursive(value, entity)
if candidate is not None:
return candidate
return None
elif isinstance(json, list):
for item in json:
candidate = _get_entity_recursive(item, entity)
if candidate is not None:
return candidate
return None
else:
return None
",[],0,[],/twitter/common.py__get_entity_recursive
5230,/home/amandapotts/git/nltk/nltk/twitter/common.py_json2csv,"def json2csv(
fp, outfile, fields, encoding=""utf8"", errors=""replace"", gzip_compress=False
",[],0,[],/twitter/common.py_json2csv
5231,/home/amandapotts/git/nltk/nltk/twitter/common.py_outf_writer_compat,"def outf_writer_compat(outfile, encoding, errors, gzip_compress=False):
""""""Get a CSV writer with optional compression.""""""
return _outf_writer(outfile, encoding, errors, gzip_compress)
",[],0,[],/twitter/common.py_outf_writer_compat
5232,/home/amandapotts/git/nltk/nltk/twitter/common.py__outf_writer,"def _outf_writer(outfile, encoding, errors, gzip_compress=False):
if gzip_compress:
outf = gzip.open(outfile, ""wt"", newline="""", encoding=encoding, errors=errors)
else:
outf = open(outfile, ""w"", newline="""", encoding=encoding, errors=errors)
writer = csv.writer(outf)
return (writer, outf)
",[],0,[],/twitter/common.py__outf_writer
5233,/home/amandapotts/git/nltk/nltk/twitter/common.py_json2csv_entities,"def json2csv_entities(
tweets_file,
outfile,
main_fields,
entity_type,
entity_fields,
encoding=""utf8"",
errors=""replace"",
gzip_compress=False,
",[],0,[],/twitter/common.py_json2csv_entities
5234,/home/amandapotts/git/nltk/nltk/twitter/common.py_get_header_field_list,"def get_header_field_list(main_fields, entity_type, entity_fields):
if _is_composed_key(entity_type):
key, value = _get_key_value_composed(entity_type)
main_entity = key
sub_entity = value
else:
main_entity = None
sub_entity = entity_type
if main_entity:
output1 = [HIER_SEPARATOR.join([main_entity, x]) for x in main_fields]
else:
output1 = main_fields
output2 = [HIER_SEPARATOR.join([sub_entity, x]) for x in entity_fields]
return output1 + output2
",[],0,[],/twitter/common.py_get_header_field_list
5235,/home/amandapotts/git/nltk/nltk/twitter/common.py__write_to_file,"def _write_to_file(object_fields, items, entity_fields, writer):
if not items:
return
if isinstance(items, dict):
row = object_fields
entity_field_values = [x for x in entity_fields if not _is_composed_key(x)]
entity_field_composed = [x for x in entity_fields if _is_composed_key(x)]
for field in entity_field_values:
value = items[field]
if isinstance(value, list):
row += value
else:
row += [value]
for d in entity_field_composed:
kd, vd = _get_key_value_composed(d)
json_dict = items[kd]
if not isinstance(json_dict, dict):
raise RuntimeError(
""""""Key {} does not contain a dictionary
in the json file"""""".format(
kd
)
)
row += [json_dict[vd]]
writer.writerow(row)
return
for item in items:
row = object_fields + extract_fields(item, entity_fields)
writer.writerow(row)
",[],0,[],/twitter/common.py__write_to_file
5236,/home/amandapotts/git/nltk/nltk/twitter/util.py_credsfromfile,"def credsfromfile(creds_file=None, subdir=None, verbose=False):
""""""
Convenience function for authentication
""""""
return Authenticate().load_creds(
creds_file=creds_file, subdir=subdir, verbose=verbose
)
",[],0,[],/twitter/util.py_credsfromfile
5237,/home/amandapotts/git/nltk/nltk/twitter/util.py___init__,"def __init__(self):
self.creds_file = ""credentials.txt""
self.creds_fullpath = None
self.oauth = {}
try:
self.twitter_dir = os.environ[""TWITTER""]
self.creds_subdir = self.twitter_dir
except KeyError:
self.twitter_dir = None
self.creds_subdir = None
",[],0,[],/twitter/util.py___init__
5238,/home/amandapotts/git/nltk/nltk/twitter/util.py_load_creds,"def load_creds(self, creds_file=None, subdir=None, verbose=False):
""""""
Read OAuth credentials from a text file.
File format for OAuth 1::
app_key=YOUR_APP_KEY
app_secret=YOUR_APP_SECRET
oauth_token=OAUTH_TOKEN
oauth_token_secret=OAUTH_TOKEN_SECRET
File format for OAuth 2::
app_key=YOUR_APP_KEY
app_secret=YOUR_APP_SECRET
access_token=ACCESS_TOKEN
:param str file_name: File containing credentials. ``None`` (default) reads
data from `TWITTER/'credentials.txt'`
""""""
if creds_file is not None:
self.creds_file = creds_file
if subdir is None:
if self.creds_subdir is None:
msg = (
""Supply a value to the 'subdir' parameter or""
+ "" set the TWITTER environment variable.""
)
raise ValueError(msg)
else:
self.creds_subdir = subdir
self.creds_fullpath = os.path.normpath(
os.path.join(self.creds_subdir, self.creds_file)
)
if not os.path.isfile(self.creds_fullpath):
raise OSError(f""Cannot find file {self.creds_fullpath}"")
with open(self.creds_fullpath) as infile:
if verbose:
print(f""Reading credentials file {self.creds_fullpath}"")
for line in infile:
if ""="" in line:
name, value = line.split(""="", 1)
self.oauth[name.strip()] = value.strip()
self._validate_creds_file(verbose=verbose)
return self.oauth
",[],0,[],/twitter/util.py_load_creds
5239,/home/amandapotts/git/nltk/nltk/twitter/util.py__validate_creds_file,"def _validate_creds_file(self, verbose=False):
""""""Check validity of a credentials file.""""""
oauth1 = False
oauth1_keys = [""app_key"", ""app_secret"", ""oauth_token"", ""oauth_token_secret""]
oauth2 = False
oauth2_keys = [""app_key"", ""app_secret"", ""access_token""]
if all(k in self.oauth for k in oauth1_keys):
oauth1 = True
elif all(k in self.oauth for k in oauth2_keys):
oauth2 = True
if not (oauth1 or oauth2):
msg = f""Missing or incorrect entries in {self.creds_file}\n""
msg += pprint.pformat(self.oauth)
raise ValueError(msg)
elif verbose:
print(f'Credentials file ""{self.creds_file}"" looks good')
",[],0,[],/twitter/util.py__validate_creds_file
5240,/home/amandapotts/git/nltk/nltk/twitter/util.py_add_access_token,"def add_access_token(creds_file=None):
""""""
For OAuth 2, retrieve an access token for an app and append it to a
credentials file.
""""""
if creds_file is None:
path = os.path.dirname(__file__)
creds_file = os.path.join(path, ""credentials2.txt"")
oauth2 = credsfromfile(creds_file=creds_file)
app_key = oauth2[""app_key""]
app_secret = oauth2[""app_secret""]
twitter = Twython(app_key, app_secret, oauth_version=2)
access_token = twitter.obtain_access_token()
tok = f""access_token={access_token}\n""
with open(creds_file, ""a"") as infile:
print(tok, file=infile)
",[],0,[],/twitter/util.py_add_access_token
5241,/home/amandapotts/git/nltk/nltk/twitter/util.py_guess_path,"def guess_path(pth):
""""""
If the path is not absolute, guess that it is a subdirectory of the
user's home directory.
:param str pth: The pathname of the directory where files of tweets should be written
""""""
if os.path.isabs(pth):
return pth
else:
return os.path.expanduser(os.path.join(""~"", pth))
",[],0,[],/twitter/util.py_guess_path
5242,/home/amandapotts/git/nltk/nltk/twitter/twitter_demo.py_verbose,"def verbose(func):
""""""Decorator for demo functions""""""
@wraps(func)
",[],0,[],/twitter/twitter_demo.py_verbose
5243,/home/amandapotts/git/nltk/nltk/twitter/twitter_demo.py_with_formatting,"def with_formatting(*args, **kwargs):
print()
print(SPACER)
print(""Using %s"" % (func.__name__))
print(SPACER)
return func(*args, **kwargs)
",[],0,[],/twitter/twitter_demo.py_with_formatting
5244,/home/amandapotts/git/nltk/nltk/twitter/twitter_demo.py_yesterday,"def yesterday():
""""""
Get yesterday's datetime as a 5-tuple.
""""""
date = datetime.datetime.now()
date -= datetime.timedelta(days=1)
date_tuple = date.timetuple()[:6]
return date_tuple
",[],0,[],/twitter/twitter_demo.py_yesterday
5245,/home/amandapotts/git/nltk/nltk/twitter/twitter_demo.py_setup,"def setup():
""""""
Initialize global variables for the demos.
""""""
global USERIDS, FIELDS
USERIDS = [""759251"", ""612473"", ""15108702"", ""6017542"", ""2673523800""]
FIELDS = [""id_str""]
",[],0,[],/twitter/twitter_demo.py_setup
5246,/home/amandapotts/git/nltk/nltk/twitter/twitter_demo.py_twitterclass_demo,"def twitterclass_demo():
""""""
Use the simplified :class:`Twitter` class to write some tweets to a file.
""""""
tw = Twitter()
print(""Track from the public stream\n"")
tw.tweets(keywords=""love, hate"", limit=10)  # public stream
print(SPACER)
print(""Search past Tweets\n"")
tw = Twitter()
tw.tweets(keywords=""love, hate"", stream=False, limit=10)  # search past tweets
print(SPACER)
print(
""Follow two accounts in the public stream""
+ "" -- be prepared to wait a few minutes\n""
)
tw = Twitter()
tw.tweets(follow=[""759251"", ""6017542""], stream=True, limit=5)  # public stream
",[],0,[],/twitter/twitter_demo.py_twitterclass_demo
5247,/home/amandapotts/git/nltk/nltk/twitter/twitter_demo.py_sampletoscreen_demo,"def sampletoscreen_demo(limit=20):
""""""
Sample from the Streaming API and send output to terminal.
""""""
oauth = credsfromfile()
client = Streamer(**oauth)
client.register(TweetViewer(limit=limit))
client.sample()
",[],0,[],/twitter/twitter_demo.py_sampletoscreen_demo
5248,/home/amandapotts/git/nltk/nltk/twitter/twitter_demo.py_tracktoscreen_demo,"def tracktoscreen_demo(track=""taylor swift"", limit=10):
""""""
Track keywords from the public Streaming API and send output to terminal.
""""""
oauth = credsfromfile()
client = Streamer(**oauth)
client.register(TweetViewer(limit=limit))
client.filter(track=track)
",[],0,[],/twitter/twitter_demo.py_tracktoscreen_demo
5249,/home/amandapotts/git/nltk/nltk/twitter/twitter_demo.py_search_demo,"def search_demo(keywords=""nltk""):
""""""
Use the REST API to search for past tweets containing a given keyword.
""""""
oauth = credsfromfile()
client = Query(**oauth)
for tweet in client.search_tweets(keywords=keywords, limit=10):
print(tweet[""text""])
",[],0,[],/twitter/twitter_demo.py_search_demo
5250,/home/amandapotts/git/nltk/nltk/twitter/twitter_demo.py_tweets_by_user_demo,"def tweets_by_user_demo(user=""NLTK_org"", count=200):
""""""
Use the REST API to search for past tweets by a given user.
""""""
oauth = credsfromfile()
client = Query(**oauth)
client.register(TweetWriter())
client.user_tweets(user, count)
",[],0,[],/twitter/twitter_demo.py_tweets_by_user_demo
5251,/home/amandapotts/git/nltk/nltk/twitter/twitter_demo.py_lookup_by_userid_demo,"def lookup_by_userid_demo():
""""""
Use the REST API to convert a userID to a screen name.
""""""
oauth = credsfromfile()
client = Query(**oauth)
user_info = client.user_info_from_id(USERIDS)
for info in user_info:
name = info[""screen_name""]
followers = info[""followers_count""]
following = info[""friends_count""]
print(f""{name}, followers: {followers}, following: {following}"")
",[],0,[],/twitter/twitter_demo.py_lookup_by_userid_demo
5252,/home/amandapotts/git/nltk/nltk/twitter/twitter_demo.py_followtoscreen_demo,"def followtoscreen_demo(limit=10):
""""""
Using the Streaming API, select just the tweets from a specified list of
userIDs.
This is will only give results in a reasonable time if the users in
question produce a high volume of tweets, and may even so show some delay.
""""""
oauth = credsfromfile()
client = Streamer(**oauth)
client.register(TweetViewer(limit=limit))
client.statuses.filter(follow=USERIDS)
",[],0,[],/twitter/twitter_demo.py_followtoscreen_demo
5253,/home/amandapotts/git/nltk/nltk/twitter/twitter_demo.py_streamtofile_demo,"def streamtofile_demo(limit=20):
""""""
Write 20 tweets sampled from the public Streaming API to a file.
""""""
oauth = credsfromfile()
client = Streamer(**oauth)
client.register(TweetWriter(limit=limit, repeat=False))
client.statuses.sample()
",[],0,[],/twitter/twitter_demo.py_streamtofile_demo
5254,/home/amandapotts/git/nltk/nltk/twitter/twitter_demo.py_limit_by_time_demo,"def limit_by_time_demo(keywords=""nltk""):
""""""
Query the REST API for Tweets about NLTK since yesterday and send
the output to terminal.
This example makes the assumption that there are sufficient Tweets since
yesterday for the date to be an effective cut-off.
""""""
date = yesterday()
dt_date = datetime.datetime(*date)
oauth = credsfromfile()
client = Query(**oauth)
client.register(TweetViewer(limit=100, lower_date_limit=date))
print(f""Cutoff date: {dt_date}\n"")
for tweet in client.search_tweets(keywords=keywords):
print(""{} "".format(tweet[""created_at""]), end="""")
client.handler.handle(tweet)
",[],0,[],/twitter/twitter_demo.py_limit_by_time_demo
5255,/home/amandapotts/git/nltk/nltk/twitter/twitter_demo.py_corpusreader_demo,"def corpusreader_demo():
""""""
Use `TwitterCorpusReader` tp read a file of tweets, and print out
""""""
from nltk.corpus import twitter_samples as tweets
print()
print(""Complete tweet documents"")
print(SPACER)
for tweet in tweets.docs(""tweets.20150430-223406.json"")[:1]:
print(json.dumps(tweet, indent=1, sort_keys=True))
print()
print(""Raw tweet strings:"")
print(SPACER)
for text in tweets.strings(""tweets.20150430-223406.json"")[:15]:
print(text)
print()
print(""Tokenized tweet strings:"")
print(SPACER)
for toks in tweets.tokenized(""tweets.20150430-223406.json"")[:15]:
print(toks)
",[],0,[],/twitter/twitter_demo.py_corpusreader_demo
5256,/home/amandapotts/git/nltk/nltk/twitter/twitter_demo.py_expand_tweetids_demo,"def expand_tweetids_demo():
""""""
Given a file object containing a list of Tweet IDs, fetch the
corresponding full Tweets, if available.
""""""
ids_f = StringIO(
""""""\
588665495492124672
588665495487909888
588665495508766721
588665495513006080
588665495517200384
588665495487811584
588665495525588992
588665495487844352
588665495492014081
588665495512948737""""""
)
oauth = credsfromfile()
client = Query(**oauth)
hydrated = client.expand_tweetids(ids_f)
for tweet in hydrated:
id_str = tweet[""id_str""]
print(f""id: {id_str}"")
text = tweet[""text""]
if text.startswith(""@null""):
text = ""[Tweet not available]""
print(text + ""\n"")
",[],0,[],/twitter/twitter_demo.py_expand_tweetids_demo
5257,/home/amandapotts/git/nltk/nltk/twitter/api.py_utcoffset,"def utcoffset(self, dt):
""""""
Access the relevant time offset.
""""""
return self.DSTOFFSET
",[],0,[],/twitter/api.py_utcoffset
5258,/home/amandapotts/git/nltk/nltk/twitter/api.py___init__,"def __init__(self, limit=20):
self.limit = limit
self.counter = 0
""""""
A flag to indicate to the client whether to stop fetching data given
some condition (e.g., reaching a date limit).
""""""
self.do_stop = False
""""""
Stores the id of the last fetched Tweet to handle pagination.
""""""
self.max_id = None
",[],0,[],/twitter/api.py___init__
5259,/home/amandapotts/git/nltk/nltk/twitter/api.py_do_continue,"def do_continue(self):
""""""
Returns `False` if the client should stop fetching Tweets.
""""""
return self.counter < self.limit and not self.do_stop
",[],0,[],/twitter/api.py_do_continue
5260,/home/amandapotts/git/nltk/nltk/twitter/api.py___init__,"def __init__(self, limit=20, upper_date_limit=None, lower_date_limit=None):
""""""
:param int limit: The number of data items to process in the current\
round of processing.
:param tuple upper_date_limit: The date at which to stop collecting\
new data. This should be entered as a tuple which can serve as the\
argument to `datetime.datetime`.\
E.g. `date_limit=(2015, 4, 1, 12, 40)` for 12:30 pm on April 1 2015.
:param tuple lower_date_limit: The date at which to stop collecting\
new data. See `upper_data_limit` for formatting.
""""""
BasicTweetHandler.__init__(self, limit)
self.upper_date_limit = None
self.lower_date_limit = None
if upper_date_limit:
self.upper_date_limit = datetime(*upper_date_limit, tzinfo=LOCAL)
if lower_date_limit:
self.lower_date_limit = datetime(*lower_date_limit, tzinfo=LOCAL)
self.startingup = True
",[],0,[],/twitter/api.py___init__
5261,/home/amandapotts/git/nltk/nltk/twitter/api.py_handle,"def handle(self, data):
""""""
Deal appropriately with data returned by the Twitter API
""""""
",[],0,[],/twitter/api.py_handle
5262,/home/amandapotts/git/nltk/nltk/twitter/api.py_on_finish,"def on_finish(self):
""""""
Actions when the tweet limit has been reached
""""""
",[],0,[],/twitter/api.py_on_finish
5263,/home/amandapotts/git/nltk/nltk/twitter/api.py_check_date_limit,"def check_date_limit(self, data, verbose=False):
""""""
Validate date limits.
""""""
if self.upper_date_limit or self.lower_date_limit:
date_fmt = ""%a %b %d %H:%M:%S +0000 %Y""
tweet_date = datetime.strptime(data[""created_at""], date_fmt).replace(
tzinfo=timezone.utc
)
if (self.upper_date_limit and tweet_date > self.upper_date_limit) or (
self.lower_date_limit and tweet_date < self.lower_date_limit
):
if self.upper_date_limit:
message = ""earlier""
date_limit = self.upper_date_limit
else:
message = ""later""
date_limit = self.lower_date_limit
if verbose:
print(
""Date limit {} is {} than date of current tweet {}"".format(
date_limit, message, tweet_date
)
)
self.do_stop = True
",[],0,[],/twitter/api.py_check_date_limit
5264,/home/amandapotts/git/nltk/nltk/twitter/twitterclient.py___init__,"def __init__(self, app_key, app_secret, oauth_token, oauth_token_secret):
self.handler = None
self.do_continue = True
TwythonStreamer.__init__(
self, app_key, app_secret, oauth_token, oauth_token_secret
)
",[],0,[],/twitter/twitterclient.py___init__
5265,/home/amandapotts/git/nltk/nltk/twitter/twitterclient.py_register,"def register(self, handler):
""""""
Register a method for handling Tweets.
:param TweetHandlerI handler: method for viewing
""""""
self.handler = handler
",[],0,[],/twitter/twitterclient.py_register
5266,/home/amandapotts/git/nltk/nltk/twitter/twitterclient.py_on_success,"def on_success(self, data):
""""""
:param data: response from Twitter API
""""""
if self.do_continue:
if self.handler is not None:
if ""text"" in data:
self.handler.counter += 1
self.handler.handle(data)
self.do_continue = self.handler.do_continue()
else:
raise ValueError(""No data handler has been registered."")
else:
self.disconnect()
self.handler.on_finish()
",[],0,[],/twitter/twitterclient.py_on_success
5267,/home/amandapotts/git/nltk/nltk/twitter/twitterclient.py_on_error,"def on_error(self, status_code, data):
""""""
:param status_code: The status code returned by the Twitter API
:param data: The response from Twitter API
""""""
print(status_code)
",[],0,[],/twitter/twitterclient.py_on_error
5268,/home/amandapotts/git/nltk/nltk/twitter/twitterclient.py_sample,"def sample(self):
""""""
Wrapper for 'statuses / sample' API call
""""""
while self.do_continue:
try:
self.statuses.sample()
except requests.exceptions.ChunkedEncodingError as e:
if e is not None:
print(f""Error (stream will continue): {e}"")
continue
",[],0,[],/twitter/twitterclient.py_sample
5269,/home/amandapotts/git/nltk/nltk/twitter/twitterclient.py_filter,"def filter(self, track="""", follow="""", lang=""en""):
""""""
Wrapper for 'statuses / filter' API call
""""""
while self.do_continue:
try:
if track == """" and follow == """":
msg = ""Please supply a value for 'track', 'follow'""
raise ValueError(msg)
self.statuses.filter(track=track, follow=follow, lang=lang)
except requests.exceptions.ChunkedEncodingError as e:
if e is not None:
print(f""Error (stream will continue): {e}"")
continue
",[],0,[],/twitter/twitterclient.py_filter
5270,/home/amandapotts/git/nltk/nltk/twitter/twitterclient.py___init__,"def __init__(self, app_key, app_secret, oauth_token, oauth_token_secret):
""""""
:param app_key: (optional) Your applications key
:param app_secret: (optional) Your applications secret key
:param oauth_token: (optional) When using **OAuth 1**, combined with
oauth_token_secret to make authenticated calls
:param oauth_token_secret: (optional) When using **OAuth 1** combined
with oauth_token to make authenticated calls
""""""
self.handler = None
self.do_continue = True
Twython.__init__(self, app_key, app_secret, oauth_token, oauth_token_secret)
",[],0,[],/twitter/twitterclient.py___init__
5271,/home/amandapotts/git/nltk/nltk/twitter/twitterclient.py_register,"def register(self, handler):
""""""
Register a method for handling Tweets.
:param TweetHandlerI handler: method for viewing or writing Tweets to a file.
""""""
self.handler = handler
",[],0,[],/twitter/twitterclient.py_register
5272,/home/amandapotts/git/nltk/nltk/twitter/twitterclient.py_expand_tweetids,"def expand_tweetids(self, ids_f, verbose=True):
""""""
Given a file object containing a list of Tweet IDs, fetch the
corresponding full Tweets from the Twitter API.
The API call `statuses/lookup` will fail to retrieve a Tweet if the
user has deleted it.
This call to the Twitter API is rate-limited. See
<https://dev.twitter.com/rest/reference/get/statuses/lookup> for details.
:param ids_f: input file object consisting of Tweet IDs, one to a line
:return: iterable of Tweet objects in JSON format
""""""
ids = [line.strip() for line in ids_f if line]
if verbose:
print(f""Counted {len(ids)} Tweet IDs in {ids_f}."")
id_chunks = [ids[i : i + 100] for i in range(0, len(ids), 100)]
chunked_tweets = (self.lookup_status(id=chunk) for chunk in id_chunks)
return itertools.chain.from_iterable(chunked_tweets)
",[],0,[],/twitter/twitterclient.py_expand_tweetids
5273,/home/amandapotts/git/nltk/nltk/twitter/twitterclient.py__search_tweets,"def _search_tweets(self, keywords, limit=100, lang=""en""):
""""""
Assumes that the handler has been informed. Fetches Tweets from
search_tweets generator output and passses them to handler
:param str keywords: A list of query terms to search for, written as\
a comma-separated string.
:param int limit: Number of Tweets to process
:param str lang: language
""""""
while True:
tweets = self.search_tweets(
keywords=keywords, limit=limit, lang=lang, max_id=self.handler.max_id
)
for tweet in tweets:
self.handler.handle(tweet)
if not (self.handler.do_continue() and self.handler.repeat):
break
self.handler.on_finish()
",[],0,[],/twitter/twitterclient.py__search_tweets
5274,/home/amandapotts/git/nltk/nltk/twitter/twitterclient.py_search_tweets,"def search_tweets(
self,
keywords,
limit=100,
lang=""en"",
max_id=None,
retries_after_twython_exception=0,
",[],0,[],/twitter/twitterclient.py_search_tweets
5275,/home/amandapotts/git/nltk/nltk/twitter/twitterclient.py_user_info_from_id,"def user_info_from_id(self, userids):
""""""
Convert a list of userIDs into a variety of information about the users.
See <https://dev.twitter.com/rest/reference/get/users/show>.
:param list userids: A list of integer strings corresponding to Twitter userIDs
:rtype: list(json)
""""""
return [self.show_user(user_id=userid) for userid in userids]
",[],0,[],/twitter/twitterclient.py_user_info_from_id
5276,/home/amandapotts/git/nltk/nltk/twitter/twitterclient.py_user_tweets,"def user_tweets(self, screen_name, limit, include_rts=""false""):
""""""
Return a collection of the most recent Tweets posted by the user
:param str user: The user's screen name
should be omitted
:param int limit: The number of Tweets to recover
:param str include_rts: Whether to include statuses which have been\
retweeted by the user
""""""
data = self.get_user_timeline(
screen_name=screen_name, count=limit, include_rts=include_rts
)
for item in data:
self.handler.handle(item)
",[],0,[],/twitter/twitterclient.py_user_tweets
5277,/home/amandapotts/git/nltk/nltk/twitter/twitterclient.py___init__,"def __init__(self):
self._oauth = credsfromfile()
self.streamer = Streamer(**self._oauth)
self.query = Query(**self._oauth)
",[],0,[],/twitter/twitterclient.py___init__
5278,/home/amandapotts/git/nltk/nltk/twitter/twitterclient.py_tweets,"def tweets(
self,
keywords="""",
follow="""",
to_screen=True,
stream=True,
limit=100,
date_limit=None,
lang=""en"",
repeat=False,
gzip_compress=False,
",[],0,[],/twitter/twitterclient.py_tweets
5279,/home/amandapotts/git/nltk/nltk/twitter/twitterclient.py_handle,"def handle(self, data):
""""""
Direct data to `sys.stdout`
:return: return ``False`` if processing should cease, otherwise return ``True``.
:rtype: bool
:param data: Tweet object returned by Twitter API
""""""
text = data[""text""]
print(text)
self.check_date_limit(data)
if self.do_stop:
return
",[],0,[],/twitter/twitterclient.py_handle
5280,/home/amandapotts/git/nltk/nltk/twitter/twitterclient.py_on_finish,"def on_finish(self):
print(f""Written {self.counter} Tweets"")
",[],0,[],/twitter/twitterclient.py_on_finish
5281,/home/amandapotts/git/nltk/nltk/twitter/twitterclient.py___init__,"def __init__(
self,
limit=2000,
upper_date_limit=None,
lower_date_limit=None,
fprefix=""tweets"",
subdir=""twitter-files"",
repeat=False,
gzip_compress=False,
",[],0,[],/twitter/twitterclient.py___init__
5282,/home/amandapotts/git/nltk/nltk/twitter/twitterclient.py_timestamped_file,"def timestamped_file(self):
""""""
:return: timestamped file name
:rtype: str
""""""
subdir = self.subdir
fprefix = self.fprefix
if subdir:
if not os.path.exists(subdir):
os.mkdir(subdir)
fname = os.path.join(subdir, fprefix)
fmt = ""%Y%m%d-%H%M%S""
timestamp = datetime.datetime.now().strftime(fmt)
if self.gzip_compress:
suffix = "".gz""
else:
suffix = """"
outfile = f""{fname}.{timestamp}.json{suffix}""
return outfile
",[],0,[],/twitter/twitterclient.py_timestamped_file
5283,/home/amandapotts/git/nltk/nltk/twitter/twitterclient.py_handle,"def handle(self, data):
""""""
Write Twitter data as line-delimited JSON into one or more files.
:return: return `False` if processing should cease, otherwise return `True`.
:param data: tweet object returned by Twitter API
""""""
if self.startingup:
if self.gzip_compress:
self.output = gzip.open(self.fname, ""w"")
else:
self.output = open(self.fname, ""w"")
print(f""Writing to {self.fname}"")
json_data = json.dumps(data)
if self.gzip_compress:
self.output.write((json_data + ""\n"").encode(""utf-8""))
else:
self.output.write(json_data + ""\n"")
self.check_date_limit(data)
if self.do_stop:
return
self.startingup = False
",[],0,[],/twitter/twitterclient.py_handle
5284,/home/amandapotts/git/nltk/nltk/twitter/twitterclient.py_on_finish,"def on_finish(self):
print(f""Written {self.counter} Tweets"")
if self.output:
self.output.close()
",[],0,[],/twitter/twitterclient.py_on_finish
5285,/home/amandapotts/git/nltk/nltk/twitter/twitterclient.py_do_continue,"def do_continue(self):
if self.repeat == False:
return TweetHandlerI.do_continue(self)
if self.do_stop:
return False
if self.counter == self.limit:
self._restart_file()
return True
",[],0,[],/twitter/twitterclient.py_do_continue
5286,/home/amandapotts/git/nltk/nltk/twitter/twitterclient.py__restart_file,"def _restart_file(self):
self.on_finish()
self.fname = self.timestamped_file()
self.startingup = True
self.counter = 0
",[],0,[],/twitter/twitterclient.py__restart_file
5287,/home/amandapotts/git/nltk/nltk/chat/__init__.py_chatbots,"def chatbots():
print(""Which chatbot would you like to talk to?"")
botcount = len(bots)
for i in range(botcount):
print(""  %d: %s"" % (i + 1, bots[i][1]))
while True:
choice = input(f""\nEnter a number in the range 1-{botcount}: "").strip()
if choice.isdigit() and (int(choice) - 1) in range(botcount):
break
else:
print(""   Error: bad chatbot number"")
chatbot = bots[int(choice) - 1][0]
chatbot()
",[],0,[],/chat/__init__.py_chatbots
5288,/home/amandapotts/git/nltk/nltk/chat/iesha.py_iesha_chat,"def iesha_chat():
print(""Iesha the TeenBoT\n---------"")
print(""Talk to the program by typing in plain English, using normal upper-"")
print('and lower-case letters and punctuation.  Enter ""quit"" when done.')
print(""="" * 72)
print(""hi!! i'm iesha! who r u??!"")
iesha_chatbot.converse()
",[],0,[],/chat/iesha.py_iesha_chat
5289,/home/amandapotts/git/nltk/nltk/chat/iesha.py_demo,"def demo():
iesha_chat()
",[],0,[],/chat/iesha.py_demo
5290,/home/amandapotts/git/nltk/nltk/chat/suntsu.py_suntsu_chat,"def suntsu_chat():
print(""Talk to the program by typing in plain English, using normal upper-"")
print('and lower-case letters and punctuation.  Enter ""quit"" when done.')
print(""="" * 72)
print(""You seek enlightenment?"")
suntsu_chatbot.converse()
",[],0,[],/chat/suntsu.py_suntsu_chat
5291,/home/amandapotts/git/nltk/nltk/chat/suntsu.py_demo,"def demo():
suntsu_chat()
",[],0,[],/chat/suntsu.py_demo
5292,/home/amandapotts/git/nltk/nltk/chat/util.py___init__,"def __init__(self, pairs, reflections={}):
""""""
Initialize the chatbot.  Pairs is a list of patterns and responses.  Each
pattern is a regular expression matching the user's statement or question,
e.g. r'I like (.*)'.  For each such pattern a list of possible responses
is given, e.g. ['Why do you like %1', 'Did you ever dislike %1'].  Material
which is matched by parenthesized sections of the patterns (e.g. .*) is mapped to
the numbered positions in the responses, e.g. %1.
:type pairs: list of tuple
:param pairs: The patterns and responses
:type reflections: dict
:param reflections: A mapping between first and second person expressions
:rtype: None
""""""
self._pairs = [(re.compile(x, re.IGNORECASE), y) for (x, y) in pairs]
self._reflections = reflections
self._regex = self._compile_reflections()
",[],0,[],/chat/util.py___init__
5293,/home/amandapotts/git/nltk/nltk/chat/util.py__compile_reflections,"def _compile_reflections(self):
sorted_refl = sorted(self._reflections, key=len, reverse=True)
return re.compile(
r""\b({})\b"".format(""|"".join(map(re.escape, sorted_refl))), re.IGNORECASE
)
",[],0,[],/chat/util.py__compile_reflections
5294,/home/amandapotts/git/nltk/nltk/chat/util.py__wildcards,"def _wildcards(self, response, match):
pos = response.find(""%"")
while pos >= 0:
num = int(response[pos + 1 : pos + 2])
response = (
response[:pos]
+ self._substitute(match.group(num))
+ response[pos + 2 :]
)
pos = response.find(""%"")
return response
",[],0,[],/chat/util.py__wildcards
5295,/home/amandapotts/git/nltk/nltk/chat/util.py_respond,"def respond(self, str):
""""""
Generate a response to the user input.
:type str: str
:param str: The string to be mapped
:rtype: str
""""""
for pattern, response in self._pairs:
match = pattern.match(str)
if match:
resp = random.choice(response)  # pick a random response
resp = self._wildcards(resp, match)  # process wildcards
if resp[-2:] == ""?."":
resp = resp[:-2] + "".""
if resp[-2:] == ""??"":
resp = resp[:-2] + ""?""
return resp
",[],0,[],/chat/util.py_respond
5296,/home/amandapotts/git/nltk/nltk/chat/util.py_converse,"def converse(self, quit=""quit""):
user_input = """"
while user_input != quit:
user_input = quit
try:
user_input = input("">"")
except EOFError:
print(user_input)
if user_input:
while user_input[-1] in ""!."":
user_input = user_input[:-1]
print(self.respond(user_input))
",[],0,[],/chat/util.py_converse
5297,/home/amandapotts/git/nltk/nltk/chat/zen.py_zen_chat,"def zen_chat():
print(""*"" * 75)
print(""Zen Chatbot!"".center(75))
print(""*"" * 75)
print('""Look beyond mere words and letters - look into your mind""'.center(75))
print(""* Talk your way to truth with Zen Chatbot."")
print(""* Type 'quit' when you have had enough."")
print(""*"" * 75)
print(""Welcome, my child."")
zen_chatbot.converse()
",[],0,[],/chat/zen.py_zen_chat
5298,/home/amandapotts/git/nltk/nltk/chat/zen.py_demo,"def demo():
zen_chat()
",[],0,[],/chat/zen.py_demo
5299,/home/amandapotts/git/nltk/nltk/chat/eliza.py_eliza_chat,"def eliza_chat():
print(""Therapist\n---------"")
print(""Talk to the program by typing in plain English, using normal upper-"")
print('and lower-case letters and punctuation.  Enter ""quit"" when done.')
print(""="" * 72)
print(""Hello.  How are you feeling today?"")
eliza_chatbot.converse()
",[],0,[],/chat/eliza.py_eliza_chat
5300,/home/amandapotts/git/nltk/nltk/chat/eliza.py_demo,"def demo():
eliza_chat()
",[],0,[],/chat/eliza.py_demo
5301,/home/amandapotts/git/nltk/nltk/chat/rude.py_rude_chat,"def rude_chat():
print(""Talk to the program by typing in plain English, using normal upper-"")
print('and lower-case letters and punctuation.  Enter ""quit"" when done.')
print(""="" * 72)
print(""I suppose I should say hello."")
rude_chatbot.converse()
",[],0,[],/chat/rude.py_rude_chat
5302,/home/amandapotts/git/nltk/nltk/chat/rude.py_demo,"def demo():
rude_chat()
",[],0,[],/chat/rude.py_demo
5303,/home/amandapotts/git/nltk/nltk/lm/preprocessing.py_padded_everygrams,"def padded_everygrams(order, sentence):
""""""Helper with some useful defaults.
Applies pad_both_ends to sentence and follows it up with everygrams.
""""""
return everygrams(list(pad_both_ends(sentence, n=order)), max_len=order)
",[],0,[],/lm/preprocessing.py_padded_everygrams
5304,/home/amandapotts/git/nltk/nltk/lm/preprocessing.py_padded_everygram_pipeline,"def padded_everygram_pipeline(order, text):
""""""Default preprocessing for a sequence of sentences.
Creates two iterators:
- sentences padded and turned into sequences of `nltk.util.everygrams`
- sentences padded as above and chained together for a flat stream of words
:param order: Largest ngram length produced by `everygrams`.
:param text: Text to iterate over. Expected to be an iterable of sentences.
:type text: Iterable[Iterable[str]]
:return: iterator over text as ngrams, iterator over text as vocabulary data
""""""
padding_fn = partial(pad_both_ends, n=order)
return (
(everygrams(list(padding_fn(sent)), max_len=order) for sent in text),
flatten(map(padding_fn, text)),
)
",[],0,[],/lm/preprocessing.py_padded_everygram_pipeline
5305,/home/amandapotts/git/nltk/nltk/lm/util.py_log_base2,"def log_base2(score):
""""""Convenience function for computing logarithms with base 2.""""""
if score == 0.0:
return NEG_INF
return log(score, 2)
",[],0,[],/lm/util.py_log_base2
5306,/home/amandapotts/git/nltk/nltk/lm/models.py_unmasked_score,"def unmasked_score(self, word, context=None):
""""""Returns the MLE score for a word given a context.
Args:
- word is expected to be a string
- context is expected to be something reasonably convertible to a tuple
""""""
return self.context_counts(context).freq(word)
",[],0,[],/lm/models.py_unmasked_score
5307,/home/amandapotts/git/nltk/nltk/lm/models.py___init__,"def __init__(self, gamma, *args, **kwargs):
super().__init__(*args, **kwargs)
self.gamma = gamma
",[],0,[],/lm/models.py___init__
5308,/home/amandapotts/git/nltk/nltk/lm/models.py_unmasked_score,"def unmasked_score(self, word, context=None):
""""""Add-one smoothing: Lidstone or Laplace.
To see what kind, look at `gamma` attribute on the class.
""""""
counts = self.context_counts(context)
word_count = counts[word]
norm_count = counts.N()
return (word_count + self.gamma) / (norm_count + len(self.vocab) * self.gamma)
",[],0,[],/lm/models.py_unmasked_score
5309,/home/amandapotts/git/nltk/nltk/lm/models.py___init__,"def __init__(self, *args, **kwargs):
super().__init__(1, *args, **kwargs)
",[],0,[],/lm/models.py___init__
5310,/home/amandapotts/git/nltk/nltk/lm/models.py___init__,"def __init__(self, alpha=0.4, *args, **kwargs):
super().__init__(*args, **kwargs)
self.alpha = alpha
",[],0,[],/lm/models.py___init__
5311,/home/amandapotts/git/nltk/nltk/lm/models.py_unmasked_score,"def unmasked_score(self, word, context=None):
if not context:
return self.counts.unigrams.freq(word)
counts = self.context_counts(context)
word_count = counts[word]
norm_count = counts.N()
if word_count > 0:
return word_count / norm_count
else:
return self.alpha * self.unmasked_score(word, context[1:])
",[],0,[],/lm/models.py_unmasked_score
5312,/home/amandapotts/git/nltk/nltk/lm/models.py___init__,"def __init__(self, smoothing_cls, order, **kwargs):
params = kwargs.pop(""params"", {})
super().__init__(order, **kwargs)
self.estimator = smoothing_cls(self.vocab, self.counts, **params)
",[],0,[],/lm/models.py___init__
5313,/home/amandapotts/git/nltk/nltk/lm/models.py_unmasked_score,"def unmasked_score(self, word, context=None):
if not context:
return self.estimator.unigram_score(word)
if not self.counts[context]:
alpha, gamma = 0, 1
else:
alpha, gamma = self.estimator.alpha_gamma(word, context)
return alpha + gamma * self.unmasked_score(word, context[1:])
",[],0,[],/lm/models.py_unmasked_score
5314,/home/amandapotts/git/nltk/nltk/lm/models.py___init__,"def __init__(self, order, **kwargs):
super().__init__(WittenBell, order, **kwargs)
",[],0,[],/lm/models.py___init__
5315,/home/amandapotts/git/nltk/nltk/lm/models.py___init__,"def __init__(self, order, discount=0.75, **kwargs):
super().__init__(
AbsoluteDiscounting, order, params={""discount"": discount}, **kwargs
)
",[],0,[],/lm/models.py___init__
5316,/home/amandapotts/git/nltk/nltk/lm/models.py___init__,"def __init__(self, order, discount=0.1, **kwargs):
if not (0 <= discount <= 1):
raise ValueError(
""Discount must be between 0 and 1 for probabilities to sum to unity.""
)
super().__init__(
KneserNey, order, params={""discount"": discount, ""order"": order}, **kwargs
)
",[],0,[],/lm/models.py___init__
5317,/home/amandapotts/git/nltk/nltk/lm/counter.py___init__,"def __init__(self, ngram_text=None):
""""""Creates a new NgramCounter.
If `ngram_text` is specified, counts ngrams from it, otherwise waits for
`update` method to be called explicitly.
:param ngram_text: Optional text containing sentences of ngrams, as for `update` method.
:type ngram_text: Iterable(Iterable(tuple(str))) or None
""""""
self._counts = defaultdict(ConditionalFreqDist)
self._counts[1] = self.unigrams = FreqDist()
if ngram_text:
self.update(ngram_text)
",[],0,[],/lm/counter.py___init__
5318,/home/amandapotts/git/nltk/nltk/lm/counter.py_update,"def update(self, ngram_text):
""""""Updates ngram counts from `ngram_text`.
Expects `ngram_text` to be a sequence of sentences (sequences).
Each sentence consists of ngrams as tuples of strings.
:param Iterable(Iterable(tuple(str))) ngram_text: Text containing sentences of ngrams.
:raises TypeError: if the ngrams are not tuples.
""""""
for sent in ngram_text:
for ngram in sent:
if not isinstance(ngram, tuple):
raise TypeError(
""Ngram <{}> isn't a tuple, "" ""but {}"".format(ngram, type(ngram))
)
ngram_order = len(ngram)
if ngram_order == 1:
self.unigrams[ngram[0]] += 1
continue
context, word = ngram[:-1], ngram[-1]
self[ngram_order][context][word] += 1
",[],0,[],/lm/counter.py_update
5319,/home/amandapotts/git/nltk/nltk/lm/counter.py_N,"def N(self):
""""""Returns grand total number of ngrams stored.
This includes ngrams from all orders, so some duplication is expected.
:rtype: int
>>> from nltk.lm import NgramCounter
>>> counts = NgramCounter([[(""a"", ""b""), (""c"",), (""d"", ""e"")]])
>>> counts.N()
3
""""""
return sum(val.N() for val in self._counts.values())
",[],0,[],/lm/counter.py_N
5320,/home/amandapotts/git/nltk/nltk/lm/counter.py___getitem__,"def __getitem__(self, item):
""""""User-friendly access to ngram counts.""""""
if isinstance(item, int):
return self._counts[item]
elif isinstance(item, str):
return self._counts.__getitem__(1)[item]
elif isinstance(item, Sequence):
return self._counts.__getitem__(len(item) + 1)[tuple(item)]
",[],0,[],/lm/counter.py___getitem__
5321,/home/amandapotts/git/nltk/nltk/lm/counter.py___str__,"def __str__(self):
return ""<{} with {} ngram orders and {} ngrams>"".format(
self.__class__.__name__, len(self._counts), self.N()
)
",[],0,[],/lm/counter.py___str__
5322,/home/amandapotts/git/nltk/nltk/lm/counter.py___len__,"def __len__(self):
return self._counts.__len__()
",[],0,[],/lm/counter.py___len__
5323,/home/amandapotts/git/nltk/nltk/lm/counter.py___contains__,"def __contains__(self, item):
return item in self._counts
",[],0,[],/lm/counter.py___contains__
5324,/home/amandapotts/git/nltk/nltk/lm/api.py___init__,"def __init__(self, vocabulary, counter):
""""""
:param vocabulary: The Ngram vocabulary object.
:type vocabulary: nltk.lm.vocab.Vocabulary
:param counter: The counts of the vocabulary items.
:type counter: nltk.lm.counter.NgramCounter
""""""
self.vocab = vocabulary
self.counts = counter
",[],0,[],/lm/api.py___init__
5325,/home/amandapotts/git/nltk/nltk/lm/api.py_unigram_score,"def unigram_score(self, word):
raise NotImplementedError()
",[],0,[],/lm/api.py_unigram_score
5326,/home/amandapotts/git/nltk/nltk/lm/api.py_alpha_gamma,"def alpha_gamma(self, word, context):
raise NotImplementedError()
",[],0,[],/lm/api.py_alpha_gamma
5327,/home/amandapotts/git/nltk/nltk/lm/api.py__mean,"def _mean(items):
""""""Return average (aka mean) for sequence of items.""""""
return sum(items) / len(items)
",[],0,[],/lm/api.py__mean
5328,/home/amandapotts/git/nltk/nltk/lm/api.py__random_generator,"def _random_generator(seed_or_generator):
if isinstance(seed_or_generator, random.Random):
return seed_or_generator
return random.Random(seed_or_generator)
",[],0,[],/lm/api.py__random_generator
5329,/home/amandapotts/git/nltk/nltk/lm/api.py__weighted_choice,"def _weighted_choice(population, weights, random_generator=None):
""""""Like random.choice, but with weights.
Heavily inspired by python 3.6 `random.choices`.
""""""
if not population:
raise ValueError(""Can't choose from empty population"")
if len(population) != len(weights):
raise ValueError(""The number of weights does not match the population"")
cum_weights = list(accumulate(weights))
total = cum_weights[-1]
threshold = random_generator.random()
return population[bisect(cum_weights, total * threshold)]
",[],0,[],/lm/api.py__weighted_choice
5330,/home/amandapotts/git/nltk/nltk/lm/api.py___init__,"def __init__(self, order, vocabulary=None, counter=None):
""""""Creates new LanguageModel.
:param vocabulary: If provided, this vocabulary will be used instead
of creating a new one when training.
:type vocabulary: `nltk.lm.Vocabulary` or None
:param counter: If provided, use this object to count ngrams.
:type counter: `nltk.lm.NgramCounter` or None
:param ngrams_fn: If given, defines how sentences in training text are turned to ngram
sequences.
:type ngrams_fn: function or None
:param pad_fn: If given, defines how sentences in training text are padded.
:type pad_fn: function or None
""""""
self.order = order
if vocabulary and not isinstance(vocabulary, Vocabulary):
warnings.warn(
f""The `vocabulary` argument passed to {self.__class__.__name__!r} ""
""must be an instance of `nltk.lm.Vocabulary`."",
stacklevel=3,
)
self.vocab = Vocabulary() if vocabulary is None else vocabulary
self.counts = NgramCounter() if counter is None else counter
",[],0,[],/lm/api.py___init__
5331,/home/amandapotts/git/nltk/nltk/lm/api.py_fit,"def fit(self, text, vocabulary_text=None):
""""""Trains the model on a text.
:param text: Training text as a sequence of sentences.
""""""
if not self.vocab:
if vocabulary_text is None:
raise ValueError(
""Cannot fit without a vocabulary or text to create it from.""
)
self.vocab.update(vocabulary_text)
self.counts.update(self.vocab.lookup(sent) for sent in text)
",[],0,[],/lm/api.py_fit
5332,/home/amandapotts/git/nltk/nltk/lm/api.py_score,"def score(self, word, context=None):
""""""Masks out of vocab (OOV) words and computes their model score.
For model-specific logic of calculating scores, see the `unmasked_score`
method.
""""""
return self.unmasked_score(
self.vocab.lookup(word), self.vocab.lookup(context) if context else None
)
",[],0,[],/lm/api.py_score
5333,/home/amandapotts/git/nltk/nltk/lm/api.py_unmasked_score,"def unmasked_score(self, word, context=None):
""""""Score a word given some optional context.
Concrete models are expected to provide an implementation.
Note that this method does not mask its arguments with the OOV label.
Use the `score` method for that.
:param str word: Word for which we want the score
:param tuple(str) context: Context the word is in.
If `None`, compute unigram score.
:param context: tuple(str) or None
:rtype: float
""""""
raise NotImplementedError()
",[],0,[],/lm/api.py_unmasked_score
5334,/home/amandapotts/git/nltk/nltk/lm/api.py_logscore,"def logscore(self, word, context=None):
""""""Evaluate the log score of this word in this context.
The arguments are the same as for `score` and `unmasked_score`.
""""""
return log_base2(self.score(word, context))
",[],0,[],/lm/api.py_logscore
5335,/home/amandapotts/git/nltk/nltk/lm/api.py_context_counts,"def context_counts(self, context):
""""""Helper method for retrieving counts for a given context.
Assumes context has been checked and oov words in it masked.
:type context: tuple(str) or None
""""""
return (
self.counts[len(context) + 1][context] if context else self.counts.unigrams
)
",[],0,[],/lm/api.py_context_counts
5336,/home/amandapotts/git/nltk/nltk/lm/api.py_entropy,"def entropy(self, text_ngrams):
""""""Calculate cross-entropy of model for given evaluation text.
This implementation is based on the Shannon-McMillan-Breiman theorem,
as used and referenced by Dan Jurafsky and Jordan Boyd-Graber.
:param Iterable(tuple(str)) text_ngrams: A sequence of ngram tuples.
:rtype: float
""""""
return -1 * _mean(
[self.logscore(ngram[-1], ngram[:-1]) for ngram in text_ngrams]
)
",[],0,[],/lm/api.py_entropy
5337,/home/amandapotts/git/nltk/nltk/lm/api.py_perplexity,"def perplexity(self, text_ngrams):
""""""Calculates the perplexity of the given text.
This is simply 2 ** cross-entropy for the text, so the arguments are the same.
""""""
return pow(2.0, self.entropy(text_ngrams))
",[],0,[],/lm/api.py_perplexity
5338,/home/amandapotts/git/nltk/nltk/lm/api.py_generate,"def generate(self, num_words=1, text_seed=None, random_seed=None):
""""""Generate words from the model.
:param int num_words: How many words to generate. By default 1.
:param text_seed: Generation can be conditioned on preceding context.
:param random_seed: A random seed or an instance of `random.Random`. If provided,
makes the random sampling part of generation reproducible.
:return: One (str) word or a list of words generated from model.
Examples:
>>> from nltk.lm import MLE
>>> lm = MLE(2)
>>> lm.fit([[(""a"", ""b""), (""b"", ""c"")]], vocabulary_text=['a', 'b', 'c'])
>>> lm.fit([[(""a"",), (""b"",), (""c"",)]])
>>> lm.generate(random_seed=3)
'a'
>>> lm.generate(text_seed=['a'])
'b'
""""""
text_seed = [] if text_seed is None else list(text_seed)
random_generator = _random_generator(random_seed)
if num_words == 1:
context = (
text_seed[-self.order + 1 :]
if len(text_seed) >= self.order
else text_seed
)
samples = self.context_counts(self.vocab.lookup(context))
while context and not samples:
context = context[1:] if len(context) > 1 else []
samples = self.context_counts(self.vocab.lookup(context))
samples = sorted(samples)
return _weighted_choice(
samples,
tuple(self.score(w, context) for w in samples),
random_generator,
)
generated = []
for _ in range(num_words):
generated.append(
self.generate(
num_words=1,
text_seed=text_seed + generated,
random_seed=random_generator,
)
)
return generated
",[],0,[],/lm/api.py_generate
5339,/home/amandapotts/git/nltk/nltk/lm/smoothing.py___init__,"def __init__(self, vocabulary, counter, **kwargs):
super().__init__(vocabulary, counter, **kwargs)
",[],0,[],/lm/smoothing.py___init__
5340,/home/amandapotts/git/nltk/nltk/lm/smoothing.py_alpha_gamma,"def alpha_gamma(self, word, context):
alpha = self.counts[context].freq(word)
gamma = self._gamma(context)
return (1.0 - gamma) * alpha, gamma
",[],0,[],/lm/smoothing.py_alpha_gamma
5341,/home/amandapotts/git/nltk/nltk/lm/smoothing.py__gamma,"def _gamma(self, context):
n_plus = _count_values_gt_zero(self.counts[context])
return n_plus / (n_plus + self.counts[context].N())
",[],0,[],/lm/smoothing.py__gamma
5342,/home/amandapotts/git/nltk/nltk/lm/smoothing.py_unigram_score,"def unigram_score(self, word):
return self.counts.unigrams.freq(word)
",[],0,[],/lm/smoothing.py_unigram_score
5343,/home/amandapotts/git/nltk/nltk/lm/smoothing.py___init__,"def __init__(self, vocabulary, counter, discount=0.75, **kwargs):
super().__init__(vocabulary, counter, **kwargs)
self.discount = discount
",[],0,[],/lm/smoothing.py___init__
5344,/home/amandapotts/git/nltk/nltk/lm/smoothing.py_alpha_gamma,"def alpha_gamma(self, word, context):
alpha = (
max(self.counts[context][word] - self.discount, 0)
/ self.counts[context].N()
)
gamma = self._gamma(context)
return alpha, gamma
",[],0,[],/lm/smoothing.py_alpha_gamma
5345,/home/amandapotts/git/nltk/nltk/lm/smoothing.py__gamma,"def _gamma(self, context):
n_plus = _count_values_gt_zero(self.counts[context])
return (self.discount * n_plus) / self.counts[context].N()
",[],0,[],/lm/smoothing.py__gamma
5346,/home/amandapotts/git/nltk/nltk/lm/smoothing.py_unigram_score,"def unigram_score(self, word):
return self.counts.unigrams.freq(word)
",[],0,[],/lm/smoothing.py_unigram_score
5347,/home/amandapotts/git/nltk/nltk/lm/smoothing.py___init__,"def __init__(self, vocabulary, counter, order, discount=0.1, **kwargs):
super().__init__(vocabulary, counter, **kwargs)
self.discount = discount
self._order = order
",[],0,[],/lm/smoothing.py___init__
5348,/home/amandapotts/git/nltk/nltk/lm/smoothing.py_unigram_score,"def unigram_score(self, word):
word_continuation_count, total_count = self._continuation_counts(word)
return word_continuation_count / total_count
",[],0,[],/lm/smoothing.py_unigram_score
5349,/home/amandapotts/git/nltk/nltk/lm/smoothing.py_alpha_gamma,"def alpha_gamma(self, word, context):
prefix_counts = self.counts[context]
word_continuation_count, total_count = (
(prefix_counts[word], prefix_counts.N())
if len(context) + 1 == self._order
else self._continuation_counts(word, context)
)
alpha = max(word_continuation_count - self.discount, 0.0) / total_count
gamma = self.discount * _count_values_gt_zero(prefix_counts) / total_count
return alpha, gamma
",[],0,[],/lm/smoothing.py_alpha_gamma
5350,/home/amandapotts/git/nltk/nltk/lm/smoothing.py__continuation_counts,"def _continuation_counts(self, word, context=tuple()):
""""""Count continuations that end with context and word.
Continuations track unique ngram ""types"", regardless of how many
instances were observed for each ""type"".
This is different than raw ngram counts which track number of instances.
""""""
higher_order_ngrams_with_context = (
counts
for prefix_ngram, counts in self.counts[len(context) + 2].items()
if prefix_ngram[1:] == context
)
higher_order_ngrams_with_word_count, total = 0, 0
for counts in higher_order_ngrams_with_context:
higher_order_ngrams_with_word_count += int(counts[word] > 0)
total += _count_values_gt_zero(counts)
return higher_order_ngrams_with_word_count, total
",[],0,[],/lm/smoothing.py__continuation_counts
5351,/home/amandapotts/git/nltk/nltk/lm/vocabulary.py__dispatched_lookup,"def _dispatched_lookup(words, vocab):
raise TypeError(f""Unsupported type for looking up in vocabulary: {type(words)}"")
",[],0,[],/lm/vocabulary.py__dispatched_lookup
5352,/home/amandapotts/git/nltk/nltk/lm/vocabulary.py__,"def _(words, vocab):
""""""Look up a sequence of words in the vocabulary.
Returns an iterator over looked up words.
""""""
return tuple(_dispatched_lookup(w, vocab) for w in words)
",[],0,[],/lm/vocabulary.py__
5353,/home/amandapotts/git/nltk/nltk/lm/vocabulary.py__string_lookup,"def _string_lookup(word, vocab):
""""""Looks up one word in the vocabulary.""""""
return word if word in vocab else vocab.unk_label
",[],0,[],/lm/vocabulary.py__string_lookup
5354,/home/amandapotts/git/nltk/nltk/lm/vocabulary.py___init__,"def __init__(self, counts=None, unk_cutoff=1, unk_label=""<UNK>""):
""""""Create a new Vocabulary.
:param counts: Optional iterable or `collections.Counter` instance to
pre-seed the Vocabulary. In case it is iterable, counts
are calculated.
:param int unk_cutoff: Words that occur less frequently than this value
are not considered part of the vocabulary.
:param unk_label: Label for marking words not part of vocabulary.
""""""
self.unk_label = unk_label
if unk_cutoff < 1:
raise ValueError(f""Cutoff value cannot be less than 1. Got: {unk_cutoff}"")
self._cutoff = unk_cutoff
self.counts = Counter()
self.update(counts if counts is not None else """")
",[],0,[],/lm/vocabulary.py___init__
5355,/home/amandapotts/git/nltk/nltk/lm/vocabulary.py_cutoff,"def cutoff(self):
""""""Cutoff value.
Items with count below this value are not considered part of vocabulary.
""""""
return self._cutoff
",[],0,[],/lm/vocabulary.py_cutoff
5356,/home/amandapotts/git/nltk/nltk/lm/vocabulary.py_update,"def update(self, *counter_args, **counter_kwargs):
""""""Update vocabulary counts.
Wraps `collections.Counter.update` method.
""""""
self.counts.update(*counter_args, **counter_kwargs)
self._len = sum(1 for _ in self)
",[],0,[],/lm/vocabulary.py_update
5357,/home/amandapotts/git/nltk/nltk/lm/vocabulary.py_lookup,"def lookup(self, words):
""""""Look up one or more words in the vocabulary.
If passed one word as a string will return that word or `self.unk_label`.
Otherwise will assume it was passed a sequence of words, will try to look
each of them up and return an iterator over the looked up words.
:param words: Word(s) to look up.
:type words: Iterable(str) or str
:rtype: generator(str) or str
:raises: TypeError for types other than strings or iterables
>>> from nltk.lm import Vocabulary
>>> vocab = Vocabulary([""a"", ""b"", ""c"", ""a"", ""b""], unk_cutoff=2)
>>> vocab.lookup(""a"")
'a'
>>> vocab.lookup(""aliens"")
'<UNK>'
>>> vocab.lookup([""a"", ""b"", ""c"", [""x"", ""b""]])
('a', 'b', '<UNK>', ('<UNK>', 'b'))
""""""
return _dispatched_lookup(words, self)
",[],0,[],/lm/vocabulary.py_lookup
5358,/home/amandapotts/git/nltk/nltk/lm/vocabulary.py___getitem__,"def __getitem__(self, item):
return self._cutoff if item == self.unk_label else self.counts[item]
",[],0,[],/lm/vocabulary.py___getitem__
5359,/home/amandapotts/git/nltk/nltk/lm/vocabulary.py___contains__,"def __contains__(self, item):
""""""Only consider items with counts GE to cutoff as being in the
vocabulary.""""""
return self[item] >= self.cutoff
",[],0,[],/lm/vocabulary.py___contains__
5360,/home/amandapotts/git/nltk/nltk/lm/vocabulary.py___iter__,"def __iter__(self):
""""""Building on membership check define how to iterate over
vocabulary.""""""
return chain(
(item for item in self.counts if item in self),
[self.unk_label] if self.counts else [],
)
",[],0,[],/lm/vocabulary.py___iter__
5361,/home/amandapotts/git/nltk/nltk/lm/vocabulary.py___len__,"def __len__(self):
""""""Computing size of vocabulary reflects the cutoff.""""""
return self._len
",[],0,[],/lm/vocabulary.py___len__
5362,/home/amandapotts/git/nltk/nltk/lm/vocabulary.py___eq__,"def __eq__(self, other):
return (
self.unk_label == other.unk_label
and self.cutoff == other.cutoff
and self.counts == other.counts
)
",[],0,[],/lm/vocabulary.py___eq__
5363,/home/amandapotts/git/nltk/nltk/lm/vocabulary.py___str__,"def __str__(self):
return ""<{} with cutoff={} unk_label='{}' and {} items>"".format(
self.__class__.__name__, self.cutoff, self.unk_label, len(self)
)
",[],0,[],/lm/vocabulary.py___str__
5364,/home/amandapotts/git/nltk/nltk/test/classify_fixt.py_setup_module,"def setup_module():
import pytest
pytest.importorskip(""numpy"")
",[],0,[],/test/classify_fixt.py_setup_module
5365,/home/amandapotts/git/nltk/nltk/test/all.py_additional_tests,"def additional_tests():
dir = os.path.dirname(__file__)
paths = glob(os.path.join(dir, ""*.doctest""))
files = [os.path.basename(path) for path in paths]
return unittest.TestSuite([doctest.DocFileSuite(file) for file in files])
",[],0,[],/test/all.py_additional_tests
5366,/home/amandapotts/git/nltk/nltk/test/probability_fixt.py_setup_module,"def setup_module():
import pytest
pytest.importorskip(""numpy"")
",[],0,[],/test/probability_fixt.py_setup_module
5367,/home/amandapotts/git/nltk/nltk/test/conftest.py_mock_plot,"def mock_plot(mocker):
""""""Disable matplotlib plotting in test code""""""
try:
import matplotlib.pyplot as plt
mocker.patch.object(plt, ""gca"")
mocker.patch.object(plt, ""show"")
except ImportError:
pass
",[],0,[],/test/conftest.py_mock_plot
5368,/home/amandapotts/git/nltk/nltk/test/conftest.py_teardown_loaded_corpora,"def teardown_loaded_corpora():
""""""
After each test session ends (either doctest or unit test),
unload any loaded corpora
""""""
yield  # first, wait for the test to end
import nltk.corpus
for name in dir(nltk.corpus):
obj = getattr(nltk.corpus, name, None)
if isinstance(obj, CorpusReader) and hasattr(obj, ""_unload""):
obj._unload()
",[],0,[],/test/conftest.py_teardown_loaded_corpora
5369,/home/amandapotts/git/nltk/nltk/test/childes_fixt.py_setup_module,"def setup_module():
import pytest
import nltk.data
try:
nltk.data.find(""corpora/childes/data-xml/Eng-USA-MOR/"")
except LookupError as e:
pytest.skip(
""The CHILDES corpus is not found. ""
""It should be manually downloaded and saved/unpacked ""
""to [NLTK_Data_Dir]/corpora/childes/""
)
",[],0,[],/test/childes_fixt.py_setup_module
5370,/home/amandapotts/git/nltk/nltk/test/portuguese_en_fixt.py_setup_module,"def setup_module():
import pytest
pytest.skip(""portuguese_en.doctest imports nltk.examples.pt which doesn't exist!"")
",[],0,[],/test/portuguese_en_fixt.py_setup_module
5371,/home/amandapotts/git/nltk/nltk/test/gluesemantics_malt_fixt.py_setup_module,"def setup_module():
import pytest
from nltk.parse.malt import MaltParser
try:
depparser = MaltParser()
except (AssertionError, LookupError) as e:
pytest.skip(""MaltParser is not available"")
",[],0,[],/test/gluesemantics_malt_fixt.py_setup_module
5372,/home/amandapotts/git/nltk/nltk/test/gensim_fixt.py_setup_module,"def setup_module():
import pytest
pytest.importorskip(""gensim"")
",[],0,[],/test/gensim_fixt.py_setup_module
5373,/home/amandapotts/git/nltk/nltk/test/setup_fixt.py_check_binary,"def check_binary(binary: str, **args):
""""""Skip a test via `pytest.skip` if the `binary` executable is not found.
Keyword arguments are passed to `nltk.internals.find_binary`.""""""
import pytest
try:
find_binary(binary, **args)
except LookupError:
pytest.skip(f""Skipping test because the {binary} binary was not found."")
",[],0,[],/test/setup_fixt.py_check_binary
5374,/home/amandapotts/git/nltk/nltk/test/setup_fixt.py_check_jar,"def check_jar(name_pattern: str, **args):
""""""Skip a test via `pytest.skip` if the `name_pattern` jar is not found.
Keyword arguments are passed to `nltk.internals.find_jar`.
TODO: Investigate why the CoreNLP tests that rely on this check_jar failed
on the CI. https://github.com/nltk/nltk/pull/3060#issuecomment-1268355108
""""""
import pytest
pytest.skip(
""Skipping test because the doctests requiring jars are inconsistent on the CI.""
)
",[],0,[],/test/setup_fixt.py_check_jar
5375,/home/amandapotts/git/nltk/nltk/test/unit/test_json2csv_corpus.py_files_are_identical,"def files_are_identical(pathA, pathB):
""""""
Compare two files, ignoring carriage returns,
leading whitespace, and trailing whitespace
""""""
f1 = [l.strip() for l in pathA.read_bytes().splitlines()]
f2 = [l.strip() for l in pathB.read_bytes().splitlines()]
return f1 == f2
",[],0,[],/test/unit/test_json2csv_corpus.py_files_are_identical
5376,/home/amandapotts/git/nltk/nltk/test/unit/test_json2csv_corpus.py_infile,"def infile():
with open(twitter_samples.abspath(""tweets.20150430-223406.json"")) as infile:
return [next(infile) for x in range(100)]
",[],0,[],/test/unit/test_json2csv_corpus.py_infile
5377,/home/amandapotts/git/nltk/nltk/test/unit/test_json2csv_corpus.py_test_textoutput,"def test_textoutput(tmp_path, infile):
ref_fn = subdir / ""tweets.20150430-223406.text.csv.ref""
outfn = tmp_path / ""tweets.20150430-223406.text.csv""
json2csv(infile, outfn, [""text""], gzip_compress=False)
assert files_are_identical(outfn, ref_fn)
",[],0,[],/test/unit/test_json2csv_corpus.py_test_textoutput
5378,/home/amandapotts/git/nltk/nltk/test/unit/test_json2csv_corpus.py_test_tweet_metadata,"def test_tweet_metadata(tmp_path, infile):
ref_fn = subdir / ""tweets.20150430-223406.tweet.csv.ref""
fields = [
""created_at"",
""favorite_count"",
""id"",
""in_reply_to_status_id"",
""in_reply_to_user_id"",
""retweet_count"",
""retweeted"",
""text"",
""truncated"",
""user.id"",
]
outfn = tmp_path / ""tweets.20150430-223406.tweet.csv""
json2csv(infile, outfn, fields, gzip_compress=False)
assert files_are_identical(outfn, ref_fn)
",[],0,[],/test/unit/test_json2csv_corpus.py_test_tweet_metadata
5379,/home/amandapotts/git/nltk/nltk/test/unit/test_json2csv_corpus.py_test_user_metadata,"def test_user_metadata(tmp_path, infile):
ref_fn = subdir / ""tweets.20150430-223406.user.csv.ref""
fields = [""id"", ""text"", ""user.id"", ""user.followers_count"", ""user.friends_count""]
outfn = tmp_path / ""tweets.20150430-223406.user.csv""
json2csv(infile, outfn, fields, gzip_compress=False)
assert files_are_identical(outfn, ref_fn)
",[],0,[],/test/unit/test_json2csv_corpus.py_test_user_metadata
5380,/home/amandapotts/git/nltk/nltk/test/unit/test_json2csv_corpus.py_test_tweet_hashtag,"def test_tweet_hashtag(tmp_path, infile):
ref_fn = subdir / ""tweets.20150430-223406.hashtag.csv.ref""
outfn = tmp_path / ""tweets.20150430-223406.hashtag.csv""
json2csv_entities(
infile,
outfn,
[""id"", ""text""],
""hashtags"",
[""text""],
gzip_compress=False,
)
assert files_are_identical(outfn, ref_fn)
",[],0,[],/test/unit/test_json2csv_corpus.py_test_tweet_hashtag
5381,/home/amandapotts/git/nltk/nltk/test/unit/test_json2csv_corpus.py_test_tweet_usermention,"def test_tweet_usermention(tmp_path, infile):
ref_fn = subdir / ""tweets.20150430-223406.usermention.csv.ref""
outfn = tmp_path / ""tweets.20150430-223406.usermention.csv""
json2csv_entities(
infile,
outfn,
[""id"", ""text""],
""user_mentions"",
[""id"", ""screen_name""],
gzip_compress=False,
)
assert files_are_identical(outfn, ref_fn)
",[],0,[],/test/unit/test_json2csv_corpus.py_test_tweet_usermention
5382,/home/amandapotts/git/nltk/nltk/test/unit/test_json2csv_corpus.py_test_tweet_media,"def test_tweet_media(tmp_path, infile):
ref_fn = subdir / ""tweets.20150430-223406.media.csv.ref""
outfn = tmp_path / ""tweets.20150430-223406.media.csv""
json2csv_entities(
infile,
outfn,
[""id""],
""media"",
[""media_url"", ""url""],
gzip_compress=False,
)
assert files_are_identical(outfn, ref_fn)
",[],0,[],/test/unit/test_json2csv_corpus.py_test_tweet_media
5383,/home/amandapotts/git/nltk/nltk/test/unit/test_json2csv_corpus.py_test_tweet_url,"def test_tweet_url(tmp_path, infile):
ref_fn = subdir / ""tweets.20150430-223406.url.csv.ref""
outfn = tmp_path / ""tweets.20150430-223406.url.csv""
json2csv_entities(
infile,
outfn,
[""id""],
""urls"",
[""url"", ""expanded_url""],
gzip_compress=False,
)
assert files_are_identical(outfn, ref_fn)
",[],0,[],/test/unit/test_json2csv_corpus.py_test_tweet_url
5384,/home/amandapotts/git/nltk/nltk/test/unit/test_json2csv_corpus.py_test_userurl,"def test_userurl(tmp_path, infile):
ref_fn = subdir / ""tweets.20150430-223406.userurl.csv.ref""
outfn = tmp_path / ""tweets.20150430-223406.userurl.csv""
json2csv_entities(
infile,
outfn,
[""id"", ""screen_name""],
""user.urls"",
[""url"", ""expanded_url""],
gzip_compress=False,
)
assert files_are_identical(outfn, ref_fn)
",[],0,[],/test/unit/test_json2csv_corpus.py_test_userurl
5385,/home/amandapotts/git/nltk/nltk/test/unit/test_json2csv_corpus.py_test_tweet_place,"def test_tweet_place(tmp_path, infile):
ref_fn = subdir / ""tweets.20150430-223406.place.csv.ref""
outfn = tmp_path / ""tweets.20150430-223406.place.csv""
json2csv_entities(
infile,
outfn,
[""id"", ""text""],
""place"",
[""name"", ""country""],
gzip_compress=False,
)
assert files_are_identical(outfn, ref_fn)
",[],0,[],/test/unit/test_json2csv_corpus.py_test_tweet_place
5386,/home/amandapotts/git/nltk/nltk/test/unit/test_json2csv_corpus.py_test_tweet_place_boundingbox,"def test_tweet_place_boundingbox(tmp_path, infile):
ref_fn = subdir / ""tweets.20150430-223406.placeboundingbox.csv.ref""
outfn = tmp_path / ""tweets.20150430-223406.placeboundingbox.csv""
json2csv_entities(
infile,
outfn,
[""id"", ""name""],
""place.bounding_box"",
[""coordinates""],
gzip_compress=False,
)
assert files_are_identical(outfn, ref_fn)
",[],0,[],/test/unit/test_json2csv_corpus.py_test_tweet_place_boundingbox
5387,/home/amandapotts/git/nltk/nltk/test/unit/test_json2csv_corpus.py_test_retweet_original_tweet,"def test_retweet_original_tweet(tmp_path, infile):
ref_fn = subdir / ""tweets.20150430-223406.retweet.csv.ref""
outfn = tmp_path / ""tweets.20150430-223406.retweet.csv""
json2csv_entities(
infile,
outfn,
[""id""],
""retweeted_status"",
[
""created_at"",
""favorite_count"",
""id"",
""in_reply_to_status_id"",
""in_reply_to_user_id"",
""retweet_count"",
""text"",
""truncated"",
""user.id"",
],
gzip_compress=False,
)
assert files_are_identical(outfn, ref_fn)
",[],0,[],/test/unit/test_json2csv_corpus.py_test_retweet_original_tweet
5388,/home/amandapotts/git/nltk/nltk/test/unit/test_json2csv_corpus.py_test_file_is_wrong,"def test_file_is_wrong(tmp_path, infile):
""""""
Sanity check that file comparison is not giving false positives.
""""""
ref_fn = subdir / ""tweets.20150430-223406.retweet.csv.ref""
outfn = tmp_path / ""tweets.20150430-223406.text.csv""
json2csv(infile, outfn, [""text""], gzip_compress=False)
assert not files_are_identical(outfn, ref_fn)
",[],0,[],/test/unit/test_json2csv_corpus.py_test_file_is_wrong
5389,/home/amandapotts/git/nltk/nltk/test/unit/test_cfd_mutation.py_test_tabulate,"def test_tabulate(self):
empty = ConditionalFreqDist()
self.assertEqual(empty.conditions(), [])
with pytest.raises(ValueError):
empty.tabulate(conditions=""BUG"")  # nonexistent keys shouldn't be added
self.assertEqual(empty.conditions(), [])
",[],0,[],/test/unit/test_cfd_mutation.py_test_tabulate
5390,/home/amandapotts/git/nltk/nltk/test/unit/test_cfd_mutation.py_test_plot,"def test_plot(self):
empty = ConditionalFreqDist()
self.assertEqual(empty.conditions(), [])
empty.plot(conditions=[""BUG""])  # nonexistent keys shouldn't be added
self.assertEqual(empty.conditions(), [])
",[],0,[],/test/unit/test_cfd_mutation.py_test_plot
5391,/home/amandapotts/git/nltk/nltk/test/unit/test_cfd_mutation.py_test_increment,"def test_increment(self):
text = ""cow cat mouse cat tiger""
cfd = ConditionalFreqDist()
for word in tokenize.word_tokenize(text):
condition = len(word)
cfd[condition][word] += 1
self.assertEqual(cfd.conditions(), [3, 5])
cfd[2][""hi""] += 1
self.assertCountEqual(cfd.conditions(), [3, 5, 2])  # new condition added
self.assertEqual(
cfd[2][""hi""], 1
)  # key's frequency incremented from 0 (unseen) to 1
",[],0,[],/test/unit/test_cfd_mutation.py_test_increment
5392,/home/amandapotts/git/nltk/nltk/test/unit/test_concordance.py_stdout_redirect,"def stdout_redirect(where):
sys.stdout = where
try:
yield where
finally:
sys.stdout = sys.__stdout__
",[],0,[],/test/unit/test_concordance.py_stdout_redirect
5393,/home/amandapotts/git/nltk/nltk/test/unit/test_concordance.py_setUpClass,"def setUpClass(cls):
cls.corpus = gutenberg.words(""melville-moby_dick.txt"")
",[],0,[],/test/unit/test_concordance.py_setUpClass
5394,/home/amandapotts/git/nltk/nltk/test/unit/test_concordance.py_tearDownClass,"def tearDownClass(cls):
pass
",[],0,[],/test/unit/test_concordance.py_tearDownClass
5395,/home/amandapotts/git/nltk/nltk/test/unit/test_concordance.py_setUp,"def setUp(self):
self.text = Text(TestConcordance.corpus)
self.query = ""monstrous""
self.maxDiff = None
self.list_out = [
""ong the former , one was of a most monstrous size . ... This came towards us , "",
'ON OF THE PSALMS . "" Touching that monstrous bulk of the whale or ork we have r',
""ll over with a heathenish array of monstrous clubs and spears . Some were thick"",
""d as you gazed , and wondered what monstrous cannibal and savage could ever hav"",
""that has survived the flood 
""they might scout at Moby Dick as a monstrous fable , or still worse and more de"",
""th of Radney .'\"" CHAPTER 55 Of the Monstrous Pictures of Whales . I shall ere l"",
""ing Scenes . In connexion with the monstrous pictures of whales , I am strongly"",
""ere to enter upon those still more monstrous stories of them which are to be fo"",
""ght have been rummaged out of this monstrous cabinet there is no telling . But "",
""of Whale - Bones 
]
",[],0,[],/test/unit/test_concordance.py_setUp
5396,/home/amandapotts/git/nltk/nltk/test/unit/test_concordance.py_tearDown,"def tearDown(self):
pass
",[],0,[],/test/unit/test_concordance.py_tearDown
5397,/home/amandapotts/git/nltk/nltk/test/unit/test_concordance.py_test_concordance_list,"def test_concordance_list(self):
concordance_out = self.text.concordance_list(self.query)
self.assertEqual(self.list_out, [c.line for c in concordance_out])
",[],0,[],/test/unit/test_concordance.py_test_concordance_list
5398,/home/amandapotts/git/nltk/nltk/test/unit/test_concordance.py_test_concordance_width,"def test_concordance_width(self):
list_out = [
""monstrous"",
""monstrous"",
""monstrous"",
""monstrous"",
""monstrous"",
""monstrous"",
""Monstrous"",
""monstrous"",
""monstrous"",
""monstrous"",
""monstrous"",
]
concordance_out = self.text.concordance_list(self.query, width=0)
self.assertEqual(list_out, [c.query for c in concordance_out])
",[],0,[],/test/unit/test_concordance.py_test_concordance_width
5399,/home/amandapotts/git/nltk/nltk/test/unit/test_concordance.py_test_concordance_lines,"def test_concordance_lines(self):
concordance_out = self.text.concordance_list(self.query, lines=3)
self.assertEqual(self.list_out[:3], [c.line for c in concordance_out])
",[],0,[],/test/unit/test_concordance.py_test_concordance_lines
5400,/home/amandapotts/git/nltk/nltk/test/unit/test_concordance.py_test_concordance_print,"def test_concordance_print(self):
print_out = """"""Displaying 11 of 11 matches:
ong the former , one was of a most monstrous size . ... This came towards us ,
ON OF THE PSALMS . "" Touching that monstrous bulk of the whale or ork we have r
ll over with a heathenish array of monstrous clubs and spears . Some were thick
d as you gazed , and wondered what monstrous cannibal and savage could ever hav
that has survived the flood 
they might scout at Moby Dick as a monstrous fable , or still worse and more de
th of Radney .'"" CHAPTER 55 Of the Monstrous Pictures of Whales . I shall ere l
ing Scenes . In connexion with the monstrous pictures of whales , I am strongly
ere to enter upon those still more monstrous stories of them which are to be fo
ght have been rummaged out of this monstrous cabinet there is no telling . But
of Whale - Bones 
""""""
with stdout_redirect(StringIO()) as stdout:
self.text.concordance(self.query)
",[],0,[],/test/unit/test_concordance.py_test_concordance_print
5401,/home/amandapotts/git/nltk/nltk/test/unit/test_concordance.py_strip_space,"def strip_space(raw_str):
return raw_str.replace("" "", """")
",[],0,[],/test/unit/test_concordance.py_strip_space
5402,/home/amandapotts/git/nltk/nltk/test/unit/test_json_serialization.py_setUp,"def setUp(self):
self.corpus = brown.tagged_sents()[:35]
self.decoder = JSONTaggedDecoder()
self.encoder = JSONTaggedEncoder()
self.default_tagger = DefaultTagger(""NN"")
",[],0,[],/test/unit/test_json_serialization.py_setUp
5403,/home/amandapotts/git/nltk/nltk/test/unit/test_json_serialization.py_test_default_tagger,"def test_default_tagger(self):
encoded = self.encoder.encode(self.default_tagger)
decoded = self.decoder.decode(encoded)
self.assertEqual(repr(self.default_tagger), repr(decoded))
self.assertEqual(self.default_tagger._tag, decoded._tag)
",[],0,[],/test/unit/test_json_serialization.py_test_default_tagger
5404,/home/amandapotts/git/nltk/nltk/test/unit/test_json_serialization.py_test_regexp_tagger,"def test_regexp_tagger(self):
tagger = RegexpTagger([(r"".*"", ""NN"")], backoff=self.default_tagger)
encoded = self.encoder.encode(tagger)
decoded = self.decoder.decode(encoded)
self.assertEqual(repr(tagger), repr(decoded))
self.assertEqual(repr(tagger.backoff), repr(decoded.backoff))
self.assertEqual(tagger._regexps, decoded._regexps)
",[],0,[],/test/unit/test_json_serialization.py_test_regexp_tagger
5405,/home/amandapotts/git/nltk/nltk/test/unit/test_json_serialization.py_test_affix_tagger,"def test_affix_tagger(self):
tagger = AffixTagger(self.corpus, backoff=self.default_tagger)
encoded = self.encoder.encode(tagger)
decoded = self.decoder.decode(encoded)
self.assertEqual(repr(tagger), repr(decoded))
self.assertEqual(repr(tagger.backoff), repr(decoded.backoff))
self.assertEqual(tagger._affix_length, decoded._affix_length)
self.assertEqual(tagger._min_word_length, decoded._min_word_length)
self.assertEqual(tagger._context_to_tag, decoded._context_to_tag)
",[],0,[],/test/unit/test_json_serialization.py_test_affix_tagger
5406,/home/amandapotts/git/nltk/nltk/test/unit/test_json_serialization.py_test_ngram_taggers,"def test_ngram_taggers(self):
unitagger = UnigramTagger(self.corpus, backoff=self.default_tagger)
bitagger = BigramTagger(self.corpus, backoff=unitagger)
tritagger = TrigramTagger(self.corpus, backoff=bitagger)
ntagger = NgramTagger(4, self.corpus, backoff=tritagger)
encoded = self.encoder.encode(ntagger)
decoded = self.decoder.decode(encoded)
self.assertEqual(repr(ntagger), repr(decoded))
self.assertEqual(repr(tritagger), repr(decoded.backoff))
self.assertEqual(repr(bitagger), repr(decoded.backoff.backoff))
self.assertEqual(repr(unitagger), repr(decoded.backoff.backoff.backoff))
self.assertEqual(
repr(self.default_tagger), repr(decoded.backoff.backoff.backoff.backoff)
)
",[],0,[],/test/unit/test_json_serialization.py_test_ngram_taggers
5407,/home/amandapotts/git/nltk/nltk/test/unit/test_json_serialization.py_test_perceptron_tagger,"def test_perceptron_tagger(self):
tagger = PerceptronTagger(load=False)
tagger.train(self.corpus)
encoded = self.encoder.encode(tagger)
decoded = self.decoder.decode(encoded)
self.assertEqual(tagger.model.weights, decoded.model.weights)
self.assertEqual(tagger.tagdict, decoded.tagdict)
self.assertEqual(tagger.classes, decoded.classes)
",[],0,[],/test/unit/test_json_serialization.py_test_perceptron_tagger
5408,/home/amandapotts/git/nltk/nltk/test/unit/test_json_serialization.py_test_brill_tagger,"def test_brill_tagger(self):
trainer = BrillTaggerTrainer(
self.default_tagger, nltkdemo18(), deterministic=True
)
tagger = trainer.train(self.corpus, max_rules=30)
encoded = self.encoder.encode(tagger)
decoded = self.decoder.decode(encoded)
self.assertEqual(repr(tagger._initial_tagger), repr(decoded._initial_tagger))
self.assertEqual(tagger._rules, decoded._rules)
self.assertEqual(tagger._training_stats, decoded._training_stats)
",[],0,[],/test/unit/test_json_serialization.py_test_brill_tagger
5409,/home/amandapotts/git/nltk/nltk/test/unit/test_wordnet.py_test_retrieve_synset,"def test_retrieve_synset(self):
move_synset = S(""go.v.21"")
self.assertEqual(move_synset.name(), ""move.v.15"")
self.assertEqual(move_synset.lemma_names(), [""move"", ""go""])
self.assertEqual(
move_synset.definition(), ""have a turn
)
self.assertEqual(move_synset.examples(), [""Can I go now?""])
",[],0,[],/test/unit/test_wordnet.py_test_retrieve_synset
5410,/home/amandapotts/git/nltk/nltk/test/unit/test_wordnet.py_test_retrieve_synsets,"def test_retrieve_synsets(self):
self.assertEqual(sorted(wn.synsets(""zap"", pos=""n"")), [S(""zap.n.01"")])
self.assertEqual(
sorted(wn.synsets(""zap"", pos=""v"")),
[S(""microwave.v.01""), S(""nuke.v.01""), S(""zap.v.01""), S(""zap.v.02"")],
)
",[],0,[],/test/unit/test_wordnet.py_test_retrieve_synsets
5411,/home/amandapotts/git/nltk/nltk/test/unit/test_wordnet.py_test_hyperhyponyms,"def test_hyperhyponyms(self):
self.assertEqual(S(""travel.v.01"").hypernyms(), [])
self.assertEqual(S(""travel.v.02"").hypernyms(), [S(""travel.v.03"")])
self.assertEqual(S(""travel.v.03"").hypernyms(), [])
self.assertEqual(S(""breakfast.n.1"").hypernyms(), [S(""meal.n.01"")])
first_five_meal_hypo = [
S(""banquet.n.02""),
S(""bite.n.04""),
S(""breakfast.n.01""),
S(""brunch.n.01""),
S(""buffet.n.02""),
]
self.assertEqual(sorted(S(""meal.n.1"").hyponyms()[:5]), first_five_meal_hypo)
self.assertEqual(S(""Austen.n.1"").instance_hypernyms(), [S(""writer.n.01"")])
first_five_composer_hypo = [
S(""ambrose.n.01""),
S(""bach.n.01""),
S(""barber.n.01""),
S(""bartok.n.01""),
S(""beethoven.n.01""),
]
self.assertEqual(
S(""composer.n.1"").instance_hyponyms()[:5], first_five_composer_hypo
)
self.assertEqual(S(""person.n.01"").root_hypernyms(), [S(""entity.n.01"")])
self.assertEqual(S(""sail.v.01"").root_hypernyms(), [S(""travel.v.01"")])
self.assertEqual(
S(""fall.v.12"").root_hypernyms(), [S(""act.v.01""), S(""fall.v.17"")]
)
",[],0,[],/test/unit/test_wordnet.py_test_hyperhyponyms
5412,/home/amandapotts/git/nltk/nltk/test/unit/test_wordnet.py_test_derivationally_related_forms,"def test_derivationally_related_forms(self):
self.assertEqual(
L(""zap.v.03.nuke"").derivationally_related_forms(),
[L(""atomic_warhead.n.01.nuke"")],
)
self.assertEqual(
L(""zap.v.03.atomize"").derivationally_related_forms(),
[L(""atomization.n.02.atomization"")],
)
self.assertEqual(
L(""zap.v.03.atomise"").derivationally_related_forms(),
[L(""atomization.n.02.atomisation"")],
)
self.assertEqual(L(""zap.v.03.zap"").derivationally_related_forms(), [])
",[],0,[],/test/unit/test_wordnet.py_test_derivationally_related_forms
5413,/home/amandapotts/git/nltk/nltk/test/unit/test_wordnet.py_test_meronyms_holonyms,"def test_meronyms_holonyms(self):
self.assertEqual(
S(""dog.n.01"").member_holonyms(), [S(""canis.n.01""), S(""pack.n.06"")]
)
self.assertEqual(S(""dog.n.01"").part_meronyms(), [S(""flag.n.07"")])
self.assertEqual(S(""faculty.n.2"").member_meronyms(), [S(""professor.n.01"")])
self.assertEqual(S(""copilot.n.1"").member_holonyms(), [S(""crew.n.01"")])
self.assertEqual(
S(""table.n.2"").part_meronyms(),
[S(""leg.n.03""), S(""tabletop.n.01""), S(""tableware.n.01"")],
)
self.assertEqual(S(""course.n.7"").part_holonyms(), [S(""meal.n.01"")])
self.assertEqual(
S(""water.n.1"").substance_meronyms(), [S(""hydrogen.n.01""), S(""oxygen.n.01"")]
)
self.assertEqual(
S(""gin.n.1"").substance_holonyms(),
[
S(""gin_and_it.n.01""),
S(""gin_and_tonic.n.01""),
S(""martini.n.01""),
S(""pink_lady.n.01""),
],
)
",[],0,[],/test/unit/test_wordnet.py_test_meronyms_holonyms
5414,/home/amandapotts/git/nltk/nltk/test/unit/test_wordnet.py_test_antonyms,"def test_antonyms(self):
self.assertEqual(
L(""leader.n.1.leader"").antonyms(), [L(""follower.n.01.follower"")]
)
self.assertEqual(
L(""increase.v.1.increase"").antonyms(), [L(""decrease.v.01.decrease"")]
)
",[],0,[],/test/unit/test_wordnet.py_test_antonyms
5415,/home/amandapotts/git/nltk/nltk/test/unit/test_wordnet.py_test_misc_relations,"def test_misc_relations(self):
self.assertEqual(S(""snore.v.1"").entailments(), [S(""sleep.v.01"")])
self.assertEqual(
S(""heavy.a.1"").similar_tos(),
[
S(""dense.s.03""),
S(""doughy.s.01""),
S(""heavier-than-air.s.01""),
S(""hefty.s.02""),
S(""massive.s.04""),
S(""non-buoyant.s.01""),
S(""ponderous.s.02""),
],
)
self.assertEqual(S(""light.a.1"").attributes(), [S(""weight.n.01"")])
self.assertEqual(S(""heavy.a.1"").attributes(), [S(""weight.n.01"")])
self.assertEqual(
L(""English.a.1.English"").pertainyms(), [L(""england.n.01.England"")]
)
",[],0,[],/test/unit/test_wordnet.py_test_misc_relations
5416,/home/amandapotts/git/nltk/nltk/test/unit/test_wordnet.py_test_lch,"def test_lch(self):
self.assertEqual(
S(""person.n.01"").lowest_common_hypernyms(S(""dog.n.01"")),
[S(""organism.n.01"")],
)
self.assertEqual(
S(""woman.n.01"").lowest_common_hypernyms(S(""girlfriend.n.02"")),
[S(""woman.n.01"")],
)
",[],0,[],/test/unit/test_wordnet.py_test_lch
5417,/home/amandapotts/git/nltk/nltk/test/unit/test_wordnet.py_test_domains,"def test_domains(self):
self.assertEqual(S(""code.n.03"").topic_domains(), [S(""computer_science.n.01"")])
self.assertEqual(S(""pukka.a.01"").region_domains(), [S(""india.n.01"")])
self.assertEqual(S(""freaky.a.01"").usage_domains(), [S(""slang.n.02"")])
",[],0,[],/test/unit/test_wordnet.py_test_domains
5418,/home/amandapotts/git/nltk/nltk/test/unit/test_wordnet.py_test_in_topic_domains,"def test_in_topic_domains(self):
self.assertEqual(
S(""computer_science.n.01"").in_topic_domains()[0], S(""access.n.05"")
)
self.assertEqual(S(""germany.n.01"").in_region_domains()[23], S(""trillion.n.02""))
self.assertEqual(S(""slang.n.02"").in_usage_domains()[1], S(""airhead.n.01""))
",[],0,[],/test/unit/test_wordnet.py_test_in_topic_domains
5419,/home/amandapotts/git/nltk/nltk/test/unit/test_wordnet.py_test_wordnet_similarities,"def test_wordnet_similarities(self):
self.assertAlmostEqual(S(""cat.n.01"").path_similarity(S(""cat.n.01"")), 1.0)
self.assertAlmostEqual(S(""dog.n.01"").path_similarity(S(""cat.n.01"")), 0.2)
self.assertAlmostEqual(
S(""car.n.01"").path_similarity(S(""automobile.v.01"")),
S(""automobile.v.01"").path_similarity(S(""car.n.01"")),
)
self.assertAlmostEqual(
S(""big.a.01"").path_similarity(S(""dog.n.01"")),
S(""dog.n.01"").path_similarity(S(""big.a.01"")),
)
self.assertAlmostEqual(
S(""big.a.01"").path_similarity(S(""long.a.01"")),
S(""long.a.01"").path_similarity(S(""big.a.01"")),
)
self.assertAlmostEqual(
S(""dog.n.01"").lch_similarity(S(""cat.n.01"")), 2.028, places=3
)
self.assertAlmostEqual(
S(""dog.n.01"").wup_similarity(S(""cat.n.01"")), 0.8571, places=3
)
self.assertAlmostEqual(
S(""car.n.01"").wup_similarity(S(""automobile.v.01"")),
S(""automobile.v.01"").wup_similarity(S(""car.n.01"")),
)
self.assertAlmostEqual(
S(""big.a.01"").wup_similarity(S(""dog.n.01"")),
S(""dog.n.01"").wup_similarity(S(""big.a.01"")),
)
self.assertAlmostEqual(
S(""big.a.01"").wup_similarity(S(""long.a.01"")),
S(""long.a.01"").wup_similarity(S(""big.a.01"")),
)
self.assertAlmostEqual(
S(""big.a.01"").lch_similarity(S(""long.a.01"")),
S(""long.a.01"").lch_similarity(S(""big.a.01"")),
)
brown_ic = wnic.ic(""ic-brown.dat"")
self.assertAlmostEqual(
S(""dog.n.01"").jcn_similarity(S(""cat.n.01""), brown_ic), 0.4497, places=3
)
semcor_ic = wnic.ic(""ic-semcor.dat"")
self.assertAlmostEqual(
S(""dog.n.01"").lin_similarity(S(""cat.n.01""), semcor_ic), 0.8863, places=3
)
",[],0,[],/test/unit/test_wordnet.py_test_wordnet_similarities
5420,/home/amandapotts/git/nltk/nltk/test/unit/test_wordnet.py_test_omw_lemma_no_trailing_underscore,"def test_omw_lemma_no_trailing_underscore(self):
expected = sorted(
[
""popolna_sprememba_v_mišljenju"",
""popoln_obrat"",
""preobrat"",
""preobrat_v_mišljenju"",
]
)
self.assertEqual(sorted(S(""about-face.n.02"").lemma_names(lang=""slv"")), expected)
",[],0,[],/test/unit/test_wordnet.py_test_omw_lemma_no_trailing_underscore
5421,/home/amandapotts/git/nltk/nltk/test/unit/test_wordnet.py_test_iterable_type_for_all_lemma_names,"def test_iterable_type_for_all_lemma_names(self):
cat_lemmas = wn.all_lemma_names(lang=""cat"")
eng_lemmas = wn.all_lemma_names(lang=""eng"")
self.assertTrue(hasattr(eng_lemmas, ""__iter__""))
self.assertTrue(hasattr(eng_lemmas, ""__next__"") or hasattr(eng_lemmas, ""next""))
self.assertTrue(eng_lemmas.__iter__() is eng_lemmas)
self.assertTrue(hasattr(cat_lemmas, ""__iter__""))
self.assertTrue(hasattr(cat_lemmas, ""__next__"") or hasattr(eng_lemmas, ""next""))
self.assertTrue(cat_lemmas.__iter__() is cat_lemmas)
",[],0,[],/test/unit/test_wordnet.py_test_iterable_type_for_all_lemma_names
5422,/home/amandapotts/git/nltk/nltk/test/unit/test_data.py_test_find_raises_exception,"def test_find_raises_exception():
with pytest.raises(LookupError):
nltk.data.find(""no_such_resource/foo"")
",[],0,[],/test/unit/test_data.py_test_find_raises_exception
5423,/home/amandapotts/git/nltk/nltk/test/unit/test_data.py_test_find_raises_exception_with_full_resource_name,"def test_find_raises_exception_with_full_resource_name():
no_such_thing = ""no_such_thing/bar""
with pytest.raises(LookupError) as exc:
nltk.data.find(no_such_thing)
assert no_such_thing in str(exc)
",[],0,[],/test/unit/test_data.py_test_find_raises_exception_with_full_resource_name
5424,/home/amandapotts/git/nltk/nltk/test/unit/test_classify.py_assert_classifier_correct,"def assert_classifier_correct(algorithm):
try:
classifier = classify.MaxentClassifier.train(
TRAIN, algorithm, trace=0, max_iter=1000
)
except (LookupError, AttributeError) as e:
pytest.skip(str(e))
for (px, py), featureset in zip(RESULTS, TEST):
pdist = classifier.prob_classify(featureset)
assert abs(pdist.prob(""x"") - px) < 1e-2, (pdist.prob(""x""), px)
assert abs(pdist.prob(""y"") - py) < 1e-2, (pdist.prob(""y""), py)
",[],0,[],/test/unit/test_classify.py_assert_classifier_correct
5425,/home/amandapotts/git/nltk/nltk/test/unit/test_classify.py_test_megam,"def test_megam():
assert_classifier_correct(""MEGAM"")
",[],0,[],/test/unit/test_classify.py_test_megam
5426,/home/amandapotts/git/nltk/nltk/test/unit/test_classify.py_test_tadm,"def test_tadm():
assert_classifier_correct(""TADM"")
",[],0,[],/test/unit/test_classify.py_test_tadm
5427,/home/amandapotts/git/nltk/nltk/test/unit/test_ribes.py_test_ribes_empty_worder,"def test_ribes_empty_worder():  # worder as in word order
hyp = ""This is a nice sentence which I quite like"".split()
ref = ""Okay well that's neat and all but the reference's different"".split()
assert word_rank_alignment(ref, hyp) == []
list_of_refs = [[ref]]
hypotheses = [hyp]
assert corpus_ribes(list_of_refs, hypotheses) == 0.0
",[],0,[],/test/unit/test_ribes.py_test_ribes_empty_worder
5428,/home/amandapotts/git/nltk/nltk/test/unit/test_ribes.py_test_ribes_one_worder,"def test_ribes_one_worder():
hyp = ""This is a nice sentence which I quite like"".split()
ref = ""Okay well that's nice and all but the reference's different"".split()
assert word_rank_alignment(ref, hyp) == [3]
list_of_refs = [[ref]]
hypotheses = [hyp]
assert corpus_ribes(list_of_refs, hypotheses) == 0.0
",[],0,[],/test/unit/test_ribes.py_test_ribes_one_worder
5429,/home/amandapotts/git/nltk/nltk/test/unit/test_ribes.py_test_ribes_two_worder,"def test_ribes_two_worder():
hyp = ""This is a nice sentence which I quite like"".split()
ref = ""Okay well that's nice and all but the reference is different"".split()
assert word_rank_alignment(ref, hyp) == [9, 3]
list_of_refs = [[ref]]
hypotheses = [hyp]
assert corpus_ribes(list_of_refs, hypotheses) == 0.0
",[],0,[],/test/unit/test_ribes.py_test_ribes_two_worder
5430,/home/amandapotts/git/nltk/nltk/test/unit/test_ribes.py_test_ribes,"def test_ribes():
hyp1 = [
""It"",
""is"",
""a"",
""guide"",
""to"",
""action"",
""which"",
""ensures"",
""that"",
""the"",
""military"",
""always"",
""obeys"",
""the"",
""commands"",
""of"",
""the"",
""party"",
]
ref1a = [
""It"",
""is"",
""a"",
""guide"",
""to"",
""action"",
""that"",
""ensures"",
""that"",
""the"",
""military"",
""will"",
""forever"",
""heed"",
""Party"",
""commands"",
]
ref1b = [
""It"",
""is"",
""the"",
""guiding"",
""principle"",
""which"",
""guarantees"",
""the"",
""military"",
""forces"",
""always"",
""being"",
""under"",
""the"",
""command"",
""of"",
""the"",
""Party"",
]
ref1c = [
""It"",
""is"",
""the"",
""practical"",
""guide"",
""for"",
""the"",
""army"",
""always"",
""to"",
""heed"",
""the"",
""directions"",
""of"",
""the"",
""party"",
]
hyp2 = [
""he"",
""read"",
""the"",
""book"",
""because"",
""he"",
""was"",
""interested"",
""in"",
""world"",
""history"",
]
ref2a = [
""he"",
""was"",
""interested"",
""in"",
""world"",
""history"",
""because"",
""he"",
""read"",
""the"",
""book"",
]
list_of_refs = [[ref1a, ref1b, ref1c], [ref2a]]
hypotheses = [hyp1, hyp2]
score = corpus_ribes(list_of_refs, hypotheses)
assert round(score, 4) == 0.3597
",[],0,[],/test/unit/test_ribes.py_test_ribes
5431,/home/amandapotts/git/nltk/nltk/test/unit/test_ribes.py_test_no_zero_div,"def test_no_zero_div():
hyp1 = [
""It"",
""is"",
""a"",
""guide"",
""to"",
""action"",
""which"",
""ensures"",
""that"",
""the"",
""military"",
""always"",
""obeys"",
""the"",
""commands"",
""of"",
""the"",
""party"",
]
ref1a = [
""It"",
""is"",
""a"",
""guide"",
""to"",
""action"",
""that"",
""ensures"",
""that"",
""the"",
""military"",
""will"",
""forever"",
""heed"",
""Party"",
""commands"",
]
ref1b = [
""It"",
""is"",
""the"",
""guiding"",
""principle"",
""which"",
""guarantees"",
""the"",
""military"",
""forces"",
""always"",
""being"",
""under"",
""the"",
""command"",
""of"",
""the"",
""Party"",
]
ref1c = [
""It"",
""is"",
""the"",
""practical"",
""guide"",
""for"",
""the"",
""army"",
""always"",
""to"",
""heed"",
""the"",
""directions"",
""of"",
""the"",
""party"",
]
hyp2 = [""he"", ""read"", ""the""]
ref2a = [""he"", ""was"", ""interested"", ""in"", ""world"", ""history"", ""because"", ""he""]
list_of_refs = [[ref1a, ref1b, ref1c], [ref2a]]
hypotheses = [hyp1, hyp2]
score = corpus_ribes(list_of_refs, hypotheses)
assert round(score, 4) == 0.1688
",[],0,[],/test/unit/test_ribes.py_test_no_zero_div
5432,/home/amandapotts/git/nltk/nltk/test/unit/test_freqdist.py_test_iterating_returns_an_iterator_ordered_by_frequency,"def test_iterating_returns_an_iterator_ordered_by_frequency():
samples = [""one"", ""two"", ""two""]
distribution = nltk.FreqDist(samples)
assert list(distribution) == [""two"", ""one""]
",[],0,[],/test/unit/test_freqdist.py_test_iterating_returns_an_iterator_ordered_by_frequency
5433,/home/amandapotts/git/nltk/nltk/test/unit/test_pl196x.py_test_corpus_reader,"def test_corpus_reader(self):
pl196x_dir = nltk.data.find(""corpora/pl196x"")
pl = pl196x.Pl196xCorpusReader(
pl196x_dir, r"".*\.xml"", textids=""textids.txt"", cat_file=""cats.txt""
)
pl.tagged_words(fileids=pl.fileids(), categories=""cats.txt"")
",[],0,[],/test/unit/test_pl196x.py_test_corpus_reader
5434,/home/amandapotts/git/nltk/nltk/test/unit/test_distance.py_test_with_transpositions,"def test_with_transpositions(
self, left: str, right: str, substitution_cost: int, expecteds: Tuple[int, int]
",[],0,[],/test/unit/test_distance.py_test_with_transpositions
5435,/home/amandapotts/git/nltk/nltk/test/unit/test_pos_tag.py_test_pos_tag_eng,"def test_pos_tag_eng(self):
text = ""John's big idea isn't all that bad.""
expected_tagged = [
(""John"", ""NNP""),
(""'s"", ""POS""),
(""big"", ""JJ""),
(""idea"", ""NN""),
(""is"", ""VBZ""),
(""n't"", ""RB""),
(""all"", ""PDT""),
(""that"", ""DT""),
(""bad"", ""JJ""),
(""."", "".""),
]
assert pos_tag(word_tokenize(text)) == expected_tagged
",[],0,[],/test/unit/test_pos_tag.py_test_pos_tag_eng
5436,/home/amandapotts/git/nltk/nltk/test/unit/test_pos_tag.py_test_pos_tag_eng_universal,"def test_pos_tag_eng_universal(self):
text = ""John's big idea isn't all that bad.""
expected_tagged = [
(""John"", ""NOUN""),
(""'s"", ""PRT""),
(""big"", ""ADJ""),
(""idea"", ""NOUN""),
(""is"", ""VERB""),
(""n't"", ""ADV""),
(""all"", ""DET""),
(""that"", ""DET""),
(""bad"", ""ADJ""),
(""."", "".""),
]
assert pos_tag(word_tokenize(text), tagset=""universal"") == expected_tagged
",[],0,[],/test/unit/test_pos_tag.py_test_pos_tag_eng_universal
5437,/home/amandapotts/git/nltk/nltk/test/unit/test_pos_tag.py_test_pos_tag_rus,"def test_pos_tag_rus(self):
text = ""Илья оторопел и дважды перечитал бумажку.""
expected_tagged = [
(""Илья"", ""S""),
(""оторопел"", ""V""),
(""и"", ""CONJ""),
(""дважды"", ""ADV""),
(""перечитал"", ""V""),
(""бумажку"", ""S""),
(""."", ""NONLEX""),
]
assert pos_tag(word_tokenize(text), lang=""rus"") == expected_tagged
",[],0,[],/test/unit/test_pos_tag.py_test_pos_tag_rus
5438,/home/amandapotts/git/nltk/nltk/test/unit/test_pos_tag.py_test_pos_tag_rus_universal,"def test_pos_tag_rus_universal(self):
text = ""Илья оторопел и дважды перечитал бумажку.""
expected_tagged = [
(""Илья"", ""NOUN""),
(""оторопел"", ""VERB""),
(""и"", ""CONJ""),
(""дважды"", ""ADV""),
(""перечитал"", ""VERB""),
(""бумажку"", ""NOUN""),
(""."", "".""),
]
assert (
pos_tag(word_tokenize(text), tagset=""universal"", lang=""rus"")
== expected_tagged
)
",[],0,[],/test/unit/test_pos_tag.py_test_pos_tag_rus_universal
5439,/home/amandapotts/git/nltk/nltk/test/unit/test_pos_tag.py_test_pos_tag_unknown_lang,"def test_pos_tag_unknown_lang(self):
text = ""모르겠 습니 다""
self.assertRaises(NotImplementedError, pos_tag, word_tokenize(text), lang=""kor"")
self.assertRaises(NotImplementedError, pos_tag, word_tokenize(text), lang=None)
",[],0,[],/test/unit/test_pos_tag.py_test_pos_tag_unknown_lang
5440,/home/amandapotts/git/nltk/nltk/test/unit/test_pos_tag.py_test_unspecified_lang,"def test_unspecified_lang(self):
text = ""모르겠 습니 다""
expected_but_wrong = [(""모르겠"", ""JJ""), (""습니"", ""NNP""), (""다"", ""NN"")]
assert pos_tag(word_tokenize(text)) == expected_but_wrong
",[],0,[],/test/unit/test_pos_tag.py_test_unspecified_lang
5441,/home/amandapotts/git/nltk/nltk/test/unit/test_naivebayes.py_test_simple,"def test_simple(self):
training_features = [
({""nice"": True, ""good"": True}, ""positive""),
({""bad"": True, ""mean"": True}, ""negative""),
]
classifier = NaiveBayesClassifier.train(training_features)
result = classifier.prob_classify({""nice"": True})
self.assertTrue(result.prob(""positive"") > result.prob(""negative""))
self.assertEqual(result.max(), ""positive"")
result = classifier.prob_classify({""bad"": True})
self.assertTrue(result.prob(""positive"") < result.prob(""negative""))
self.assertEqual(result.max(), ""negative"")
",[],0,[],/test/unit/test_naivebayes.py_test_simple
5442,/home/amandapotts/git/nltk/nltk/test/unit/test_aline.py_test_aline,"def test_aline():
result = aline.align(""θin"", ""tenwis"")
expected = [[(""θ"", ""t""), (""i"", ""e""), (""n"", ""n"")]]
assert result == expected
result = aline.align(""jo"", ""ʒə"")
expected = [[(""j"", ""ʒ""), (""o"", ""ə"")]]
assert result == expected
result = aline.align(""pematesiweni"", ""pematesewen"")
expected = [
[
(""p"", ""p""),
(""e"", ""e""),
(""m"", ""m""),
(""a"", ""a""),
(""t"", ""t""),
(""e"", ""e""),
(""s"", ""s""),
(""i"", ""e""),
(""w"", ""w""),
(""e"", ""e""),
(""n"", ""n""),
]
]
assert result == expected
result = aline.align(""tuwθ"", ""dentis"")
expected = [[(""t"", ""t""), (""u"", ""i""), (""w"", ""-""), (""θ"", ""s"")]]
assert result == expected
",[],0,[],/test/unit/test_aline.py_test_aline
5443,/home/amandapotts/git/nltk/nltk/test/unit/test_aline.py_test_aline_delta,"def test_aline_delta():
""""""
Test aline for computing the difference between two segments
""""""
assert aline.delta(""p"", ""q"") == 20.0
assert aline.delta(""a"", ""A"") == 0.0
",[],0,[],/test/unit/test_aline.py_test_aline_delta
5444,/home/amandapotts/git/nltk/nltk/test/unit/test_tokenize.py_load_stanford_segmenter,"def load_stanford_segmenter():
try:
seg = StanfordSegmenter()
seg.default_config(""ar"")
seg.default_config(""zh"")
return True
except LookupError:
return False
",[],0,[],/test/unit/test_tokenize.py_load_stanford_segmenter
5445,/home/amandapotts/git/nltk/nltk/test/unit/test_tokenize.py_test_tweet_tokenizer,"def test_tweet_tokenizer(self):
""""""
Test TweetTokenizer using words with special and accented characters.
""""""
tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)
s9 = ""@myke: Let's test these words: resumé España München français""
tokens = tokenizer.tokenize(s9)
expected = [
"":"",
""Let's"",
""test"",
""these"",
""words"",
"":"",
""resumé"",
""España"",
""München"",
""français"",
]
assert tokens == expected
",[],0,[],/test/unit/test_tokenize.py_test_tweet_tokenizer
5446,/home/amandapotts/git/nltk/nltk/test/unit/test_tokenize.py_test_tweet_tokenizer_expanded,"def test_tweet_tokenizer_expanded(
self, test_input: str, expecteds: Tuple[List[str], List[str]]
",[],0,[],/test/unit/test_tokenize.py_test_tweet_tokenizer_expanded
5447,/home/amandapotts/git/nltk/nltk/test/unit/test_tokenize.py_test_sonority_sequencing_syllable_tokenizer,"def test_sonority_sequencing_syllable_tokenizer(self):
""""""
Test SyllableTokenizer tokenizer.
""""""
tokenizer = SyllableTokenizer()
tokens = tokenizer.tokenize(""justification"")
assert tokens == [""jus"", ""ti"", ""fi"", ""ca"", ""tion""]
",[],0,[],/test/unit/test_tokenize.py_test_sonority_sequencing_syllable_tokenizer
5448,/home/amandapotts/git/nltk/nltk/test/unit/test_tokenize.py_test_syllable_tokenizer_numbers,"def test_syllable_tokenizer_numbers(self):
""""""
Test SyllableTokenizer tokenizer.
""""""
tokenizer = SyllableTokenizer()
text = ""9"" * 10000
tokens = tokenizer.tokenize(text)
assert tokens == [text]
",[],0,[],/test/unit/test_tokenize.py_test_syllable_tokenizer_numbers
5449,/home/amandapotts/git/nltk/nltk/test/unit/test_tokenize.py_test_legality_principle_syllable_tokenizer,"def test_legality_principle_syllable_tokenizer(self):
""""""
Test LegalitySyllableTokenizer tokenizer.
""""""
from nltk.corpus import words
test_word = ""wonderful""
tokenizer = LegalitySyllableTokenizer(words.words())
tokens = tokenizer.tokenize(test_word)
assert tokens == [""won"", ""der"", ""ful""]
",[],0,[],/test/unit/test_tokenize.py_test_legality_principle_syllable_tokenizer
5450,/home/amandapotts/git/nltk/nltk/test/unit/test_tokenize.py_test_stanford_segmenter_arabic,"def test_stanford_segmenter_arabic(self):
""""""
Test the Stanford Word Segmenter for Arabic (default config)
""""""
seg = StanfordSegmenter()
seg.default_config(""ar"")
sent = ""يبحث علم الحاسوب استخدام الحوسبة بجميع اشكالها لحل المشكلات""
segmented_sent = seg.segment(sent.split())
assert segmented_sent.split() == [
""يبحث"",
""علم"",
""الحاسوب"",
""استخدام"",
""الحوسبة"",
""ب"",
""جميع"",
""اشكال"",
""ها"",
""ل"",
""حل"",
""المشكلات"",
]
",[],0,[],/test/unit/test_tokenize.py_test_stanford_segmenter_arabic
5451,/home/amandapotts/git/nltk/nltk/test/unit/test_tokenize.py_test_stanford_segmenter_chinese,"def test_stanford_segmenter_chinese(self):
""""""
Test the Stanford Word Segmenter for Chinese (default config)
""""""
seg = StanfordSegmenter()
seg.default_config(""zh"")
sent = ""这是斯坦福中文分词器测试""
segmented_sent = seg.segment(sent.split())
assert segmented_sent.split() == [""这"", ""是"", ""斯坦福"", ""中文"", ""分词器"", ""测试""]
",[],0,[],/test/unit/test_tokenize.py_test_stanford_segmenter_chinese
5452,/home/amandapotts/git/nltk/nltk/test/unit/test_tokenize.py_test_phone_tokenizer,"def test_phone_tokenizer(self):
""""""
Test a string that resembles a phone number but contains a newline
""""""
tokenizer = TweetTokenizer()
test1 = ""(393)  928 -3010""
expected = [""(393)  928 -3010""]
result = tokenizer.tokenize(test1)
assert result == expected
test2 = ""(393)\n928 -3010""
expected = [""("", ""393"", "")"", ""928 -3010""]
result = tokenizer.tokenize(test2)
assert result == expected
",[],0,[],/test/unit/test_tokenize.py_test_phone_tokenizer
5453,/home/amandapotts/git/nltk/nltk/test/unit/test_tokenize.py_test_emoji_tokenizer,"def test_emoji_tokenizer(self):
""""""
Test a string that contains Emoji ZWJ Sequences and skin tone modifier
""""""
tokenizer = TweetTokenizer()
test1 = ""👨‍👩‍👧‍👧""
expected = [""👨‍👩‍👧‍👧""]
result = tokenizer.tokenize(test1)
assert result == expected
test2 = ""👨🏿""
expected = [""👨🏿""]
result = tokenizer.tokenize(test2)
assert result == expected
test3 = ""🤔 🙈 me así, se😌 ds 💕👭👙 hello 👩🏾‍🎓 emoji hello 👨‍👩‍👦‍👦 how are 😊 you today🙅🏽🙅🏽""
expected = [
""🤔"",
""🙈"",
""me"",
""así"",
"","",
""se"",
""😌"",
""ds"",
""💕"",
""👭"",
""👙"",
""hello"",
""👩🏾\u200d🎓"",
""emoji"",
""hello"",
""👨\u200d👩\u200d👦\u200d👦"",
""how"",
""are"",
""😊"",
""you"",
""today"",
""🙅🏽"",
""🙅🏽"",
]
result = tokenizer.tokenize(test3)
assert result == expected
test4 = ""🇦🇵🇵🇱🇪""
expected = [""🇦🇵"", ""🇵🇱"", ""🇪""]
result = tokenizer.tokenize(test4)
assert result == expected
test5 = ""Hi 🇨🇦, 😍!!""
expected = [""Hi"", ""🇨🇦"", "","", ""😍"", ""!"", ""!""]
result = tokenizer.tokenize(test5)
assert result == expected
test6 = ""<3 🇨🇦 🤝 🇵🇱 <3""
expected = [""<3"", ""🇨🇦"", ""🤝"", ""🇵🇱"", ""<3""]
result = tokenizer.tokenize(test6)
assert result == expected
",[],0,[],/test/unit/test_tokenize.py_test_emoji_tokenizer
5454,/home/amandapotts/git/nltk/nltk/test/unit/test_tokenize.py_test_pad_asterisk,"def test_pad_asterisk(self):
""""""
Test padding of asterisk for word tokenization.
""""""
text = ""This is a, *weird sentence with *asterisks in it.""
expected = [
""This"",
""is"",
""a"",
"","",
""*"",
""weird"",
""sentence"",
""with"",
""*"",
""asterisks"",
""in"",
""it"",
""."",
]
assert word_tokenize(text) == expected
",[],0,[],/test/unit/test_tokenize.py_test_pad_asterisk
5455,/home/amandapotts/git/nltk/nltk/test/unit/test_tokenize.py_test_pad_dotdot,"def test_pad_dotdot(self):
""""""
Test padding of dotdot* for word tokenization.
""""""
text = ""Why did dotdot.. not get tokenized but dotdotdot... did? How about manydots.....""
expected = [
""Why"",
""did"",
""dotdot"",
"".."",
""not"",
""get"",
""tokenized"",
""but"",
""dotdotdot"",
""..."",
""did"",
""?"",
""How"",
""about"",
""manydots"",
""....."",
]
assert word_tokenize(text) == expected
",[],0,[],/test/unit/test_tokenize.py_test_pad_dotdot
5456,/home/amandapotts/git/nltk/nltk/test/unit/test_tokenize.py_test_remove_handle,"def test_remove_handle(self):
""""""
Test remove_handle() from casual.py with specially crafted edge cases
""""""
tokenizer = TweetTokenizer(strip_handles=True)
test1 = ""@twitter hello @twi_tter_. hi @12345 @123news""
expected = [""hello"", ""."", ""hi""]
result = tokenizer.tokenize(test1)
assert result == expected
test2 = ""@n`@n~@n(@n)@n-@n=@n+@n\\@n|@n[@n]@n{@n}@n
expected = [
""`"",
""~"",
""("",
"")"",
""-"",
""="",
""+"",
""\\"",
""|"",
""["",
""]"",
""{"",
""}"",
""
"":"",
""'"",
'""',
""/"",
""?"",
""."",
"","",
""<"",
"">"",
""ñ"",
""."",
""ü"",
""."",
""ç"",
""."",
]
result = tokenizer.tokenize(test2)
assert result == expected
test3 = ""a@n j@n z@n A@n L@n Z@n 1@n 4@n 7@n 9@n 0@n _@n !@n @@n #@n $@n %@n &@n *@n""
expected = [
""a"",
""@n"",
""j"",
""@n"",
""z"",
""@n"",
""A"",
""@n"",
""L"",
""@n"",
""Z"",
""@n"",
""1"",
""@n"",
""4"",
""@n"",
""7"",
""@n"",
""9"",
""@n"",
""0"",
""@n"",
""_"",
""@n"",
""!"",
""@n"",
""@"",
""@n"",
""#"",
""@n"",
""$"",
""@n"",
""%"",
""@n"",
""&"",
""@n"",
""*"",
""@n"",
]
result = tokenizer.tokenize(test3)
assert result == expected
test4 = ""@n!a @n#a @n$a @n%a @n&a @n*a""
expected = [""!"", ""a"", ""#"", ""a"", ""$"", ""a"", ""%"", ""a"", ""&"", ""a"", ""*"", ""a""]
result = tokenizer.tokenize(test4)
assert result == expected
test5 = ""@n!@n @n#@n @n$@n @n%@n @n&@n @n*@n @n@n @@n @n@@n @n_@n @n7@n @nj@n""
expected = [
""!"",
""@n"",
""#"",
""@n"",
""$"",
""@n"",
""%"",
""@n"",
""&"",
""@n"",
""*"",
""@n"",
""@n"",
""@n"",
""@"",
""@n"",
""@n"",
""@"",
""@n"",
""@n_"",
""@n"",
""@n7"",
""@n"",
""@nj"",
""@n"",
]
result = tokenizer.tokenize(test5)
assert result == expected
test6 = ""@abcdefghijklmnopqrstuvwxyz @abcdefghijklmno1234 @abcdefghijklmno_ @abcdefghijklmnoendofhandle""
expected = [""pqrstuvwxyz"", ""1234"", ""_"", ""endofhandle""]
result = tokenizer.tokenize(test6)
assert result == expected
test7 = ""@abcdefghijklmnop@abcde @abcdefghijklmno@abcde @abcdefghijklmno_@abcde @abcdefghijklmno5@abcde""
expected = [
""p"",
""@abcde"",
""@abcdefghijklmno"",
""@abcde"",
""_"",
""@abcde"",
""5"",
""@abcde"",
]
result = tokenizer.tokenize(test7)
assert result == expected
",[],0,[],/test/unit/test_tokenize.py_test_remove_handle
5457,/home/amandapotts/git/nltk/nltk/test/unit/test_tokenize.py_test_treebank_span_tokenizer,"def test_treebank_span_tokenizer(self):
""""""
Test TreebankWordTokenizer.span_tokenize function
""""""
tokenizer = TreebankWordTokenizer()
test1 = ""Good muffins cost $3.88\nin New (York).  Please (buy) me\ntwo of them.\n(Thanks).""
expected = [
(0, 4),
(5, 12),
(13, 17),
(18, 19),
(19, 23),
(24, 26),
(27, 30),
(31, 32),
(32, 36),
(36, 37),
(37, 38),
(40, 46),
(47, 48),
(48, 51),
(51, 52),
(53, 55),
(56, 59),
(60, 62),
(63, 68),
(69, 70),
(70, 76),
(76, 77),
(77, 78),
]
result = list(tokenizer.span_tokenize(test1))
assert result == expected
test2 = 'The DUP is similar to the ""religious right"" in the United States and takes a hardline stance on social issues'
expected = [
(0, 3),
(4, 7),
(8, 10),
(11, 18),
(19, 21),
(22, 25),
(26, 27),
(27, 36),
(37, 42),
(42, 43),
(44, 46),
(47, 50),
(51, 57),
(58, 64),
(65, 68),
(69, 74),
(75, 76),
(77, 85),
(86, 92),
(93, 95),
(96, 102),
(103, 109),
]
result = list(tokenizer.span_tokenize(test2))
assert result == expected
test3 = ""The DUP is similar to the \""religious right\"" in the United States and takes a ``hardline'' stance on social issues""
expected = [
(0, 3),
(4, 7),
(8, 10),
(11, 18),
(19, 21),
(22, 25),
(26, 27),
(27, 36),
(37, 42),
(42, 43),
(44, 46),
(47, 50),
(51, 57),
(58, 64),
(65, 68),
(69, 74),
(75, 76),
(77, 79),
(79, 87),
(87, 89),
(90, 96),
(97, 99),
(100, 106),
(107, 113),
]
result = list(tokenizer.span_tokenize(test3))
assert result == expected
",[],0,[],/test/unit/test_tokenize.py_test_treebank_span_tokenizer
5458,/home/amandapotts/git/nltk/nltk/test/unit/test_tokenize.py_test_word_tokenize,"def test_word_tokenize(self):
""""""
Test word_tokenize function
""""""
sentence = ""The 'v', I've been fooled but I'll seek revenge.""
expected = [
""The"",
""'"",
""v"",
""'"",
"","",
""I"",
""'ve"",
""been"",
""fooled"",
""but"",
""I"",
""'ll"",
""seek"",
""revenge"",
""."",
]
assert word_tokenize(sentence) == expected
sentence = ""'v' 're'""
expected = [""'"", ""v"", ""'"", ""'re"", ""'""]
assert word_tokenize(sentence) == expected
",[],0,[],/test/unit/test_tokenize.py_test_word_tokenize
5459,/home/amandapotts/git/nltk/nltk/test/unit/test_tokenize.py_test_punkt_pair_iter,"def test_punkt_pair_iter(self):
test_cases = [
(""12"", [(""1"", ""2""), (""2"", None)]),
(""123"", [(""1"", ""2""), (""2"", ""3""), (""3"", None)]),
(""1234"", [(""1"", ""2""), (""2"", ""3""), (""3"", ""4""), (""4"", None)]),
]
for test_input, expected_output in test_cases:
actual_output = [x for x in punkt._pair_iter(test_input)]
assert actual_output == expected_output
",[],0,[],/test/unit/test_tokenize.py_test_punkt_pair_iter
5460,/home/amandapotts/git/nltk/nltk/test/unit/test_tokenize.py_test_punkt_pair_iter_handles_stop_iteration_exception,"def test_punkt_pair_iter_handles_stop_iteration_exception(self):
it = iter([])
gen = punkt._pair_iter(it)
list(gen)
",[],0,[],/test/unit/test_tokenize.py_test_punkt_pair_iter_handles_stop_iteration_exception
5461,/home/amandapotts/git/nltk/nltk/test/unit/test_tokenize.py_test_punkt_tokenize_words_handles_stop_iteration_exception,"def test_punkt_tokenize_words_handles_stop_iteration_exception(self):
obj = punkt.PunktBaseClass()
",[],0,[],/test/unit/test_tokenize.py_test_punkt_tokenize_words_handles_stop_iteration_exception
5462,/home/amandapotts/git/nltk/nltk/test/unit/test_tokenize.py_TestPunktTokenizeWordsMock:,"class TestPunktTokenizeWordsMock:
",[],0,[],/test/unit/test_tokenize.py_TestPunktTokenizeWordsMock
5463,/home/amandapotts/git/nltk/nltk/test/unit/test_tokenize.py_word_tokenize,"def word_tokenize(self, s):
return iter([])
",[],0,[],/test/unit/test_tokenize.py_word_tokenize
5464,/home/amandapotts/git/nltk/nltk/test/unit/test_tokenize.py_test_punkt_tokenize_custom_lang_vars,"def test_punkt_tokenize_custom_lang_vars(self):
",[],0,[],/test/unit/test_tokenize.py_test_punkt_tokenize_custom_lang_vars
5465,/home/amandapotts/git/nltk/nltk/test/unit/test_tokenize.py_BengaliLanguageVars,"class BengaliLanguageVars(punkt.PunktLanguageVars):
sent_end_chars = (""."", ""?"", ""!"", ""\u0964"")
",[],0,[],/test/unit/test_tokenize.py_BengaliLanguageVars
5466,/home/amandapotts/git/nltk/nltk/test/unit/test_tokenize.py_test_punkt_tokenize_no_custom_lang_vars,"def test_punkt_tokenize_no_custom_lang_vars(self):
obj = punkt.PunktSentenceTokenizer()
sentences = ""উপরাষ্ট্রপতি শ্রী এম ভেঙ্কাইয়া নাইডু সোমবার আই আই টি দিল্লির হীরক জয়ন্তী উদযাপনের উদ্বোধন করেছেন। অনলাইনের মাধ্যমে এই অনুষ্ঠানে কেন্দ্রীয় মানব সম্পদ উন্নয়নমন্ত্রী শ্রী রমেশ পোখরিয়াল ‘নিশাঙ্ক’  উপস্থিত ছিলেন। এই উপলক্ষ্যে উপরাষ্ট্রপতি হীরকজয়ন্তীর লোগো এবং ২০৩০-এর জন্য প্রতিষ্ঠানের লক্ষ্য ও পরিকল্পনার নথি প্রকাশ করেছেন।""
expected = [
""উপরাষ্ট্রপতি শ্রী এম ভেঙ্কাইয়া নাইডু সোমবার আই আই টি দিল্লির হীরক জয়ন্তী উদযাপনের উদ্বোধন করেছেন। অনলাইনের মাধ্যমে এই অনুষ্ঠানে কেন্দ্রীয় মানব সম্পদ উন্নয়নমন্ত্রী শ্রী রমেশ পোখরিয়াল ‘নিশাঙ্ক’  উপস্থিত ছিলেন। এই উপলক্ষ্যে উপরাষ্ট্রপতি হীরকজয়ন্তীর লোগো এবং ২০৩০-এর জন্য প্রতিষ্ঠানের লক্ষ্য ও পরিকল্পনার নথি প্রকাশ করেছেন।""
]
assert obj.tokenize(sentences) == expected
",[],0,[],/test/unit/test_tokenize.py_test_punkt_tokenize_no_custom_lang_vars
5467,/home/amandapotts/git/nltk/nltk/test/unit/test_tokenize.py_punkt_debug_decisions,"def punkt_debug_decisions(self, input_text, n_sents, n_splits, lang_vars=None):
tokenizer = punkt.PunktSentenceTokenizer()
if lang_vars != None:
tokenizer._lang_vars = lang_vars
assert len(tokenizer.tokenize(input_text)) == n_sents
assert len(list(tokenizer.debug_decisions(input_text))) == n_splits
",[],0,[],/test/unit/test_tokenize.py_punkt_debug_decisions
5468,/home/amandapotts/git/nltk/nltk/test/unit/test_tokenize.py_test_punkt_debug_decisions_custom_end,"def test_punkt_debug_decisions_custom_end(self):
",[],0,[],/test/unit/test_tokenize.py_test_punkt_debug_decisions_custom_end
5469,/home/amandapotts/git/nltk/nltk/test/unit/test_tokenize.py_ExtLangVars,"class ExtLangVars(punkt.PunktLanguageVars):
sent_end_chars = (""."", ""?"", ""!"", ""^"")
",[],0,[],/test/unit/test_tokenize.py_ExtLangVars
5470,/home/amandapotts/git/nltk/nltk/test/unit/test_tokenize.py_test_sent_tokenize,"def test_sent_tokenize(self, sentences: str, expected: List[str]):
assert sent_tokenize(sentences) == expected
",[],0,[],/test/unit/test_tokenize.py_test_sent_tokenize
5471,/home/amandapotts/git/nltk/nltk/test/unit/test_tokenize.py_test_string_tokenizer,"def test_string_tokenizer(self) -> None:
sentence = ""Hello there""
tokenizer = CharTokenizer()
assert tokenizer.tokenize(sentence) == list(sentence)
assert list(tokenizer.span_tokenize(sentence)) == [
(0, 1),
(1, 2),
(2, 3),
(3, 4),
(4, 5),
(5, 6),
(6, 7),
(7, 8),
(8, 9),
(9, 10),
(10, 11),
]
",[],0,[],/test/unit/test_tokenize.py_test_string_tokenizer
5472,/home/amandapotts/git/nltk/nltk/test/unit/test_tokenize.py_test_punkt_train,"def test_punkt_train(self) -> None:
trainer = punkt.PunktTrainer()
trainer.train(""This is a test."")
",[],0,[],/test/unit/test_tokenize.py_test_punkt_train
5473,/home/amandapotts/git/nltk/nltk/test/unit/test_tokenize.py_test_punkt_train_single_word,"def test_punkt_train_single_word(self) -> None:
trainer = punkt.PunktTrainer()
trainer.train(""This."")
",[],0,[],/test/unit/test_tokenize.py_test_punkt_train_single_word
5474,/home/amandapotts/git/nltk/nltk/test/unit/test_tokenize.py_test_punkt_train_no_punc,"def test_punkt_train_no_punc(self) -> None:
trainer = punkt.PunktTrainer()
trainer.train(""This is a test"")
",[],0,[],/test/unit/test_tokenize.py_test_punkt_train_no_punc
5475,/home/amandapotts/git/nltk/nltk/test/unit/test_metrics.py_test_lr_bigram,"def test_lr_bigram(self):
self.assertAlmostEqual(
BigramAssocMeasures.likelihood_ratio(2, (4, 4), 20),
2.4142743368419755,
delta=_DELTA,
)
self.assertAlmostEqual(
BigramAssocMeasures.likelihood_ratio(1, (1, 1), 1), 0.0, delta=_DELTA
)
self.assertRaises(
ValueError,
BigramAssocMeasures.likelihood_ratio,
)
",[],0,[],/test/unit/test_metrics.py_test_lr_bigram
5476,/home/amandapotts/git/nltk/nltk/test/unit/test_metrics.py_test_lr_trigram,"def test_lr_trigram(self):
self.assertAlmostEqual(
TrigramAssocMeasures.likelihood_ratio(1, (1, 1, 1), (1, 1, 1), 2),
5.545177444479562,
delta=_DELTA,
)
self.assertAlmostEqual(
TrigramAssocMeasures.likelihood_ratio(1, (1, 1, 1), (1, 1, 1), 1),
0.0,
delta=_DELTA,
)
self.assertRaises(
ValueError,
TrigramAssocMeasures.likelihood_ratio,
)
",[],0,[],/test/unit/test_metrics.py_test_lr_trigram
5477,/home/amandapotts/git/nltk/nltk/test/unit/test_metrics.py_test_lr_quadgram,"def test_lr_quadgram(self):
self.assertAlmostEqual(
QuadgramAssocMeasures.likelihood_ratio(
1, (1, 1, 1, 1), (1, 1, 1, 1, 1, 1), (1, 1, 1, 1), 2
),
8.317766166719343,
delta=_DELTA,
)
self.assertAlmostEqual(
QuadgramAssocMeasures.likelihood_ratio(
1, (1, 1, 1, 1), (1, 1, 1, 1, 1, 1), (1, 1, 1, 1), 1
),
0.0,
delta=_DELTA,
)
self.assertRaises(
ValueError,
QuadgramAssocMeasures.likelihood_ratio,
)
",[],0,[],/test/unit/test_metrics.py_test_lr_quadgram
5478,/home/amandapotts/git/nltk/nltk/test/unit/test_twitter_auth.py_auth,"def auth():
return Authenticate()
",[],0,[],/test/unit/test_twitter_auth.py_auth
5479,/home/amandapotts/git/nltk/nltk/test/unit/test_twitter_auth.py_setup_class,"def setup_class(self):
self.subdir = os.path.join(os.path.dirname(__file__), ""files"")
os.environ[""TWITTER""] = ""twitter-files""
",[],0,[],/test/unit/test_twitter_auth.py_setup_class
5480,/home/amandapotts/git/nltk/nltk/test/unit/test_twitter_auth.py_test_environment,"def test_environment(self, auth):
""""""
Test that environment variable has been read correctly.
""""""
fn = os.path.basename(auth.creds_subdir)
assert fn == os.environ[""TWITTER""]
",[],0,[],/test/unit/test_twitter_auth.py_test_environment
5481,/home/amandapotts/git/nltk/nltk/test/unit/test_twitter_auth.py_test_scenarios_that_should_raise_errors,"def test_scenarios_that_should_raise_errors(self, kwargs, auth):
""""""Various scenarios that should raise errors""""""
try:
auth.load_creds(**kwargs)
except (OSError, ValueError):
pass
except Exception as e:
pytest.fail(""Unexpected exception thrown: %s"" % e)
else:
pytest.fail(""OSError exception not thrown."")
",[],0,[],/test/unit/test_twitter_auth.py_test_scenarios_that_should_raise_errors
5482,/home/amandapotts/git/nltk/nltk/test/unit/test_twitter_auth.py_test_correct_file,"def test_correct_file(self, auth):
""""""Test that a proper file succeeds and is read correctly""""""
oauth = auth.load_creds(subdir=self.subdir)
assert auth.creds_fullpath == os.path.join(self.subdir, auth.creds_file)
assert auth.creds_file == ""credentials.txt""
assert oauth[""app_key""] == ""a""
",[],0,[],/test/unit/test_twitter_auth.py_test_correct_file
5483,/home/amandapotts/git/nltk/nltk/test/unit/test_stem.py_test_arabic,"def test_arabic(self):
""""""
this unit testing for test the snowball arabic light stemmer
this stemmer deals with prefixes and suffixes
""""""
ar_stemmer = SnowballStemmer(""arabic"", True)
assert ar_stemmer.stem(""الْعَرَبِــــــيَّة"") == ""عرب""
assert ar_stemmer.stem(""العربية"") == ""عرب""
assert ar_stemmer.stem(""فقالوا"") == ""قال""
assert ar_stemmer.stem(""الطالبات"") == ""طالب""
assert ar_stemmer.stem(""فالطالبات"") == ""طالب""
assert ar_stemmer.stem(""والطالبات"") == ""طالب""
assert ar_stemmer.stem(""الطالبون"") == ""طالب""
assert ar_stemmer.stem(""اللذان"") == ""اللذان""
assert ar_stemmer.stem(""من"") == ""من""
ar_stemmer = SnowballStemmer(""arabic"", False)
assert ar_stemmer.stem(""اللذان"") == ""اللذ""  # this is a stop word
assert ar_stemmer.stem(""الطالبات"") == ""طالب""
assert ar_stemmer.stem(""الكلمات"") == ""كلم""
ar_stemmer = SnowballStemmer(""arabic"")
assert ar_stemmer.stem(""الْعَرَبِــــــيَّة"") == ""عرب""
assert ar_stemmer.stem(""العربية"") == ""عرب""
assert ar_stemmer.stem(""فقالوا"") == ""قال""
assert ar_stemmer.stem(""الطالبات"") == ""طالب""
assert ar_stemmer.stem(""الكلمات"") == ""كلم""
",[],0,[],/test/unit/test_stem.py_test_arabic
5484,/home/amandapotts/git/nltk/nltk/test/unit/test_stem.py_test_russian,"def test_russian(self):
stemmer_russian = SnowballStemmer(""russian"")
assert stemmer_russian.stem(""авантненькая"") == ""авантненьк""
",[],0,[],/test/unit/test_stem.py_test_russian
5485,/home/amandapotts/git/nltk/nltk/test/unit/test_stem.py_test_german,"def test_german(self):
stemmer_german = SnowballStemmer(""german"")
stemmer_german2 = SnowballStemmer(""german"", ignore_stopwords=True)
assert stemmer_german.stem(""Schr\xe4nke"") == ""schrank""
assert stemmer_german2.stem(""Schr\xe4nke"") == ""schrank""
assert stemmer_german.stem(""keinen"") == ""kein""
assert stemmer_german2.stem(""keinen"") == ""keinen""
",[],0,[],/test/unit/test_stem.py_test_german
5486,/home/amandapotts/git/nltk/nltk/test/unit/test_stem.py_test_spanish,"def test_spanish(self):
stemmer = SnowballStemmer(""spanish"")
assert stemmer.stem(""Visionado"") == ""vision""
assert stemmer.stem(""algue"") == ""algu""
",[],0,[],/test/unit/test_stem.py_test_spanish
5487,/home/amandapotts/git/nltk/nltk/test/unit/test_stem.py_test_short_strings_bug,"def test_short_strings_bug(self):
stemmer = SnowballStemmer(""english"")
assert stemmer.stem(""y's"") == ""y""
",[],0,[],/test/unit/test_stem.py_test_short_strings_bug
5488,/home/amandapotts/git/nltk/nltk/test/unit/test_stem.py__vocabulary,"def _vocabulary(self):
with closing(
data.find(""stemmers/porter_test/porter_vocabulary.txt"").open(
encoding=""utf-8""
)
) as fp:
return fp.read().splitlines()
",[],0,[],/test/unit/test_stem.py__vocabulary
5489,/home/amandapotts/git/nltk/nltk/test/unit/test_stem.py__test_against_expected_output,"def _test_against_expected_output(self, stemmer_mode, expected_stems):
stemmer = PorterStemmer(mode=stemmer_mode)
for word, true_stem in zip(self._vocabulary(), expected_stems):
our_stem = stemmer.stem(word)
assert (
our_stem == true_stem
), ""{} should stem to {} in {} mode but got {}"".format(
word,
true_stem,
stemmer_mode,
our_stem,
)
",[],0,[],/test/unit/test_stem.py__test_against_expected_output
5490,/home/amandapotts/git/nltk/nltk/test/unit/test_stem.py_test_vocabulary_martin_mode,"def test_vocabulary_martin_mode(self):
""""""Tests all words from the test vocabulary provided by M Porter
The sample vocabulary and output were sourced from
https://tartarus.org/martin/PorterStemmer/voc.txt and
https://tartarus.org/martin/PorterStemmer/output.txt
and are linked to from the Porter Stemmer algorithm's homepage
at https://tartarus.org/martin/PorterStemmer/
""""""
with closing(
data.find(""stemmers/porter_test/porter_martin_output.txt"").open(
encoding=""utf-8""
)
) as fp:
self._test_against_expected_output(
PorterStemmer.MARTIN_EXTENSIONS, fp.read().splitlines()
)
",[],0,[],/test/unit/test_stem.py_test_vocabulary_martin_mode
5491,/home/amandapotts/git/nltk/nltk/test/unit/test_stem.py_test_vocabulary_nltk_mode,"def test_vocabulary_nltk_mode(self):
with closing(
data.find(""stemmers/porter_test/porter_nltk_output.txt"").open(
encoding=""utf-8""
)
) as fp:
self._test_against_expected_output(
PorterStemmer.NLTK_EXTENSIONS, fp.read().splitlines()
)
",[],0,[],/test/unit/test_stem.py_test_vocabulary_nltk_mode
5492,/home/amandapotts/git/nltk/nltk/test/unit/test_stem.py_test_vocabulary_original_mode,"def test_vocabulary_original_mode(self):
with closing(
data.find(""stemmers/porter_test/porter_original_output.txt"").open(
encoding=""utf-8""
)
) as fp:
self._test_against_expected_output(
PorterStemmer.ORIGINAL_ALGORITHM, fp.read().splitlines()
)
self._test_against_expected_output(
PorterStemmer.ORIGINAL_ALGORITHM,
data.find(""stemmers/porter_test/porter_original_output.txt"")
.open(encoding=""utf-8"")
.read()
.splitlines(),
)
",[],0,[],/test/unit/test_stem.py_test_vocabulary_original_mode
5493,/home/amandapotts/git/nltk/nltk/test/unit/test_stem.py_test_oed_bug,"def test_oed_bug(self):
""""""Test for bug https://github.com/nltk/nltk/issues/1581
Ensures that 'oed' can be stemmed without throwing an error.
""""""
assert PorterStemmer().stem(""oed"") == ""o""
",[],0,[],/test/unit/test_stem.py_test_oed_bug
5494,/home/amandapotts/git/nltk/nltk/test/unit/test_stem.py_test_lowercase_option,"def test_lowercase_option(self):
""""""Test for improvement on https://github.com/nltk/nltk/issues/2507
Ensures that stems are lowercased when `to_lowercase=True`
""""""
porter = PorterStemmer()
assert porter.stem(""On"") == ""on""
assert porter.stem(""I"") == ""i""
assert porter.stem(""I"", to_lowercase=False) == ""I""
assert porter.stem(""Github"") == ""github""
assert porter.stem(""Github"", to_lowercase=False) == ""Github""
",[],0,[],/test/unit/test_stem.py_test_lowercase_option
5495,/home/amandapotts/git/nltk/nltk/test/unit/test_downloader.py_test_downloader_using_existing_parent_download_dir,"def test_downloader_using_existing_parent_download_dir(tmp_path):
""""""Test that download works properly when the parent folder of the download_dir exists""""""
download_dir = str(tmp_path.joinpath(""another_dir""))
download_status = download(""mwa_ppdb"", download_dir)
assert download_status is True
",[],0,[],/test/unit/test_downloader.py_test_downloader_using_existing_parent_download_dir
5496,/home/amandapotts/git/nltk/nltk/test/unit/test_downloader.py_test_downloader_using_non_existing_parent_download_dir,"def test_downloader_using_non_existing_parent_download_dir(tmp_path):
""""""Test that download works properly when the parent folder of the download_dir does not exist""""""
download_dir = str(
tmp_path.joinpath(""non-existing-parent-folder"", ""another-non-existing-folder"")
)
download_status = download(""mwa_ppdb"", download_dir)
assert download_status is True
",[],0,[],/test/unit/test_downloader.py_test_downloader_using_non_existing_parent_download_dir
5497,/home/amandapotts/git/nltk/nltk/test/unit/test_corenlp.py_setup_module,"def setup_module(module):
global server
try:
server = corenlp.CoreNLPServer(port=9000)
except LookupError:
pytest.skip(""Could not instantiate CoreNLPServer."")
try:
server.start()
except corenlp.CoreNLPServerError as e:
pytest.skip(
""Skipping CoreNLP tests because the server could not be started. ""
""Make sure that the 9000 port is free. ""
""{}"".format(e.strerror)
)
",[],0,[],/test/unit/test_corenlp.py_setup_module
5498,/home/amandapotts/git/nltk/nltk/test/unit/test_corenlp.py_teardown_module,"def teardown_module(module):
server.stop()
",[],0,[],/test/unit/test_corenlp.py_teardown_module
5499,/home/amandapotts/git/nltk/nltk/test/unit/test_corenlp.py_test_tokenize,"def test_tokenize(self):
corenlp_tokenizer = corenlp.CoreNLPParser()
api_return_value = {
""sentences"": [
{
""index"": 0,
""tokens"": [
{
""after"": "" "",
""before"": """",
""characterOffsetBegin"": 0,
""characterOffsetEnd"": 4,
""index"": 1,
""originalText"": ""Good"",
""word"": ""Good"",
},
{
""after"": "" "",
""before"": "" "",
""characterOffsetBegin"": 5,
""characterOffsetEnd"": 12,
""index"": 2,
""originalText"": ""muffins"",
""word"": ""muffins"",
},
{
""after"": "" "",
""before"": "" "",
""characterOffsetBegin"": 13,
""characterOffsetEnd"": 17,
""index"": 3,
""originalText"": ""cost"",
""word"": ""cost"",
},
{
""after"": """",
""before"": "" "",
""characterOffsetBegin"": 18,
""characterOffsetEnd"": 19,
""index"": 4,
""originalText"": ""$"",
""word"": ""$"",
},
{
""after"": ""\n"",
""before"": """",
""characterOffsetBegin"": 19,
""characterOffsetEnd"": 23,
""index"": 5,
""originalText"": ""3.88"",
""word"": ""3.88"",
},
{
""after"": "" "",
""before"": ""\n"",
""characterOffsetBegin"": 24,
""characterOffsetEnd"": 26,
""index"": 6,
""originalText"": ""in"",
""word"": ""in"",
},
{
""after"": "" "",
""before"": "" "",
""characterOffsetBegin"": 27,
""characterOffsetEnd"": 30,
""index"": 7,
""originalText"": ""New"",
""word"": ""New"",
},
{
""after"": """",
""before"": "" "",
""characterOffsetBegin"": 31,
""characterOffsetEnd"": 35,
""index"": 8,
""originalText"": ""York"",
""word"": ""York"",
},
{
""after"": ""  "",
""before"": """",
""characterOffsetBegin"": 35,
""characterOffsetEnd"": 36,
""index"": 9,
""originalText"": ""."",
""word"": ""."",
},
],
},
{
""index"": 1,
""tokens"": [
{
""after"": "" "",
""before"": ""  "",
""characterOffsetBegin"": 38,
""characterOffsetEnd"": 44,
""index"": 1,
""originalText"": ""Please"",
""word"": ""Please"",
},
{
""after"": "" "",
""before"": "" "",
""characterOffsetBegin"": 45,
""characterOffsetEnd"": 48,
""index"": 2,
""originalText"": ""buy"",
""word"": ""buy"",
},
{
""after"": ""\n"",
""before"": "" "",
""characterOffsetBegin"": 49,
""characterOffsetEnd"": 51,
""index"": 3,
""originalText"": ""me"",
""word"": ""me"",
},
{
""after"": "" "",
""before"": ""\n"",
""characterOffsetBegin"": 52,
""characterOffsetEnd"": 55,
""index"": 4,
""originalText"": ""two"",
""word"": ""two"",
},
{
""after"": "" "",
""before"": "" "",
""characterOffsetBegin"": 56,
""characterOffsetEnd"": 58,
""index"": 5,
""originalText"": ""of"",
""word"": ""of"",
},
{
""after"": """",
""before"": "" "",
""characterOffsetBegin"": 59,
""characterOffsetEnd"": 63,
""index"": 6,
""originalText"": ""them"",
""word"": ""them"",
},
{
""after"": ""\n"",
""before"": """",
""characterOffsetBegin"": 63,
""characterOffsetEnd"": 64,
""index"": 7,
""originalText"": ""."",
""word"": ""."",
},
],
},
{
""index"": 2,
""tokens"": [
{
""after"": """",
""before"": ""\n"",
""characterOffsetBegin"": 65,
""characterOffsetEnd"": 71,
""index"": 1,
""originalText"": ""Thanks"",
""word"": ""Thanks"",
},
{
""after"": """",
""before"": """",
""characterOffsetBegin"": 71,
""characterOffsetEnd"": 72,
""index"": 2,
""originalText"": ""."",
""word"": ""."",
},
],
},
]
}
corenlp_tokenizer.api_call = MagicMock(return_value=api_return_value)
input_string = ""Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.\nThanks.""
expected_output = [
""Good"",
""muffins"",
""cost"",
""$"",
""3.88"",
""in"",
""New"",
""York"",
""."",
""Please"",
""buy"",
""me"",
""two"",
""of"",
""them"",
""."",
""Thanks"",
""."",
]
tokenized_output = list(corenlp_tokenizer.tokenize(input_string))
corenlp_tokenizer.api_call.assert_called_once_with(
""Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.\nThanks."",
properties={""annotators"": ""tokenize,ssplit""},
)
self.assertEqual(expected_output, tokenized_output)
",[],0,[],/test/unit/test_corenlp.py_test_tokenize
5500,/home/amandapotts/git/nltk/nltk/test/unit/test_corenlp.py_test_pos_tagger,"def test_pos_tagger(self):
corenlp_tagger = corenlp.CoreNLPParser(tagtype=""pos"")
api_return_value = {
""sentences"": [
{
""basicDependencies"": [
{
""dep"": ""ROOT"",
""dependent"": 1,
""dependentGloss"": ""What"",
""governor"": 0,
""governorGloss"": ""ROOT"",
},
{
""dep"": ""cop"",
""dependent"": 2,
""dependentGloss"": ""is"",
""governor"": 1,
""governorGloss"": ""What"",
},
{
""dep"": ""det"",
""dependent"": 3,
""dependentGloss"": ""the"",
""governor"": 4,
""governorGloss"": ""airspeed"",
},
{
""dep"": ""nsubj"",
""dependent"": 4,
""dependentGloss"": ""airspeed"",
""governor"": 1,
""governorGloss"": ""What"",
},
{
""dep"": ""case"",
""dependent"": 5,
""dependentGloss"": ""of"",
""governor"": 8,
""governorGloss"": ""swallow"",
},
{
""dep"": ""det"",
""dependent"": 6,
""dependentGloss"": ""an"",
""governor"": 8,
""governorGloss"": ""swallow"",
},
{
""dep"": ""compound"",
""dependent"": 7,
""dependentGloss"": ""unladen"",
""governor"": 8,
""governorGloss"": ""swallow"",
},
{
""dep"": ""nmod"",
""dependent"": 8,
""dependentGloss"": ""swallow"",
""governor"": 4,
""governorGloss"": ""airspeed"",
},
{
""dep"": ""punct"",
""dependent"": 9,
""dependentGloss"": ""?"",
""governor"": 1,
""governorGloss"": ""What"",
},
],
""enhancedDependencies"": [
{
""dep"": ""ROOT"",
""dependent"": 1,
""dependentGloss"": ""What"",
""governor"": 0,
""governorGloss"": ""ROOT"",
},
{
""dep"": ""cop"",
""dependent"": 2,
""dependentGloss"": ""is"",
""governor"": 1,
""governorGloss"": ""What"",
},
{
""dep"": ""det"",
""dependent"": 3,
""dependentGloss"": ""the"",
""governor"": 4,
""governorGloss"": ""airspeed"",
},
{
""dep"": ""nsubj"",
""dependent"": 4,
""dependentGloss"": ""airspeed"",
""governor"": 1,
""governorGloss"": ""What"",
},
{
""dep"": ""case"",
""dependent"": 5,
""dependentGloss"": ""of"",
""governor"": 8,
""governorGloss"": ""swallow"",
},
{
""dep"": ""det"",
""dependent"": 6,
""dependentGloss"": ""an"",
""governor"": 8,
""governorGloss"": ""swallow"",
},
{
""dep"": ""compound"",
""dependent"": 7,
""dependentGloss"": ""unladen"",
""governor"": 8,
""governorGloss"": ""swallow"",
},
{
""dep"": ""nmod:of"",
""dependent"": 8,
""dependentGloss"": ""swallow"",
""governor"": 4,
""governorGloss"": ""airspeed"",
},
{
""dep"": ""punct"",
""dependent"": 9,
""dependentGloss"": ""?"",
""governor"": 1,
""governorGloss"": ""What"",
},
],
""enhancedPlusPlusDependencies"": [
{
""dep"": ""ROOT"",
""dependent"": 1,
""dependentGloss"": ""What"",
""governor"": 0,
""governorGloss"": ""ROOT"",
},
{
""dep"": ""cop"",
""dependent"": 2,
""dependentGloss"": ""is"",
""governor"": 1,
""governorGloss"": ""What"",
},
{
""dep"": ""det"",
""dependent"": 3,
""dependentGloss"": ""the"",
""governor"": 4,
""governorGloss"": ""airspeed"",
},
{
""dep"": ""nsubj"",
""dependent"": 4,
""dependentGloss"": ""airspeed"",
""governor"": 1,
""governorGloss"": ""What"",
},
{
""dep"": ""case"",
""dependent"": 5,
""dependentGloss"": ""of"",
""governor"": 8,
""governorGloss"": ""swallow"",
},
{
""dep"": ""det"",
""dependent"": 6,
""dependentGloss"": ""an"",
""governor"": 8,
""governorGloss"": ""swallow"",
},
{
""dep"": ""compound"",
""dependent"": 7,
""dependentGloss"": ""unladen"",
""governor"": 8,
""governorGloss"": ""swallow"",
},
{
""dep"": ""nmod:of"",
""dependent"": 8,
""dependentGloss"": ""swallow"",
""governor"": 4,
""governorGloss"": ""airspeed"",
},
{
""dep"": ""punct"",
""dependent"": 9,
""dependentGloss"": ""?"",
""governor"": 1,
""governorGloss"": ""What"",
},
],
""index"": 0,
""parse"": ""(ROOT\n  (SBARQ\n    (WHNP (WP What))\n    (SQ (VBZ is)\n      (NP\n        (NP (DT the) (NN airspeed))\n        (PP (IN of)\n          (NP (DT an) (NN unladen) (NN swallow)))))\n    (. ?)))"",
""tokens"": [
{
""after"": "" "",
""before"": """",
""characterOffsetBegin"": 0,
""characterOffsetEnd"": 4,
""index"": 1,
""lemma"": ""what"",
""originalText"": ""What"",
""pos"": ""WP"",
""word"": ""What"",
},
{
""after"": "" "",
""before"": "" "",
""characterOffsetBegin"": 5,
""characterOffsetEnd"": 7,
""index"": 2,
""lemma"": ""be"",
""originalText"": ""is"",
""pos"": ""VBZ"",
""word"": ""is"",
},
{
""after"": "" "",
""before"": "" "",
""characterOffsetBegin"": 8,
""characterOffsetEnd"": 11,
""index"": 3,
""lemma"": ""the"",
""originalText"": ""the"",
""pos"": ""DT"",
""word"": ""the"",
},
{
""after"": "" "",
""before"": "" "",
""characterOffsetBegin"": 12,
""characterOffsetEnd"": 20,
""index"": 4,
""lemma"": ""airspeed"",
""originalText"": ""airspeed"",
""pos"": ""NN"",
""word"": ""airspeed"",
},
{
""after"": "" "",
""before"": "" "",
""characterOffsetBegin"": 21,
""characterOffsetEnd"": 23,
""index"": 5,
""lemma"": ""of"",
""originalText"": ""of"",
""pos"": ""IN"",
""word"": ""of"",
},
{
""after"": "" "",
""before"": "" "",
""characterOffsetBegin"": 24,
""characterOffsetEnd"": 26,
""index"": 6,
""lemma"": ""a"",
""originalText"": ""an"",
""pos"": ""DT"",
""word"": ""an"",
},
{
""after"": "" "",
""before"": "" "",
""characterOffsetBegin"": 27,
""characterOffsetEnd"": 34,
""index"": 7,
""lemma"": ""unladen"",
""originalText"": ""unladen"",
""pos"": ""JJ"",
""word"": ""unladen"",
},
{
""after"": "" "",
""before"": "" "",
""characterOffsetBegin"": 35,
""characterOffsetEnd"": 42,
""index"": 8,
""lemma"": ""swallow"",
""originalText"": ""swallow"",
""pos"": ""VB"",
""word"": ""swallow"",
},
{
""after"": """",
""before"": "" "",
""characterOffsetBegin"": 43,
""characterOffsetEnd"": 44,
""index"": 9,
""lemma"": ""?"",
""originalText"": ""?"",
""pos"": ""."",
""word"": ""?"",
},
],
}
]
}
corenlp_tagger.api_call = MagicMock(return_value=api_return_value)
input_tokens = ""What is the airspeed of an unladen swallow ?"".split()
expected_output = [
(""What"", ""WP""),
(""is"", ""VBZ""),
(""the"", ""DT""),
(""airspeed"", ""NN""),
(""of"", ""IN""),
(""an"", ""DT""),
(""unladen"", ""JJ""),
(""swallow"", ""VB""),
(""?"", "".""),
]
tagged_output = corenlp_tagger.tag(input_tokens)
corenlp_tagger.api_call.assert_called_once_with(
""What is the airspeed of an unladen swallow ?"",
properties={
""ssplit.isOneSentence"": ""true"",
""annotators"": ""tokenize,ssplit,pos"",
},
)
self.assertEqual(expected_output, tagged_output)
",[],0,[],/test/unit/test_corenlp.py_test_pos_tagger
5501,/home/amandapotts/git/nltk/nltk/test/unit/test_corenlp.py_test_ner_tagger,"def test_ner_tagger(self):
corenlp_tagger = corenlp.CoreNLPParser(tagtype=""ner"")
api_return_value = {
""sentences"": [
{
""index"": 0,
""tokens"": [
{
""after"": "" "",
""before"": """",
""characterOffsetBegin"": 0,
""characterOffsetEnd"": 4,
""index"": 1,
""lemma"": ""Rami"",
""ner"": ""PERSON"",
""originalText"": ""Rami"",
""pos"": ""NNP"",
""word"": ""Rami"",
},
{
""after"": "" "",
""before"": "" "",
""characterOffsetBegin"": 5,
""characterOffsetEnd"": 8,
""index"": 2,
""lemma"": ""Eid"",
""ner"": ""PERSON"",
""originalText"": ""Eid"",
""pos"": ""NNP"",
""word"": ""Eid"",
},
{
""after"": "" "",
""before"": "" "",
""characterOffsetBegin"": 9,
""characterOffsetEnd"": 11,
""index"": 3,
""lemma"": ""be"",
""ner"": ""O"",
""originalText"": ""is"",
""pos"": ""VBZ"",
""word"": ""is"",
},
{
""after"": "" "",
""before"": "" "",
""characterOffsetBegin"": 12,
""characterOffsetEnd"": 20,
""index"": 4,
""lemma"": ""study"",
""ner"": ""O"",
""originalText"": ""studying"",
""pos"": ""VBG"",
""word"": ""studying"",
},
{
""after"": "" "",
""before"": "" "",
""characterOffsetBegin"": 21,
""characterOffsetEnd"": 23,
""index"": 5,
""lemma"": ""at"",
""ner"": ""O"",
""originalText"": ""at"",
""pos"": ""IN"",
""word"": ""at"",
},
{
""after"": "" "",
""before"": "" "",
""characterOffsetBegin"": 24,
""characterOffsetEnd"": 29,
""index"": 6,
""lemma"": ""Stony"",
""ner"": ""ORGANIZATION"",
""originalText"": ""Stony"",
""pos"": ""NNP"",
""word"": ""Stony"",
},
{
""after"": "" "",
""before"": "" "",
""characterOffsetBegin"": 30,
""characterOffsetEnd"": 35,
""index"": 7,
""lemma"": ""Brook"",
""ner"": ""ORGANIZATION"",
""originalText"": ""Brook"",
""pos"": ""NNP"",
""word"": ""Brook"",
},
{
""after"": "" "",
""before"": "" "",
""characterOffsetBegin"": 36,
""characterOffsetEnd"": 46,
""index"": 8,
""lemma"": ""University"",
""ner"": ""ORGANIZATION"",
""originalText"": ""University"",
""pos"": ""NNP"",
""word"": ""University"",
},
{
""after"": "" "",
""before"": "" "",
""characterOffsetBegin"": 47,
""characterOffsetEnd"": 49,
""index"": 9,
""lemma"": ""in"",
""ner"": ""O"",
""originalText"": ""in"",
""pos"": ""IN"",
""word"": ""in"",
},
{
""after"": """",
""before"": "" "",
""characterOffsetBegin"": 50,
""characterOffsetEnd"": 52,
""index"": 10,
""lemma"": ""NY"",
""ner"": ""O"",
""originalText"": ""NY"",
""pos"": ""NNP"",
""word"": ""NY"",
},
],
}
]
}
corenlp_tagger.api_call = MagicMock(return_value=api_return_value)
input_tokens = ""Rami Eid is studying at Stony Brook University in NY"".split()
expected_output = [
(""Rami"", ""PERSON""),
(""Eid"", ""PERSON""),
(""is"", ""O""),
(""studying"", ""O""),
(""at"", ""O""),
(""Stony"", ""ORGANIZATION""),
(""Brook"", ""ORGANIZATION""),
(""University"", ""ORGANIZATION""),
(""in"", ""O""),
(""NY"", ""O""),
]
tagged_output = corenlp_tagger.tag(input_tokens)
corenlp_tagger.api_call.assert_called_once_with(
""Rami Eid is studying at Stony Brook University in NY"",
properties={
""ssplit.isOneSentence"": ""true"",
""annotators"": ""tokenize,ssplit,ner"",
},
)
self.assertEqual(expected_output, tagged_output)
",[],0,[],/test/unit/test_corenlp.py_test_ner_tagger
5502,/home/amandapotts/git/nltk/nltk/test/unit/test_corenlp.py_test_unexpected_tagtype,"def test_unexpected_tagtype(self):
with self.assertRaises(ValueError):
corenlp_tagger = corenlp.CoreNLPParser(tagtype=""test"")
",[],0,[],/test/unit/test_corenlp.py_test_unexpected_tagtype
5503,/home/amandapotts/git/nltk/nltk/test/unit/test_corenlp.py_test_parse,"def test_parse(self):
corenlp_parser = corenlp.CoreNLPParser()
api_return_value = {
""sentences"": [
{
""basicDependencies"": [
{
""dep"": ""ROOT"",
""dependent"": 4,
""dependentGloss"": ""fox"",
""governor"": 0,
""governorGloss"": ""ROOT"",
},
{
""dep"": ""det"",
""dependent"": 1,
""dependentGloss"": ""The"",
""governor"": 4,
""governorGloss"": ""fox"",
},
{
""dep"": ""amod"",
""dependent"": 2,
""dependentGloss"": ""quick"",
""governor"": 4,
""governorGloss"": ""fox"",
},
{
""dep"": ""amod"",
""dependent"": 3,
""dependentGloss"": ""brown"",
""governor"": 4,
""governorGloss"": ""fox"",
},
{
""dep"": ""dep"",
""dependent"": 5,
""dependentGloss"": ""jumps"",
""governor"": 4,
""governorGloss"": ""fox"",
},
{
""dep"": ""case"",
""dependent"": 6,
""dependentGloss"": ""over"",
""governor"": 9,
""governorGloss"": ""dog"",
},
{
""dep"": ""det"",
""dependent"": 7,
""dependentGloss"": ""the"",
""governor"": 9,
""governorGloss"": ""dog"",
},
{
""dep"": ""amod"",
""dependent"": 8,
""dependentGloss"": ""lazy"",
""governor"": 9,
""governorGloss"": ""dog"",
},
{
""dep"": ""nmod"",
""dependent"": 9,
""dependentGloss"": ""dog"",
""governor"": 5,
""governorGloss"": ""jumps"",
},
],
""enhancedDependencies"": [
{
""dep"": ""ROOT"",
""dependent"": 4,
""dependentGloss"": ""fox"",
""governor"": 0,
""governorGloss"": ""ROOT"",
},
{
""dep"": ""det"",
""dependent"": 1,
""dependentGloss"": ""The"",
""governor"": 4,
""governorGloss"": ""fox"",
},
{
""dep"": ""amod"",
""dependent"": 2,
""dependentGloss"": ""quick"",
""governor"": 4,
""governorGloss"": ""fox"",
},
{
""dep"": ""amod"",
""dependent"": 3,
""dependentGloss"": ""brown"",
""governor"": 4,
""governorGloss"": ""fox"",
},
{
""dep"": ""dep"",
""dependent"": 5,
""dependentGloss"": ""jumps"",
""governor"": 4,
""governorGloss"": ""fox"",
},
{
""dep"": ""case"",
""dependent"": 6,
""dependentGloss"": ""over"",
""governor"": 9,
""governorGloss"": ""dog"",
},
{
""dep"": ""det"",
""dependent"": 7,
""dependentGloss"": ""the"",
""governor"": 9,
""governorGloss"": ""dog"",
},
{
""dep"": ""amod"",
""dependent"": 8,
""dependentGloss"": ""lazy"",
""governor"": 9,
""governorGloss"": ""dog"",
},
{
""dep"": ""nmod:over"",
""dependent"": 9,
""dependentGloss"": ""dog"",
""governor"": 5,
""governorGloss"": ""jumps"",
},
],
""enhancedPlusPlusDependencies"": [
{
""dep"": ""ROOT"",
""dependent"": 4,
""dependentGloss"": ""fox"",
""governor"": 0,
""governorGloss"": ""ROOT"",
},
{
""dep"": ""det"",
""dependent"": 1,
""dependentGloss"": ""The"",
""governor"": 4,
""governorGloss"": ""fox"",
},
{
""dep"": ""amod"",
""dependent"": 2,
""dependentGloss"": ""quick"",
""governor"": 4,
""governorGloss"": ""fox"",
},
{
""dep"": ""amod"",
""dependent"": 3,
""dependentGloss"": ""brown"",
""governor"": 4,
""governorGloss"": ""fox"",
},
{
""dep"": ""dep"",
""dependent"": 5,
""dependentGloss"": ""jumps"",
""governor"": 4,
""governorGloss"": ""fox"",
},
{
""dep"": ""case"",
""dependent"": 6,
""dependentGloss"": ""over"",
""governor"": 9,
""governorGloss"": ""dog"",
},
{
""dep"": ""det"",
""dependent"": 7,
""dependentGloss"": ""the"",
""governor"": 9,
""governorGloss"": ""dog"",
},
{
""dep"": ""amod"",
""dependent"": 8,
""dependentGloss"": ""lazy"",
""governor"": 9,
""governorGloss"": ""dog"",
},
{
""dep"": ""nmod:over"",
""dependent"": 9,
""dependentGloss"": ""dog"",
""governor"": 5,
""governorGloss"": ""jumps"",
},
],
""index"": 0,
""parse"": ""(ROOT\n  (NP\n    (NP (DT The) (JJ quick) (JJ brown) (NN fox))\n    (NP\n      (NP (NNS jumps))\n      (PP (IN over)\n        (NP (DT the) (JJ lazy) (NN dog))))))"",
""tokens"": [
{
""after"": "" "",
""before"": """",
""characterOffsetBegin"": 0,
""characterOffsetEnd"": 3,
""index"": 1,
""lemma"": ""the"",
""originalText"": ""The"",
""pos"": ""DT"",
""word"": ""The"",
},
{
""after"": "" "",
""before"": "" "",
""characterOffsetBegin"": 4,
""characterOffsetEnd"": 9,
""index"": 2,
""lemma"": ""quick"",
""originalText"": ""quick"",
""pos"": ""JJ"",
""word"": ""quick"",
},
{
""after"": "" "",
""before"": "" "",
""characterOffsetBegin"": 10,
""characterOffsetEnd"": 15,
""index"": 3,
""lemma"": ""brown"",
""originalText"": ""brown"",
""pos"": ""JJ"",
""word"": ""brown"",
},
{
""after"": "" "",
""before"": "" "",
""characterOffsetBegin"": 16,
""characterOffsetEnd"": 19,
""index"": 4,
""lemma"": ""fox"",
""originalText"": ""fox"",
""pos"": ""NN"",
""word"": ""fox"",
},
{
""after"": "" "",
""before"": "" "",
""characterOffsetBegin"": 20,
""characterOffsetEnd"": 25,
""index"": 5,
""lemma"": ""jump"",
""originalText"": ""jumps"",
""pos"": ""VBZ"",
""word"": ""jumps"",
},
{
""after"": "" "",
""before"": "" "",
""characterOffsetBegin"": 26,
""characterOffsetEnd"": 30,
""index"": 6,
""lemma"": ""over"",
""originalText"": ""over"",
""pos"": ""IN"",
""word"": ""over"",
},
{
""after"": "" "",
""before"": "" "",
""characterOffsetBegin"": 31,
""characterOffsetEnd"": 34,
""index"": 7,
""lemma"": ""the"",
""originalText"": ""the"",
""pos"": ""DT"",
""word"": ""the"",
},
{
""after"": "" "",
""before"": "" "",
""characterOffsetBegin"": 35,
""characterOffsetEnd"": 39,
""index"": 8,
""lemma"": ""lazy"",
""originalText"": ""lazy"",
""pos"": ""JJ"",
""word"": ""lazy"",
},
{
""after"": """",
""before"": "" "",
""characterOffsetBegin"": 40,
""characterOffsetEnd"": 43,
""index"": 9,
""lemma"": ""dog"",
""originalText"": ""dog"",
""pos"": ""NN"",
""word"": ""dog"",
},
],
}
]
}
corenlp_parser.api_call = MagicMock(return_value=api_return_value)
input_string = ""The quick brown fox jumps over the lazy dog"".split()
expected_output = Tree(
""ROOT"",
[
Tree(
""NP"",
[
Tree(
""NP"",
[
Tree(""DT"", [""The""]),
Tree(""JJ"", [""quick""]),
Tree(""JJ"", [""brown""]),
Tree(""NN"", [""fox""]),
],
),
Tree(
""NP"",
[
Tree(""NP"", [Tree(""NNS"", [""jumps""])]),
Tree(
""PP"",
[
Tree(""IN"", [""over""]),
Tree(
""NP"",
[
Tree(""DT"", [""the""]),
Tree(""JJ"", [""lazy""]),
Tree(""NN"", [""dog""]),
],
),
],
),
],
),
],
)
],
)
parsed_data = next(corenlp_parser.parse(input_string))
corenlp_parser.api_call.assert_called_once_with(
""The quick brown fox jumps over the lazy dog"",
properties={""ssplit.eolonly"": ""true""},
)
self.assertEqual(expected_output, parsed_data)
",[],0,[],/test/unit/test_corenlp.py_test_parse
5504,/home/amandapotts/git/nltk/nltk/test/unit/test_corenlp.py_test_dependency_parser,"def test_dependency_parser(self):
corenlp_parser = corenlp.CoreNLPDependencyParser()
api_return_value = {
""sentences"": [
{
""basicDependencies"": [
{
""dep"": ""ROOT"",
""dependent"": 5,
""dependentGloss"": ""jumps"",
""governor"": 0,
""governorGloss"": ""ROOT"",
},
{
""dep"": ""det"",
""dependent"": 1,
""dependentGloss"": ""The"",
""governor"": 4,
""governorGloss"": ""fox"",
},
{
""dep"": ""amod"",
""dependent"": 2,
""dependentGloss"": ""quick"",
""governor"": 4,
""governorGloss"": ""fox"",
},
{
""dep"": ""amod"",
""dependent"": 3,
""dependentGloss"": ""brown"",
""governor"": 4,
""governorGloss"": ""fox"",
},
{
""dep"": ""nsubj"",
""dependent"": 4,
""dependentGloss"": ""fox"",
""governor"": 5,
""governorGloss"": ""jumps"",
},
{
""dep"": ""case"",
""dependent"": 6,
""dependentGloss"": ""over"",
""governor"": 9,
""governorGloss"": ""dog"",
},
{
""dep"": ""det"",
""dependent"": 7,
""dependentGloss"": ""the"",
""governor"": 9,
""governorGloss"": ""dog"",
},
{
""dep"": ""amod"",
""dependent"": 8,
""dependentGloss"": ""lazy"",
""governor"": 9,
""governorGloss"": ""dog"",
},
{
""dep"": ""nmod"",
""dependent"": 9,
""dependentGloss"": ""dog"",
""governor"": 5,
""governorGloss"": ""jumps"",
},
],
""enhancedDependencies"": [
{
""dep"": ""ROOT"",
""dependent"": 5,
""dependentGloss"": ""jumps"",
""governor"": 0,
""governorGloss"": ""ROOT"",
},
{
""dep"": ""det"",
""dependent"": 1,
""dependentGloss"": ""The"",
""governor"": 4,
""governorGloss"": ""fox"",
},
{
""dep"": ""amod"",
""dependent"": 2,
""dependentGloss"": ""quick"",
""governor"": 4,
""governorGloss"": ""fox"",
},
{
""dep"": ""amod"",
""dependent"": 3,
""dependentGloss"": ""brown"",
""governor"": 4,
""governorGloss"": ""fox"",
},
{
""dep"": ""nsubj"",
""dependent"": 4,
""dependentGloss"": ""fox"",
""governor"": 5,
""governorGloss"": ""jumps"",
},
{
""dep"": ""case"",
""dependent"": 6,
""dependentGloss"": ""over"",
""governor"": 9,
""governorGloss"": ""dog"",
},
{
""dep"": ""det"",
""dependent"": 7,
""dependentGloss"": ""the"",
""governor"": 9,
""governorGloss"": ""dog"",
},
{
""dep"": ""amod"",
""dependent"": 8,
""dependentGloss"": ""lazy"",
""governor"": 9,
""governorGloss"": ""dog"",
},
{
""dep"": ""nmod:over"",
""dependent"": 9,
""dependentGloss"": ""dog"",
""governor"": 5,
""governorGloss"": ""jumps"",
},
],
""enhancedPlusPlusDependencies"": [
{
""dep"": ""ROOT"",
""dependent"": 5,
""dependentGloss"": ""jumps"",
""governor"": 0,
""governorGloss"": ""ROOT"",
},
{
""dep"": ""det"",
""dependent"": 1,
""dependentGloss"": ""The"",
""governor"": 4,
""governorGloss"": ""fox"",
},
{
""dep"": ""amod"",
""dependent"": 2,
""dependentGloss"": ""quick"",
""governor"": 4,
""governorGloss"": ""fox"",
},
{
""dep"": ""amod"",
""dependent"": 3,
""dependentGloss"": ""brown"",
""governor"": 4,
""governorGloss"": ""fox"",
},
{
""dep"": ""nsubj"",
""dependent"": 4,
""dependentGloss"": ""fox"",
""governor"": 5,
""governorGloss"": ""jumps"",
},
{
""dep"": ""case"",
""dependent"": 6,
""dependentGloss"": ""over"",
""governor"": 9,
""governorGloss"": ""dog"",
},
{
""dep"": ""det"",
""dependent"": 7,
""dependentGloss"": ""the"",
""governor"": 9,
""governorGloss"": ""dog"",
},
{
""dep"": ""amod"",
""dependent"": 8,
""dependentGloss"": ""lazy"",
""governor"": 9,
""governorGloss"": ""dog"",
},
{
""dep"": ""nmod:over"",
""dependent"": 9,
""dependentGloss"": ""dog"",
""governor"": 5,
""governorGloss"": ""jumps"",
},
],
""index"": 0,
""tokens"": [
{
""after"": "" "",
""before"": """",
""characterOffsetBegin"": 0,
""characterOffsetEnd"": 3,
""index"": 1,
""lemma"": ""the"",
""originalText"": ""The"",
""pos"": ""DT"",
""word"": ""The"",
},
{
""after"": "" "",
""before"": "" "",
""characterOffsetBegin"": 4,
""characterOffsetEnd"": 9,
""index"": 2,
""lemma"": ""quick"",
""originalText"": ""quick"",
""pos"": ""JJ"",
""word"": ""quick"",
},
{
""after"": "" "",
""before"": "" "",
""characterOffsetBegin"": 10,
""characterOffsetEnd"": 15,
""index"": 3,
""lemma"": ""brown"",
""originalText"": ""brown"",
""pos"": ""JJ"",
""word"": ""brown"",
},
{
""after"": "" "",
""before"": "" "",
""characterOffsetBegin"": 16,
""characterOffsetEnd"": 19,
""index"": 4,
""lemma"": ""fox"",
""originalText"": ""fox"",
""pos"": ""NN"",
""word"": ""fox"",
},
{
""after"": "" "",
""before"": "" "",
""characterOffsetBegin"": 20,
""characterOffsetEnd"": 25,
""index"": 5,
""lemma"": ""jump"",
""originalText"": ""jumps"",
""pos"": ""VBZ"",
""word"": ""jumps"",
},
{
""after"": "" "",
""before"": "" "",
""characterOffsetBegin"": 26,
""characterOffsetEnd"": 30,
""index"": 6,
""lemma"": ""over"",
""originalText"": ""over"",
""pos"": ""IN"",
""word"": ""over"",
},
{
""after"": "" "",
""before"": "" "",
""characterOffsetBegin"": 31,
""characterOffsetEnd"": 34,
""index"": 7,
""lemma"": ""the"",
""originalText"": ""the"",
""pos"": ""DT"",
""word"": ""the"",
},
{
""after"": "" "",
""before"": "" "",
""characterOffsetBegin"": 35,
""characterOffsetEnd"": 39,
""index"": 8,
""lemma"": ""lazy"",
""originalText"": ""lazy"",
""pos"": ""JJ"",
""word"": ""lazy"",
},
{
""after"": """",
""before"": "" "",
""characterOffsetBegin"": 40,
""characterOffsetEnd"": 43,
""index"": 9,
""lemma"": ""dog"",
""originalText"": ""dog"",
""pos"": ""NN"",
""word"": ""dog"",
},
],
}
]
}
corenlp_parser.api_call = MagicMock(return_value=api_return_value)
input_string = ""The quick brown fox jumps over the lazy dog"".split()
expected_output = Tree(
""jumps"",
[
Tree(""fox"", [""The"", ""quick"", ""brown""]),
Tree(""dog"", [""over"", ""the"", ""lazy""]),
],
)
parsed_data = next(corenlp_parser.parse(input_string))
corenlp_parser.api_call.assert_called_once_with(
""The quick brown fox jumps over the lazy dog"",
properties={""ssplit.eolonly"": ""true""},
)
self.assertEqual(expected_output, parsed_data.tree())
",[],0,[],/test/unit/test_corenlp.py_test_dependency_parser
5505,/home/amandapotts/git/nltk/nltk/test/unit/test_nombank.py_test_numbers,"def test_numbers(self):
self.assertEqual(len(nombank.instances()), 114574)
self.assertEqual(len(nombank.rolesets()), 5577)
self.assertEqual(len(nombank.nouns()), 4704)
",[],0,[],/test/unit/test_nombank.py_test_numbers
5506,/home/amandapotts/git/nltk/nltk/test/unit/test_nombank.py_test_instance,"def test_instance(self):
self.assertEqual(nombank.instances()[0].roleset, ""perc-sign.01"")
",[],0,[],/test/unit/test_nombank.py_test_instance
5507,/home/amandapotts/git/nltk/nltk/test/unit/test_nombank.py_test_framefiles_fileids,"def test_framefiles_fileids(self):
self.assertEqual(len(nombank.fileids()), 4705)
self.assertTrue(all(fileid.endswith("".xml"") for fileid in nombank.fileids()))
",[],0,[],/test/unit/test_nombank.py_test_framefiles_fileids
5508,/home/amandapotts/git/nltk/nltk/test/unit/test_bllip.py_parser,"def parser():
model_dir = find(""models/bllip_wsj_no_aux"").path
return BllipParser.from_unified_model_dir(model_dir)
",[],0,[],/test/unit/test_bllip.py_parser
5509,/home/amandapotts/git/nltk/nltk/test/unit/test_bllip.py_setup_module,"def setup_module():
pytest.importorskip(""bllipparser"")
",[],0,[],/test/unit/test_bllip.py_setup_module
5510,/home/amandapotts/git/nltk/nltk/test/unit/test_bllip.py_test_parser_loads_a_valid_tree,"def test_parser_loads_a_valid_tree(self, parser):
parsed = parser.parse(""I saw the man with the telescope"")
tree = next(parsed)
assert isinstance(tree, Tree)
assert (
tree.pformat()
== """"""
",[],0,[],/test/unit/test_bllip.py_test_parser_loads_a_valid_tree
5511,/home/amandapotts/git/nltk/nltk/test/unit/test_bllip.py_test_tagged_parse_finds_matching_element,"def test_tagged_parse_finds_matching_element(self, parser):
parsed = parser.parse(""I saw the man with the telescope"")
tagged_tree = next(parser.tagged_parse([(""telescope"", ""NN"")]))
assert isinstance(tagged_tree, Tree)
assert tagged_tree.pformat() == ""(S1 (NP (NN telescope)))""
",[],0,[],/test/unit/test_bllip.py_test_tagged_parse_finds_matching_element
5512,/home/amandapotts/git/nltk/nltk/test/unit/test_disagreement.py_test_easy,"def test_easy(self):
""""""
Simple test, based on
https://github.com/foolswood/krippendorffs_alpha/raw/master/krippendorff.pdf.
""""""
data = [
(""coder1"", ""dress1"", ""YES""),
(""coder2"", ""dress1"", ""NO""),
(""coder3"", ""dress1"", ""NO""),
(""coder1"", ""dress2"", ""YES""),
(""coder2"", ""dress2"", ""NO""),
(""coder3"", ""dress3"", ""NO""),
]
annotation_task = AnnotationTask(data)
self.assertAlmostEqual(annotation_task.alpha(), -0.3333333)
",[],0,[],/test/unit/test_disagreement.py_test_easy
5513,/home/amandapotts/git/nltk/nltk/test/unit/test_disagreement.py_test_easy2,"def test_easy2(self):
""""""
Same simple test with 1 rating removed.
Removal of that rating should not matter: K-Apha ignores items with
only 1 rating.
""""""
data = [
(""coder1"", ""dress1"", ""YES""),
(""coder2"", ""dress1"", ""NO""),
(""coder3"", ""dress1"", ""NO""),
(""coder1"", ""dress2"", ""YES""),
(""coder2"", ""dress2"", ""NO""),
]
annotation_task = AnnotationTask(data)
self.assertAlmostEqual(annotation_task.alpha(), -0.3333333)
",[],0,[],/test/unit/test_disagreement.py_test_easy2
5514,/home/amandapotts/git/nltk/nltk/test/unit/test_disagreement.py_test_advanced,"def test_advanced(self):
""""""
More advanced test, based on
http://www.agreestat.com/research_papers/onkrippendorffalpha.pdf
""""""
data = [
(""A"", ""1"", ""1""),
(""B"", ""1"", ""1""),
(""D"", ""1"", ""1""),
(""A"", ""2"", ""2""),
(""B"", ""2"", ""2""),
(""C"", ""2"", ""3""),
(""D"", ""2"", ""2""),
(""A"", ""3"", ""3""),
(""B"", ""3"", ""3""),
(""C"", ""3"", ""3""),
(""D"", ""3"", ""3""),
(""A"", ""4"", ""3""),
(""B"", ""4"", ""3""),
(""C"", ""4"", ""3""),
(""D"", ""4"", ""3""),
(""A"", ""5"", ""2""),
(""B"", ""5"", ""2""),
(""C"", ""5"", ""2""),
(""D"", ""5"", ""2""),
(""A"", ""6"", ""1""),
(""B"", ""6"", ""2""),
(""C"", ""6"", ""3""),
(""D"", ""6"", ""4""),
(""A"", ""7"", ""4""),
(""B"", ""7"", ""4""),
(""C"", ""7"", ""4""),
(""D"", ""7"", ""4""),
(""A"", ""8"", ""1""),
(""B"", ""8"", ""1""),
(""C"", ""8"", ""2""),
(""D"", ""8"", ""1""),
(""A"", ""9"", ""2""),
(""B"", ""9"", ""2""),
(""C"", ""9"", ""2""),
(""D"", ""9"", ""2""),
(""B"", ""10"", ""5""),
(""C"", ""10"", ""5""),
(""D"", ""10"", ""5""),
(""C"", ""11"", ""1""),
(""D"", ""11"", ""1""),
(""C"", ""12"", ""3""),
]
annotation_task = AnnotationTask(data)
self.assertAlmostEqual(annotation_task.alpha(), 0.743421052632)
",[],0,[],/test/unit/test_disagreement.py_test_advanced
5515,/home/amandapotts/git/nltk/nltk/test/unit/test_disagreement.py_test_advanced2,"def test_advanced2(self):
""""""
Same more advanced example, but with 1 rating removed.
Again, removal of that 1 rating should not matter.
""""""
data = [
(""A"", ""1"", ""1""),
(""B"", ""1"", ""1""),
(""D"", ""1"", ""1""),
(""A"", ""2"", ""2""),
(""B"", ""2"", ""2""),
(""C"", ""2"", ""3""),
(""D"", ""2"", ""2""),
(""A"", ""3"", ""3""),
(""B"", ""3"", ""3""),
(""C"", ""3"", ""3""),
(""D"", ""3"", ""3""),
(""A"", ""4"", ""3""),
(""B"", ""4"", ""3""),
(""C"", ""4"", ""3""),
(""D"", ""4"", ""3""),
(""A"", ""5"", ""2""),
(""B"", ""5"", ""2""),
(""C"", ""5"", ""2""),
(""D"", ""5"", ""2""),
(""A"", ""6"", ""1""),
(""B"", ""6"", ""2""),
(""C"", ""6"", ""3""),
(""D"", ""6"", ""4""),
(""A"", ""7"", ""4""),
(""B"", ""7"", ""4""),
(""C"", ""7"", ""4""),
(""D"", ""7"", ""4""),
(""A"", ""8"", ""1""),
(""B"", ""8"", ""1""),
(""C"", ""8"", ""2""),
(""D"", ""8"", ""1""),
(""A"", ""9"", ""2""),
(""B"", ""9"", ""2""),
(""C"", ""9"", ""2""),
(""D"", ""9"", ""2""),
(""B"", ""10"", ""5""),
(""C"", ""10"", ""5""),
(""D"", ""10"", ""5""),
(""C"", ""11"", ""1""),
(""D"", ""11"", ""1""),
(""C"", ""12"", ""3""),
]
annotation_task = AnnotationTask(data)
self.assertAlmostEqual(annotation_task.alpha(), 0.743421052632)
",[],0,[],/test/unit/test_disagreement.py_test_advanced2
5516,/home/amandapotts/git/nltk/nltk/test/unit/test_seekable_unicode_stream_reader.py_check_reader,"def check_reader(unicode_string, encoding):
bytestr = unicode_string.encode(encoding)
stream = BytesIO(bytestr)
reader = SeekableUnicodeStreamReader(stream, encoding)
assert reader.tell() == 0
assert unicode_string == """".join(reader.readlines())
stream.seek(0, os.SEEK_END)
assert reader.tell() == stream.tell()
reader.seek(0)  # go back to start
contents = """"
char = None
while char != """":
char = reader.read(1)
contents += char
assert unicode_string == contents
",[],0,[],/test/unit/test_seekable_unicode_stream_reader.py_check_reader
5517,/home/amandapotts/git/nltk/nltk/test/unit/test_seekable_unicode_stream_reader.py_test_reader,"def test_reader(string):
for encoding in ENCODINGS:
try:
string.encode(encoding)
except UnicodeEncodeError:
continue
check_reader(string, encoding)
",[],0,[],/test/unit/test_seekable_unicode_stream_reader.py_test_reader
5518,/home/amandapotts/git/nltk/nltk/test/unit/test_seekable_unicode_stream_reader.py_test_reader_stream_closes_when_deleted,"def test_reader_stream_closes_when_deleted():
reader = SeekableUnicodeStreamReader(BytesIO(b""""), ""ascii"")
assert not reader.stream.closed
reader.__del__()
assert reader.stream.closed
",[],0,[],/test/unit/test_seekable_unicode_stream_reader.py_test_reader_stream_closes_when_deleted
5519,/home/amandapotts/git/nltk/nltk/test/unit/test_seekable_unicode_stream_reader.py_teardown_module,"def teardown_module(module=None):
import gc
gc.collect()
",[],0,[],/test/unit/test_seekable_unicode_stream_reader.py_teardown_module
5520,/home/amandapotts/git/nltk/nltk/test/unit/test_tag.py_test_basic,"def test_basic():
from nltk.tag import pos_tag
from nltk.tokenize import word_tokenize
result = pos_tag(word_tokenize(""John's big idea isn't all that bad.""))
assert result == [
(""John"", ""NNP""),
(""'s"", ""POS""),
(""big"", ""JJ""),
(""idea"", ""NN""),
(""is"", ""VBZ""),
(""n't"", ""RB""),
(""all"", ""PDT""),
(""that"", ""DT""),
(""bad"", ""JJ""),
(""."", "".""),
]
",[],0,[],/test/unit/test_tag.py_test_basic
5521,/home/amandapotts/git/nltk/nltk/test/unit/test_tag.py_setup_module,"def setup_module(module):
import pytest
pytest.importorskip(""numpy"")
",[],0,[],/test/unit/test_tag.py_setup_module
5522,/home/amandapotts/git/nltk/nltk/test/unit/test_senna.py_test_senna_pipeline,"def test_senna_pipeline(self):
""""""Senna pipeline interface""""""
pipeline = Senna(SENNA_EXECUTABLE_PATH, [""pos"", ""chk"", ""ner""])
sent = ""Dusseldorf is an international business center"".split()
result = [
(token[""word""], token[""chk""], token[""ner""], token[""pos""])
for token in pipeline.tag(sent)
]
expected = [
(""Dusseldorf"", ""B-NP"", ""B-LOC"", ""NNP""),
(""is"", ""B-VP"", ""O"", ""VBZ""),
(""an"", ""B-NP"", ""O"", ""DT""),
(""international"", ""I-NP"", ""O"", ""JJ""),
(""business"", ""I-NP"", ""O"", ""NN""),
(""center"", ""I-NP"", ""O"", ""NN""),
]
self.assertEqual(result, expected)
",[],0,[],/test/unit/test_senna.py_test_senna_pipeline
5523,/home/amandapotts/git/nltk/nltk/test/unit/test_senna.py_test_senna_tagger,"def test_senna_tagger(self):
tagger = SennaTagger(SENNA_EXECUTABLE_PATH)
result = tagger.tag(""What is the airspeed of an unladen swallow ?"".split())
expected = [
(""What"", ""WP""),
(""is"", ""VBZ""),
(""the"", ""DT""),
(""airspeed"", ""NN""),
(""of"", ""IN""),
(""an"", ""DT""),
(""unladen"", ""NN""),
(""swallow"", ""NN""),
(""?"", "".""),
]
self.assertEqual(result, expected)
",[],0,[],/test/unit/test_senna.py_test_senna_tagger
5524,/home/amandapotts/git/nltk/nltk/test/unit/test_senna.py_test_senna_chunk_tagger,"def test_senna_chunk_tagger(self):
chktagger = SennaChunkTagger(SENNA_EXECUTABLE_PATH)
result_1 = chktagger.tag(""What is the airspeed of an unladen swallow ?"".split())
expected_1 = [
(""What"", ""B-NP""),
(""is"", ""B-VP""),
(""the"", ""B-NP""),
(""airspeed"", ""I-NP""),
(""of"", ""B-PP""),
(""an"", ""B-NP""),
(""unladen"", ""I-NP""),
(""swallow"", ""I-NP""),
(""?"", ""O""),
]
result_2 = list(chktagger.bio_to_chunks(result_1, chunk_type=""NP""))
expected_2 = [
(""What"", ""0""),
(""the airspeed"", ""2-3""),
(""an unladen swallow"", ""5-6-7""),
]
self.assertEqual(result_1, expected_1)
self.assertEqual(result_2, expected_2)
",[],0,[],/test/unit/test_senna.py_test_senna_chunk_tagger
5525,/home/amandapotts/git/nltk/nltk/test/unit/test_senna.py_test_senna_ner_tagger,"def test_senna_ner_tagger(self):
nertagger = SennaNERTagger(SENNA_EXECUTABLE_PATH)
result_1 = nertagger.tag(""Shakespeare theatre was in London ."".split())
expected_1 = [
(""Shakespeare"", ""B-PER""),
(""theatre"", ""O""),
(""was"", ""O""),
(""in"", ""O""),
(""London"", ""B-LOC""),
(""."", ""O""),
]
result_2 = nertagger.tag(""UN headquarters are in NY , USA ."".split())
expected_2 = [
(""UN"", ""B-ORG""),
(""headquarters"", ""O""),
(""are"", ""O""),
(""in"", ""O""),
(""NY"", ""B-LOC""),
("","", ""O""),
(""USA"", ""B-LOC""),
(""."", ""O""),
]
self.assertEqual(result_1, expected_1)
self.assertEqual(result_2, expected_2)
",[],0,[],/test/unit/test_senna.py_test_senna_ner_tagger
5526,/home/amandapotts/git/nltk/nltk/test/unit/test_hmm.py__wikipedia_example_hmm,"def _wikipedia_example_hmm():
states = [""rain"", ""no rain""]
symbols = [""umbrella"", ""no umbrella""]
A = [[0.7, 0.3], [0.3, 0.7]]  # transition probabilities
B = [[0.9, 0.1], [0.2, 0.8]]  # emission probabilities
pi = [0.5, 0.5]  # initial probabilities
seq = [""umbrella"", ""umbrella"", ""no umbrella"", ""umbrella"", ""umbrella""]
seq = list(zip(seq, [None] * len(seq)))
model = hmm._create_hmm_tagger(states, symbols, A, B, pi)
return model, states, symbols, seq
",[],0,[],/test/unit/test_hmm.py__wikipedia_example_hmm
5527,/home/amandapotts/git/nltk/nltk/test/unit/test_hmm.py_test_forward_probability,"def test_forward_probability():
from numpy.testing import assert_array_almost_equal
model, states, symbols = hmm._market_hmm_example()
seq = [(""up"", None), (""up"", None)]
expected = [[0.35, 0.02, 0.09], [0.1792, 0.0085, 0.0357]]
fp = 2 ** model._forward_probability(seq)
assert_array_almost_equal(fp, expected)
",[],0,[],/test/unit/test_hmm.py_test_forward_probability
5528,/home/amandapotts/git/nltk/nltk/test/unit/test_hmm.py_test_forward_probability2,"def test_forward_probability2():
from numpy.testing import assert_array_almost_equal
model, states, symbols, seq = _wikipedia_example_hmm()
fp = 2 ** model._forward_probability(seq)
fp = (fp.T / fp.sum(axis=1)).T
wikipedia_results = [
[0.8182, 0.1818],
[0.8834, 0.1166],
[0.1907, 0.8093],
[0.7308, 0.2692],
[0.8673, 0.1327],
]
assert_array_almost_equal(wikipedia_results, fp, 4)
",[],0,[],/test/unit/test_hmm.py_test_forward_probability2
5529,/home/amandapotts/git/nltk/nltk/test/unit/test_hmm.py_test_backward_probability,"def test_backward_probability():
from numpy.testing import assert_array_almost_equal
model, states, symbols, seq = _wikipedia_example_hmm()
bp = 2 ** model._backward_probability(seq)
bp = (bp.T / bp.sum(axis=1)).T
wikipedia_results = [
[0.5923, 0.4077],
[0.3763, 0.6237],
[0.6533, 0.3467],
[0.6273, 0.3727],
[0.5, 0.5],
]
assert_array_almost_equal(wikipedia_results, bp, 4)
",[],0,[],/test/unit/test_hmm.py_test_backward_probability
5530,/home/amandapotts/git/nltk/nltk/test/unit/test_hmm.py_setup_module,"def setup_module(module):
pytest.importorskip(""numpy"")
",[],0,[],/test/unit/test_hmm.py_setup_module
5531,/home/amandapotts/git/nltk/nltk/test/unit/test_corpora.py_test_words,"def test_words(self):
for name in udhr.fileids():
words = list(udhr.words(name))
self.assertTrue(words)
",[],0,[],/test/unit/test_corpora.py_test_words
5532,/home/amandapotts/git/nltk/nltk/test/unit/test_corpora.py_test_raw_unicode,"def test_raw_unicode(self):
for name in udhr.fileids():
txt = udhr.raw(name)
assert not isinstance(txt, bytes), name
",[],0,[],/test/unit/test_corpora.py_test_raw_unicode
5533,/home/amandapotts/git/nltk/nltk/test/unit/test_corpora.py_test_polish_encoding,"def test_polish_encoding(self):
text_pl = udhr.raw(""Polish-Latin2"")[:164]
text_ppl = udhr.raw(""Polish_Polski-Latin2"")[:164]
expected = """"""POWSZECHNA DEKLARACJA PRAW CZŁOWIEKA
",[],0,[],/test/unit/test_corpora.py_test_polish_encoding
5534,/home/amandapotts/git/nltk/nltk/test/unit/test_corpora.py_test_words,"def test_words(self):
words = indian.words()[:3]
self.assertEqual(words, [""মহিষের"", ""সন্তান"", "":""])
",[],0,[],/test/unit/test_corpora.py_test_words
5535,/home/amandapotts/git/nltk/nltk/test/unit/test_corpora.py_test_tagged_words,"def test_tagged_words(self):
tagged_words = indian.tagged_words()[:3]
self.assertEqual(
tagged_words, [(""মহিষের"", ""NN""), (""সন্তান"", ""NN""), ("":"", ""SYM"")]
)
",[],0,[],/test/unit/test_corpora.py_test_tagged_words
5536,/home/amandapotts/git/nltk/nltk/test/unit/test_corpora.py_test_catalan,"def test_catalan(self):
words = cess_cat.words()[:15]
txt = ""El Tribunal_Suprem -Fpa- TS -Fpt- ha confirmat la condemna a quatre anys d' inhabilitació especial""
self.assertEqual(words, txt.split())
self.assertEqual(cess_cat.tagged_sents()[0][34][0], ""càrrecs"")
",[],0,[],/test/unit/test_corpora.py_test_catalan
5537,/home/amandapotts/git/nltk/nltk/test/unit/test_corpora.py_test_esp,"def test_esp(self):
words = cess_esp.words()[:15]
txt = ""El grupo estatal Electricité_de_France -Fpa- EDF -Fpt- anunció hoy , jueves , la compra del""
self.assertEqual(words, txt.split())
self.assertEqual(cess_esp.words()[115], ""años"")
",[],0,[],/test/unit/test_corpora.py_test_esp
5538,/home/amandapotts/git/nltk/nltk/test/unit/test_corpora.py_test_words,"def test_words(self):
words = floresta.words()[:10]
txt = ""Um revivalismo refrescante O 7_e_Meio é um ex-libris de a""
self.assertEqual(words, txt.split())
",[],0,[],/test/unit/test_corpora.py_test_words
5539,/home/amandapotts/git/nltk/nltk/test/unit/test_corpora.py_test_sents,"def test_sents(self):
first_3_sents = sinica_treebank.sents()[:3]
self.assertEqual(
first_3_sents, [[""一""], [""友情""], [""嘉珍"", ""和"", ""我"", ""住在"", ""同一條"", ""巷子""]]
)
",[],0,[],/test/unit/test_corpora.py_test_sents
5540,/home/amandapotts/git/nltk/nltk/test/unit/test_corpora.py_test_parsed_sents,"def test_parsed_sents(self):
parsed_sents = sinica_treebank.parsed_sents()[25]
self.assertEqual(
parsed_sents,
Tree(
""S"",
[
Tree(""NP"", [Tree(""Nba"", [""嘉珍""])]),
Tree(""V‧地"", [Tree(""VA11"", [""不停""]), Tree(""DE"", [""的""])]),
Tree(""VA4"", [""哭泣""]),
],
),
)
",[],0,[],/test/unit/test_corpora.py_test_parsed_sents
5541,/home/amandapotts/git/nltk/nltk/test/unit/test_corpora.py_test_sents,"def test_sents(self):
sents = conll2007.sents(""esp.train"")[0]
self.assertEqual(
sents[:6], [""El"", ""aumento"", ""del"", ""índice"", ""de"", ""desempleo""]
)
",[],0,[],/test/unit/test_corpora.py_test_sents
5542,/home/amandapotts/git/nltk/nltk/test/unit/test_corpora.py_test_parsed_sents,"def test_parsed_sents(self):
parsed_sents = conll2007.parsed_sents(""esp.train"")[0]
self.assertEqual(
parsed_sents.tree(),
Tree(
""fortaleció"",
[
Tree(
""aumento"",
[
""El"",
Tree(
""del"",
[
Tree(
""índice"",
[
Tree(
""de"",
[Tree(""desempleo"", [""estadounidense""])],
)
],
)
],
),
],
),
""hoy"",
""considerablemente"",
Tree(
""al"",
[
Tree(
""euro"",
[
Tree(
""cotizaba"",
[
"","",
""que"",
Tree(""a"", [Tree(""15.35"", [""las"", ""GMT""])]),
""se"",
Tree(
""en"",
[
Tree(
""mercado"",
[
""el"",
Tree(""de"", [""divisas""]),
Tree(""de"", [""Fráncfort""]),
],
)
],
),
Tree(""a"", [""0,9452_dólares""]),
Tree(
""frente_a"",
[
"","",
Tree(
""0,9349_dólares"",
[
""los"",
Tree(
""de"",
[
Tree(
""mañana"",
[""esta""],
)
],
),
],
),
],
),
],
)
],
)
],
),
""."",
],
),
)
",[],0,[],/test/unit/test_corpora.py_test_parsed_sents
5543,/home/amandapotts/git/nltk/nltk/test/unit/test_corpora.py_test_fileids,"def test_fileids(self):
self.assertEqual(
ptb.fileids()[:4],
[
""BROWN/CF/CF01.MRG"",
""BROWN/CF/CF02.MRG"",
""BROWN/CF/CF03.MRG"",
""BROWN/CF/CF04.MRG"",
],
)
",[],0,[],/test/unit/test_corpora.py_test_fileids
5544,/home/amandapotts/git/nltk/nltk/test/unit/test_corpora.py_test_words,"def test_words(self):
self.assertEqual(
ptb.words(""WSJ/00/WSJ_0003.MRG"")[:7],
[""A"", ""form"", ""of"", ""asbestos"", ""once"", ""used"", ""*""],
)
",[],0,[],/test/unit/test_corpora.py_test_words
5545,/home/amandapotts/git/nltk/nltk/test/unit/test_corpora.py_test_tagged_words,"def test_tagged_words(self):
self.assertEqual(
ptb.tagged_words(""WSJ/00/WSJ_0003.MRG"")[:3],
[(""A"", ""DT""), (""form"", ""NN""), (""of"", ""IN"")],
)
",[],0,[],/test/unit/test_corpora.py_test_tagged_words
5546,/home/amandapotts/git/nltk/nltk/test/unit/test_corpora.py_test_categories,"def test_categories(self):
self.assertEqual(
ptb.categories(),
[
""adventure"",
""belles_lettres"",
""fiction"",
""humor"",
""lore"",
""mystery"",
""news"",
""romance"",
""science_fiction"",
],
)
",[],0,[],/test/unit/test_corpora.py_test_categories
5547,/home/amandapotts/git/nltk/nltk/test/unit/test_corpora.py_test_news_fileids,"def test_news_fileids(self):
self.assertEqual(
ptb.fileids(""news"")[:3],
[""WSJ/00/WSJ_0001.MRG"", ""WSJ/00/WSJ_0002.MRG"", ""WSJ/00/WSJ_0003.MRG""],
)
",[],0,[],/test/unit/test_corpora.py_test_news_fileids
5548,/home/amandapotts/git/nltk/nltk/test/unit/test_corpora.py_test_category_words,"def test_category_words(self):
self.assertEqual(
ptb.words(categories=[""humor"", ""fiction""])[:6],
[""Thirty-three"", ""Scotty"", ""did"", ""not"", ""go"", ""back""],
)
",[],0,[],/test/unit/test_corpora.py_test_category_words
5549,/home/amandapotts/git/nltk/nltk/test/unit/test_corpora.py_test_fileids,"def test_fileids(self):
self.assertEqual(
mwa_ppdb.fileids(), [""ppdb-1.0-xxxl-lexical.extended.synonyms.uniquepairs""]
)
",[],0,[],/test/unit/test_corpora.py_test_fileids
5550,/home/amandapotts/git/nltk/nltk/test/unit/test_corpora.py_test_entries,"def test_entries(self):
self.assertEqual(
mwa_ppdb.entries()[:10],
[
(""10/17/01"", ""17/10/2001""),
(""102,70"", ""102.70""),
(""13,53"", ""13.53""),
(""3.2.5.3.2.1"", ""3.2.5.3.2.1.""),
(""53,76"", ""53.76""),
(""6.9.5"", ""6.9.5.""),
(""7.7.6.3"", ""7.7.6.3.""),
(""76,20"", ""76.20""),
(""79,85"", ""79.85""),
(""93,65"", ""93.65""),
],
)
",[],0,[],/test/unit/test_corpora.py_test_entries
5551,/home/amandapotts/git/nltk/nltk/test/unit/test_tgrep.py_test_tokenize_simple,"def test_tokenize_simple(self):
""""""
Simple test of tokenization.
""""""
tokens = tgrep.tgrep_tokenize(""A .. (B !< C . D) | ![<< (E , F) $ G]"")
self.assertEqual(
tokens,
[
""A"",
"".."",
""("",
""B"",
""!"",
""<"",
""C"",
""."",
""D"",
"")"",
""|"",
""!"",
""["",
""<<"",
""("",
""E"",
"","",
""F"",
"")"",
""$"",
""G"",
""]"",
],
)
",[],0,[],/test/unit/test_tgrep.py_test_tokenize_simple
5552,/home/amandapotts/git/nltk/nltk/test/unit/test_tgrep.py_test_tokenize_encoding,"def test_tokenize_encoding(self):
""""""
Test that tokenization handles bytes and strs the same way.
""""""
self.assertEqual(
tgrep.tgrep_tokenize(b""A .. (B !< C . D) | ![<< (E , F) $ G]""),
tgrep.tgrep_tokenize(""A .. (B !< C . D) | ![<< (E , F) $ G]""),
)
",[],0,[],/test/unit/test_tgrep.py_test_tokenize_encoding
5553,/home/amandapotts/git/nltk/nltk/test/unit/test_tgrep.py_test_tokenize_link_types,"def test_tokenize_link_types(self):
""""""
Test tokenization of basic link types.
""""""
self.assertEqual(tgrep.tgrep_tokenize(""A<B""), [""A"", ""<"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A>B""), [""A"", "">"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A<3B""), [""A"", ""<3"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A>3B""), [""A"", "">3"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A<,B""), [""A"", ""<,"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A>,B""), [""A"", "">,"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A<-3B""), [""A"", ""<-3"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A>-3B""), [""A"", "">-3"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A<-B""), [""A"", ""<-"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A>-B""), [""A"", "">-"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A<'B""), [""A"", ""<'"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A>'B""), [""A"", "">'"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A<:B""), [""A"", ""<:"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A>:B""), [""A"", "">:"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A<<B""), [""A"", ""<<"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A>>B""), [""A"", "">>"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A<<,B""), [""A"", ""<<,"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A>>,B""), [""A"", "">>,"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A<<'B""), [""A"", ""<<'"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A>>'B""), [""A"", "">>'"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A<<:B""), [""A"", ""<<:"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A>>:B""), [""A"", "">>:"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A.B""), [""A"", ""."", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A,B""), [""A"", "","", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A..B""), [""A"", "".."", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A,,B""), [""A"", "",,"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A$B""), [""A"", ""$"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A$.B""), [""A"", ""$."", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A$,B""), [""A"", ""$,"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A$..B""), [""A"", ""$.."", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A$,,B""), [""A"", ""$,,"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A!<B""), [""A"", ""!"", ""<"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A!>B""), [""A"", ""!"", "">"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A!<3B""), [""A"", ""!"", ""<3"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A!>3B""), [""A"", ""!"", "">3"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A!<,B""), [""A"", ""!"", ""<,"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A!>,B""), [""A"", ""!"", "">,"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A!<-3B""), [""A"", ""!"", ""<-3"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A!>-3B""), [""A"", ""!"", "">-3"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A!<-B""), [""A"", ""!"", ""<-"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A!>-B""), [""A"", ""!"", "">-"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A!<'B""), [""A"", ""!"", ""<'"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A!>'B""), [""A"", ""!"", "">'"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A!<:B""), [""A"", ""!"", ""<:"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A!>:B""), [""A"", ""!"", "">:"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A!<<B""), [""A"", ""!"", ""<<"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A!>>B""), [""A"", ""!"", "">>"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A!<<,B""), [""A"", ""!"", ""<<,"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A!>>,B""), [""A"", ""!"", "">>,"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A!<<'B""), [""A"", ""!"", ""<<'"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A!>>'B""), [""A"", ""!"", "">>'"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A!<<:B""), [""A"", ""!"", ""<<:"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A!>>:B""), [""A"", ""!"", "">>:"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A!.B""), [""A"", ""!"", ""."", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A!,B""), [""A"", ""!"", "","", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A!..B""), [""A"", ""!"", "".."", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A!,,B""), [""A"", ""!"", "",,"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A!$B""), [""A"", ""!"", ""$"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A!$.B""), [""A"", ""!"", ""$."", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A!$,B""), [""A"", ""!"", ""$,"", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A!$..B""), [""A"", ""!"", ""$.."", ""B""])
self.assertEqual(tgrep.tgrep_tokenize(""A!$,,B""), [""A"", ""!"", ""$,,"", ""B""])
",[],0,[],/test/unit/test_tgrep.py_test_tokenize_link_types
5554,/home/amandapotts/git/nltk/nltk/test/unit/test_tgrep.py_test_tokenize_examples,"def test_tokenize_examples(self):
""""""
Test tokenization of the TGrep2 manual example patterns.
""""""
self.assertEqual(tgrep.tgrep_tokenize(""NP < PP""), [""NP"", ""<"", ""PP""])
self.assertEqual(tgrep.tgrep_tokenize(""/^NP/""), [""/^NP/""])
self.assertEqual(
tgrep.tgrep_tokenize(""NP << PP . VP""), [""NP"", ""<<"", ""PP"", ""."", ""VP""]
)
self.assertEqual(
tgrep.tgrep_tokenize(""NP << PP | . VP""), [""NP"", ""<<"", ""PP"", ""|"", ""."", ""VP""]
)
self.assertEqual(
tgrep.tgrep_tokenize(""NP !<< PP [> NP | >> VP]""),
[""NP"", ""!"", ""<<"", ""PP"", ""["", "">"", ""NP"", ""|"", "">>"", ""VP"", ""]""],
)
self.assertEqual(
tgrep.tgrep_tokenize(""NP << (PP . VP)""),
[""NP"", ""<<"", ""("", ""PP"", ""."", ""VP"", "")""],
)
self.assertEqual(
tgrep.tgrep_tokenize(""NP <' (PP <, (IN < on))""),
[""NP"", ""<'"", ""("", ""PP"", ""<,"", ""("", ""IN"", ""<"", ""on"", "")"", "")""],
)
self.assertEqual(
tgrep.tgrep_tokenize(""S < (A < B) < C""),
[""S"", ""<"", ""("", ""A"", ""<"", ""B"", "")"", ""<"", ""C""],
)
self.assertEqual(
tgrep.tgrep_tokenize(""S < ((A < B) < C)""),
[""S"", ""<"", ""("", ""("", ""A"", ""<"", ""B"", "")"", ""<"", ""C"", "")""],
)
self.assertEqual(
tgrep.tgrep_tokenize(""S < (A < B < C)""),
[""S"", ""<"", ""("", ""A"", ""<"", ""B"", ""<"", ""C"", "")""],
)
self.assertEqual(tgrep.tgrep_tokenize(""A<B&.C""), [""A"", ""<"", ""B"", ""&"", ""."", ""C""])
",[],0,[],/test/unit/test_tgrep.py_test_tokenize_examples
5555,/home/amandapotts/git/nltk/nltk/test/unit/test_tgrep.py_test_tokenize_quoting,"def test_tokenize_quoting(self):
""""""
Test tokenization of quoting.
""""""
self.assertEqual(
tgrep.tgrep_tokenize('""A<<:B""<<:""A $.. B""<""A>3B""<C'),
['""A<<:B""', ""<<:"", '""A $.. B""', ""<"", '""A>3B""', ""<"", ""C""],
)
",[],0,[],/test/unit/test_tgrep.py_test_tokenize_quoting
5556,/home/amandapotts/git/nltk/nltk/test/unit/test_tgrep.py_test_tokenize_nodenames,"def test_tokenize_nodenames(self):
""""""
Test tokenization of node names.
""""""
self.assertEqual(tgrep.tgrep_tokenize(""Robert""), [""Robert""])
self.assertEqual(tgrep.tgrep_tokenize(""/^[Bb]ob/""), [""/^[Bb]ob/""])
self.assertEqual(tgrep.tgrep_tokenize(""*""), [""*""])
self.assertEqual(tgrep.tgrep_tokenize(""__""), [""__""])
self.assertEqual(tgrep.tgrep_tokenize(""N()""), [""N("", "")""])
self.assertEqual(tgrep.tgrep_tokenize(""N(0,)""), [""N("", ""0"", "","", "")""])
self.assertEqual(tgrep.tgrep_tokenize(""N(0,0)""), [""N("", ""0"", "","", ""0"", "")""])
self.assertEqual(
tgrep.tgrep_tokenize(""N(0,0,)""), [""N("", ""0"", "","", ""0"", "","", "")""]
)
",[],0,[],/test/unit/test_tgrep.py_test_tokenize_nodenames
5557,/home/amandapotts/git/nltk/nltk/test/unit/test_tgrep.py_test_tokenize_macros,"def test_tokenize_macros(self):
""""""
Test tokenization of macro definitions.
""""""
self.assertEqual(
tgrep.tgrep_tokenize(
""@ NP /^NP/
),
[
""@"",
""NP"",
""/^NP/"",
""
""@"",
""NN"",
""/^NN/"",
""
""@NP"",
""["",
""!"",
""<"",
""NP"",
""|"",
""<"",
""@NN"",
""]"",
""!"",
""$.."",
""@NN"",
],
)
",[],0,[],/test/unit/test_tgrep.py_test_tokenize_macros
5558,/home/amandapotts/git/nltk/nltk/test/unit/test_tgrep.py_test_node_simple,"def test_node_simple(self):
""""""
Test a simple use of tgrep for finding nodes matching a given
pattern.
""""""
tree = ParentedTree.fromstring(
""(S (NP (DT the) (JJ big) (NN dog)) "" ""(VP bit) (NP (DT a) (NN cat)))""
)
self.assertEqual(list(tgrep.tgrep_positions(""NN"", [tree])), [[(0, 2), (2, 1)]])
self.assertEqual(
list(tgrep.tgrep_nodes(""NN"", [tree])), [[tree[0, 2], tree[2, 1]]]
)
self.assertEqual(
list(tgrep.tgrep_positions(""NN|JJ"", [tree])), [[(0, 1), (0, 2), (2, 1)]]
)
",[],0,[],/test/unit/test_tgrep.py_test_node_simple
5559,/home/amandapotts/git/nltk/nltk/test/unit/test_tgrep.py_test_node_printing,"def test_node_printing(self):
""""""Test that the tgrep print operator ' is properly ignored.""""""
tree = ParentedTree.fromstring(""(S (n x) (N x))"")
self.assertEqual(
list(tgrep.tgrep_positions(""N"", [tree])),
list(tgrep.tgrep_positions(""'N"", [tree])),
)
self.assertEqual(
list(tgrep.tgrep_positions(""/[Nn]/"", [tree])),
list(tgrep.tgrep_positions(""'/[Nn]/"", [tree])),
)
",[],0,[],/test/unit/test_tgrep.py_test_node_printing
5560,/home/amandapotts/git/nltk/nltk/test/unit/test_tgrep.py_test_node_encoding,"def test_node_encoding(self):
""""""
Test that tgrep search strings handles bytes and strs the same
way.
""""""
tree = ParentedTree.fromstring(
""(S (NP (DT the) (JJ big) (NN dog)) "" ""(VP bit) (NP (DT a) (NN cat)))""
)
self.assertEqual(
list(tgrep.tgrep_positions(b""NN"", [tree])),
list(tgrep.tgrep_positions(b""NN"", [tree])),
)
self.assertEqual(
list(tgrep.tgrep_nodes(b""NN"", [tree])),
list(tgrep.tgrep_nodes(""NN"", [tree])),
)
self.assertEqual(
list(tgrep.tgrep_positions(b""NN|JJ"", [tree])),
list(tgrep.tgrep_positions(""NN|JJ"", [tree])),
)
",[],0,[],/test/unit/test_tgrep.py_test_node_encoding
5561,/home/amandapotts/git/nltk/nltk/test/unit/test_tgrep.py_test_node_nocase,"def test_node_nocase(self):
""""""
Test selecting nodes using case insensitive node names.
""""""
tree = ParentedTree.fromstring(""(S (n x) (N x))"")
self.assertEqual(list(tgrep.tgrep_positions('""N""', [tree])), [[(1,)]])
self.assertEqual(list(tgrep.tgrep_positions('i@""N""', [tree])), [[(0,), (1,)]])
",[],0,[],/test/unit/test_tgrep.py_test_node_nocase
5562,/home/amandapotts/git/nltk/nltk/test/unit/test_tgrep.py_test_node_quoted,"def test_node_quoted(self):
""""""
Test selecting nodes using quoted node names.
""""""
tree = ParentedTree.fromstring('(N (""N"" x) (N"" x) (""\\"" x))')
self.assertEqual(list(tgrep.tgrep_positions('""N""', [tree])), [[()]])
self.assertEqual(list(tgrep.tgrep_positions('""\\""N\\""""', [tree])), [[(0,)]])
self.assertEqual(list(tgrep.tgrep_positions('""N\\""""', [tree])), [[(1,)]])
self.assertEqual(list(tgrep.tgrep_positions('""\\""\\\\\\""""', [tree])), [[(2,)]])
",[],0,[],/test/unit/test_tgrep.py_test_node_quoted
5563,/home/amandapotts/git/nltk/nltk/test/unit/test_tgrep.py_test_node_regex,"def test_node_regex(self):
""""""
Test regex matching on nodes.
""""""
tree = ParentedTree.fromstring(""(S (NP-SBJ x) (NP x) (NNP x) (VP x))"")
self.assertEqual(list(tgrep.tgrep_positions(""/^NP/"", [tree])), [[(0,), (1,)]])
",[],0,[],/test/unit/test_tgrep.py_test_node_regex
5564,/home/amandapotts/git/nltk/nltk/test/unit/test_tgrep.py_test_node_regex_2,"def test_node_regex_2(self):
""""""
Test regex matching on nodes.
""""""
tree = ParentedTree.fromstring(""(S (SBJ x) (SBJ1 x) (NP-SBJ x))"")
self.assertEqual(list(tgrep.tgrep_positions(""/^SBJ/"", [tree])), [[(0,), (1,)]])
self.assertEqual(
list(tgrep.tgrep_positions(""/SBJ/"", [tree])), [[(0,), (1,), (2,)]]
)
",[],0,[],/test/unit/test_tgrep.py_test_node_regex_2
5565,/home/amandapotts/git/nltk/nltk/test/unit/test_tgrep.py_test_node_tree_position,"def test_node_tree_position(self):
""""""
Test matching on nodes based on NLTK tree position.
""""""
tree = ParentedTree.fromstring(""(S (NP-SBJ x) (NP x) (NNP x) (VP x))"")
leaf_positions = {tree.leaf_treeposition(x) for x in range(len(tree.leaves()))}
tree_positions = [x for x in tree.treepositions() if x not in leaf_positions]
for position in tree_positions:
node_id = f""N{position}""
tgrep_positions = list(tgrep.tgrep_positions(node_id, [tree]))
self.assertEqual(len(tgrep_positions[0]), 1)
self.assertEqual(tgrep_positions[0][0], position)
",[],0,[],/test/unit/test_tgrep.py_test_node_tree_position
5566,/home/amandapotts/git/nltk/nltk/test/unit/test_tgrep.py_test_node_noleaves,"def test_node_noleaves(self):
""""""
Test node name matching with the search_leaves flag set to False.
""""""
tree = ParentedTree.fromstring(""(S (A (T x)) (B (N x)))"")
self.assertEqual(
list(tgrep.tgrep_positions(""x"", [tree])), [[(0, 0, 0), (1, 0, 0)]]
)
self.assertEqual(list(tgrep.tgrep_positions(""x"", [tree], False)), [[]])
",[],0,[],/test/unit/test_tgrep.py_test_node_noleaves
5567,/home/amandapotts/git/nltk/nltk/test/unit/test_tgrep.py_tests_rel_dominance,"def tests_rel_dominance(self):
""""""
Test matching nodes based on dominance relations.
""""""
tree = ParentedTree.fromstring(""(S (A (T x)) (B (N x)))"")
self.assertEqual(list(tgrep.tgrep_positions(""* < T"", [tree])), [[(0,)]])
self.assertEqual(list(tgrep.tgrep_positions(""* < T > S"", [tree])), [[(0,)]])
self.assertEqual(
list(tgrep.tgrep_positions(""* !< T"", [tree])),
[[(), (0, 0), (0, 0, 0), (1,), (1, 0), (1, 0, 0)]],
)
self.assertEqual(list(tgrep.tgrep_positions(""* !< T > S"", [tree])), [[(1,)]])
self.assertEqual(list(tgrep.tgrep_positions(""* > A"", [tree])), [[(0, 0)]])
self.assertEqual(list(tgrep.tgrep_positions(""* > B"", [tree])), [[(1, 0)]])
self.assertEqual(
list(tgrep.tgrep_positions(""* !> B"", [tree])),
[[(), (0,), (0, 0), (0, 0, 0), (1,), (1, 0, 0)]],
)
self.assertEqual(
list(tgrep.tgrep_positions(""* !> B >> S"", [tree])), [[(0,), (0, 0), (1,)]]
)
self.assertEqual(
list(tgrep.tgrep_positions(""* >> S"", [tree])),
[[(0,), (0, 0), (1,), (1, 0)]],
)
self.assertEqual(
list(tgrep.tgrep_positions(""* >>, S"", [tree])), [[(0,), (0, 0)]]
)
self.assertEqual(
list(tgrep.tgrep_positions(""* >>' S"", [tree])), [[(1,), (1, 0)]]
)
self.assertEqual(list(tgrep.tgrep_positions(""* << T"", [tree])), [[(), (0,)]])
self.assertEqual(list(tgrep.tgrep_positions(""* <<' T"", [tree])), [[(0,)]])
self.assertEqual(list(tgrep.tgrep_positions(""* <<1 N"", [tree])), [[(1,)]])
self.assertEqual(
list(tgrep.tgrep_positions(""* !<< T"", [tree])),
[[(0, 0), (0, 0, 0), (1,), (1, 0), (1, 0, 0)]],
)
tree = ParentedTree.fromstring(""(S (A (T x)) (B (T x) (N x )))"")
self.assertEqual(list(tgrep.tgrep_positions(""* <: T"", [tree])), [[(0,)]])
self.assertEqual(list(tgrep.tgrep_positions(""* < T"", [tree])), [[(0,), (1,)]])
self.assertEqual(
list(tgrep.tgrep_positions(""* !<: T"", [tree])),
[[(), (0, 0), (0, 0, 0), (1,), (1, 0), (1, 0, 0), (1, 1), (1, 1, 0)]],
)
self.assertEqual(list(tgrep.tgrep_positions(""* !<: T > S"", [tree])), [[(1,)]])
tree = ParentedTree.fromstring(""(S (T (A x) (B x)) (T (C x)))"")
self.assertEqual(list(tgrep.tgrep_positions(""* >: T"", [tree])), [[(1, 0)]])
self.assertEqual(
list(tgrep.tgrep_positions(""* !>: T"", [tree])),
[[(), (0,), (0, 0), (0, 0, 0), (0, 1), (0, 1, 0), (1,), (1, 0, 0)]],
)
tree = ParentedTree.fromstring(
""(S (A (B (C (D (E (T x))))))"" "" (A (B (C (D (E (T x))) (N x)))))""
)
self.assertEqual(
list(tgrep.tgrep_positions(""* <<: T"", [tree])),
[
[
(0,),
(0, 0),
(0, 0, 0),
(0, 0, 0, 0),
(0, 0, 0, 0, 0),
(1, 0, 0, 0),
(1, 0, 0, 0, 0),
]
],
)
self.assertEqual(
list(tgrep.tgrep_positions(""* >>: A"", [tree])),
[
[
(0, 0),
(0, 0, 0),
(0, 0, 0, 0),
(0, 0, 0, 0, 0),
(0, 0, 0, 0, 0, 0),
(1, 0),
(1, 0, 0),
]
],
)
",[],0,[],/test/unit/test_tgrep.py_tests_rel_dominance
5568,/home/amandapotts/git/nltk/nltk/test/unit/test_tgrep.py_test_bad_operator,"def test_bad_operator(self):
""""""
Test error handling of undefined tgrep operators.
""""""
tree = ParentedTree.fromstring(""(S (A (T x)) (B (N x)))"")
self.assertRaises(
tgrep.TgrepException, list, tgrep.tgrep_positions(""* >>> S"", [tree])
)
",[],0,[],/test/unit/test_tgrep.py_test_bad_operator
5569,/home/amandapotts/git/nltk/nltk/test/unit/test_tgrep.py_test_comments,"def test_comments(self):
""""""
Test that comments are correctly filtered out of tgrep search
strings.
""""""
tree = ParentedTree.fromstring(""(S (NN x) (NP x) (NN x))"")
search1 = """"""
@ NP /^NP/
@ NN /^NN/
@NN
""""""
self.assertEqual(list(tgrep.tgrep_positions(search1, [tree])), [[(0,), (2,)]])
search2 = """"""
@ NP /^NP/
@ NN /^NN/
@NN
""""""
self.assertEqual(list(tgrep.tgrep_positions(search2, [tree])), [[(0,), (2,)]])
",[],0,[],/test/unit/test_tgrep.py_test_comments
5570,/home/amandapotts/git/nltk/nltk/test/unit/test_tgrep.py_test_rel_sister_nodes,"def test_rel_sister_nodes(self):
""""""
Test matching sister nodes in a tree.
""""""
tree = ParentedTree.fromstring(""(S (A x) (B x) (C x))"")
self.assertEqual(list(tgrep.tgrep_positions(""* $. B"", [tree])), [[(0,)]])
self.assertEqual(list(tgrep.tgrep_positions(""* $.. B"", [tree])), [[(0,)]])
self.assertEqual(list(tgrep.tgrep_positions(""* $, B"", [tree])), [[(2,)]])
self.assertEqual(list(tgrep.tgrep_positions(""* $,, B"", [tree])), [[(2,)]])
self.assertEqual(list(tgrep.tgrep_positions(""* $ B"", [tree])), [[(0,), (2,)]])
",[],0,[],/test/unit/test_tgrep.py_test_rel_sister_nodes
5571,/home/amandapotts/git/nltk/nltk/test/unit/test_tgrep.py_tests_rel_indexed_children,"def tests_rel_indexed_children(self):
""""""
Test matching nodes based on their index in their parent node.
""""""
tree = ParentedTree.fromstring(""(S (A x) (B x) (C x))"")
self.assertEqual(list(tgrep.tgrep_positions(""* >, S"", [tree])), [[(0,)]])
self.assertEqual(list(tgrep.tgrep_positions(""* >1 S"", [tree])), [[(0,)]])
self.assertEqual(list(tgrep.tgrep_positions(""* >2 S"", [tree])), [[(1,)]])
self.assertEqual(list(tgrep.tgrep_positions(""* >3 S"", [tree])), [[(2,)]])
self.assertEqual(list(tgrep.tgrep_positions(""* >' S"", [tree])), [[(2,)]])
self.assertEqual(list(tgrep.tgrep_positions(""* >-1 S"", [tree])), [[(2,)]])
self.assertEqual(list(tgrep.tgrep_positions(""* >-2 S"", [tree])), [[(1,)]])
self.assertEqual(list(tgrep.tgrep_positions(""* >-3 S"", [tree])), [[(0,)]])
tree = ParentedTree.fromstring(
""(S (D (A x) (B x) (C x)) (E (B x) (C x) (A x)) "" ""(F (C x) (A x) (B x)))""
)
self.assertEqual(list(tgrep.tgrep_positions(""* <, A"", [tree])), [[(0,)]])
self.assertEqual(list(tgrep.tgrep_positions(""* <1 A"", [tree])), [[(0,)]])
self.assertEqual(list(tgrep.tgrep_positions(""* <2 A"", [tree])), [[(2,)]])
self.assertEqual(list(tgrep.tgrep_positions(""* <3 A"", [tree])), [[(1,)]])
self.assertEqual(list(tgrep.tgrep_positions(""* <' A"", [tree])), [[(1,)]])
self.assertEqual(list(tgrep.tgrep_positions(""* <-1 A"", [tree])), [[(1,)]])
self.assertEqual(list(tgrep.tgrep_positions(""* <-2 A"", [tree])), [[(2,)]])
self.assertEqual(list(tgrep.tgrep_positions(""* <-3 A"", [tree])), [[(0,)]])
",[],0,[],/test/unit/test_tgrep.py_tests_rel_indexed_children
5572,/home/amandapotts/git/nltk/nltk/test/unit/test_tgrep.py_test_rel_precedence,"def test_rel_precedence(self):
""""""
Test matching nodes based on precedence relations.
""""""
tree = ParentedTree.fromstring(
""(S (NP (NP (PP x)) (NP (AP x)))""
"" (VP (AP (X (PP x)) (Y (AP x))))""
"" (NP (RC (NP (AP x)))))""
)
self.assertEqual(
list(tgrep.tgrep_positions(""* . X"", [tree])), [[(0,), (0, 1), (0, 1, 0)]]
)
self.assertEqual(
list(tgrep.tgrep_positions(""* . Y"", [tree])), [[(1, 0, 0), (1, 0, 0, 0)]]
)
self.assertEqual(
list(tgrep.tgrep_positions(""* .. X"", [tree])),
[[(0,), (0, 0), (0, 0, 0), (0, 1), (0, 1, 0)]],
)
self.assertEqual(
list(tgrep.tgrep_positions(""* .. Y"", [tree])),
[[(0,), (0, 0), (0, 0, 0), (0, 1), (0, 1, 0), (1, 0, 0), (1, 0, 0, 0)]],
)
self.assertEqual(
list(tgrep.tgrep_positions(""* , X"", [tree])), [[(1, 0, 1), (1, 0, 1, 0)]]
)
self.assertEqual(
list(tgrep.tgrep_positions(""* , Y"", [tree])),
[[(2,), (2, 0), (2, 0, 0), (2, 0, 0, 0)]],
)
self.assertEqual(
list(tgrep.tgrep_positions(""* ,, X"", [tree])),
[[(1, 0, 1), (1, 0, 1, 0), (2,), (2, 0), (2, 0, 0), (2, 0, 0, 0)]],
)
self.assertEqual(
list(tgrep.tgrep_positions(""* ,, Y"", [tree])),
[[(2,), (2, 0), (2, 0, 0), (2, 0, 0, 0)]],
)
",[],0,[],/test/unit/test_tgrep.py_test_rel_precedence
5573,/home/amandapotts/git/nltk/nltk/test/unit/test_tgrep.py_test_examples,"def test_examples(self):
""""""
Test the Basic Examples from the TGrep2 manual.
""""""
tree = ParentedTree.fromstring(""(S (NP (AP x)) (NP (PP x)))"")
self.assertEqual(list(tgrep.tgrep_positions(""NP < PP"", [tree])), [[(1,)]])
tree = ParentedTree.fromstring(""(S (NP x) (VP x) (NP (PP x)) (VP x))"")
self.assertEqual(list(tgrep.tgrep_positions(""NP << PP . VP"", [tree])), [[(2,)]])
tree = ParentedTree.fromstring(
""(S (NP (AP x)) (NP (PP x)) "" ""(NP (DET x) (NN x)) (VP x))""
)
self.assertEqual(
list(tgrep.tgrep_positions(""NP << PP | . VP"", [tree])), [[(1,), (2,)]]
)
tree = ParentedTree.fromstring(
""(S (NP (NP (PP x)) (NP (AP x)))""
"" (VP (AP (NP (PP x)) (NP (AP x))))""
"" (NP (RC (NP (AP x)))))""
)
self.assertEqual(
list(tgrep.tgrep_positions(""NP !<< PP [> NP | >> VP]"", [tree])),
[[(0, 1), (1, 0, 1)]],
)
tree = ParentedTree.fromstring(
""(S (NP (AP (PP x) (VP x))) "" ""(NP (AP (PP x) (NP x))) (NP x))""
)
self.assertEqual(
list(tgrep.tgrep_positions(""NP << (PP . VP)"", [tree])), [[(0,)]]
)
tree = ParentedTree.fromstring(
""(S (NP (DET a) (NN cat) (PP (IN on) (NP x)))""
"" (NP (DET a) (NN cat) (PP (IN on) (NP x)) (PP x))""
"" (NP x))""
)
self.assertEqual(
list(tgrep.tgrep_positions(""NP <' (PP <, (IN < on))"", [tree])), [[(0,)]]
)
tree = ParentedTree.fromstring(
""(S (S (C x) (A (B x))) (S (C x) (A x)) "" ""(S (D x) (A (B x))))""
)
self.assertEqual(
list(tgrep.tgrep_positions(""S < (A < B) < C"", [tree])), [[(0,)]]
)
tree = ParentedTree.fromstring(
""(S (S (A (B x) (C x))) (S (S (C x) (A (B x)))))""
)
self.assertEqual(
list(tgrep.tgrep_positions(""S < ((A < B) < C)"", [tree])), [[(0,)]]
)
self.assertEqual(
list(tgrep.tgrep_positions(""S < (A < B < C)"", [tree])), [[(0,)]]
)
",[],0,[],/test/unit/test_tgrep.py_test_examples
5574,/home/amandapotts/git/nltk/nltk/test/unit/test_tgrep.py_test_use_macros,"def test_use_macros(self):
""""""
Test defining and using tgrep2 macros.
""""""
tree = ParentedTree.fromstring(
""(VP (VB sold) (NP (DET the) ""
""(NN heiress)) (NP (NN deed) (PREP to) ""
""(NP (DET the) (NN school) (NN house))))""
)
self.assertEqual(
list(
tgrep.tgrep_positions(
""@ NP /^NP/
)
),
[[(1,), (2, 2)]],
)
self.assertRaises(
tgrep.TgrepException,
list,
tgrep.tgrep_positions(
""@ NP /^NP/
),
)
",[],0,[],/test/unit/test_tgrep.py_test_use_macros
5575,/home/amandapotts/git/nltk/nltk/test/unit/test_tgrep.py_test_tokenize_node_labels,"def test_tokenize_node_labels(self):
""""""Test tokenization of labeled nodes.""""""
self.assertEqual(
tgrep.tgrep_tokenize(""S < @SBJ < (@VP < (@VB $.. @OBJ))""),
[
""S"",
""<"",
""@SBJ"",
""<"",
""("",
""@VP"",
""<"",
""("",
""@VB"",
""$.."",
""@OBJ"",
"")"",
"")"",
],
)
self.assertEqual(
tgrep.tgrep_tokenize(""S < @SBJ=s < (@VP=v < (@VB $.. @OBJ))""),
[
""S"",
""<"",
""@SBJ"",
""="",
""s"",
""<"",
""("",
""@VP"",
""="",
""v"",
""<"",
""("",
""@VB"",
""$.."",
""@OBJ"",
"")"",
"")"",
],
)
",[],0,[],/test/unit/test_tgrep.py_test_tokenize_node_labels
5576,/home/amandapotts/git/nltk/nltk/test/unit/test_tgrep.py_test_tokenize_segmented_patterns,"def test_tokenize_segmented_patterns(self):
""""""Test tokenization of segmented patterns.""""""
self.assertEqual(
tgrep.tgrep_tokenize(""S < @SBJ=s < (@VP=v < (@VB $.. @OBJ)) : =s .. =v""),
[
""S"",
""<"",
""@SBJ"",
""="",
""s"",
""<"",
""("",
""@VP"",
""="",
""v"",
""<"",
""("",
""@VB"",
""$.."",
""@OBJ"",
"")"",
"")"",
"":"",
""=s"",
"".."",
""=v"",
],
)
",[],0,[],/test/unit/test_tgrep.py_test_tokenize_segmented_patterns
5577,/home/amandapotts/git/nltk/nltk/test/unit/test_tgrep.py_test_labeled_nodes,"def test_labeled_nodes(self):
""""""
Test labeled nodes.
Test case from Emily M. Bender.
""""""
search = """"""
@ SBJ /SBJ/
@ VP /VP/
@ VB /VB/
@ VPoB /V[PB]/
@ OBJ /OBJ/
S < @SBJ=s < (@VP=v < (@VB $.. @OBJ)) : =s .. =v""""""
sent1 = ParentedTree.fromstring(
""(S (NP-SBJ I) (VP (VB eat) (NP-OBJ (NNS apples))))""
)
sent2 = ParentedTree.fromstring(
""(S (VP (VB eat) (NP-OBJ (NNS apples))) (NP-SBJ I))""
)
search_firsthalf = search.split(""\n\n"")[0] + ""S < @SBJ < (@VP < (@VB $.. @OBJ))""
search_rewrite = ""S < (/.*SBJ/ $.. (/VP/ < (/VB/ $.. /.*OBJ/)))""
self.assertTrue(list(tgrep.tgrep_positions(search_firsthalf, [sent1]))[0])
self.assertTrue(list(tgrep.tgrep_positions(search, [sent1]))[0])
self.assertTrue(list(tgrep.tgrep_positions(search_rewrite, [sent1]))[0])
self.assertEqual(
list(tgrep.tgrep_positions(search, [sent1])),
list(tgrep.tgrep_positions(search_rewrite, [sent1])),
)
self.assertTrue(list(tgrep.tgrep_positions(search_firsthalf, [sent2]))[0])
self.assertFalse(list(tgrep.tgrep_positions(search, [sent2]))[0])
self.assertFalse(list(tgrep.tgrep_positions(search_rewrite, [sent2]))[0])
self.assertEqual(
list(tgrep.tgrep_positions(search, [sent2])),
list(tgrep.tgrep_positions(search_rewrite, [sent2])),
)
",[],0,[],/test/unit/test_tgrep.py_test_labeled_nodes
5578,/home/amandapotts/git/nltk/nltk/test/unit/test_tgrep.py_test_multiple_conjs,"def test_multiple_conjs(self):
""""""
Test that multiple (3 or more) conjunctions of node relations are
handled properly.
""""""
sent = ParentedTree.fromstring(""((A (B b) (C c)) (A (B b) (C c) (D d)))"")
self.assertEqual(
list(tgrep.tgrep_positions(""(A < B < C < D)"", [sent])), [[(1,)]]
)
self.assertEqual(
list(tgrep.tgrep_positions(""(A < B < C)"", [sent])), [[(0,), (1,)]]
)
",[],0,[],/test/unit/test_tgrep.py_test_multiple_conjs
5579,/home/amandapotts/git/nltk/nltk/test/unit/test_tgrep.py_test_trailing_semicolon,"def test_trailing_semicolon(self):
""""""
Test that semicolons at the end of a tgrep2 search string won't
cause a parse failure.
""""""
tree = ParentedTree.fromstring(
""(S (NP (DT the) (JJ big) (NN dog)) "" ""(VP bit) (NP (DT a) (NN cat)))""
)
self.assertEqual(list(tgrep.tgrep_positions(""NN"", [tree])), [[(0, 2), (2, 1)]])
self.assertEqual(list(tgrep.tgrep_positions(""NN
self.assertEqual(
list(tgrep.tgrep_positions(""NN
)
",[],0,[],/test/unit/test_tgrep.py_test_trailing_semicolon
5580,/home/amandapotts/git/nltk/nltk/test/unit/test_cfg2chomsky.py_test_simple,"def test_simple(self):
grammar = CFG.fromstring(
""""""
S -> NP VP
PP -> P NP
NP -> Det N | NP PP P
VP -> V NP | VP PP
VP -> Det
Det -> 'a' | 'the'
N -> 'dog' | 'cat'
V -> 'chased' | 'sat'
P -> 'on' | 'in'
""""""
)
self.assertFalse(grammar.is_flexible_chomsky_normal_form())
self.assertFalse(grammar.is_chomsky_normal_form())
grammar = grammar.chomsky_normal_form(flexible=True)
self.assertTrue(grammar.is_flexible_chomsky_normal_form())
self.assertFalse(grammar.is_chomsky_normal_form())
grammar2 = CFG.fromstring(
""""""
S -> NP VP
NP -> VP N P
VP -> P
N -> 'dog' | 'cat'
P -> 'on' | 'in'
""""""
)
self.assertFalse(grammar2.is_flexible_chomsky_normal_form())
self.assertFalse(grammar2.is_chomsky_normal_form())
grammar2 = grammar2.chomsky_normal_form()
self.assertTrue(grammar2.is_flexible_chomsky_normal_form())
self.assertTrue(grammar2.is_chomsky_normal_form())
",[],0,[],/test/unit/test_cfg2chomsky.py_test_simple
5581,/home/amandapotts/git/nltk/nltk/test/unit/test_cfg2chomsky.py_test_complex,"def test_complex(self):
grammar = nltk.data.load(""grammars/large_grammars/atis.cfg"")
self.assertFalse(grammar.is_flexible_chomsky_normal_form())
self.assertFalse(grammar.is_chomsky_normal_form())
grammar = grammar.chomsky_normal_form(flexible=True)
self.assertTrue(grammar.is_flexible_chomsky_normal_form())
self.assertFalse(grammar.is_chomsky_normal_form())
",[],0,[],/test/unit/test_cfg2chomsky.py_test_complex
5582,/home/amandapotts/git/nltk/nltk/test/unit/test_corpus_views.py_data,"def data(self):
for name in self.names:
f = nltk.data.find(name)
with f.open() as fp:
file_data = fp.read().decode(""utf8"")
yield f, file_data
",[],0,[],/test/unit/test_corpus_views.py_data
5583,/home/amandapotts/git/nltk/nltk/test/unit/test_corpus_views.py_test_correct_values,"def test_correct_values(self):
for f, file_data in self.data():
v = StreamBackedCorpusView(f, read_whitespace_block)
self.assertEqual(list(v), file_data.split())
v = StreamBackedCorpusView(f, read_line_block)
self.assertEqual(list(v), self.linetok.tokenize(file_data))
",[],0,[],/test/unit/test_corpus_views.py_test_correct_values
5584,/home/amandapotts/git/nltk/nltk/test/unit/test_corpus_views.py_test_correct_length,"def test_correct_length(self):
for f, file_data in self.data():
v = StreamBackedCorpusView(f, read_whitespace_block)
self.assertEqual(len(v), len(file_data.split()))
v = StreamBackedCorpusView(f, read_line_block)
self.assertEqual(len(v), len(self.linetok.tokenize(file_data)))
",[],0,[],/test/unit/test_corpus_views.py_test_correct_length
5585,/home/amandapotts/git/nltk/nltk/test/unit/test_brill.py_test_pos_template,"def test_pos_template(self):
train_sents = treebank.tagged_sents()[:1000]
tagger = UnigramTagger(train_sents)
trainer = brill_trainer.BrillTaggerTrainer(
tagger, [brill.Template(brill.Pos([-1]))]
)
brill_tagger = trainer.train(train_sents)
result = brill_tagger.tag(""This is a foo bar sentence"".split())
expected = [
(""This"", ""DT""),
(""is"", ""VBZ""),
(""a"", ""DT""),
(""foo"", None),
(""bar"", ""NN""),
(""sentence"", None),
]
self.assertEqual(result, expected)
",[],0,[],/test/unit/test_brill.py_test_pos_template
5586,/home/amandapotts/git/nltk/nltk/test/unit/test_brill.py_test_brill_demo,"def test_brill_demo(self):
demo()
",[],0,[],/test/unit/test_brill.py_test_brill_demo
5587,/home/amandapotts/git/nltk/nltk/test/unit/test_util.py_everygram_input,"def everygram_input():
""""""Form test data for tests.""""""
return iter([""a"", ""b"", ""c""])
",[],0,[],/test/unit/test_util.py_everygram_input
5588,/home/amandapotts/git/nltk/nltk/test/unit/test_util.py_test_everygrams_without_padding,"def test_everygrams_without_padding(everygram_input):
expected_output = [
(""a"",),
(""a"", ""b""),
(""a"", ""b"", ""c""),
(""b"",),
(""b"", ""c""),
(""c"",),
]
output = list(everygrams(everygram_input))
assert output == expected_output
",[],0,[],/test/unit/test_util.py_test_everygrams_without_padding
5589,/home/amandapotts/git/nltk/nltk/test/unit/test_util.py_test_everygrams_max_len,"def test_everygrams_max_len(everygram_input):
expected_output = [
(""a"",),
(""a"", ""b""),
(""b"",),
(""b"", ""c""),
(""c"",),
]
output = list(everygrams(everygram_input, max_len=2))
assert output == expected_output
",[],0,[],/test/unit/test_util.py_test_everygrams_max_len
5590,/home/amandapotts/git/nltk/nltk/test/unit/test_util.py_test_everygrams_min_len,"def test_everygrams_min_len(everygram_input):
expected_output = [
(""a"", ""b""),
(""a"", ""b"", ""c""),
(""b"", ""c""),
]
output = list(everygrams(everygram_input, min_len=2))
assert output == expected_output
",[],0,[],/test/unit/test_util.py_test_everygrams_min_len
5591,/home/amandapotts/git/nltk/nltk/test/unit/test_util.py_test_everygrams_pad_right,"def test_everygrams_pad_right(everygram_input):
expected_output = [
(""a"",),
(""a"", ""b""),
(""a"", ""b"", ""c""),
(""b"",),
(""b"", ""c""),
(""b"", ""c"", None),
(""c"",),
(""c"", None),
(""c"", None, None),
(None,),
(None, None),
(None,),
]
output = list(everygrams(everygram_input, max_len=3, pad_right=True))
assert output == expected_output
",[],0,[],/test/unit/test_util.py_test_everygrams_pad_right
5592,/home/amandapotts/git/nltk/nltk/test/unit/test_util.py_test_everygrams_pad_left,"def test_everygrams_pad_left(everygram_input):
expected_output = [
(None,),
(None, None),
(None, None, ""a""),
(None,),
(None, ""a""),
(None, ""a"", ""b""),
(""a"",),
(""a"", ""b""),
(""a"", ""b"", ""c""),
(""b"",),
(""b"", ""c""),
(""c"",),
]
output = list(everygrams(everygram_input, max_len=3, pad_left=True))
assert output == expected_output
",[],0,[],/test/unit/test_util.py_test_everygrams_pad_left
5593,/home/amandapotts/git/nltk/nltk/test/unit/test_collocations.py_close_enough,"def close_enough(x, y):
""""""Verify that two sequences of n-gram association values are within
_EPSILON of each other.
""""""
return all(abs(x1[1] - y1[1]) <= _EPSILON for x1, y1 in zip(x, y))
",[],0,[],/test/unit/test_collocations.py_close_enough
5594,/home/amandapotts/git/nltk/nltk/test/unit/test_collocations.py_test_bigram2,"def test_bigram2():
b = BigramCollocationFinder.from_words(SENT)
assert sorted(b.ngram_fd.items()) == [
((""a"", ""a""), 1),
((""a"", ""test""), 1),
((""is"", ""a""), 1),
((""is"", ""is""), 1),
((""test"", ""test""), 1),
((""this"", ""is""), 1),
((""this"", ""this""), 1),
]
assert sorted(b.word_fd.items()) == [(""a"", 2), (""is"", 2), (""test"", 2), (""this"", 2)]
assert len(SENT) == sum(b.word_fd.values()) == sum(b.ngram_fd.values()) + 1
assert close_enough(
sorted(b.score_ngrams(BigramAssocMeasures.pmi)),
[
((""a"", ""a""), 1.0),
((""a"", ""test""), 1.0),
((""is"", ""a""), 1.0),
((""is"", ""is""), 1.0),
((""test"", ""test""), 1.0),
((""this"", ""is""), 1.0),
((""this"", ""this""), 1.0),
],
)
",[],0,[],/test/unit/test_collocations.py_test_bigram2
5595,/home/amandapotts/git/nltk/nltk/test/unit/test_collocations.py_test_bigram3,"def test_bigram3():
b = BigramCollocationFinder.from_words(SENT, window_size=3)
assert sorted(b.ngram_fd.items()) == sorted(
[
((""a"", ""test""), 3),
((""is"", ""a""), 3),
((""this"", ""is""), 3),
((""a"", ""a""), 1),
((""is"", ""is""), 1),
((""test"", ""test""), 1),
((""this"", ""this""), 1),
]
)
assert sorted(b.word_fd.items()) == sorted(
[(""a"", 2), (""is"", 2), (""test"", 2), (""this"", 2)]
)
assert (
len(SENT) == sum(b.word_fd.values()) == (sum(b.ngram_fd.values()) + 2 + 1) / 2.0
)
assert close_enough(
sorted(b.score_ngrams(BigramAssocMeasures.pmi)),
sorted(
[
((""a"", ""test""), 1.584962500721156),
((""is"", ""a""), 1.584962500721156),
((""this"", ""is""), 1.584962500721156),
((""a"", ""a""), 0.0),
((""is"", ""is""), 0.0),
((""test"", ""test""), 0.0),
((""this"", ""this""), 0.0),
]
),
)
",[],0,[],/test/unit/test_collocations.py_test_bigram3
5596,/home/amandapotts/git/nltk/nltk/test/unit/test_collocations.py_test_bigram5,"def test_bigram5():
b = BigramCollocationFinder.from_words(SENT, window_size=5)
assert sorted(b.ngram_fd.items()) == sorted(
[
((""a"", ""test""), 4),
((""is"", ""a""), 4),
((""this"", ""is""), 4),
((""is"", ""test""), 3),
((""this"", ""a""), 3),
((""a"", ""a""), 1),
((""is"", ""is""), 1),
((""test"", ""test""), 1),
((""this"", ""this""), 1),
]
)
assert sorted(b.word_fd.items()) == sorted(
[(""a"", 2), (""is"", 2), (""test"", 2), (""this"", 2)]
)
n_word_fd = sum(b.word_fd.values())
n_ngram_fd = (sum(b.ngram_fd.values()) + 4 + 3 + 2 + 1) / 4.0
assert len(SENT) == n_word_fd == n_ngram_fd
assert close_enough(
sorted(b.score_ngrams(BigramAssocMeasures.pmi)),
sorted(
[
((""a"", ""test""), 1.0),
((""is"", ""a""), 1.0),
((""this"", ""is""), 1.0),
((""is"", ""test""), 0.5849625007211562),
((""this"", ""a""), 0.5849625007211562),
((""a"", ""a""), -1.0),
((""is"", ""is""), -1.0),
((""test"", ""test""), -1.0),
((""this"", ""this""), -1.0),
]
),
)
",[],0,[],/test/unit/test_collocations.py_test_bigram5
5597,/home/amandapotts/git/nltk/nltk/test/unit/test_rte_classify.py_test_rte_feature_extraction,"def test_rte_feature_extraction(self):
pairs = rte_corpus.pairs([""rte1_dev.xml""])[:6]
test_output = [
f""{key:<15} => {rte_features(pair)[key]}""
for pair in pairs
for key in sorted(rte_features(pair))
]
expected_output = expected_from_rte_feature_extration.strip().split(""\n"")
expected_output = list(filter(None, expected_output))
assert test_output == expected_output
",[],0,[],/test/unit/test_rte_classify.py_test_rte_feature_extraction
5598,/home/amandapotts/git/nltk/nltk/test/unit/test_rte_classify.py_test_feature_extractor_object,"def test_feature_extractor_object(self):
rtepair = rte_corpus.pairs([""rte3_dev.xml""])[33]
extractor = RTEFeatureExtractor(rtepair)
assert extractor.hyp_words == {""member"", ""China"", ""SCO.""}
assert extractor.overlap(""word"") == set()
assert extractor.overlap(""ne"") == {""China""}
assert extractor.hyp_extra(""word"") == {""member""}
",[],0,[],/test/unit/test_rte_classify.py_test_feature_extractor_object
5599,/home/amandapotts/git/nltk/nltk/test/unit/test_rte_classify.py_test_rte_classification_without_megam,"def test_rte_classification_without_megam(self):
clf = rte_classifier(""IIS"", sample_N=100)
clf = rte_classifier(""GIS"", sample_N=100)
",[],0,[],/test/unit/test_rte_classify.py_test_rte_classification_without_megam
5600,/home/amandapotts/git/nltk/nltk/test/unit/test_rte_classify.py_test_rte_classification_with_megam,"def test_rte_classification_with_megam(self):
try:
config_megam()
except (LookupError, AttributeError) as e:
pytest.skip(""Skipping tests with dependencies on MEGAM"")
clf = rte_classifier(""megam"", sample_N=100)
",[],0,[],/test/unit/test_rte_classify.py_test_rte_classification_with_megam
5601,/home/amandapotts/git/nltk/nltk/test/unit/test_chunk.py_test_tag_pattern2re_pattern_quantifier,"def test_tag_pattern2re_pattern_quantifier(self):
""""""Test for bug https://github.com/nltk/nltk/issues/1597
Ensures that curly bracket quantifiers can be used inside a chunk rule.
This type of quantifier has been used for the supplementary example
in https://www.nltk.org/book/ch07.html#exploring-text-corpora.
""""""
sent = [
(""The"", ""AT""),
(""September-October"", ""NP""),
(""term"", ""NN""),
(""jury"", ""NN""),
(""had"", ""HVD""),
(""been"", ""BEN""),
(""charged"", ""VBN""),
(""by"", ""IN""),
(""Fulton"", ""NP-TL""),
(""Superior"", ""JJ-TL""),
(""Court"", ""NN-TL""),
(""Judge"", ""NN-TL""),
(""Durwood"", ""NP""),
(""Pye"", ""NP""),
(""to"", ""TO""),
(""investigate"", ""VB""),
(""reports"", ""NNS""),
(""of"", ""IN""),
(""possible"", ""JJ""),
(""``"", ""``""),
(""irregularities"", ""NNS""),
(""''"", ""''""),
(""in"", ""IN""),
(""the"", ""AT""),
(""hard-fought"", ""JJ""),
(""primary"", ""NN""),
(""which"", ""WDT""),
(""was"", ""BEDZ""),
(""won"", ""VBN""),
(""by"", ""IN""),
(""Mayor-nominate"", ""NN-TL""),
(""Ivan"", ""NP""),
(""Allen"", ""NP""),
(""Jr."", ""NP""),
(""."", "".""),
]  # source: brown corpus
cp = RegexpParser(""CHUNK: {<N.*>{4,}}"")
tree = cp.parse(sent)
assert (
tree.pformat()
== """"""(S
",[],0,[],/test/unit/test_chunk.py_test_tag_pattern2re_pattern_quantifier
5602,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_meteor.py_test_meteor,"def test_meteor(self):
score = meteor_score(self.reference, self.candidate, preprocess=str.lower)
assert score == 0.9921875
",[],0,[],/test/unit/translate/test_meteor.py_test_meteor
5603,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_meteor.py_test_reference_type_check,"def test_reference_type_check(self):
str_reference = ["" "".join(ref) for ref in self.reference]
self.assertRaises(TypeError, meteor_score, str_reference, self.candidate)
",[],0,[],/test/unit/translate/test_meteor.py_test_reference_type_check
5604,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_meteor.py_test_candidate_type_check,"def test_candidate_type_check(self):
str_candidate = "" "".join(self.candidate)
self.assertRaises(TypeError, meteor_score, self.reference, str_candidate)
",[],0,[],/test/unit/translate/test_meteor.py_test_candidate_type_check
5605,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_stack_decoder.py_test_find_all_src_phrases,"def test_find_all_src_phrases(self):
phrase_table = TestStackDecoder.create_fake_phrase_table()
stack_decoder = StackDecoder(phrase_table, None)
sentence = (""my"", ""hovercraft"", ""is"", ""full"", ""of"", ""eels"")
src_phrase_spans = stack_decoder.find_all_src_phrases(sentence)
self.assertEqual(src_phrase_spans[0], [2])  # 'my hovercraft'
self.assertEqual(src_phrase_spans[1], [2])  # 'hovercraft'
self.assertEqual(src_phrase_spans[2], [3])  # 'is'
self.assertEqual(src_phrase_spans[3], [5, 6])  # 'full of', 'full of eels'
self.assertFalse(src_phrase_spans[4])  # no entry starting with 'of'
self.assertEqual(src_phrase_spans[5], [6])  # 'eels'
",[],0,[],/test/unit/translate/test_stack_decoder.py_test_find_all_src_phrases
5606,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_stack_decoder.py_test_distortion_score,"def test_distortion_score(self):
stack_decoder = StackDecoder(None, None)
stack_decoder.distortion_factor = 0.5
hypothesis = _Hypothesis()
hypothesis.src_phrase_span = (3, 5)
score = stack_decoder.distortion_score(hypothesis, (8, 10))
expected_score = log(stack_decoder.distortion_factor) * (8 - 5)
self.assertEqual(score, expected_score)
",[],0,[],/test/unit/translate/test_stack_decoder.py_test_distortion_score
5607,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_stack_decoder.py_test_distortion_score_of_first_expansion,"def test_distortion_score_of_first_expansion(self):
stack_decoder = StackDecoder(None, None)
stack_decoder.distortion_factor = 0.5
hypothesis = _Hypothesis()
score = stack_decoder.distortion_score(hypothesis, (8, 10))
self.assertEqual(score, 0.0)
",[],0,[],/test/unit/translate/test_stack_decoder.py_test_distortion_score_of_first_expansion
5608,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_stack_decoder.py_test_compute_future_costs,"def test_compute_future_costs(self):
phrase_table = TestStackDecoder.create_fake_phrase_table()
language_model = TestStackDecoder.create_fake_language_model()
stack_decoder = StackDecoder(phrase_table, language_model)
sentence = (""my"", ""hovercraft"", ""is"", ""full"", ""of"", ""eels"")
future_scores = stack_decoder.compute_future_scores(sentence)
self.assertEqual(
future_scores[1][2],
(
phrase_table.translations_for((""hovercraft"",))[0].log_prob
+ language_model.probability((""hovercraft"",))
),
)
self.assertEqual(
future_scores[0][2],
(
phrase_table.translations_for((""my"", ""hovercraft""))[0].log_prob
+ language_model.probability((""my"", ""hovercraft""))
),
)
",[],0,[],/test/unit/translate/test_stack_decoder.py_test_compute_future_costs
5609,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_stack_decoder.py_test_compute_future_costs_for_phrases_not_in_phrase_table,"def test_compute_future_costs_for_phrases_not_in_phrase_table(self):
phrase_table = TestStackDecoder.create_fake_phrase_table()
language_model = TestStackDecoder.create_fake_language_model()
stack_decoder = StackDecoder(phrase_table, language_model)
sentence = (""my"", ""hovercraft"", ""is"", ""full"", ""of"", ""eels"")
future_scores = stack_decoder.compute_future_scores(sentence)
self.assertEqual(
future_scores[1][3],  # 'hovercraft is' is not in phrase table
future_scores[1][2] + future_scores[2][3],
)  # backoff
",[],0,[],/test/unit/translate/test_stack_decoder.py_test_compute_future_costs_for_phrases_not_in_phrase_table
5610,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_stack_decoder.py_create_fake_phrase_table,"def create_fake_phrase_table():
phrase_table = PhraseTable()
phrase_table.add((""hovercraft"",), ("""",), 0.8)
phrase_table.add((""my"", ""hovercraft""), ("""", """"), 0.7)
phrase_table.add((""my"", ""cheese""), ("""", """"), 0.7)
phrase_table.add((""is"",), ("""",), 0.8)
phrase_table.add((""is"",), ("""",), 0.5)
phrase_table.add((""full"", ""of""), ("""", """"), 0.01)
phrase_table.add((""full"", ""of"", ""eels""), ("""", """", """"), 0.5)
phrase_table.add((""full"", ""of"", ""spam""), ("""", """"), 0.5)
phrase_table.add((""eels"",), ("""",), 0.5)
phrase_table.add((""spam"",), ("""",), 0.5)
return phrase_table
",[],0,[],/test/unit/translate/test_stack_decoder.py_create_fake_phrase_table
5611,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_stack_decoder.py_setUp,"def setUp(self):
root = _Hypothesis()
child = _Hypothesis(
raw_score=0.5,
src_phrase_span=(3, 7),
trg_phrase=(""hello"", ""world""),
previous=root,
)
grandchild = _Hypothesis(
raw_score=0.4,
src_phrase_span=(1, 2),
trg_phrase=(""and"", ""goodbye""),
previous=child,
)
self.hypothesis_chain = grandchild
",[],0,[],/test/unit/translate/test_stack_decoder.py_setUp
5612,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_stack_decoder.py_test_translation_so_far,"def test_translation_so_far(self):
translation = self.hypothesis_chain.translation_so_far()
self.assertEqual(translation, [""hello"", ""world"", ""and"", ""goodbye""])
",[],0,[],/test/unit/translate/test_stack_decoder.py_test_translation_so_far
5613,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_stack_decoder.py_test_translation_so_far_for_empty_hypothesis,"def test_translation_so_far_for_empty_hypothesis(self):
hypothesis = _Hypothesis()
translation = hypothesis.translation_so_far()
self.assertEqual(translation, [])
",[],0,[],/test/unit/translate/test_stack_decoder.py_test_translation_so_far_for_empty_hypothesis
5614,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_stack_decoder.py_test_total_translated_words,"def test_total_translated_words(self):
total_translated_words = self.hypothesis_chain.total_translated_words()
self.assertEqual(total_translated_words, 5)
",[],0,[],/test/unit/translate/test_stack_decoder.py_test_total_translated_words
5615,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_stack_decoder.py_test_translated_positions,"def test_translated_positions(self):
translated_positions = self.hypothesis_chain.translated_positions()
translated_positions.sort()
self.assertEqual(translated_positions, [1, 3, 4, 5, 6])
",[],0,[],/test/unit/translate/test_stack_decoder.py_test_translated_positions
5616,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_stack_decoder.py_test_untranslated_spans,"def test_untranslated_spans(self):
untranslated_spans = self.hypothesis_chain.untranslated_spans(10)
self.assertEqual(untranslated_spans, [(0, 1), (2, 3), (7, 10)])
",[],0,[],/test/unit/translate/test_stack_decoder.py_test_untranslated_spans
5617,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_stack_decoder.py_test_untranslated_spans_for_empty_hypothesis,"def test_untranslated_spans_for_empty_hypothesis(self):
hypothesis = _Hypothesis()
untranslated_spans = hypothesis.untranslated_spans(10)
self.assertEqual(untranslated_spans, [(0, 10)])
",[],0,[],/test/unit/translate/test_stack_decoder.py_test_untranslated_spans_for_empty_hypothesis
5618,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_stack_decoder.py_test_push_bumps_off_worst_hypothesis_when_stack_is_full,"def test_push_bumps_off_worst_hypothesis_when_stack_is_full(self):
stack = _Stack(3)
poor_hypothesis = _Hypothesis(0.01)
stack.push(_Hypothesis(0.2))
stack.push(poor_hypothesis)
stack.push(_Hypothesis(0.1))
stack.push(_Hypothesis(0.3))
self.assertFalse(poor_hypothesis in stack)
",[],0,[],/test/unit/translate/test_stack_decoder.py_test_push_bumps_off_worst_hypothesis_when_stack_is_full
5619,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_stack_decoder.py_test_push_removes_hypotheses_that_fall_below_beam_threshold,"def test_push_removes_hypotheses_that_fall_below_beam_threshold(self):
stack = _Stack(3, 0.5)
poor_hypothesis = _Hypothesis(0.01)
worse_hypothesis = _Hypothesis(0.009)
stack.push(poor_hypothesis)
stack.push(worse_hypothesis)
stack.push(_Hypothesis(0.9))  # greatly superior hypothesis
self.assertFalse(poor_hypothesis in stack)
self.assertFalse(worse_hypothesis in stack)
",[],0,[],/test/unit/translate/test_stack_decoder.py_test_push_removes_hypotheses_that_fall_below_beam_threshold
5620,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_stack_decoder.py_test_push_does_not_add_hypothesis_that_falls_below_beam_threshold,"def test_push_does_not_add_hypothesis_that_falls_below_beam_threshold(self):
stack = _Stack(3, 0.5)
poor_hypothesis = _Hypothesis(0.01)
stack.push(_Hypothesis(0.9))  # greatly superior hypothesis
stack.push(poor_hypothesis)
self.assertFalse(poor_hypothesis in stack)
",[],0,[],/test/unit/translate/test_stack_decoder.py_test_push_does_not_add_hypothesis_that_falls_below_beam_threshold
5621,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_stack_decoder.py_test_best_returns_the_best_hypothesis,"def test_best_returns_the_best_hypothesis(self):
stack = _Stack(3)
best_hypothesis = _Hypothesis(0.99)
stack.push(_Hypothesis(0.0))
stack.push(best_hypothesis)
stack.push(_Hypothesis(0.5))
self.assertEqual(stack.best(), best_hypothesis)
",[],0,[],/test/unit/translate/test_stack_decoder.py_test_best_returns_the_best_hypothesis
5622,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_stack_decoder.py_test_best_returns_none_when_stack_is_empty,"def test_best_returns_none_when_stack_is_empty(self):
stack = _Stack(3)
self.assertEqual(stack.best(), None)
",[],0,[],/test/unit/translate/test_stack_decoder.py_test_best_returns_none_when_stack_is_empty
5623,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_ibm5.py_test_set_uniform_vacancy_probabilities_of_max_displacements,"def test_set_uniform_vacancy_probabilities_of_max_displacements(self):
src_classes = {""schinken"": 0, ""eier"": 0, ""spam"": 1}
trg_classes = {""ham"": 0, ""eggs"": 1, ""spam"": 2}
corpus = [
AlignedSent([""ham"", ""eggs""], [""schinken"", ""schinken"", ""eier""]),
AlignedSent([""spam"", ""spam"", ""spam"", ""spam""], [""spam"", ""spam""]),
]
model5 = IBMModel5(corpus, 0, src_classes, trg_classes)
model5.set_uniform_probabilities(corpus)
expected_prob = 1.0 / (2 * 4)
self.assertEqual(model5.head_vacancy_table[4][4][0], expected_prob)
self.assertEqual(model5.head_vacancy_table[-3][1][2], expected_prob)
self.assertEqual(model5.non_head_vacancy_table[4][4][0], expected_prob)
self.assertEqual(model5.non_head_vacancy_table[-3][1][2], expected_prob)
",[],0,[],/test/unit/translate/test_ibm5.py_test_set_uniform_vacancy_probabilities_of_max_displacements
5624,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_ibm5.py_test_set_uniform_vacancy_probabilities_of_non_domain_values,"def test_set_uniform_vacancy_probabilities_of_non_domain_values(self):
src_classes = {""schinken"": 0, ""eier"": 0, ""spam"": 1}
trg_classes = {""ham"": 0, ""eggs"": 1, ""spam"": 2}
corpus = [
AlignedSent([""ham"", ""eggs""], [""schinken"", ""schinken"", ""eier""]),
AlignedSent([""spam"", ""spam"", ""spam"", ""spam""], [""spam"", ""spam""]),
]
model5 = IBMModel5(corpus, 0, src_classes, trg_classes)
model5.set_uniform_probabilities(corpus)
self.assertEqual(model5.head_vacancy_table[5][4][0], IBMModel.MIN_PROB)
self.assertEqual(model5.head_vacancy_table[-4][1][2], IBMModel.MIN_PROB)
self.assertEqual(model5.head_vacancy_table[4][0][0], IBMModel.MIN_PROB)
self.assertEqual(model5.non_head_vacancy_table[5][4][0], IBMModel.MIN_PROB)
self.assertEqual(model5.non_head_vacancy_table[-4][1][2], IBMModel.MIN_PROB)
",[],0,[],/test/unit/translate/test_ibm5.py_test_set_uniform_vacancy_probabilities_of_non_domain_values
5625,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_bleu.py_test_modified_precision,"def test_modified_precision(self):
""""""
Examples from the original BLEU paper
https://www.aclweb.org/anthology/P02-1040.pdf
""""""
ref1 = ""the cat is on the mat"".split()
ref2 = ""there is a cat on the mat"".split()
hyp1 = ""the the the the the the the"".split()
references = [ref1, ref2]
hyp1_unigram_precision = float(modified_precision(references, hyp1, n=1))
assert round(hyp1_unigram_precision, 4) == 0.2857
self.assertAlmostEqual(hyp1_unigram_precision, 0.28571428, places=4)
assert float(modified_precision(references, hyp1, n=2)) == 0.0
ref1 = str(
""It is a guide to action that ensures that the military ""
""will forever heed Party commands""
).split()
ref2 = str(
""It is the guiding principle which guarantees the military ""
""forces always being under the command of the Party""
).split()
ref3 = str(
""It is the practical guide for the army always to heed ""
""the directions of the party""
).split()
hyp1 = ""of the"".split()
references = [ref1, ref2, ref3]
assert float(modified_precision(references, hyp1, n=1)) == 1.0
assert float(modified_precision(references, hyp1, n=2)) == 1.0
hyp1 = str(
""It is a guide to action which ensures that the military ""
""always obeys the commands of the party""
).split()
hyp2 = str(
""It is to insure the troops forever hearing the activity ""
""guidebook that party direct""
).split()
references = [ref1, ref2, ref3]
hyp1_unigram_precision = float(modified_precision(references, hyp1, n=1))
hyp2_unigram_precision = float(modified_precision(references, hyp2, n=1))
self.assertAlmostEqual(hyp1_unigram_precision, 0.94444444, places=4)
self.assertAlmostEqual(hyp2_unigram_precision, 0.57142857, places=4)
assert round(hyp1_unigram_precision, 4) == 0.9444
assert round(hyp2_unigram_precision, 4) == 0.5714
hyp1_bigram_precision = float(modified_precision(references, hyp1, n=2))
hyp2_bigram_precision = float(modified_precision(references, hyp2, n=2))
self.assertAlmostEqual(hyp1_bigram_precision, 0.58823529, places=4)
self.assertAlmostEqual(hyp2_bigram_precision, 0.07692307, places=4)
assert round(hyp1_bigram_precision, 4) == 0.5882
assert round(hyp2_bigram_precision, 4) == 0.0769
",[],0,[],/test/unit/translate/test_bleu.py_test_modified_precision
5626,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_bleu.py_test_brevity_penalty,"def test_brevity_penalty(self):
references = [[""a""] * 11, [""a""] * 8]
hypothesis = [""a""] * 7
hyp_len = len(hypothesis)
closest_ref_len = closest_ref_length(references, hyp_len)
self.assertAlmostEqual(
brevity_penalty(closest_ref_len, hyp_len), 0.8669, places=4
)
references = [[""a""] * 11, [""a""] * 8, [""a""] * 6, [""a""] * 7]
hypothesis = [""a""] * 7
hyp_len = len(hypothesis)
closest_ref_len = closest_ref_length(references, hyp_len)
assert brevity_penalty(closest_ref_len, hyp_len) == 1.0
",[],0,[],/test/unit/translate/test_bleu.py_test_brevity_penalty
5627,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_bleu.py_test_zero_matches,"def test_zero_matches(self):
references = [""The candidate has no alignment to any of the references"".split()]
hypothesis = ""John loves Mary"".split()
for n in range(1, len(hypothesis)):
weights = (1.0 / n,) * n  # Uniform weights.
assert sentence_bleu(references, hypothesis, weights) == 0
",[],0,[],/test/unit/translate/test_bleu.py_test_zero_matches
5628,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_bleu.py_test_full_matches,"def test_full_matches(self):
references = [""John loves Mary"".split()]
hypothesis = ""John loves Mary"".split()
for n in range(1, len(hypothesis)):
weights = (1.0 / n,) * n  # Uniform weights.
assert sentence_bleu(references, hypothesis, weights) == 1.0
",[],0,[],/test/unit/translate/test_bleu.py_test_full_matches
5629,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_bleu.py_test_partial_matches_hypothesis_longer_than_reference,"def test_partial_matches_hypothesis_longer_than_reference(self):
references = [""John loves Mary"".split()]
hypothesis = ""John loves Mary who loves Mike"".split()
self.assertAlmostEqual(sentence_bleu(references, hypothesis), 0.0, places=4)
try:
self.assertWarns(UserWarning, sentence_bleu, references, hypothesis)
except AttributeError:
pass  # unittest.TestCase.assertWarns is only supported in Python >= 3.2.
",[],0,[],/test/unit/translate/test_bleu.py_test_partial_matches_hypothesis_longer_than_reference
5630,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_bleu.py_test_case_where_n_is_bigger_than_hypothesis_length,"def test_case_where_n_is_bigger_than_hypothesis_length(self):
references = [""John loves Mary ?"".split()]
hypothesis = ""John loves Mary"".split()
n = len(hypothesis) + 1  #
weights = (1.0 / n,) * n  # Uniform weights.
self.assertAlmostEqual(
sentence_bleu(references, hypothesis, weights), 0.0, places=4
)
try:
self.assertWarns(UserWarning, sentence_bleu, references, hypothesis)
except AttributeError:
pass  # unittest.TestCase.assertWarns is only supported in Python >= 3.2.
references = [""John loves Mary"".split()]
hypothesis = ""John loves Mary"".split()
self.assertAlmostEqual(
sentence_bleu(references, hypothesis, weights), 0.0, places=4
)
",[],0,[],/test/unit/translate/test_bleu.py_test_case_where_n_is_bigger_than_hypothesis_length
5631,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_bleu.py_test_empty_hypothesis,"def test_empty_hypothesis(self):
references = [""The candidate has no alignment to any of the references"".split()]
hypothesis = []
assert sentence_bleu(references, hypothesis) == 0
",[],0,[],/test/unit/translate/test_bleu.py_test_empty_hypothesis
5632,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_bleu.py_test_length_one_hypothesis,"def test_length_one_hypothesis(self):
references = [""The candidate has no alignment to any of the references"".split()]
hypothesis = [""Foo""]
method4 = SmoothingFunction().method4
try:
sentence_bleu(references, hypothesis, smoothing_function=method4)
except ValueError:
pass  # unittest.TestCase.assertWarns is only supported in Python >= 3.2.
",[],0,[],/test/unit/translate/test_bleu.py_test_length_one_hypothesis
5633,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_bleu.py_test_empty_references,"def test_empty_references(self):
references = [[]]
hypothesis = ""John loves Mary"".split()
assert sentence_bleu(references, hypothesis) == 0
",[],0,[],/test/unit/translate/test_bleu.py_test_empty_references
5634,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_bleu.py_test_empty_references_and_hypothesis,"def test_empty_references_and_hypothesis(self):
references = [[]]
hypothesis = []
assert sentence_bleu(references, hypothesis) == 0
",[],0,[],/test/unit/translate/test_bleu.py_test_empty_references_and_hypothesis
5635,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_bleu.py_test_reference_or_hypothesis_shorter_than_fourgrams,"def test_reference_or_hypothesis_shorter_than_fourgrams(self):
references = [""let it go"".split()]
hypothesis = ""let go it"".split()
self.assertAlmostEqual(sentence_bleu(references, hypothesis), 0.0, places=4)
try:
self.assertWarns(UserWarning, sentence_bleu, references, hypothesis)
except AttributeError:
pass  # unittest.TestCase.assertWarns is only supported in Python >= 3.2.
",[],0,[],/test/unit/translate/test_bleu.py_test_reference_or_hypothesis_shorter_than_fourgrams
5636,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_bleu.py_test_numpy_weights,"def test_numpy_weights(self):
references = [""The candidate has no alignment to any of the references"".split()]
hypothesis = ""John loves Mary"".split()
weights = np.array([0.25] * 4)
assert sentence_bleu(references, hypothesis, weights) == 0
",['array'],1,['array([0.25] * 4)'],/test/unit/translate/test_bleu.py_test_numpy_weights
5637,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_bleu.py_test_corpus_bleu_with_bad_sentence,"def test_corpus_bleu_with_bad_sentence(self):
hyp = ""Teo S yb , oe uNb , R , T t , , t Tue Ar saln S , , 5istsi l , 5oe R ulO sae oR R""
ref = str(
""Their tasks include changing a pump on the faulty stokehold .""
""Likewise , two species that are very similar in morphology ""
""were distinguished using genetics .""
)
references = [[ref.split()]]
hypotheses = [hyp.split()]
try:  # Check that the warning is raised since no. of 2-grams < 0.
with self.assertWarns(UserWarning):
self.assertAlmostEqual(
corpus_bleu(references, hypotheses), 0.0, places=4
)
except (
AttributeError
):  # unittest.TestCase.assertWarns is only supported in Python >= 3.2.
self.assertAlmostEqual(corpus_bleu(references, hypotheses), 0.0, places=4)
",[],0,[],/test/unit/translate/test_bleu.py_test_corpus_bleu_with_bad_sentence
5638,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_bleu.py_test_corpus_bleu_with_multiple_weights,"def test_corpus_bleu_with_multiple_weights(self):
hyp1 = [
""It"",
""is"",
""a"",
""guide"",
""to"",
""action"",
""which"",
""ensures"",
""that"",
""the"",
""military"",
""always"",
""obeys"",
""the"",
""commands"",
""of"",
""the"",
""party"",
]
ref1a = [
""It"",
""is"",
""a"",
""guide"",
""to"",
""action"",
""that"",
""ensures"",
""that"",
""the"",
""military"",
""will"",
""forever"",
""heed"",
""Party"",
""commands"",
]
ref1b = [
""It"",
""is"",
""the"",
""guiding"",
""principle"",
""which"",
""guarantees"",
""the"",
""military"",
""forces"",
""always"",
""being"",
""under"",
""the"",
""command"",
""of"",
""the"",
""Party"",
]
ref1c = [
""It"",
""is"",
""the"",
""practical"",
""guide"",
""for"",
""the"",
""army"",
""always"",
""to"",
""heed"",
""the"",
""directions"",
""of"",
""the"",
""party"",
]
hyp2 = [
""he"",
""read"",
""the"",
""book"",
""because"",
""he"",
""was"",
""interested"",
""in"",
""world"",
""history"",
]
ref2a = [
""he"",
""was"",
""interested"",
""in"",
""world"",
""history"",
""because"",
""he"",
""read"",
""the"",
""book"",
]
weight_1 = (1, 0, 0, 0)
weight_2 = (0.25, 0.25, 0.25, 0.25)
weight_3 = (0, 0, 0, 0, 1)
bleu_scores = corpus_bleu(
list_of_references=[[ref1a, ref1b, ref1c], [ref2a]],
hypotheses=[hyp1, hyp2],
weights=[weight_1, weight_2, weight_3],
)
assert bleu_scores[0] == corpus_bleu(
[[ref1a, ref1b, ref1c], [ref2a]], [hyp1, hyp2], weight_1
)
assert bleu_scores[1] == corpus_bleu(
[[ref1a, ref1b, ref1c], [ref2a]], [hyp1, hyp2], weight_2
)
assert bleu_scores[2] == corpus_bleu(
[[ref1a, ref1b, ref1c], [ref2a]], [hyp1, hyp2], weight_3
)
",[],0,[],/test/unit/translate/test_bleu.py_test_corpus_bleu_with_multiple_weights
5639,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_ibm4.py_test_set_uniform_distortion_probabilities_of_max_displacements,"def test_set_uniform_distortion_probabilities_of_max_displacements(self):
src_classes = {""schinken"": 0, ""eier"": 0, ""spam"": 1}
trg_classes = {""ham"": 0, ""eggs"": 1, ""spam"": 2}
corpus = [
AlignedSent([""ham"", ""eggs""], [""schinken"", ""schinken"", ""eier""]),
AlignedSent([""spam"", ""spam"", ""spam"", ""spam""], [""spam"", ""spam""]),
]
model4 = IBMModel4(corpus, 0, src_classes, trg_classes)
model4.set_uniform_probabilities(corpus)
expected_prob = 1.0 / (2 * (4 - 1))
self.assertEqual(model4.head_distortion_table[3][0][0], expected_prob)
self.assertEqual(model4.head_distortion_table[-3][1][2], expected_prob)
self.assertEqual(model4.non_head_distortion_table[3][0], expected_prob)
self.assertEqual(model4.non_head_distortion_table[-3][2], expected_prob)
",[],0,[],/test/unit/translate/test_ibm4.py_test_set_uniform_distortion_probabilities_of_max_displacements
5640,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_ibm4.py_test_set_uniform_distortion_probabilities_of_non_domain_values,"def test_set_uniform_distortion_probabilities_of_non_domain_values(self):
src_classes = {""schinken"": 0, ""eier"": 0, ""spam"": 1}
trg_classes = {""ham"": 0, ""eggs"": 1, ""spam"": 2}
corpus = [
AlignedSent([""ham"", ""eggs""], [""schinken"", ""schinken"", ""eier""]),
AlignedSent([""spam"", ""spam"", ""spam"", ""spam""], [""spam"", ""spam""]),
]
model4 = IBMModel4(corpus, 0, src_classes, trg_classes)
model4.set_uniform_probabilities(corpus)
self.assertEqual(model4.head_distortion_table[4][0][0], IBMModel.MIN_PROB)
self.assertEqual(model4.head_distortion_table[100][1][2], IBMModel.MIN_PROB)
self.assertEqual(model4.non_head_distortion_table[4][0], IBMModel.MIN_PROB)
self.assertEqual(model4.non_head_distortion_table[100][2], IBMModel.MIN_PROB)
",[],0,[],/test/unit/translate/test_ibm4.py_test_set_uniform_distortion_probabilities_of_non_domain_values
5641,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_ibm1.py_test_set_uniform_translation_probabilities,"def test_set_uniform_translation_probabilities(self):
corpus = [
AlignedSent([""ham"", ""eggs""], [""schinken"", ""schinken"", ""eier""]),
AlignedSent([""spam"", ""spam"", ""spam"", ""spam""], [""spam"", ""spam""]),
]
model1 = IBMModel1(corpus, 0)
model1.set_uniform_probabilities(corpus)
self.assertEqual(model1.translation_table[""ham""][""eier""], 1.0 / 3)
self.assertEqual(model1.translation_table[""eggs""][None], 1.0 / 3)
",[],0,[],/test/unit/translate/test_ibm1.py_test_set_uniform_translation_probabilities
5642,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_ibm1.py_test_set_uniform_translation_probabilities_of_non_domain_values,"def test_set_uniform_translation_probabilities_of_non_domain_values(self):
corpus = [
AlignedSent([""ham"", ""eggs""], [""schinken"", ""schinken"", ""eier""]),
AlignedSent([""spam"", ""spam"", ""spam"", ""spam""], [""spam"", ""spam""]),
]
model1 = IBMModel1(corpus, 0)
model1.set_uniform_probabilities(corpus)
self.assertEqual(model1.translation_table[""parrot""][""eier""], IBMModel.MIN_PROB)
",[],0,[],/test/unit/translate/test_ibm1.py_test_set_uniform_translation_probabilities_of_non_domain_values
5643,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_gdfa.py_test_from_eflomal_outputs,"def test_from_eflomal_outputs(self):
""""""
Testing GDFA with first 10 eflomal outputs from issue #1829
https://github.com/nltk/nltk/issues/1829
""""""
forwards = [
""0-0 1-2"",
""0-0 1-1"",
""0-0 2-1 3-2 4-3 5-4 6-5 7-6 8-7 7-8 9-9 10-10 9-11 11-12 12-13 13-14"",
""0-0 1-1 1-2 2-3 3-4 4-5 4-6 5-7 6-8 8-9 9-10"",
""0-0 14-1 15-2 16-3 20-5 21-6 22-7 5-8 6-9 7-10 8-11 9-12 10-13 11-14 12-15 13-16 14-17 17-18 18-19 19-20 20-21 23-22 24-23 25-24 26-25 27-27 28-28 29-29 30-30 31-31"",
""0-0 1-1 0-2 2-3"",
""0-0 2-2 4-4"",
""0-0 1-1 2-3 3-4 5-5 7-6 8-7 9-8 10-9 11-10 12-11 13-12 14-13 15-14 16-16 17-17 18-18 19-19 20-20"",
""3-0 4-1 6-2 5-3 6-4 7-5 8-6 9-7 10-8 11-9 16-10 9-12 10-13 12-14"",
""1-0"",
]
backwards = [
""0-0 1-2"",
""0-0 1-1"",
""0-0 2-1 3-2 4-3 5-4 6-5 7-6 8-7 9-8 10-10 11-12 12-11 13-13"",
""0-0 1-2 2-3 3-4 4-6 6-8 7-5 8-7 9-8"",
""0-0 1-8 2-9 3-10 4-11 5-12 6-11 8-13 9-14 10-15 11-16 12-17 13-18 14-19 15-20 16-21 17-22 18-23 19-24 20-29 21-30 22-31 23-2 24-3 25-4 26-5 27-5 28-6 29-7 30-28 31-31"",
""0-0 1-1 2-3"",
""0-0 1-1 2-3 4-4"",
""0-0 1-1 2-3 3-4 5-5 7-6 8-7 9-8 10-9 11-10 12-11 13-12 14-13 15-14 16-16 17-17 18-18 19-19 20-16 21-18"",
""0-0 1-1 3-2 4-1 5-3 6-4 7-5 8-6 9-7 10-8 11-9 12-8 13-9 14-8 15-9 16-10"",
""1-0"",
]
source_lens = [2, 3, 3, 15, 11, 33, 4, 6, 23, 18]
target_lens = [2, 4, 3, 16, 12, 33, 5, 6, 22, 16]
expected = [
[(0, 0), (1, 2)],
[(0, 0), (1, 1)],
[
(0, 0),
(2, 1),
(3, 2),
(4, 3),
(5, 4),
(6, 5),
(7, 6),
(8, 7),
(10, 10),
(11, 12),
],
[
(0, 0),
(1, 1),
(1, 2),
(2, 3),
(3, 4),
(4, 5),
(4, 6),
(5, 7),
(6, 8),
(7, 5),
(8, 7),
(8, 9),
(9, 8),
(9, 10),
],
[
(0, 0),
(1, 8),
(2, 9),
(3, 10),
(4, 11),
(5, 8),
(6, 9),
(6, 11),
(7, 10),
(8, 11),
(31, 31),
],
[(0, 0), (0, 2), (1, 1), (2, 3)],
[(0, 0), (1, 1), (2, 2), (2, 3), (4, 4)],
[
(0, 0),
(1, 1),
(2, 3),
(3, 4),
(5, 5),
(7, 6),
(8, 7),
(9, 8),
(10, 9),
(11, 10),
(12, 11),
(13, 12),
(14, 13),
(15, 14),
(16, 16),
(17, 17),
(18, 18),
(19, 19),
],
[
(0, 0),
(1, 1),
(3, 0),
(3, 2),
(4, 1),
(5, 3),
(6, 2),
(6, 4),
(7, 5),
(8, 6),
(9, 7),
(9, 12),
(10, 8),
(10, 13),
(11, 9),
(12, 8),
(12, 14),
(13, 9),
(14, 8),
(15, 9),
(16, 10),
],
[(1, 0)],
[
(0, 0),
(1, 1),
(3, 2),
(4, 3),
(5, 4),
(6, 5),
(7, 6),
(9, 10),
(10, 12),
(11, 13),
(12, 14),
(13, 15),
],
]
for fw, bw, src_len, trg_len, expect in zip(
forwards, backwards, source_lens, target_lens, expected
):
self.assertListEqual(expect, grow_diag_final_and(src_len, trg_len, fw, bw))
",[],0,[],/test/unit/translate/test_gdfa.py_test_from_eflomal_outputs
5644,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_ibm2.py_test_set_uniform_alignment_probabilities,"def test_set_uniform_alignment_probabilities(self):
corpus = [
AlignedSent([""ham"", ""eggs""], [""schinken"", ""schinken"", ""eier""]),
AlignedSent([""spam"", ""spam"", ""spam"", ""spam""], [""spam"", ""spam""]),
]
model2 = IBMModel2(corpus, 0)
model2.set_uniform_probabilities(corpus)
self.assertEqual(model2.alignment_table[0][1][3][2], 1.0 / 4)
self.assertEqual(model2.alignment_table[2][4][2][4], 1.0 / 3)
",[],0,[],/test/unit/translate/test_ibm2.py_test_set_uniform_alignment_probabilities
5645,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_ibm2.py_test_set_uniform_alignment_probabilities_of_non_domain_values,"def test_set_uniform_alignment_probabilities_of_non_domain_values(self):
corpus = [
AlignedSent([""ham"", ""eggs""], [""schinken"", ""schinken"", ""eier""]),
AlignedSent([""spam"", ""spam"", ""spam"", ""spam""], [""spam"", ""spam""]),
]
model2 = IBMModel2(corpus, 0)
model2.set_uniform_probabilities(corpus)
self.assertEqual(model2.alignment_table[99][1][3][2], IBMModel.MIN_PROB)
self.assertEqual(model2.alignment_table[2][99][2][4], IBMModel.MIN_PROB)
",[],0,[],/test/unit/translate/test_ibm2.py_test_set_uniform_alignment_probabilities_of_non_domain_values
5646,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_ibm3.py_test_set_uniform_distortion_probabilities,"def test_set_uniform_distortion_probabilities(self):
corpus = [
AlignedSent([""ham"", ""eggs""], [""schinken"", ""schinken"", ""eier""]),
AlignedSent([""spam"", ""spam"", ""spam"", ""spam""], [""spam"", ""spam""]),
]
model3 = IBMModel3(corpus, 0)
model3.set_uniform_probabilities(corpus)
self.assertEqual(model3.distortion_table[1][0][3][2], 1.0 / 2)
self.assertEqual(model3.distortion_table[4][2][2][4], 1.0 / 4)
",[],0,[],/test/unit/translate/test_ibm3.py_test_set_uniform_distortion_probabilities
5647,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_ibm3.py_test_set_uniform_distortion_probabilities_of_non_domain_values,"def test_set_uniform_distortion_probabilities_of_non_domain_values(self):
corpus = [
AlignedSent([""ham"", ""eggs""], [""schinken"", ""schinken"", ""eier""]),
AlignedSent([""spam"", ""spam"", ""spam"", ""spam""], [""spam"", ""spam""]),
]
model3 = IBMModel3(corpus, 0)
model3.set_uniform_probabilities(corpus)
self.assertEqual(model3.distortion_table[0][0][3][2], IBMModel.MIN_PROB)
self.assertEqual(model3.distortion_table[9][2][2][4], IBMModel.MIN_PROB)
self.assertEqual(model3.distortion_table[2][9][2][4], IBMModel.MIN_PROB)
",[],0,[],/test/unit/translate/test_ibm3.py_test_set_uniform_distortion_probabilities_of_non_domain_values
5648,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_ibm_model.py_test_vocabularies_are_initialized,"def test_vocabularies_are_initialized(self):
parallel_corpora = [
AlignedSent([""one"", ""two"", ""three"", ""four""], [""un"", ""deux"", ""trois""]),
AlignedSent([""five"", ""one"", ""six""], [""quatre"", ""cinq"", ""six""]),
AlignedSent([], [""sept""]),
]
ibm_model = IBMModel(parallel_corpora)
self.assertEqual(len(ibm_model.src_vocab), 8)
self.assertEqual(len(ibm_model.trg_vocab), 6)
",[],0,[],/test/unit/translate/test_ibm_model.py_test_vocabularies_are_initialized
5649,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_ibm_model.py_test_vocabularies_are_initialized_even_with_empty_corpora,"def test_vocabularies_are_initialized_even_with_empty_corpora(self):
parallel_corpora = []
ibm_model = IBMModel(parallel_corpora)
self.assertEqual(len(ibm_model.src_vocab), 1)  # addition of NULL token
self.assertEqual(len(ibm_model.trg_vocab), 0)
",[],0,[],/test/unit/translate/test_ibm_model.py_test_vocabularies_are_initialized_even_with_empty_corpora
5650,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_ibm_model.py_test_best_model2_alignment_handles_empty_src_sentence,"def test_best_model2_alignment_handles_empty_src_sentence(self):
sentence_pair = AlignedSent(TestIBMModel.__TEST_TRG_SENTENCE, [])
ibm_model = IBMModel([])
a_info = ibm_model.best_model2_alignment(sentence_pair)
self.assertEqual(a_info.alignment[1:], (0, 0, 0))
self.assertEqual(a_info.cepts, [[1, 2, 3]])
",[],0,[],/test/unit/translate/test_ibm_model.py_test_best_model2_alignment_handles_empty_src_sentence
5651,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_ibm_model.py_test_best_model2_alignment_handles_empty_trg_sentence,"def test_best_model2_alignment_handles_empty_trg_sentence(self):
sentence_pair = AlignedSent([], TestIBMModel.__TEST_SRC_SENTENCE)
ibm_model = IBMModel([])
a_info = ibm_model.best_model2_alignment(sentence_pair)
self.assertEqual(a_info.alignment[1:], ())
self.assertEqual(a_info.cepts, [[], [], [], [], []])
",[],0,[],/test/unit/translate/test_ibm_model.py_test_best_model2_alignment_handles_empty_trg_sentence
5652,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_ibm_model.py_test_neighboring_finds_neighbor_alignments,"def test_neighboring_finds_neighbor_alignments(self):
a_info = AlignmentInfo(
(0, 3, 2),
(None, ""des"", ""œufs"", ""verts""),
(""UNUSED"", ""green"", ""eggs""),
[[], [], [2], [1]],
)
ibm_model = IBMModel([])
neighbors = ibm_model.neighboring(a_info)
neighbor_alignments = set()
for neighbor in neighbors:
neighbor_alignments.add(neighbor.alignment)
expected_alignments = {
(0, 0, 2),
(0, 1, 2),
(0, 2, 2),
(0, 3, 0),
(0, 3, 1),
(0, 3, 3),
(0, 2, 3),
(0, 3, 2),
}
self.assertEqual(neighbor_alignments, expected_alignments)
",[],0,[],/test/unit/translate/test_ibm_model.py_test_neighboring_finds_neighbor_alignments
5653,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_ibm_model.py_test_neighboring_sets_neighbor_alignment_info,"def test_neighboring_sets_neighbor_alignment_info(self):
a_info = AlignmentInfo(
(0, 3, 2),
(None, ""des"", ""œufs"", ""verts""),
(""UNUSED"", ""green"", ""eggs""),
[[], [], [2], [1]],
)
ibm_model = IBMModel([])
neighbors = ibm_model.neighboring(a_info)
for neighbor in neighbors:
if neighbor.alignment == (0, 2, 2):
moved_alignment = neighbor
elif neighbor.alignment == (0, 3, 2):
swapped_alignment = neighbor
self.assertEqual(moved_alignment.cepts, [[], [], [1, 2], []])
self.assertEqual(swapped_alignment.cepts, [[], [], [2], [1]])
",[],0,[],/test/unit/translate/test_ibm_model.py_test_neighboring_sets_neighbor_alignment_info
5654,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_ibm_model.py_test_neighboring_returns_neighbors_with_pegged_alignment,"def test_neighboring_returns_neighbors_with_pegged_alignment(self):
a_info = AlignmentInfo(
(0, 3, 2),
(None, ""des"", ""œufs"", ""verts""),
(""UNUSED"", ""green"", ""eggs""),
[[], [], [2], [1]],
)
ibm_model = IBMModel([])
neighbors = ibm_model.neighboring(a_info, 2)
neighbor_alignments = set()
for neighbor in neighbors:
neighbor_alignments.add(neighbor.alignment)
expected_alignments = {
(0, 0, 2),
(0, 1, 2),
(0, 2, 2),
(0, 3, 2),
}
self.assertEqual(neighbor_alignments, expected_alignments)
",[],0,[],/test/unit/translate/test_ibm_model.py_test_neighboring_returns_neighbors_with_pegged_alignment
5655,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_ibm_model.py_test_hillclimb,"def test_hillclimb(self):
initial_alignment = AlignmentInfo((0, 3, 2), None, None, None)
",[],0,[],/test/unit/translate/test_ibm_model.py_test_hillclimb
5656,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_ibm_model.py_neighboring_mock,"def neighboring_mock(a, j):
if a.alignment == (0, 3, 2):
return {
AlignmentInfo((0, 2, 2), None, None, None),
AlignmentInfo((0, 1, 1), None, None, None),
}
elif a.alignment == (0, 2, 2):
return {
AlignmentInfo((0, 3, 3), None, None, None),
AlignmentInfo((0, 4, 4), None, None, None),
}
return set()
",[],0,[],/test/unit/translate/test_ibm_model.py_neighboring_mock
5657,/home/amandapotts/git/nltk/nltk/test/unit/translate/test_ibm_model.py_prob_t_a_given_s_mock,"def prob_t_a_given_s_mock(a):
prob_values = {
(0, 3, 2): 0.5,
(0, 2, 2): 0.6,
(0, 1, 1): 0.4,
(0, 3, 3): 0.6,
(0, 4, 4): 0.7,
}
return prob_values.get(a.alignment, 0.01)
",[],0,[],/test/unit/translate/test_ibm_model.py_prob_t_a_given_s_mock
5658,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_vocabulary,"def vocabulary():
return Vocabulary([""a"", ""b"", ""c"", ""d"", ""z"", ""<s>"", ""</s>""], unk_cutoff=1)
",[],0,[],/test/unit/lm/test_models.py_vocabulary
5659,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_training_data,"def training_data():
return [[""a"", ""b"", ""c"", ""d""], [""e"", ""g"", ""a"", ""d"", ""b"", ""e""]]
",[],0,[],/test/unit/lm/test_models.py_training_data
5660,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_bigram_training_data,"def bigram_training_data(training_data):
return [list(padded_everygrams(2, sent)) for sent in training_data]
",[],0,[],/test/unit/lm/test_models.py_bigram_training_data
5661,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_trigram_training_data,"def trigram_training_data(training_data):
return [list(padded_everygrams(3, sent)) for sent in training_data]
",[],0,[],/test/unit/lm/test_models.py_trigram_training_data
5662,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_mle_bigram_model,"def mle_bigram_model(vocabulary, bigram_training_data):
model = MLE(2, vocabulary=vocabulary)
model.fit(bigram_training_data)
return model
",[],0,[],/test/unit/lm/test_models.py_mle_bigram_model
5663,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_test_mle_bigram_scores,"def test_mle_bigram_scores(mle_bigram_model, word, context, expected_score):
assert pytest.approx(mle_bigram_model.score(word, context), 1e-4) == expected_score
",[],0,[],/test/unit/lm/test_models.py_test_mle_bigram_scores
5664,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_test_mle_bigram_logscore_for_zero_score,"def test_mle_bigram_logscore_for_zero_score(mle_bigram_model):
assert math.isinf(mle_bigram_model.logscore(""d"", [""e""]))
",[],0,[],/test/unit/lm/test_models.py_test_mle_bigram_logscore_for_zero_score
5665,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_test_mle_bigram_entropy_perplexity_seen,"def test_mle_bigram_entropy_perplexity_seen(mle_bigram_model):
trained = [
(""<s>"", ""a""),
(""a"", ""b""),
(""b"", ""<UNK>""),
(""<UNK>"", ""a""),
(""a"", ""d""),
(""d"", ""</s>""),
]
H = 1.0975
perplexity = 2.1398
assert pytest.approx(mle_bigram_model.entropy(trained), 1e-4) == H
assert pytest.approx(mle_bigram_model.perplexity(trained), 1e-4) == perplexity
",[],0,[],/test/unit/lm/test_models.py_test_mle_bigram_entropy_perplexity_seen
5666,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_test_mle_bigram_entropy_perplexity_unseen,"def test_mle_bigram_entropy_perplexity_unseen(mle_bigram_model):
untrained = [(""<s>"", ""a""), (""a"", ""c""), (""c"", ""d""), (""d"", ""</s>"")]
assert math.isinf(mle_bigram_model.entropy(untrained))
assert math.isinf(mle_bigram_model.perplexity(untrained))
",[],0,[],/test/unit/lm/test_models.py_test_mle_bigram_entropy_perplexity_unseen
5667,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_test_mle_bigram_entropy_perplexity_unigrams,"def test_mle_bigram_entropy_perplexity_unigrams(mle_bigram_model):
H = 3.0095
perplexity = 8.0529
text = [(""<s>"",), (""a"",), (""c"",), (""-"",), (""d"",), (""c"",), (""</s>"",)]
assert pytest.approx(mle_bigram_model.entropy(text), 1e-4) == H
assert pytest.approx(mle_bigram_model.perplexity(text), 1e-4) == perplexity
",[],0,[],/test/unit/lm/test_models.py_test_mle_bigram_entropy_perplexity_unigrams
5668,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_mle_trigram_model,"def mle_trigram_model(trigram_training_data, vocabulary):
model = MLE(order=3, vocabulary=vocabulary)
model.fit(trigram_training_data)
return model
",[],0,[],/test/unit/lm/test_models.py_mle_trigram_model
5669,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_test_mle_trigram_scores,"def test_mle_trigram_scores(mle_trigram_model, word, context, expected_score):
assert pytest.approx(mle_trigram_model.score(word, context), 1e-4) == expected_score
",[],0,[],/test/unit/lm/test_models.py_test_mle_trigram_scores
5670,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_lidstone_bigram_model,"def lidstone_bigram_model(bigram_training_data, vocabulary):
model = Lidstone(0.1, order=2, vocabulary=vocabulary)
model.fit(bigram_training_data)
return model
",[],0,[],/test/unit/lm/test_models.py_lidstone_bigram_model
5671,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_test_lidstone_bigram_score,"def test_lidstone_bigram_score(lidstone_bigram_model, word, context, expected_score):
assert (
pytest.approx(lidstone_bigram_model.score(word, context), 1e-4)
== expected_score
)
",[],0,[],/test/unit/lm/test_models.py_test_lidstone_bigram_score
5672,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_test_lidstone_entropy_perplexity,"def test_lidstone_entropy_perplexity(lidstone_bigram_model):
text = [
(""<s>"", ""a""),
(""a"", ""c""),
(""c"", ""<UNK>""),
(""<UNK>"", ""d""),
(""d"", ""c""),
(""c"", ""</s>""),
]
H = 4.0917
perplexity = 17.0504
assert pytest.approx(lidstone_bigram_model.entropy(text), 1e-4) == H
assert pytest.approx(lidstone_bigram_model.perplexity(text), 1e-4) == perplexity
",[],0,[],/test/unit/lm/test_models.py_test_lidstone_entropy_perplexity
5673,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_lidstone_trigram_model,"def lidstone_trigram_model(trigram_training_data, vocabulary):
model = Lidstone(0.1, order=3, vocabulary=vocabulary)
model.fit(trigram_training_data)
return model
",[],0,[],/test/unit/lm/test_models.py_lidstone_trigram_model
5674,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_test_lidstone_trigram_score,"def test_lidstone_trigram_score(lidstone_trigram_model, word, context, expected_score):
assert (
pytest.approx(lidstone_trigram_model.score(word, context), 1e-4)
== expected_score
)
",[],0,[],/test/unit/lm/test_models.py_test_lidstone_trigram_score
5675,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_laplace_bigram_model,"def laplace_bigram_model(bigram_training_data, vocabulary):
model = Laplace(2, vocabulary=vocabulary)
model.fit(bigram_training_data)
return model
",[],0,[],/test/unit/lm/test_models.py_laplace_bigram_model
5676,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_test_laplace_bigram_score,"def test_laplace_bigram_score(laplace_bigram_model, word, context, expected_score):
assert (
pytest.approx(laplace_bigram_model.score(word, context), 1e-4) == expected_score
)
",[],0,[],/test/unit/lm/test_models.py_test_laplace_bigram_score
5677,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_test_laplace_bigram_entropy_perplexity,"def test_laplace_bigram_entropy_perplexity(laplace_bigram_model):
text = [
(""<s>"", ""a""),
(""a"", ""c""),
(""c"", ""<UNK>""),
(""<UNK>"", ""d""),
(""d"", ""c""),
(""c"", ""</s>""),
]
H = 3.1275
perplexity = 8.7393
assert pytest.approx(laplace_bigram_model.entropy(text), 1e-4) == H
assert pytest.approx(laplace_bigram_model.perplexity(text), 1e-4) == perplexity
",[],0,[],/test/unit/lm/test_models.py_test_laplace_bigram_entropy_perplexity
5678,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_test_laplace_gamma,"def test_laplace_gamma(laplace_bigram_model):
assert laplace_bigram_model.gamma == 1
",[],0,[],/test/unit/lm/test_models.py_test_laplace_gamma
5679,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_wittenbell_trigram_model,"def wittenbell_trigram_model(trigram_training_data, vocabulary):
model = WittenBellInterpolated(3, vocabulary=vocabulary)
model.fit(trigram_training_data)
return model
",[],0,[],/test/unit/lm/test_models.py_wittenbell_trigram_model
5680,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_test_wittenbell_trigram_score,"def test_wittenbell_trigram_score(
wittenbell_trigram_model, word, context, expected_score
",[],0,[],/test/unit/lm/test_models.py_test_wittenbell_trigram_score
5681,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_kneserney_trigram_model,"def kneserney_trigram_model(trigram_training_data, vocabulary):
model = KneserNeyInterpolated(order=3, discount=0.75, vocabulary=vocabulary)
model.fit(trigram_training_data)
return model
",[],0,[],/test/unit/lm/test_models.py_kneserney_trigram_model
5682,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_test_kneserney_trigram_score,"def test_kneserney_trigram_score(
kneserney_trigram_model, word, context, expected_score
",[],0,[],/test/unit/lm/test_models.py_test_kneserney_trigram_score
5683,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_absolute_discounting_trigram_model,"def absolute_discounting_trigram_model(trigram_training_data, vocabulary):
model = AbsoluteDiscountingInterpolated(order=3, vocabulary=vocabulary)
model.fit(trigram_training_data)
return model
",[],0,[],/test/unit/lm/test_models.py_absolute_discounting_trigram_model
5684,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_test_absolute_discounting_trigram_score,"def test_absolute_discounting_trigram_score(
absolute_discounting_trigram_model, word, context, expected_score
",[],0,[],/test/unit/lm/test_models.py_test_absolute_discounting_trigram_score
5685,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_stupid_backoff_trigram_model,"def stupid_backoff_trigram_model(trigram_training_data, vocabulary):
model = StupidBackoff(order=3, vocabulary=vocabulary)
model.fit(trigram_training_data)
return model
",[],0,[],/test/unit/lm/test_models.py_stupid_backoff_trigram_model
5686,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_test_stupid_backoff_trigram_score,"def test_stupid_backoff_trigram_score(
stupid_backoff_trigram_model, word, context, expected_score
",[],0,[],/test/unit/lm/test_models.py_test_stupid_backoff_trigram_score
5687,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_kneserney_bigram_model,"def kneserney_bigram_model(bigram_training_data, vocabulary):
model = KneserNeyInterpolated(order=2, vocabulary=vocabulary)
model.fit(bigram_training_data)
return model
",[],0,[],/test/unit/lm/test_models.py_kneserney_bigram_model
5688,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_test_sums_to_1,"def test_sums_to_1(model_fixture, context, request):
model = request.getfixturevalue(model_fixture)
scores_for_context = sum(model.score(w, context) for w in model.vocab)
assert pytest.approx(scores_for_context, 1e-7) == 1.0
",[],0,[],/test/unit/lm/test_models.py_test_sums_to_1
5689,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_test_generate_one_no_context,"def test_generate_one_no_context(mle_trigram_model):
assert mle_trigram_model.generate(random_seed=3) == ""<UNK>""
",[],0,[],/test/unit/lm/test_models.py_test_generate_one_no_context
5690,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_test_generate_one_from_limiting_context,"def test_generate_one_from_limiting_context(mle_trigram_model):
assert mle_trigram_model.generate(text_seed=[""c""]) == ""d""
assert mle_trigram_model.generate(text_seed=[""b"", ""c""]) == ""d""
assert mle_trigram_model.generate(text_seed=[""a"", ""c""]) == ""d""
",[],0,[],/test/unit/lm/test_models.py_test_generate_one_from_limiting_context
5691,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_test_generate_one_from_varied_context,"def test_generate_one_from_varied_context(mle_trigram_model):
assert mle_trigram_model.generate(text_seed=(""a"", ""<s>""), random_seed=2) == ""a""
",[],0,[],/test/unit/lm/test_models.py_test_generate_one_from_varied_context
5692,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_test_generate_cycle,"def test_generate_cycle(mle_trigram_model):
more_training_text = [padded_everygrams(mle_trigram_model.order, list(""bdbdbd""))]
mle_trigram_model.fit(more_training_text)
assert mle_trigram_model.generate(7, text_seed=(""b"", ""d""), random_seed=5) == [
""b"",
""d"",
""b"",
""d"",
""b"",
""d"",
""</s>"",
]
",[],0,[],/test/unit/lm/test_models.py_test_generate_cycle
5693,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_test_generate_with_text_seed,"def test_generate_with_text_seed(mle_trigram_model):
assert mle_trigram_model.generate(5, text_seed=(""<s>"", ""e""), random_seed=3) == [
""<UNK>"",
""a"",
""d"",
""b"",
""<UNK>"",
]
",[],0,[],/test/unit/lm/test_models.py_test_generate_with_text_seed
5694,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_test_generate_oov_text_seed,"def test_generate_oov_text_seed(mle_trigram_model):
assert mle_trigram_model.generate(
text_seed=(""aliens"",), random_seed=3
) == mle_trigram_model.generate(text_seed=(""<UNK>"",), random_seed=3)
",[],0,[],/test/unit/lm/test_models.py_test_generate_oov_text_seed
5695,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_models.py_test_generate_None_text_seed,"def test_generate_None_text_seed(mle_trigram_model):
with pytest.raises(TypeError):
mle_trigram_model.generate(text_seed=(None,))
assert mle_trigram_model.generate(
text_seed=None, random_seed=3
) == mle_trigram_model.generate(random_seed=3)
",[],0,[],/test/unit/lm/test_models.py_test_generate_None_text_seed
5696,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_preprocessing.py_test_padded_everygram_pipeline,"def test_padded_everygram_pipeline(self):
expected_train = [
[
(""<s>"",),
(""<s>"", ""a""),
(""a"",),
(""a"", ""b""),
(""b"",),
(""b"", ""c""),
(""c"",),
(""c"", ""</s>""),
(""</s>"",),
]
]
expected_vocab = [""<s>"", ""a"", ""b"", ""c"", ""</s>""]
train_data, vocab_data = padded_everygram_pipeline(2, [[""a"", ""b"", ""c""]])
self.assertEqual([list(sent) for sent in train_data], expected_train)
self.assertEqual(list(vocab_data), expected_vocab)
",[],0,[],/test/unit/lm/test_preprocessing.py_test_padded_everygram_pipeline
5697,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_vocabulary.py_setUpClass,"def setUpClass(cls):
cls.vocab = Vocabulary(
[""z"", ""a"", ""b"", ""c"", ""f"", ""d"", ""e"", ""g"", ""a"", ""d"", ""b"", ""e"", ""w""],
unk_cutoff=2,
)
",[],0,[],/test/unit/lm/test_vocabulary.py_setUpClass
5698,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_vocabulary.py_test_truthiness,"def test_truthiness(self):
self.assertTrue(self.vocab)
",[],0,[],/test/unit/lm/test_vocabulary.py_test_truthiness
5699,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_vocabulary.py_test_cutoff_value_set_correctly,"def test_cutoff_value_set_correctly(self):
self.assertEqual(self.vocab.cutoff, 2)
",[],0,[],/test/unit/lm/test_vocabulary.py_test_cutoff_value_set_correctly
5700,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_vocabulary.py_test_unable_to_change_cutoff,"def test_unable_to_change_cutoff(self):
with self.assertRaises(AttributeError):
self.vocab.cutoff = 3
",[],0,[],/test/unit/lm/test_vocabulary.py_test_unable_to_change_cutoff
5701,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_vocabulary.py_test_cutoff_setter_checks_value,"def test_cutoff_setter_checks_value(self):
with self.assertRaises(ValueError) as exc_info:
Vocabulary(""abc"", unk_cutoff=0)
expected_error_msg = ""Cutoff value cannot be less than 1. Got: 0""
self.assertEqual(expected_error_msg, str(exc_info.exception))
",[],0,[],/test/unit/lm/test_vocabulary.py_test_cutoff_setter_checks_value
5702,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_vocabulary.py_test_counts_set_correctly,"def test_counts_set_correctly(self):
self.assertEqual(self.vocab.counts[""a""], 2)
self.assertEqual(self.vocab.counts[""b""], 2)
self.assertEqual(self.vocab.counts[""c""], 1)
",[],0,[],/test/unit/lm/test_vocabulary.py_test_counts_set_correctly
5703,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_vocabulary.py_test_membership_check_respects_cutoff,"def test_membership_check_respects_cutoff(self):
self.assertTrue(""a"" in self.vocab)
self.assertFalse(""c"" in self.vocab)
self.assertFalse(""z"" in self.vocab)
",[],0,[],/test/unit/lm/test_vocabulary.py_test_membership_check_respects_cutoff
5704,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_vocabulary.py_test_vocab_len_respects_cutoff,"def test_vocab_len_respects_cutoff(self):
self.assertEqual(5, len(self.vocab))
",[],0,[],/test/unit/lm/test_vocabulary.py_test_vocab_len_respects_cutoff
5705,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_vocabulary.py_test_vocab_iter_respects_cutoff,"def test_vocab_iter_respects_cutoff(self):
vocab_counts = [""a"", ""b"", ""c"", ""d"", ""e"", ""f"", ""g"", ""w"", ""z""]
vocab_items = [""a"", ""b"", ""d"", ""e"", ""<UNK>""]
self.assertCountEqual(vocab_counts, list(self.vocab.counts.keys()))
self.assertCountEqual(vocab_items, list(self.vocab))
",[],0,[],/test/unit/lm/test_vocabulary.py_test_vocab_iter_respects_cutoff
5706,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_vocabulary.py_test_update_empty_vocab,"def test_update_empty_vocab(self):
empty = Vocabulary(unk_cutoff=2)
self.assertEqual(len(empty), 0)
self.assertFalse(empty)
self.assertIn(empty.unk_label, empty)
empty.update(list(""abcde""))
self.assertIn(empty.unk_label, empty)
",[],0,[],/test/unit/lm/test_vocabulary.py_test_update_empty_vocab
5707,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_vocabulary.py_test_lookup,"def test_lookup(self):
self.assertEqual(self.vocab.lookup(""a""), ""a"")
self.assertEqual(self.vocab.lookup(""c""), ""<UNK>"")
",[],0,[],/test/unit/lm/test_vocabulary.py_test_lookup
5708,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_vocabulary.py_test_lookup_iterables,"def test_lookup_iterables(self):
self.assertEqual(self.vocab.lookup([""a"", ""b""]), (""a"", ""b""))
self.assertEqual(self.vocab.lookup((""a"", ""b"")), (""a"", ""b""))
self.assertEqual(self.vocab.lookup((""a"", ""c"")), (""a"", ""<UNK>""))
self.assertEqual(
self.vocab.lookup(map(str, range(3))), (""<UNK>"", ""<UNK>"", ""<UNK>"")
)
",[],0,[],/test/unit/lm/test_vocabulary.py_test_lookup_iterables
5709,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_vocabulary.py_test_lookup_empty_iterables,"def test_lookup_empty_iterables(self):
self.assertEqual(self.vocab.lookup(()), ())
self.assertEqual(self.vocab.lookup([]), ())
self.assertEqual(self.vocab.lookup(iter([])), ())
self.assertEqual(self.vocab.lookup(n for n in range(0, 0)), ())
",[],0,[],/test/unit/lm/test_vocabulary.py_test_lookup_empty_iterables
5710,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_vocabulary.py_test_lookup_recursive,"def test_lookup_recursive(self):
self.assertEqual(
self.vocab.lookup([[""a"", ""b""], [""a"", ""c""]]), ((""a"", ""b""), (""a"", ""<UNK>""))
)
self.assertEqual(self.vocab.lookup([[""a"", ""b""], ""c""]), ((""a"", ""b""), ""<UNK>""))
self.assertEqual(self.vocab.lookup([[[[[""a"", ""b""]]]]]), (((((""a"", ""b""),),),),))
",[],0,[],/test/unit/lm/test_vocabulary.py_test_lookup_recursive
5711,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_vocabulary.py_test_lookup_None,"def test_lookup_None(self):
with self.assertRaises(TypeError):
self.vocab.lookup(None)
with self.assertRaises(TypeError):
list(self.vocab.lookup([None, None]))
",[],0,[],/test/unit/lm/test_vocabulary.py_test_lookup_None
5712,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_vocabulary.py_test_lookup_int,"def test_lookup_int(self):
with self.assertRaises(TypeError):
self.vocab.lookup(1)
with self.assertRaises(TypeError):
list(self.vocab.lookup([1, 2]))
",[],0,[],/test/unit/lm/test_vocabulary.py_test_lookup_int
5713,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_vocabulary.py_test_lookup_empty_str,"def test_lookup_empty_str(self):
self.assertEqual(self.vocab.lookup(""""), ""<UNK>"")
",[],0,[],/test/unit/lm/test_vocabulary.py_test_lookup_empty_str
5714,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_vocabulary.py_test_eqality,"def test_eqality(self):
v1 = Vocabulary([""a"", ""b"", ""c""], unk_cutoff=1)
v2 = Vocabulary([""a"", ""b"", ""c""], unk_cutoff=1)
v3 = Vocabulary([""a"", ""b"", ""c""], unk_cutoff=1, unk_label=""blah"")
v4 = Vocabulary([""a"", ""b""], unk_cutoff=1)
self.assertEqual(v1, v2)
self.assertNotEqual(v1, v3)
self.assertNotEqual(v1, v4)
",[],0,[],/test/unit/lm/test_vocabulary.py_test_eqality
5715,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_vocabulary.py_test_str,"def test_str(self):
self.assertEqual(
str(self.vocab), ""<Vocabulary with cutoff=2 unk_label='<UNK>' and 5 items>""
)
",[],0,[],/test/unit/lm/test_vocabulary.py_test_str
5716,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_vocabulary.py_test_creation_with_counter,"def test_creation_with_counter(self):
self.assertEqual(
self.vocab,
Vocabulary(
Counter(
[""z"", ""a"", ""b"", ""c"", ""f"", ""d"", ""e"", ""g"", ""a"", ""d"", ""b"", ""e"", ""w""]
),
unk_cutoff=2,
),
)
",[],0,[],/test/unit/lm/test_vocabulary.py_test_creation_with_counter
5717,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_vocabulary.py_test_len_is_constant,"def test_len_is_constant(self):
small_vocab = Vocabulary(""abcde"")
from nltk.corpus.europarl_raw import english
large_vocab = Vocabulary(english.words())
small_vocab_len_time = timeit(""len(small_vocab)"", globals=locals())
large_vocab_len_time = timeit(""len(large_vocab)"", globals=locals())
self.assertAlmostEqual(small_vocab_len_time, large_vocab_len_time, places=1)
",[],0,[],/test/unit/lm/test_vocabulary.py_test_len_is_constant
5718,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_counter.py_setup_class,"def setup_class(self):
text = [list(""abcd""), list(""egdbe"")]
self.trigram_counter = NgramCounter(
everygrams(sent, max_len=3) for sent in text
)
self.bigram_counter = NgramCounter(everygrams(sent, max_len=2) for sent in text)
self.case = unittest.TestCase()
",[],0,[],/test/unit/lm/test_counter.py_setup_class
5719,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_counter.py_test_N,"def test_N(self):
assert self.bigram_counter.N() == 16
assert self.trigram_counter.N() == 21
",[],0,[],/test/unit/lm/test_counter.py_test_N
5720,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_counter.py_test_counter_len_changes_with_lookup,"def test_counter_len_changes_with_lookup(self):
assert len(self.bigram_counter) == 2
self.bigram_counter[50]
assert len(self.bigram_counter) == 3
",[],0,[],/test/unit/lm/test_counter.py_test_counter_len_changes_with_lookup
5721,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_counter.py_test_ngram_order_access_unigrams,"def test_ngram_order_access_unigrams(self):
assert self.bigram_counter[1] == self.bigram_counter.unigrams
",[],0,[],/test/unit/lm/test_counter.py_test_ngram_order_access_unigrams
5722,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_counter.py_test_ngram_conditional_freqdist,"def test_ngram_conditional_freqdist(self):
case = unittest.TestCase()
expected_trigram_contexts = [
(""a"", ""b""),
(""b"", ""c""),
(""e"", ""g""),
(""g"", ""d""),
(""d"", ""b""),
]
expected_bigram_contexts = [(""a"",), (""b"",), (""d"",), (""e"",), (""c"",), (""g"",)]
bigrams = self.trigram_counter[2]
trigrams = self.trigram_counter[3]
self.case.assertCountEqual(expected_bigram_contexts, bigrams.conditions())
self.case.assertCountEqual(expected_trigram_contexts, trigrams.conditions())
",[],0,[],/test/unit/lm/test_counter.py_test_ngram_conditional_freqdist
5723,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_counter.py_test_bigram_counts_seen_ngrams,"def test_bigram_counts_seen_ngrams(self):
assert self.bigram_counter[[""a""]][""b""] == 1
assert self.bigram_counter[[""b""]][""c""] == 1
",[],0,[],/test/unit/lm/test_counter.py_test_bigram_counts_seen_ngrams
5724,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_counter.py_test_bigram_counts_unseen_ngrams,"def test_bigram_counts_unseen_ngrams(self):
assert self.bigram_counter[[""b""]][""z""] == 0
",[],0,[],/test/unit/lm/test_counter.py_test_bigram_counts_unseen_ngrams
5725,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_counter.py_test_unigram_counts_seen_words,"def test_unigram_counts_seen_words(self):
assert self.bigram_counter[""b""] == 2
",[],0,[],/test/unit/lm/test_counter.py_test_unigram_counts_seen_words
5726,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_counter.py_test_unigram_counts_completely_unseen_words,"def test_unigram_counts_completely_unseen_words(self):
assert self.bigram_counter[""z""] == 0
",[],0,[],/test/unit/lm/test_counter.py_test_unigram_counts_completely_unseen_words
5727,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_counter.py_setup_class,"def setup_class(self):
self.counter = NgramCounter()
self.case = unittest.TestCase()
",[],0,[],/test/unit/lm/test_counter.py_setup_class
5728,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_counter.py_test_empty_inputs,"def test_empty_inputs(self, case):
test = NgramCounter(case)
assert 2 not in test
assert test[1] == FreqDist()
",[],0,[],/test/unit/lm/test_counter.py_test_empty_inputs
5729,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_counter.py_test_train_on_unigrams,"def test_train_on_unigrams(self):
words = list(""abcd"")
counter = NgramCounter([[(w,) for w in words]])
assert not counter[3]
assert not counter[2]
self.case.assertCountEqual(words, counter[1].keys())
",[],0,[],/test/unit/lm/test_counter.py_test_train_on_unigrams
5730,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_counter.py_test_train_on_illegal_sentences,"def test_train_on_illegal_sentences(self):
str_sent = [""Check"", ""this"", ""out"", ""!""]
list_sent = [[""Check"", ""this""], [""this"", ""out""], [""out"", ""!""]]
with pytest.raises(TypeError):
NgramCounter([str_sent])
with pytest.raises(TypeError):
NgramCounter([list_sent])
",[],0,[],/test/unit/lm/test_counter.py_test_train_on_illegal_sentences
5731,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_counter.py_test_train_on_bigrams,"def test_train_on_bigrams(self):
bigram_sent = [(""a"", ""b""), (""c"", ""d"")]
counter = NgramCounter([bigram_sent])
assert not bool(counter[3])
",[],0,[],/test/unit/lm/test_counter.py_test_train_on_bigrams
5732,/home/amandapotts/git/nltk/nltk/test/unit/lm/test_counter.py_test_train_on_mix,"def test_train_on_mix(self):
mixed_sent = [(""a"", ""b""), (""c"", ""d""), (""e"", ""f"", ""g""), (""h"",)]
counter = NgramCounter([mixed_sent])
unigrams = [""h""]
bigram_contexts = [(""a"",), (""c"",)]
trigram_contexts = [(""e"", ""f"")]
self.case.assertCountEqual(unigrams, counter[1].keys())
self.case.assertCountEqual(bigram_contexts, counter[2].keys())
self.case.assertCountEqual(trigram_contexts, counter[3].keys())
",[],0,[],/test/unit/lm/test_counter.py_test_train_on_mix
5733,/home/amandapotts/git/nltk/nltk/classify/senna.py___init__,"def __init__(self, senna_path, operations, encoding=""utf-8""):
self._encoding = encoding
self._path = path.normpath(senna_path) + sep
exe_file_1 = self.executable(self._path)
if not path.isfile(exe_file_1):
if ""SENNA"" in environ:
self._path = path.normpath(environ[""SENNA""]) + sep
exe_file_2 = self.executable(self._path)
if not path.isfile(exe_file_2):
raise LookupError(
""Senna executable expected at %s or %s but not found""
% (exe_file_1, exe_file_2)
)
self.operations = operations
",[],0,[],/classify/senna.py___init__
5734,/home/amandapotts/git/nltk/nltk/classify/senna.py_executable,"def executable(self, base_path):
""""""
The function that determines the system specific binary that should be
used in the pipeline. In case, the system is not known the default senna binary will
be used.
""""""
os_name = system()
if os_name == ""Linux"":
bits = architecture()[0]
if bits == ""64bit"":
return path.join(base_path, ""senna-linux64"")
return path.join(base_path, ""senna-linux32"")
if os_name == ""Windows"":
return path.join(base_path, ""senna-win32.exe"")
if os_name == ""Darwin"":
return path.join(base_path, ""senna-osx"")
return path.join(base_path, ""senna"")
",[],0,[],/classify/senna.py_executable
5735,/home/amandapotts/git/nltk/nltk/classify/senna.py__map,"def _map(self):
""""""
A method that calculates the order of the columns that SENNA pipeline
will output the tags into. This depends on the operations being ordered.
""""""
_map = {}
i = 1
for operation in Senna.SUPPORTED_OPERATIONS:
if operation in self.operations:
_map[operation] = i
i += 1
return _map
",[],0,[],/classify/senna.py__map
5736,/home/amandapotts/git/nltk/nltk/classify/senna.py_tag,"def tag(self, tokens):
""""""
Applies the specified operation(s) on a list of tokens.
""""""
return self.tag_sents([tokens])[0]
",[],0,[],/classify/senna.py_tag
5737,/home/amandapotts/git/nltk/nltk/classify/senna.py_tag_sents,"def tag_sents(self, sentences):
""""""
Applies the tag method over a list of sentences. This method will return a
list of dictionaries. Every dictionary will contain a word with its
calculated annotations/tags.
""""""
encoding = self._encoding
if not path.isfile(self.executable(self._path)):
raise LookupError(
""Senna executable expected at %s but not found""
% self.executable(self._path)
)
_senna_cmd = [
self.executable(self._path),
""-path"",
self._path,
""-usrtokens"",
""-iobtags"",
]
_senna_cmd.extend([""-"" + op for op in self.operations])
_input = ""\n"".join("" "".join(x) for x in sentences) + ""\n""
if isinstance(_input, str) and encoding:
_input = _input.encode(encoding)
p = Popen(_senna_cmd, stdin=PIPE, stdout=PIPE, stderr=PIPE)
(stdout, stderr) = p.communicate(input=_input)
senna_output = stdout
if p.returncode != 0:
raise RuntimeError(""Senna command failed! Details: %s"" % stderr)
if encoding:
senna_output = stdout.decode(encoding)
map_ = self._map()
tagged_sentences = [[]]
sentence_index = 0
token_index = 0
for tagged_word in senna_output.strip().split(""\n""):
if not tagged_word:
tagged_sentences.append([])
sentence_index += 1
token_index = 0
continue
tags = tagged_word.split(""\t"")
result = {}
for tag in map_:
result[tag] = tags[map_[tag]].strip()
try:
result[""word""] = sentences[sentence_index][token_index]
except IndexError as e:
raise IndexError(
""Misalignment error occurred at sentence number %d. Possible reason""
"" is that the sentence size exceeded the maximum size. Check the ""
""documentation of Senna class for more information.""
% sentence_index
) from e
tagged_sentences[-1].append(result)
token_index += 1
return tagged_sentences
",[],0,[],/classify/senna.py_tag_sents
5738,/home/amandapotts/git/nltk/nltk/classify/textcat.py___init__,"def __init__(self):
if not re:
raise OSError(
""classify.textcat requires the regex module that ""
""supports unicode. Try '$ pip install regex' and ""
""see https://pypi.python.org/pypi/regex for ""
""further details.""
)
from nltk.corpus import crubadan
self._corpus = crubadan
for lang in self._corpus.langs():
self._corpus.lang_freq(lang)
",[],0,[],/classify/textcat.py___init__
5739,/home/amandapotts/git/nltk/nltk/classify/textcat.py_remove_punctuation,"def remove_punctuation(self, text):
""""""Get rid of punctuation except apostrophes""""""
return re.sub(r""[^\P{P}\']+"", """", text)
",[],0,[],/classify/textcat.py_remove_punctuation
5740,/home/amandapotts/git/nltk/nltk/classify/textcat.py_profile,"def profile(self, text):
""""""Create FreqDist of trigrams within text""""""
from nltk import FreqDist, word_tokenize
clean_text = self.remove_punctuation(text)
tokens = word_tokenize(clean_text)
fingerprint = FreqDist()
for t in tokens:
token_trigram_tuples = trigrams(self._START_CHAR + t + self._END_CHAR)
token_trigrams = ["""".join(tri) for tri in token_trigram_tuples]
for cur_trigram in token_trigrams:
if cur_trigram in fingerprint:
fingerprint[cur_trigram] += 1
else:
fingerprint[cur_trigram] = 1
return fingerprint
",[],0,[],/classify/textcat.py_profile
5741,/home/amandapotts/git/nltk/nltk/classify/textcat.py_calc_dist,"def calc_dist(self, lang, trigram, text_profile):
""""""Calculate the ""out-of-place"" measure between the
text and language profile for a single trigram""""""
lang_fd = self._corpus.lang_freq(lang)
dist = 0
if trigram in lang_fd:
idx_lang_profile = list(lang_fd.keys()).index(trigram)
idx_text = list(text_profile.keys()).index(trigram)
dist = abs(idx_lang_profile - idx_text)
else:
dist = maxsize
return dist
",[],0,[],/classify/textcat.py_calc_dist
5742,/home/amandapotts/git/nltk/nltk/classify/textcat.py_lang_dists,"def lang_dists(self, text):
""""""Calculate the ""out-of-place"" measure between
the text and all languages""""""
distances = {}
profile = self.profile(text)
for lang in self._corpus._all_lang_freq.keys():
lang_dist = 0
for trigram in profile:
lang_dist += self.calc_dist(lang, trigram, profile)
distances[lang] = lang_dist
return distances
",[],0,[],/classify/textcat.py_lang_dists
5743,/home/amandapotts/git/nltk/nltk/classify/textcat.py_guess_language,"def guess_language(self, text):
""""""Find the language with the min distance
to the text and return its ISO 639-3 code""""""
self.last_distances = self.lang_dists(text)
return min(self.last_distances, key=self.last_distances.get)
",[],0,[],/classify/textcat.py_guess_language
5744,/home/amandapotts/git/nltk/nltk/classify/textcat.py_demo,"def demo():
from nltk.corpus import udhr
langs = [
""Kurdish-UTF8"",
""Abkhaz-UTF8"",
""Farsi_Persian-UTF8"",
""Hindi-UTF8"",
""Hawaiian-UTF8"",
""Russian-UTF8"",
""Vietnamese-UTF8"",
""Serbian_Srpski-UTF8"",
""Esperanto-UTF8"",
]
friendly = {
""kmr"": ""Northern Kurdish"",
""abk"": ""Abkhazian"",
""pes"": ""Iranian Persian"",
""hin"": ""Hindi"",
""haw"": ""Hawaiian"",
""rus"": ""Russian"",
""vie"": ""Vietnamese"",
""srp"": ""Serbian"",
""epo"": ""Esperanto"",
}
tc = TextCat()
for cur_lang in langs:
raw_sentences = udhr.sents(cur_lang)
rows = len(raw_sentences) - 1
cols = list(map(len, raw_sentences))
sample = """"
for i in range(0, rows):
cur_sent = "" "" + "" "".join([raw_sentences[i][j] for j in range(0, cols[i])])
sample += cur_sent
print(""Language snippet: "" + sample[0:140] + ""..."")
guess = tc.guess_language(sample)
print(f""Language detection: {guess} ({friendly[guess]})"")
print(""#"" * 140)
",[],0,[],/classify/textcat.py_demo
5745,/home/amandapotts/git/nltk/nltk/classify/decisiontree.py___init__,"def __init__(self, label, feature_name=None, decisions=None, default=None):
""""""
:param label: The most likely label for tokens that reach
this node in the decision tree.  If this decision tree
has no children, then this label will be assigned to
any token that reaches this decision tree.
:param feature_name: The name of the feature that this
decision tree selects for.
:param decisions: A dictionary mapping from feature values
for the feature identified by ``feature_name`` to
child decision trees.
:param default: The child that will be used if the value of
feature ``feature_name`` does not match any of the keys in
``decisions``.  This is used when constructing binary
decision trees.
""""""
self._label = label
self._fname = feature_name
self._decisions = decisions
self._default = default
",[],0,[],/classify/decisiontree.py___init__
5746,/home/amandapotts/git/nltk/nltk/classify/decisiontree.py_labels,"def labels(self):
labels = [self._label]
if self._decisions is not None:
for dt in self._decisions.values():
labels.extend(dt.labels())
if self._default is not None:
labels.extend(self._default.labels())
return list(set(labels))
",[],0,[],/classify/decisiontree.py_labels
5747,/home/amandapotts/git/nltk/nltk/classify/decisiontree.py_classify,"def classify(self, featureset):
if self._fname is None:
return self._label
fval = featureset.get(self._fname)
if fval in self._decisions:
return self._decisions[fval].classify(featureset)
elif self._default is not None:
return self._default.classify(featureset)
else:
return self._label
",[],0,[],/classify/decisiontree.py_classify
5748,/home/amandapotts/git/nltk/nltk/classify/decisiontree.py_error,"def error(self, labeled_featuresets):
errors = 0
for featureset, label in labeled_featuresets:
if self.classify(featureset) != label:
errors += 1
return errors / len(labeled_featuresets)
",[],0,[],/classify/decisiontree.py_error
5749,/home/amandapotts/git/nltk/nltk/classify/decisiontree.py___str__,"def __str__(self):
return self.pretty_format()
",[],0,[],/classify/decisiontree.py___str__
5750,/home/amandapotts/git/nltk/nltk/classify/decisiontree.py_train,"def train(
labeled_featuresets,
entropy_cutoff=0.05,
depth_cutoff=100,
support_cutoff=10,
binary=False,
feature_values=None,
verbose=False,
",[],0,[],/classify/decisiontree.py_train
5751,/home/amandapotts/git/nltk/nltk/classify/decisiontree.py_leaf,"def leaf(labeled_featuresets):
label = FreqDist(label for (featureset, label) in labeled_featuresets).max()
return DecisionTreeClassifier(label)
",[],0,[],/classify/decisiontree.py_leaf
5752,/home/amandapotts/git/nltk/nltk/classify/decisiontree.py_stump,"def stump(feature_name, labeled_featuresets):
label = FreqDist(label for (featureset, label) in labeled_featuresets).max()
freqs = defaultdict(FreqDist)  # freq(label|value)
for featureset, label in labeled_featuresets:
feature_value = featureset.get(feature_name)
freqs[feature_value][label] += 1
decisions = {val: DecisionTreeClassifier(freqs[val].max()) for val in freqs}
return DecisionTreeClassifier(label, feature_name, decisions)
",[],0,[],/classify/decisiontree.py_stump
5753,/home/amandapotts/git/nltk/nltk/classify/decisiontree.py_refine,"def refine(
self,
labeled_featuresets,
entropy_cutoff,
depth_cutoff,
support_cutoff,
binary=False,
feature_values=None,
verbose=False,
",[],0,[],/classify/decisiontree.py_refine
5754,/home/amandapotts/git/nltk/nltk/classify/decisiontree.py_best_stump,"def best_stump(feature_names, labeled_featuresets, verbose=False):
best_stump = DecisionTreeClassifier.leaf(labeled_featuresets)
best_error = best_stump.error(labeled_featuresets)
for fname in feature_names:
stump = DecisionTreeClassifier.stump(fname, labeled_featuresets)
stump_error = stump.error(labeled_featuresets)
if stump_error < best_error:
best_error = stump_error
best_stump = stump
if verbose:
print(
""best stump for {:6d} toks uses {:20} err={:6.4f}"".format(
len(labeled_featuresets), best_stump._fname, best_error
)
)
return best_stump
",[],0,[],/classify/decisiontree.py_best_stump
5755,/home/amandapotts/git/nltk/nltk/classify/decisiontree.py_binary_stump,"def binary_stump(feature_name, feature_value, labeled_featuresets):
label = FreqDist(label for (featureset, label) in labeled_featuresets).max()
pos_fdist = FreqDist()
neg_fdist = FreqDist()
for featureset, label in labeled_featuresets:
if featureset.get(feature_name) == feature_value:
pos_fdist[label] += 1
else:
neg_fdist[label] += 1
decisions = {}
default = label
if pos_fdist.N() > 0:
decisions = {feature_value: DecisionTreeClassifier(pos_fdist.max())}
if neg_fdist.N() > 0:
default = DecisionTreeClassifier(neg_fdist.max())
return DecisionTreeClassifier(label, feature_name, decisions, default)
",[],0,[],/classify/decisiontree.py_binary_stump
5756,/home/amandapotts/git/nltk/nltk/classify/decisiontree.py_best_binary_stump,"def best_binary_stump(
feature_names, labeled_featuresets, feature_values, verbose=False
",[],0,[],/classify/decisiontree.py_best_binary_stump
5757,/home/amandapotts/git/nltk/nltk/classify/decisiontree.py_f,"def f(x):
return DecisionTreeClassifier.train(x, binary=True, verbose=True)
",[],0,[],/classify/decisiontree.py_f
5758,/home/amandapotts/git/nltk/nltk/classify/decisiontree.py_demo,"def demo():
from nltk.classify.util import binary_names_demo_features, names_demo
classifier = names_demo(
f, binary_names_demo_features  # DecisionTreeClassifier.train,
)
print(classifier.pretty_format(depth=7))
print(classifier.pseudocode(depth=7))
",[],0,[],/classify/decisiontree.py_demo
5759,/home/amandapotts/git/nltk/nltk/classify/naivebayes.py___init__,"def __init__(self, label_probdist, feature_probdist):
""""""
:param label_probdist: P(label), the probability distribution
over labels.  It is expressed as a ``ProbDistI`` whose
samples are labels.  I.e., P(label) =
``label_probdist.prob(label)``.
:param feature_probdist: P(fname=fval|label), the probability
distribution for feature values, given labels.  It is
expressed as a dictionary whose keys are ``(label, fname)``
pairs and whose values are ``ProbDistI`` objects over feature
values.  I.e., P(fname=fval|label) =
``feature_probdist[label,fname].prob(fval)``.  If a given
``(label,fname)`` is not a key in ``feature_probdist``, then
it is assumed that the corresponding P(fname=fval|label)
is 0 for all values of ``fval``.
""""""
self._label_probdist = label_probdist
self._feature_probdist = feature_probdist
self._labels = list(label_probdist.samples())
",[],0,[],/classify/naivebayes.py___init__
5760,/home/amandapotts/git/nltk/nltk/classify/naivebayes.py_labels,"def labels(self):
return self._labels
",[],0,[],/classify/naivebayes.py_labels
5761,/home/amandapotts/git/nltk/nltk/classify/naivebayes.py_classify,"def classify(self, featureset):
return self.prob_classify(featureset).max()
",[],0,[],/classify/naivebayes.py_classify
5762,/home/amandapotts/git/nltk/nltk/classify/naivebayes.py_prob_classify,"def prob_classify(self, featureset):
featureset = featureset.copy()
for fname in list(featureset.keys()):
for label in self._labels:
if (label, fname) in self._feature_probdist:
break
else:
del featureset[fname]
logprob = {}
for label in self._labels:
logprob[label] = self._label_probdist.logprob(label)
for label in self._labels:
for fname, fval in featureset.items():
if (label, fname) in self._feature_probdist:
feature_probs = self._feature_probdist[label, fname]
logprob[label] += feature_probs.logprob(fval)
else:
logprob[label] += sum_logs([])  # = -INF.
return DictionaryProbDist(logprob, normalize=True, log=True)
",[],0,[],/classify/naivebayes.py_prob_classify
5763,/home/amandapotts/git/nltk/nltk/classify/naivebayes.py_show_most_informative_features,"def show_most_informative_features(self, n=10):
cpdist = self._feature_probdist
print(""Most Informative Features"")
for fname, fval in self.most_informative_features(n):
",[],0,[],/classify/naivebayes.py_show_most_informative_features
5764,/home/amandapotts/git/nltk/nltk/classify/naivebayes.py_train,"def train(cls, labeled_featuresets, estimator=ELEProbDist):
""""""
:param labeled_featuresets: A list of classified featuresets,
i.e., a list of tuples ``(featureset, label)``.
""""""
label_freqdist = FreqDist()
feature_freqdist = defaultdict(FreqDist)
feature_values = defaultdict(set)
fnames = set()
for featureset, label in labeled_featuresets:
label_freqdist[label] += 1
for fname, fval in featureset.items():
feature_freqdist[label, fname][fval] += 1
feature_values[fname].add(fval)
fnames.add(fname)
for label in label_freqdist:
num_samples = label_freqdist[label]
for fname in fnames:
count = feature_freqdist[label, fname].N()
if num_samples - count > 0:
feature_freqdist[label, fname][None] += num_samples - count
feature_values[fname].add(None)
label_probdist = estimator(label_freqdist)
feature_probdist = {}
for (label, fname), freqdist in feature_freqdist.items():
probdist = estimator(freqdist, bins=len(feature_values[fname]))
feature_probdist[label, fname] = probdist
return cls(label_probdist, feature_probdist)
",[],0,[],/classify/naivebayes.py_train
5765,/home/amandapotts/git/nltk/nltk/classify/naivebayes.py_demo,"def demo():
from nltk.classify.util import names_demo
classifier = names_demo(NaiveBayesClassifier.train)
classifier.show_most_informative_features()
",[],0,[],/classify/naivebayes.py_demo
5766,/home/amandapotts/git/nltk/nltk/classify/rte_classify.py___init__,"def __init__(self, rtepair, stop=True, use_lemmatize=False):
""""""
:param rtepair: a ``RTEPair`` from which features should be extracted
:param stop: if ``True``, stopwords are thrown away.
:type stop: bool
""""""
self.stop = stop
self.stopwords = {
""a"",
""the"",
""it"",
""they"",
""of"",
""in"",
""to"",
""is"",
""have"",
""are"",
""were"",
""and"",
""very"",
""."",
"","",
}
self.negwords = {""no"", ""not"", ""never"", ""failed"", ""rejected"", ""denied""}
tokenizer = RegexpTokenizer(r""[\w.@:/]+|\w+|\$[\d.]+"")
self.text_tokens = tokenizer.tokenize(rtepair.text)
self.hyp_tokens = tokenizer.tokenize(rtepair.hyp)
self.text_words = set(self.text_tokens)
self.hyp_words = set(self.hyp_tokens)
if use_lemmatize:
self.text_words = {self._lemmatize(token) for token in self.text_tokens}
self.hyp_words = {self._lemmatize(token) for token in self.hyp_tokens}
if self.stop:
self.text_words = self.text_words - self.stopwords
self.hyp_words = self.hyp_words - self.stopwords
self._overlap = self.hyp_words & self.text_words
self._hyp_extra = self.hyp_words - self.text_words
self._txt_extra = self.text_words - self.hyp_words
",[],0,[],/classify/rte_classify.py___init__
5767,/home/amandapotts/git/nltk/nltk/classify/rte_classify.py_overlap,"def overlap(self, toktype, debug=False):
""""""
Compute the overlap between text and hypothesis.
:param toktype: distinguish Named Entities from ordinary words
:type toktype: 'ne' or 'word'
""""""
ne_overlap = {token for token in self._overlap if self._ne(token)}
if toktype == ""ne"":
if debug:
print(""ne overlap"", ne_overlap)
return ne_overlap
elif toktype == ""word"":
if debug:
print(""word overlap"", self._overlap - ne_overlap)
return self._overlap - ne_overlap
else:
raise ValueError(""Type not recognized:'%s'"" % toktype)
",[],0,[],/classify/rte_classify.py_overlap
5768,/home/amandapotts/git/nltk/nltk/classify/rte_classify.py_hyp_extra,"def hyp_extra(self, toktype, debug=True):
""""""
Compute the extraneous material in the hypothesis.
:param toktype: distinguish Named Entities from ordinary words
:type toktype: 'ne' or 'word'
""""""
ne_extra = {token for token in self._hyp_extra if self._ne(token)}
if toktype == ""ne"":
return ne_extra
elif toktype == ""word"":
return self._hyp_extra - ne_extra
else:
raise ValueError(""Type not recognized: '%s'"" % toktype)
",[],0,[],/classify/rte_classify.py_hyp_extra
5769,/home/amandapotts/git/nltk/nltk/classify/rte_classify.py__ne,"def _ne(token):
""""""
This just assumes that words in all caps or titles are
named entities.
:type token: str
""""""
if token.istitle() or token.isupper():
return True
return False
",[],0,[],/classify/rte_classify.py__ne
5770,/home/amandapotts/git/nltk/nltk/classify/rte_classify.py__lemmatize,"def _lemmatize(word):
""""""
Use morphy from WordNet to find the base form of verbs.
""""""
from nltk.corpus import wordnet as wn
lemma = wn.morphy(word, pos=wn.VERB)
if lemma is not None:
return lemma
return word
",[],0,[],/classify/rte_classify.py__lemmatize
5771,/home/amandapotts/git/nltk/nltk/classify/rte_classify.py_rte_features,"def rte_features(rtepair):
extractor = RTEFeatureExtractor(rtepair)
features = {}
features[""alwayson""] = True
features[""word_overlap""] = len(extractor.overlap(""word""))
features[""word_hyp_extra""] = len(extractor.hyp_extra(""word""))
features[""ne_overlap""] = len(extractor.overlap(""ne""))
features[""ne_hyp_extra""] = len(extractor.hyp_extra(""ne""))
features[""neg_txt""] = len(extractor.negwords & extractor.text_words)
features[""neg_hyp""] = len(extractor.negwords & extractor.hyp_words)
return features
",[],0,[],/classify/rte_classify.py_rte_features
5772,/home/amandapotts/git/nltk/nltk/classify/rte_classify.py_rte_featurize,"def rte_featurize(rte_pairs):
return [(rte_features(pair), pair.value) for pair in rte_pairs]
",[],0,[],/classify/rte_classify.py_rte_featurize
5773,/home/amandapotts/git/nltk/nltk/classify/rte_classify.py_rte_classifier,"def rte_classifier(algorithm, sample_N=None):
from nltk.corpus import rte as rte_corpus
train_set = rte_corpus.pairs([""rte1_dev.xml"", ""rte2_dev.xml"", ""rte3_dev.xml""])
test_set = rte_corpus.pairs([""rte1_test.xml"", ""rte2_test.xml"", ""rte3_test.xml""])
if sample_N is not None:
train_set = train_set[:sample_N]
test_set = test_set[:sample_N]
featurized_train_set = rte_featurize(train_set)
featurized_test_set = rte_featurize(test_set)
print(""Training classifier..."")
if algorithm in [""megam""]:  # MEGAM based algorithms.
clf = MaxentClassifier.train(featurized_train_set, algorithm)
elif algorithm in [""GIS"", ""IIS""]:  # Use default GIS/IIS MaxEnt algorithm
clf = MaxentClassifier.train(featurized_train_set, algorithm)
else:
err_msg = str(
""RTEClassifier only supports these algorithms:\n ""
""'megam', 'GIS', 'IIS'.\n""
)
raise Exception(err_msg)
print(""Testing classifier..."")
acc = accuracy(clf, featurized_test_set)
print(""Accuracy: %6.4f"" % acc)
return clf
",[],0,[],/classify/rte_classify.py_rte_classifier
5774,/home/amandapotts/git/nltk/nltk/classify/maxent.py___init__,"def __init__(self, encoding, weights, logarithmic=True):
""""""
Construct a new maxent classifier model.  Typically, new
classifier models are created using the ``train()`` method.
:type encoding: MaxentFeatureEncodingI
:param encoding: An encoding that is used to convert the
featuresets that are given to the ``classify`` method into
joint-feature vectors, which are used by the maxent
classifier model.
:type weights: list of float
:param weights:  The feature weight vector for this classifier.
:type logarithmic: bool
:param logarithmic: If false, then use non-logarithmic weights.
""""""
self._encoding = encoding
self._weights = weights
self._logarithmic = logarithmic
assert encoding.length() == len(weights)
",[],0,[],/classify/maxent.py___init__
5775,/home/amandapotts/git/nltk/nltk/classify/maxent.py_labels,"def labels(self):
return self._encoding.labels()
",[],0,[],/classify/maxent.py_labels
5776,/home/amandapotts/git/nltk/nltk/classify/maxent.py_set_weights,"def set_weights(self, new_weights):
""""""
Set the feature weight vector for this classifier.
:param new_weights: The new feature weight vector.
:type new_weights: list of float
""""""
self._weights = new_weights
assert self._encoding.length() == len(new_weights)
",[],0,[],/classify/maxent.py_set_weights
5777,/home/amandapotts/git/nltk/nltk/classify/maxent.py_weights,"def weights(self):
""""""
:return: The feature weight vector for this classifier.
:rtype: list of float
""""""
return self._weights
",[],0,[],/classify/maxent.py_weights
5778,/home/amandapotts/git/nltk/nltk/classify/maxent.py_classify,"def classify(self, featureset):
return self.prob_classify(featureset).max()
",[],0,[],/classify/maxent.py_classify
5779,/home/amandapotts/git/nltk/nltk/classify/maxent.py_prob_classify,"def prob_classify(self, featureset):
prob_dict = {}
for label in self._encoding.labels():
feature_vector = self._encoding.encode(featureset, label)
if self._logarithmic:
total = 0.0
for f_id, f_val in feature_vector:
total += self._weights[f_id] * f_val
prob_dict[label] = total
else:
prod = 1.0
for f_id, f_val in feature_vector:
prod *= self._weights[f_id] ** f_val
prob_dict[label] = prod
return DictionaryProbDist(prob_dict, log=self._logarithmic, normalize=True)
",[],0,[],/classify/maxent.py_prob_classify
5780,/home/amandapotts/git/nltk/nltk/classify/maxent.py_show_most_informative_features,"def show_most_informative_features(self, n=10, show=""all""):
""""""
:param show: all, neg, or pos (for negative-only or positive-only)
:type show: str
:param n: The no. of top features
:type n: int
""""""
fids = self.most_informative_features(None)
if show == ""pos"":
fids = [fid for fid in fids if self._weights[fid] > 0]
elif show == ""neg"":
fids = [fid for fid in fids if self._weights[fid] < 0]
for fid in fids[:n]:
print(f""{self._weights[fid]:8.3f} {self._encoding.describe(fid)}"")
",[],0,[],/classify/maxent.py_show_most_informative_features
5781,/home/amandapotts/git/nltk/nltk/classify/maxent.py___repr__,"def __repr__(self):
return ""<ConditionalExponentialClassifier: %d labels, %d features>"" % (
len(self._encoding.labels()),
self._encoding.length(),
)
",[],0,[],/classify/maxent.py___repr__
5782,/home/amandapotts/git/nltk/nltk/classify/maxent.py_train,"def train(
cls,
train_toks,
algorithm=None,
trace=3,
encoding=None,
labels=None,
gaussian_prior_sigma=0,
",[],0,[],/classify/maxent.py_train
5783,/home/amandapotts/git/nltk/nltk/classify/maxent.py_encode,"def encode(self, featureset, label):
""""""
Given a (featureset, label) pair, return the corresponding
vector of joint-feature values.  This vector is represented as
a list of ``(index, value)`` tuples, specifying the value of
each non-zero joint-feature.
:type featureset: dict
:rtype: list(tuple(int, int))
""""""
raise NotImplementedError()
",[],0,[],/classify/maxent.py_encode
5784,/home/amandapotts/git/nltk/nltk/classify/maxent.py_length,"def length(self):
""""""
:return: The size of the fixed-length joint-feature vectors
that are generated by this encoding.
:rtype: int
""""""
raise NotImplementedError()
",[],0,[],/classify/maxent.py_length
5785,/home/amandapotts/git/nltk/nltk/classify/maxent.py_labels,"def labels(self):
""""""
:return: A list of the \""known labels\"" -- i.e., all labels
``l`` such that ``self.encode(fs,l)`` can be a nonzero
joint-feature vector for some value of ``fs``.
:rtype: list
""""""
raise NotImplementedError()
",[],0,[],/classify/maxent.py_labels
5786,/home/amandapotts/git/nltk/nltk/classify/maxent.py_describe,"def describe(self, fid):
""""""
:return: A string describing the value of the joint-feature
whose index in the generated feature vectors is ``fid``.
:rtype: str
""""""
raise NotImplementedError()
",[],0,[],/classify/maxent.py_describe
5787,/home/amandapotts/git/nltk/nltk/classify/maxent.py_train,"def train(cls, train_toks):
""""""
Construct and return new feature encoding, based on a given
training corpus ``train_toks``.
:type train_toks: list(tuple(dict, str))
:param train_toks: Training data, represented as a list of
pairs, the first member of which is a feature dictionary,
and the second of which is a classification label.
""""""
raise NotImplementedError()
",[],0,[],/classify/maxent.py_train
5788,/home/amandapotts/git/nltk/nltk/classify/maxent.py___init__,"def __init__(self, func, length, labels):
""""""
Construct a new feature encoding based on the given function.
:type func: (callable)
:param func: A function that takes two arguments, a featureset
and a label, and returns the sparse joint feature vector
that encodes them::
func(featureset, label) -> feature_vector
This sparse joint feature vector (``feature_vector``) is a
list of ``(index,value)`` tuples.
:type length: int
:param length: The size of the fixed-length joint-feature
vectors that are generated by this encoding.
:type labels: list
:param labels: A list of the \""known labels\"" for this
encoding -- i.e., all labels ``l`` such that
``self.encode(fs,l)`` can be a nonzero joint-feature vector
for some value of ``fs``.
""""""
self._length = length
self._func = func
self._labels = labels
",[],0,[],/classify/maxent.py___init__
5789,/home/amandapotts/git/nltk/nltk/classify/maxent.py_encode,"def encode(self, featureset, label):
return self._func(featureset, label)
",[],0,[],/classify/maxent.py_encode
5790,/home/amandapotts/git/nltk/nltk/classify/maxent.py_length,"def length(self):
return self._length
",[],0,[],/classify/maxent.py_length
5791,/home/amandapotts/git/nltk/nltk/classify/maxent.py_labels,"def labels(self):
return self._labels
",[],0,[],/classify/maxent.py_labels
5792,/home/amandapotts/git/nltk/nltk/classify/maxent.py_describe,"def describe(self, fid):
return ""no description available""
",[],0,[],/classify/maxent.py_describe
5793,/home/amandapotts/git/nltk/nltk/classify/maxent.py___init__,"def __init__(self, labels, mapping, unseen_features=False, alwayson_features=False):
""""""
:param labels: A list of the \""known labels\"" for this encoding.
:param mapping: A dictionary mapping from ``(fname,fval,label)``
tuples to corresponding joint-feature indexes.  These
indexes must be the set of integers from 0...len(mapping).
If ``mapping[fname,fval,label]=id``, then
``self.encode(..., fname:fval, ..., label)[id]`` is 1
otherwise, it is 0.
:param unseen_features: If true, then include unseen value
features in the generated joint-feature vectors.
:param alwayson_features: If true, then include always-on
features in the generated joint-feature vectors.
""""""
if set(mapping.values()) != set(range(len(mapping))):
raise ValueError(
""Mapping values must be exactly the ""
""set of integers from 0...len(mapping)""
)
self._labels = list(labels)
""""""A list of attested labels.""""""
self._mapping = mapping
""""""dict mapping from (fname,fval,label) -> fid""""""
self._length = len(mapping)
""""""The length of generated joint feature vectors.""""""
self._alwayson = None
""""""dict mapping from label -> fid""""""
self._unseen = None
""""""dict mapping from fname -> fid""""""
if alwayson_features:
self._alwayson = {
label: i + self._length for (i, label) in enumerate(labels)
}
self._length += len(self._alwayson)
if unseen_features:
fnames = {fname for (fname, fval, label) in mapping}
self._unseen = {fname: i + self._length for (i, fname) in enumerate(fnames)}
self._length += len(fnames)
",[],0,[],/classify/maxent.py___init__
5794,/home/amandapotts/git/nltk/nltk/classify/maxent.py_encode,"def encode(self, featureset, label):
encoding = []
for fname, fval in featureset.items():
if (fname, fval, label) in self._mapping:
encoding.append((self._mapping[fname, fval, label], 1))
elif self._unseen:
for label2 in self._labels:
if (fname, fval, label2) in self._mapping:
break  # we've seen this fname/fval combo
else:
if fname in self._unseen:
encoding.append((self._unseen[fname], 1))
if self._alwayson and label in self._alwayson:
encoding.append((self._alwayson[label], 1))
return encoding
",[],0,[],/classify/maxent.py_encode
5795,/home/amandapotts/git/nltk/nltk/classify/maxent.py_describe,"def describe(self, f_id):
if not isinstance(f_id, int):
raise TypeError(""describe() expected an int"")
try:
self._inv_mapping
except AttributeError:
self._inv_mapping = [-1] * len(self._mapping)
for info, i in self._mapping.items():
self._inv_mapping[i] = info
if f_id < len(self._mapping):
(fname, fval, label) = self._inv_mapping[f_id]
return f""{fname}=={fval!r} and label is {label!r}""
elif self._alwayson and f_id in self._alwayson.values():
for label, f_id2 in self._alwayson.items():
if f_id == f_id2:
return ""label is %r"" % label
elif self._unseen and f_id in self._unseen.values():
for fname, f_id2 in self._unseen.items():
if f_id == f_id2:
return ""%s is unseen"" % fname
else:
raise ValueError(""Bad feature id"")
",[],0,[],/classify/maxent.py_describe
5796,/home/amandapotts/git/nltk/nltk/classify/maxent.py_labels,"def labels(self):
return self._labels
",[],0,[],/classify/maxent.py_labels
5797,/home/amandapotts/git/nltk/nltk/classify/maxent.py_length,"def length(self):
return self._length
",[],0,[],/classify/maxent.py_length
5798,/home/amandapotts/git/nltk/nltk/classify/maxent.py_train,"def train(cls, train_toks, count_cutoff=0, labels=None, **options):
""""""
Construct and return new feature encoding, based on a given
training corpus ``train_toks``.  See the class description
``BinaryMaxentFeatureEncoding`` for a description of the
joint-features that will be included in this encoding.
:type train_toks: list(tuple(dict, str))
:param train_toks: Training data, represented as a list of
pairs, the first member of which is a feature dictionary,
and the second of which is a classification label.
:type count_cutoff: int
:param count_cutoff: A cutoff value that is used to discard
rare joint-features.  If a joint-feature's value is 1
fewer than ``count_cutoff`` times in the training corpus,
then that joint-feature is not included in the generated
encoding.
:type labels: list
:param labels: A list of labels that should be used by the
classifier.  If not specified, then the set of labels
attested in ``train_toks`` will be used.
:param options: Extra parameters for the constructor, such as
``unseen_features`` and ``alwayson_features``.
""""""
mapping = {}  # maps (fname, fval, label) -> fid
seen_labels = set()  # The set of labels we've encountered
count = defaultdict(int)  # maps (fname, fval) -> count
for tok, label in train_toks:
if labels and label not in labels:
raise ValueError(""Unexpected label %s"" % label)
seen_labels.add(label)
for fname, fval in tok.items():
count[fname, fval] += 1
if count[fname, fval] >= count_cutoff:
if (fname, fval, label) not in mapping:
mapping[fname, fval, label] = len(mapping)
if labels is None:
labels = seen_labels
return cls(labels, mapping, **options)
",[],0,[],/classify/maxent.py_train
5799,/home/amandapotts/git/nltk/nltk/classify/maxent.py___init__,"def __init__(
self, labels, mapping, unseen_features=False, alwayson_features=False, C=None
",[],0,[],/classify/maxent.py___init__
5800,/home/amandapotts/git/nltk/nltk/classify/maxent.py_C,"def C(self):
""""""The non-negative constant that all encoded feature vectors
will sum to.""""""
return self._C
",[],0,[],/classify/maxent.py_C
5801,/home/amandapotts/git/nltk/nltk/classify/maxent.py_encode,"def encode(self, featureset, label):
encoding = BinaryMaxentFeatureEncoding.encode(self, featureset, label)
base_length = BinaryMaxentFeatureEncoding.length(self)
total = sum(v for (f, v) in encoding)
if total >= self._C:
raise ValueError(""Correction feature is not high enough!"")
encoding.append((base_length, self._C - total))
return encoding
",[],0,[],/classify/maxent.py_encode
5802,/home/amandapotts/git/nltk/nltk/classify/maxent.py_length,"def length(self):
return BinaryMaxentFeatureEncoding.length(self) + 1
",[],0,[],/classify/maxent.py_length
5803,/home/amandapotts/git/nltk/nltk/classify/maxent.py_describe,"def describe(self, f_id):
if f_id == BinaryMaxentFeatureEncoding.length(self):
return ""Correction feature (%s)"" % self._C
else:
return BinaryMaxentFeatureEncoding.describe(self, f_id)
",[],0,[],/classify/maxent.py_describe
5804,/home/amandapotts/git/nltk/nltk/classify/maxent.py___init__,"def __init__(self, labels, mapping, unseen_features=False, alwayson_features=False):
self._mapping = OrderedDict(mapping)
self._label_mapping = OrderedDict()
BinaryMaxentFeatureEncoding.__init__(
self, labels, self._mapping, unseen_features, alwayson_features
)
",[],0,[],/classify/maxent.py___init__
5805,/home/amandapotts/git/nltk/nltk/classify/maxent.py_encode,"def encode(self, featureset, label):
encoding = []
for feature, value in featureset.items():
if (feature, label) not in self._mapping:
self._mapping[(feature, label)] = len(self._mapping)
if value not in self._label_mapping:
if not isinstance(value, int):
self._label_mapping[value] = len(self._label_mapping)
else:
self._label_mapping[value] = value
encoding.append(
(self._mapping[(feature, label)], self._label_mapping[value])
)
return encoding
",[],0,[],/classify/maxent.py_encode
5806,/home/amandapotts/git/nltk/nltk/classify/maxent.py_labels,"def labels(self):
return self._labels
",[],0,[],/classify/maxent.py_labels
5807,/home/amandapotts/git/nltk/nltk/classify/maxent.py_describe,"def describe(self, fid):
for feature, label in self._mapping:
if self._mapping[(feature, label)] == fid:
return (feature, label)
",[],0,[],/classify/maxent.py_describe
5808,/home/amandapotts/git/nltk/nltk/classify/maxent.py_length,"def length(self):
return len(self._mapping)
",[],0,[],/classify/maxent.py_length
5809,/home/amandapotts/git/nltk/nltk/classify/maxent.py_train,"def train(cls, train_toks, count_cutoff=0, labels=None, **options):
mapping = OrderedDict()
if not labels:
labels = []
train_toks = list(train_toks)
for featureset, label in train_toks:
if label not in labels:
labels.append(label)
for featureset, label in train_toks:
for label in labels:
for feature in featureset:
if (feature, label) not in mapping:
mapping[(feature, label)] = len(mapping)
return cls(labels, mapping, **options)
",[],0,[],/classify/maxent.py_train
5810,/home/amandapotts/git/nltk/nltk/classify/maxent.py___init__,"def __init__(self, labels, mapping, unseen_features=False, alwayson_features=False):
""""""
:param labels: A list of the \""known labels\"" for this encoding.
:param mapping: A dictionary mapping from ``(fname,fval,label)``
tuples to corresponding joint-feature indexes.  These
indexes must be the set of integers from 0...len(mapping).
If ``mapping[fname,fval,label]=id``, then
``self.encode({..., fname:fval, ...``, label)[id]} is 1
otherwise, it is 0.
:param unseen_features: If true, then include unseen value
features in the generated joint-feature vectors.
:param alwayson_features: If true, then include always-on
features in the generated joint-feature vectors.
""""""
if set(mapping.values()) != set(range(len(mapping))):
raise ValueError(
""Mapping values must be exactly the ""
""set of integers from 0...len(mapping)""
)
self._labels = list(labels)
""""""A list of attested labels.""""""
self._mapping = mapping
""""""dict mapping from (fname,fval,label) -> fid""""""
self._length = len(mapping)
""""""The length of generated joint feature vectors.""""""
self._alwayson = None
""""""dict mapping from label -> fid""""""
self._unseen = None
""""""dict mapping from fname -> fid""""""
if alwayson_features:
self._alwayson = {
label: i + self._length for (i, label) in enumerate(labels)
}
self._length += len(self._alwayson)
if unseen_features:
fnames = {fname for (fname, fval, label) in mapping}
self._unseen = {fname: i + self._length for (i, fname) in enumerate(fnames)}
self._length += len(fnames)
",[],0,[],/classify/maxent.py___init__
5811,/home/amandapotts/git/nltk/nltk/classify/maxent.py_encode,"def encode(self, featureset, label):
encoding = []
for fname, fval in featureset.items():
if isinstance(fval, (int, float)):
if (fname, type(fval), label) in self._mapping:
encoding.append((self._mapping[fname, type(fval), label], fval))
else:
if (fname, fval, label) in self._mapping:
encoding.append((self._mapping[fname, fval, label], 1))
elif self._unseen:
for label2 in self._labels:
if (fname, fval, label2) in self._mapping:
break  # we've seen this fname/fval combo
else:
if fname in self._unseen:
encoding.append((self._unseen[fname], 1))
if self._alwayson and label in self._alwayson:
encoding.append((self._alwayson[label], 1))
return encoding
",[],0,[],/classify/maxent.py_encode
5812,/home/amandapotts/git/nltk/nltk/classify/maxent.py_describe,"def describe(self, f_id):
if not isinstance(f_id, int):
raise TypeError(""describe() expected an int"")
try:
self._inv_mapping
except AttributeError:
self._inv_mapping = [-1] * len(self._mapping)
for info, i in self._mapping.items():
self._inv_mapping[i] = info
if f_id < len(self._mapping):
(fname, fval, label) = self._inv_mapping[f_id]
return f""{fname}=={fval!r} and label is {label!r}""
elif self._alwayson and f_id in self._alwayson.values():
for label, f_id2 in self._alwayson.items():
if f_id == f_id2:
return ""label is %r"" % label
elif self._unseen and f_id in self._unseen.values():
for fname, f_id2 in self._unseen.items():
if f_id == f_id2:
return ""%s is unseen"" % fname
else:
raise ValueError(""Bad feature id"")
",[],0,[],/classify/maxent.py_describe
5813,/home/amandapotts/git/nltk/nltk/classify/maxent.py_labels,"def labels(self):
return self._labels
",[],0,[],/classify/maxent.py_labels
5814,/home/amandapotts/git/nltk/nltk/classify/maxent.py_length,"def length(self):
return self._length
",[],0,[],/classify/maxent.py_length
5815,/home/amandapotts/git/nltk/nltk/classify/maxent.py_train,"def train(cls, train_toks, count_cutoff=0, labels=None, **options):
""""""
Construct and return new feature encoding, based on a given
training corpus ``train_toks``.  See the class description
``TypedMaxentFeatureEncoding`` for a description of the
joint-features that will be included in this encoding.
Note: recognized feature values types are (int, float), over
types are interpreted as regular binary features.
:type train_toks: list(tuple(dict, str))
:param train_toks: Training data, represented as a list of
pairs, the first member of which is a feature dictionary,
and the second of which is a classification label.
:type count_cutoff: int
:param count_cutoff: A cutoff value that is used to discard
rare joint-features.  If a joint-feature's value is 1
fewer than ``count_cutoff`` times in the training corpus,
then that joint-feature is not included in the generated
encoding.
:type labels: list
:param labels: A list of labels that should be used by the
classifier.  If not specified, then the set of labels
attested in ``train_toks`` will be used.
:param options: Extra parameters for the constructor, such as
``unseen_features`` and ``alwayson_features``.
""""""
mapping = {}  # maps (fname, fval, label) -> fid
seen_labels = set()  # The set of labels we've encountered
count = defaultdict(int)  # maps (fname, fval) -> count
for tok, label in train_toks:
if labels and label not in labels:
raise ValueError(""Unexpected label %s"" % label)
seen_labels.add(label)
for fname, fval in tok.items():
if type(fval) in (int, float):
fval = type(fval)
count[fname, fval] += 1
if count[fname, fval] >= count_cutoff:
if (fname, fval, label) not in mapping:
mapping[fname, fval, label] = len(mapping)
if labels is None:
labels = seen_labels
return cls(labels, mapping, **options)
",[],0,[],/classify/maxent.py_train
5816,/home/amandapotts/git/nltk/nltk/classify/maxent.py_train_maxent_classifier_with_gis,"def train_maxent_classifier_with_gis(
train_toks, trace=3, encoding=None, labels=None, **cutoffs
",[],0,[],/classify/maxent.py_train_maxent_classifier_with_gis
5817,/home/amandapotts/git/nltk/nltk/classify/maxent.py_calculate_empirical_fcount,"def calculate_empirical_fcount(train_toks, encoding):
fcount = numpy.zeros(encoding.length(), ""d"")
for tok, label in train_toks:
for index, val in encoding.encode(tok, label):
fcount[index] += val
return fcount
",[],0,[],/classify/maxent.py_calculate_empirical_fcount
5818,/home/amandapotts/git/nltk/nltk/classify/maxent.py_calculate_estimated_fcount,"def calculate_estimated_fcount(classifier, train_toks, encoding):
fcount = numpy.zeros(encoding.length(), ""d"")
for tok, label in train_toks:
pdist = classifier.prob_classify(tok)
for label in pdist.samples():
prob = pdist.prob(label)
for fid, fval in encoding.encode(tok, label):
fcount[fid] += prob * fval
return fcount
",[],0,[],/classify/maxent.py_calculate_estimated_fcount
5819,/home/amandapotts/git/nltk/nltk/classify/maxent.py_train_maxent_classifier_with_iis,"def train_maxent_classifier_with_iis(
train_toks, trace=3, encoding=None, labels=None, **cutoffs
",[],0,[],/classify/maxent.py_train_maxent_classifier_with_iis
5820,/home/amandapotts/git/nltk/nltk/classify/maxent.py_calculate_nfmap,"def calculate_nfmap(train_toks, encoding):
""""""
Construct a map that can be used to compress ``nf`` (which is
typically sparse).
This represents the number of features that are active for a
given labeled text.  This method finds all values of *nf(t)*
that are attested for at least one token in the given list of
training tokens
attested values to a continuous range *0...N*.  For example,
if the only values of *nf()* that were attested were 3, 5, and
7, then ``_nfmap`` might return the dictionary ``{3:0, 5:1, 7:2}``.
:return: A map that can be used to compress ``nf`` to a dense
vector.
:rtype: dict(int -> int)
""""""
nfset = set()
for tok, _ in train_toks:
for label in encoding.labels():
nfset.add(sum(val for (id, val) in encoding.encode(tok, label)))
return {nf: i for (i, nf) in enumerate(nfset)}
",[],0,[],/classify/maxent.py_calculate_nfmap
5821,/home/amandapotts/git/nltk/nltk/classify/maxent.py_calculate_deltas,"def calculate_deltas(
train_toks,
classifier,
unattested,
ffreq_empirical,
nfmap,
nfarray,
nftranspose,
encoding,
",[],0,[],/classify/maxent.py_calculate_deltas
5822,/home/amandapotts/git/nltk/nltk/classify/maxent.py_train,"def train(cls, train_toks, **kwargs):
algorithm = kwargs.get(""algorithm"", ""tao_lmvm"")
trace = kwargs.get(""trace"", 3)
encoding = kwargs.get(""encoding"", None)
labels = kwargs.get(""labels"", None)
sigma = kwargs.get(""gaussian_prior_sigma"", 0)
count_cutoff = kwargs.get(""count_cutoff"", 0)
max_iter = kwargs.get(""max_iter"")
ll_delta = kwargs.get(""min_lldelta"")
if not encoding:
encoding = TadmEventMaxentFeatureEncoding.train(
train_toks, count_cutoff, labels=labels
)
trainfile_fd, trainfile_name = tempfile.mkstemp(
prefix=""nltk-tadm-events-"", suffix="".gz""
)
weightfile_fd, weightfile_name = tempfile.mkstemp(prefix=""nltk-tadm-weights-"")
trainfile = gzip_open_unicode(trainfile_name, ""w"")
write_tadm_file(train_toks, encoding, trainfile)
trainfile.close()
options = []
options.extend([""-monitor""])
options.extend([""-method"", algorithm])
if sigma:
options.extend([""-l2"", ""%.6f"" % sigma**2])
if max_iter:
options.extend([""-max_it"", ""%d"" % max_iter])
if ll_delta:
options.extend([""-fatol"", ""%.6f"" % abs(ll_delta)])
options.extend([""-events_in"", trainfile_name])
options.extend([""-params_out"", weightfile_name])
if trace < 3:
options.extend([""2>&1""])
else:
options.extend([""-summary""])
call_tadm(options)
with open(weightfile_name) as weightfile:
weights = parse_tadm_weights(weightfile)
os.remove(trainfile_name)
os.remove(weightfile_name)
weights *= numpy.log2(numpy.e)
return cls(encoding, weights)
",[],0,[],/classify/maxent.py_train
5823,/home/amandapotts/git/nltk/nltk/classify/maxent.py_demo,"def demo():
from nltk.classify.util import names_demo
classifier = names_demo(MaxentClassifier.train)
",[],0,[],/classify/maxent.py_demo
5824,/home/amandapotts/git/nltk/nltk/classify/util.py_apply_features,"def apply_features(feature_func, toks, labeled=None):
""""""
Use the ``LazyMap`` class to construct a lazy list-like
object that is analogous to ``map(feature_func, toks)``.  In
particular, if ``labeled=False``, then the returned list-like
object's values are equal to::
[feature_func(tok) for tok in toks]
If ``labeled=True``, then the returned list-like object's values
are equal to::
[(feature_func(tok), label) for (tok, label) in toks]
The primary purpose of this function is to avoid the memory
overhead involved in storing all the featuresets for every token
in a corpus.  Instead, these featuresets are constructed lazily,
as-needed.  The reduction in memory overhead can be especially
significant when the underlying list of tokens is itself lazy (as
is the case with many corpus readers).
:param feature_func: The function that will be applied to each
token.  It should return a featureset -- i.e., a dict
mapping feature names to feature values.
:param toks: The list of tokens to which ``feature_func`` should be
applied.  If ``labeled=True``, then the list elements will be
passed directly to ``feature_func()``.  If ``labeled=False``,
then the list elements should be tuples ``(tok,label)``, and
``tok`` will be passed to ``feature_func()``.
:param labeled: If true, then ``toks`` contains labeled tokens --
i.e., tuples of the form ``(tok, label)``.  (Default:
auto-detect based on types.)
""""""
if labeled is None:
labeled = toks and isinstance(toks[0], (tuple, list))
if labeled:
",[],0,[],/classify/util.py_apply_features
5825,/home/amandapotts/git/nltk/nltk/classify/util.py_lazy_func,"def lazy_func(labeled_token):
return (feature_func(labeled_token[0]), labeled_token[1])
",[],0,[],/classify/util.py_lazy_func
5826,/home/amandapotts/git/nltk/nltk/classify/util.py_attested_labels,"def attested_labels(tokens):
""""""
:return: A list of all labels that are attested in the given list
of tokens.
:rtype: list of (immutable)
:param tokens: The list of classified tokens from which to extract
labels.  A classified token has the form ``(token, label)``.
:type tokens: list
""""""
return tuple({label for (tok, label) in tokens})
",[],0,[],/classify/util.py_attested_labels
5827,/home/amandapotts/git/nltk/nltk/classify/util.py_log_likelihood,"def log_likelihood(classifier, gold):
results = classifier.prob_classify_many([fs for (fs, l) in gold])
ll = [pdist.prob(l) for ((fs, l), pdist) in zip(gold, results)]
return math.log(sum(ll) / len(ll))
",[],0,[],/classify/util.py_log_likelihood
5828,/home/amandapotts/git/nltk/nltk/classify/util.py_accuracy,"def accuracy(classifier, gold):
results = classifier.classify_many([fs for (fs, l) in gold])
correct = [l == r for ((fs, l), r) in zip(gold, results)]
if correct:
return sum(correct) / len(correct)
else:
return 0
",[],0,[],/classify/util.py_accuracy
5829,/home/amandapotts/git/nltk/nltk/classify/util.py___init__,"def __init__(self, cutoffs):
self.cutoffs = cutoffs.copy()
if ""min_ll"" in cutoffs:
cutoffs[""min_ll""] = -abs(cutoffs[""min_ll""])
if ""min_lldelta"" in cutoffs:
cutoffs[""min_lldelta""] = abs(cutoffs[""min_lldelta""])
self.ll = None
self.acc = None
self.iter = 1
",[],0,[],/classify/util.py___init__
5830,/home/amandapotts/git/nltk/nltk/classify/util.py_check,"def check(self, classifier, train_toks):
cutoffs = self.cutoffs
self.iter += 1
if ""max_iter"" in cutoffs and self.iter >= cutoffs[""max_iter""]:
return True  # iteration cutoff.
new_ll = nltk.classify.util.log_likelihood(classifier, train_toks)
if math.isnan(new_ll):
return True
if ""min_ll"" in cutoffs or ""min_lldelta"" in cutoffs:
if ""min_ll"" in cutoffs and new_ll >= cutoffs[""min_ll""]:
return True  # log likelihood cutoff
if (
""min_lldelta"" in cutoffs
and self.ll
and ((new_ll - self.ll) <= abs(cutoffs[""min_lldelta""]))
):
return True  # log likelihood delta cutoff
self.ll = new_ll
if ""max_acc"" in cutoffs or ""min_accdelta"" in cutoffs:
new_acc = nltk.classify.util.log_likelihood(classifier, train_toks)
if ""max_acc"" in cutoffs and new_acc >= cutoffs[""max_acc""]:
return True  # log likelihood cutoff
if (
""min_accdelta"" in cutoffs
and self.acc
and ((new_acc - self.acc) <= abs(cutoffs[""min_accdelta""]))
):
return True  # log likelihood delta cutoff
self.acc = new_acc
return False  # no cutoff reached.
",[],0,[],/classify/util.py_check
5831,/home/amandapotts/git/nltk/nltk/classify/util.py_names_demo_features,"def names_demo_features(name):
features = {}
features[""alwayson""] = True
features[""startswith""] = name[0].lower()
features[""endswith""] = name[-1].lower()
for letter in ""abcdefghijklmnopqrstuvwxyz"":
features[""count(%s)"" % letter] = name.lower().count(letter)
features[""has(%s)"" % letter] = letter in name.lower()
return features
",[],0,[],/classify/util.py_names_demo_features
5832,/home/amandapotts/git/nltk/nltk/classify/util.py_binary_names_demo_features,"def binary_names_demo_features(name):
features = {}
features[""alwayson""] = True
features[""startswith(vowel)""] = name[0].lower() in ""aeiouy""
features[""endswith(vowel)""] = name[-1].lower() in ""aeiouy""
for letter in ""abcdefghijklmnopqrstuvwxyz"":
features[""count(%s)"" % letter] = name.lower().count(letter)
features[""has(%s)"" % letter] = letter in name.lower()
features[""startswith(%s)"" % letter] = letter == name[0].lower()
features[""endswith(%s)"" % letter] = letter == name[-1].lower()
return features
",[],0,[],/classify/util.py_binary_names_demo_features
5833,/home/amandapotts/git/nltk/nltk/classify/util.py_names_demo,"def names_demo(trainer, features=names_demo_features):
import random
from nltk.corpus import names
namelist = [(name, ""male"") for name in names.words(""male.txt"")] + [
(name, ""female"") for name in names.words(""female.txt"")
]
random.seed(123456)
random.shuffle(namelist)
train = namelist[:5000]
test = namelist[5000:5500]
print(""Training classifier..."")
classifier = trainer([(features(n), g) for (n, g) in train])
print(""Testing classifier..."")
acc = accuracy(classifier, [(features(n), g) for (n, g) in test])
print(""Accuracy: %6.4f"" % acc)
try:
test_featuresets = [features(n) for (n, g) in test]
pdists = classifier.prob_classify_many(test_featuresets)
ll = [pdist.logprob(gold) for ((name, gold), pdist) in zip(test, pdists)]
print(""Avg. log likelihood: %6.4f"" % (sum(ll) / len(test)))
print()
print(""Unseen Names      P(Male)  P(Female)\n"" + ""-"" * 40)
for (name, gender), pdist in list(zip(test, pdists))[:5]:
if gender == ""male"":
fmt = ""  %-15s *%6.4f   %6.4f""
else:
fmt = ""  %-15s  %6.4f  *%6.4f""
print(fmt % (name, pdist.prob(""male""), pdist.prob(""female"")))
except NotImplementedError:
pass
return classifier
",[],0,[],/classify/util.py_names_demo
5834,/home/amandapotts/git/nltk/nltk/classify/util.py_partial_names_demo,"def partial_names_demo(trainer, features=names_demo_features):
import random
from nltk.corpus import names
male_names = names.words(""male.txt"")
female_names = names.words(""female.txt"")
random.seed(654321)
random.shuffle(male_names)
random.shuffle(female_names)
positive = map(features, male_names[:2000])
unlabeled = map(features, male_names[2000:2500] + female_names[:500])
test = [(name, True) for name in male_names[2500:2750]] + [
(name, False) for name in female_names[500:750]
]
random.shuffle(test)
print(""Training classifier..."")
classifier = trainer(positive, unlabeled)
print(""Testing classifier..."")
acc = accuracy(classifier, [(features(n), m) for (n, m) in test])
print(""Accuracy: %6.4f"" % acc)
try:
test_featuresets = [features(n) for (n, m) in test]
pdists = classifier.prob_classify_many(test_featuresets)
ll = [pdist.logprob(gold) for ((name, gold), pdist) in zip(test, pdists)]
print(""Avg. log likelihood: %6.4f"" % (sum(ll) / len(test)))
print()
print(""Unseen Names      P(Male)  P(Female)\n"" + ""-"" * 40)
for (name, is_male), pdist in zip(test, pdists)[:5]:
if is_male == True:
fmt = ""  %-15s *%6.4f   %6.4f""
else:
fmt = ""  %-15s  %6.4f  *%6.4f""
print(fmt % (name, pdist.prob(True), pdist.prob(False)))
except NotImplementedError:
pass
return classifier
",[],0,[],/classify/util.py_partial_names_demo
5835,/home/amandapotts/git/nltk/nltk/classify/util.py_wsd_demo,"def wsd_demo(trainer, word, features, n=1000):
import random
from nltk.corpus import senseval
print(""Reading data..."")
global _inst_cache
if word not in _inst_cache:
_inst_cache[word] = [(i, i.senses[0]) for i in senseval.instances(word)]
instances = _inst_cache[word][:]
if n > len(instances):
n = len(instances)
senses = list({l for (i, l) in instances})
print(""  Senses: "" + "" "".join(senses))
print(""Splitting into test & train..."")
random.seed(123456)
random.shuffle(instances)
train = instances[: int(0.8 * n)]
test = instances[int(0.8 * n) : n]
print(""Training classifier..."")
classifier = trainer([(features(i), l) for (i, l) in train])
print(""Testing classifier..."")
acc = accuracy(classifier, [(features(i), l) for (i, l) in test])
print(""Accuracy: %6.4f"" % acc)
try:
test_featuresets = [features(i) for (i, n) in test]
pdists = classifier.prob_classify_many(test_featuresets)
ll = [pdist.logprob(gold) for ((name, gold), pdist) in zip(test, pdists)]
print(""Avg. log likelihood: %6.4f"" % (sum(ll) / len(test)))
except NotImplementedError:
pass
return classifier
",[],0,[],/classify/util.py_wsd_demo
5836,/home/amandapotts/git/nltk/nltk/classify/util.py_check_megam_config,"def check_megam_config():
""""""
Checks whether the MEGAM binary is configured.
""""""
try:
_megam_bin
except NameError as e:
err_msg = str(
""Please configure your megam binary first, e.g.\n""
"">>> nltk.config_megam('/usr/bin/local/megam')""
)
raise NameError(err_msg) from e
",[],0,[],/classify/util.py_check_megam_config
5837,/home/amandapotts/git/nltk/nltk/classify/megam.py_config_megam,"def config_megam(bin=None):
""""""
Configure NLTK's interface to the ``megam`` maxent optimization
package.
:param bin: The full path to the ``megam`` binary.  If not specified,
then nltk will search the system for a ``megam`` binary
one is not found, it will raise a ``LookupError`` exception.
:type bin: str
""""""
global _megam_bin
_megam_bin = find_binary(
""megam"",
bin,
env_vars=[""MEGAM""],
binary_names=[""megam.opt"", ""megam"", ""megam_686"", ""megam_i686.opt""],
url=""https://www.umiacs.umd.edu/~hal/megam/index.html"",
)
",[],0,[],/classify/megam.py_config_megam
5838,/home/amandapotts/git/nltk/nltk/classify/megam.py_write_megam_file,"def write_megam_file(train_toks, encoding, stream, bernoulli=True, explicit=True):
""""""
Generate an input file for ``megam`` based on the given corpus of
classified tokens.
:type train_toks: list(tuple(dict, str))
:param train_toks: Training data, represented as a list of
pairs, the first member of which is a feature dictionary,
and the second of which is a classification label.
:type encoding: MaxentFeatureEncodingI
:param encoding: A feature encoding, used to convert featuresets
into feature vectors. May optionally implement a cost() method
in order to assign different costs to different class predictions.
:type stream: stream
:param stream: The stream to which the megam input file should be
written.
:param bernoulli: If true, then use the 'bernoulli' format.  I.e.,
all joint features have binary values, and are listed iff they
are true.  Otherwise, list feature values explicitly.  If
``bernoulli=False``, then you must call ``megam`` with the
``-fvals`` option.
:param explicit: If true, then use the 'explicit' format.  I.e.,
list the features that would fire for any of the possible
labels, for each token.  If ``explicit=True``, then you must
call ``megam`` with the ``-explicit`` option.
""""""
labels = encoding.labels()
labelnum = {label: i for (i, label) in enumerate(labels)}
for featureset, label in train_toks:
if hasattr(encoding, ""cost""):
stream.write(
"":"".join(str(encoding.cost(featureset, label, l)) for l in labels)
)
else:
stream.write(""%d"" % labelnum[label])
if not explicit:
_write_megam_features(encoding.encode(featureset, label), stream, bernoulli)
else:
for l in labels:
stream.write("" #"")
_write_megam_features(encoding.encode(featureset, l), stream, bernoulli)
stream.write(""\n"")
",[],0,[],/classify/megam.py_write_megam_file
5839,/home/amandapotts/git/nltk/nltk/classify/megam.py_parse_megam_weights,"def parse_megam_weights(s, features_count, explicit=True):
""""""
Given the stdout output generated by ``megam`` when training a
model, return a ``numpy`` array containing the corresponding weight
vector.  This function does not currently handle bias features.
""""""
if numpy is None:
raise ValueError(""This function requires that numpy be installed"")
assert explicit, ""non-explicit not supported yet""
lines = s.strip().split(""\n"")
weights = numpy.zeros(features_count, ""d"")
for line in lines:
if line.strip():
fid, weight = line.split()
weights[int(fid)] = float(weight)
return weights
",[],0,[],/classify/megam.py_parse_megam_weights
5840,/home/amandapotts/git/nltk/nltk/classify/megam.py__write_megam_features,"def _write_megam_features(vector, stream, bernoulli):
if not vector:
raise ValueError(
""MEGAM classifier requires the use of an "" ""always-on feature.""
)
for fid, fval in vector:
if bernoulli:
if fval == 1:
stream.write("" %s"" % fid)
elif fval != 0:
raise ValueError(
""If bernoulli=True, then all"" ""features must be binary.""
)
else:
stream.write(f"" {fid} {fval}"")
",[],0,[],/classify/megam.py__write_megam_features
5841,/home/amandapotts/git/nltk/nltk/classify/megam.py_call_megam,"def call_megam(args):
""""""
Call the ``megam`` binary with the given arguments.
""""""
if isinstance(args, str):
raise TypeError(""args should be a list of strings"")
if _megam_bin is None:
config_megam()
cmd = [_megam_bin] + args
p = subprocess.Popen(cmd, stdout=subprocess.PIPE)
(stdout, stderr) = p.communicate()
if p.returncode != 0:
print()
print(stderr)
raise OSError(""megam command failed!"")
if isinstance(stdout, str):
return stdout
else:
return stdout.decode(""utf-8"")
",[],0,[],/classify/megam.py_call_megam
5842,/home/amandapotts/git/nltk/nltk/classify/positivenaivebayes.py_train,"def train(
positive_featuresets,
unlabeled_featuresets,
positive_prob_prior=0.5,
estimator=ELEProbDist,
",[],0,[],/classify/positivenaivebayes.py_train
5843,/home/amandapotts/git/nltk/nltk/classify/positivenaivebayes.py_demo,"def demo():
from nltk.classify.util import partial_names_demo
classifier = partial_names_demo(PositiveNaiveBayesClassifier.train)
classifier.show_most_informative_features()
",[],0,[],/classify/positivenaivebayes.py_demo
5844,/home/amandapotts/git/nltk/nltk/classify/api.py_labels,"def labels(self):
""""""
:return: the list of category labels used by this classifier.
:rtype: list of (immutable)
""""""
raise NotImplementedError()
",[],0,[],/classify/api.py_labels
5845,/home/amandapotts/git/nltk/nltk/classify/api.py_classify,"def classify(self, featureset):
""""""
:return: the most appropriate label for the given featureset.
:rtype: label
""""""
if overridden(self.classify_many):
return self.classify_many([featureset])[0]
else:
raise NotImplementedError()
",[],0,[],/classify/api.py_classify
5846,/home/amandapotts/git/nltk/nltk/classify/api.py_prob_classify,"def prob_classify(self, featureset):
""""""
:return: a probability distribution over labels for the given
featureset.
:rtype: ProbDistI
""""""
if overridden(self.prob_classify_many):
return self.prob_classify_many([featureset])[0]
else:
raise NotImplementedError()
",[],0,[],/classify/api.py_prob_classify
5847,/home/amandapotts/git/nltk/nltk/classify/api.py_classify_many,"def classify_many(self, featuresets):
""""""
Apply ``self.classify()`` to each element of ``featuresets``.  I.e.:
return [self.classify(fs) for fs in featuresets]
:rtype: list(label)
""""""
return [self.classify(fs) for fs in featuresets]
",[],0,[],/classify/api.py_classify_many
5848,/home/amandapotts/git/nltk/nltk/classify/api.py_prob_classify_many,"def prob_classify_many(self, featuresets):
""""""
Apply ``self.prob_classify()`` to each element of ``featuresets``.  I.e.:
return [self.prob_classify(fs) for fs in featuresets]
:rtype: list(ProbDistI)
""""""
return [self.prob_classify(fs) for fs in featuresets]
",[],0,[],/classify/api.py_prob_classify_many
5849,/home/amandapotts/git/nltk/nltk/classify/api.py_labels,"def labels(self):
""""""
:return: the list of category labels used by this classifier.
:rtype: list of (immutable)
""""""
raise NotImplementedError()
",[],0,[],/classify/api.py_labels
5850,/home/amandapotts/git/nltk/nltk/classify/api.py_classify,"def classify(self, featureset):
""""""
:return: the most appropriate set of labels for the given featureset.
:rtype: set(label)
""""""
if overridden(self.classify_many):
return self.classify_many([featureset])[0]
else:
raise NotImplementedError()
",[],0,[],/classify/api.py_classify
5851,/home/amandapotts/git/nltk/nltk/classify/api.py_prob_classify,"def prob_classify(self, featureset):
""""""
:return: a probability distribution over sets of labels for the
given featureset.
:rtype: ProbDistI
""""""
if overridden(self.prob_classify_many):
return self.prob_classify_many([featureset])[0]
else:
raise NotImplementedError()
",[],0,[],/classify/api.py_prob_classify
5852,/home/amandapotts/git/nltk/nltk/classify/api.py_classify_many,"def classify_many(self, featuresets):
""""""
Apply ``self.classify()`` to each element of ``featuresets``.  I.e.:
return [self.classify(fs) for fs in featuresets]
:rtype: list(set(label))
""""""
return [self.classify(fs) for fs in featuresets]
",[],0,[],/classify/api.py_classify_many
5853,/home/amandapotts/git/nltk/nltk/classify/api.py_prob_classify_many,"def prob_classify_many(self, featuresets):
""""""
Apply ``self.prob_classify()`` to each element of ``featuresets``.  I.e.:
return [self.prob_classify(fs) for fs in featuresets]
:rtype: list(ProbDistI)
""""""
return [self.prob_classify(fs) for fs in featuresets]
",[],0,[],/classify/api.py_prob_classify_many
5854,/home/amandapotts/git/nltk/nltk/classify/svm.py___init__,"def __init__(self, *args, **kwargs):
raise NotImplementedError(__doc__)
",[],0,[],/classify/svm.py___init__
5855,/home/amandapotts/git/nltk/nltk/classify/tadm.py_config_tadm,"def config_tadm(bin=None):
global _tadm_bin
_tadm_bin = find_binary(
""tadm"", bin, env_vars=[""TADM""], binary_names=[""tadm""], url=""http://tadm.sf.net""
)
",[],0,[],/classify/tadm.py_config_tadm
5856,/home/amandapotts/git/nltk/nltk/classify/tadm.py_write_tadm_file,"def write_tadm_file(train_toks, encoding, stream):
""""""
Generate an input file for ``tadm`` based on the given corpus of
classified tokens.
:type train_toks: list(tuple(dict, str))
:param train_toks: Training data, represented as a list of
pairs, the first member of which is a feature dictionary,
and the second of which is a classification label.
:type encoding: TadmEventMaxentFeatureEncoding
:param encoding: A feature encoding, used to convert featuresets
into feature vectors.
:type stream: stream
:param stream: The stream to which the ``tadm`` input file should be
written.
""""""
labels = encoding.labels()
for featureset, label in train_toks:
length_line = ""%d\n"" % len(labels)
stream.write(length_line)
for known_label in labels:
v = encoding.encode(featureset, known_label)
line = ""%d %d %s\n"" % (
int(label == known_label),
len(v),
"" "".join(""%d %d"" % u for u in v),
)
stream.write(line)
",[],0,[],/classify/tadm.py_write_tadm_file
5857,/home/amandapotts/git/nltk/nltk/classify/tadm.py_parse_tadm_weights,"def parse_tadm_weights(paramfile):
""""""
Given the stdout output generated by ``tadm`` when training a
model, return a ``numpy`` array containing the corresponding weight
vector.
""""""
weights = []
for line in paramfile:
weights.append(float(line.strip()))
return numpy.array(weights, ""d"")
",[],0,[],/classify/tadm.py_parse_tadm_weights
5858,/home/amandapotts/git/nltk/nltk/classify/tadm.py_call_tadm,"def call_tadm(args):
""""""
Call the ``tadm`` binary with the given arguments.
""""""
if isinstance(args, str):
raise TypeError(""args should be a list of strings"")
if _tadm_bin is None:
config_tadm()
cmd = [_tadm_bin] + args
p = subprocess.Popen(cmd, stdout=sys.stdout)
(stdout, stderr) = p.communicate()
if p.returncode != 0:
print()
print(stderr)
raise OSError(""tadm command failed!"")
",[],0,[],/classify/tadm.py_call_tadm
5859,/home/amandapotts/git/nltk/nltk/classify/tadm.py_names_demo,"def names_demo():
from nltk.classify.maxent import TadmMaxentClassifier
from nltk.classify.util import names_demo
classifier = names_demo(TadmMaxentClassifier.train)
",[],0,[],/classify/tadm.py_names_demo
5860,/home/amandapotts/git/nltk/nltk/classify/tadm.py_encoding_demo,"def encoding_demo():
import sys
from nltk.classify.maxent import TadmEventMaxentFeatureEncoding
tokens = [
({""f0"": 1, ""f1"": 1, ""f3"": 1}, ""A""),
({""f0"": 1, ""f2"": 1, ""f4"": 1}, ""B""),
({""f0"": 2, ""f2"": 1, ""f3"": 1, ""f4"": 1}, ""A""),
]
encoding = TadmEventMaxentFeatureEncoding.train(tokens)
write_tadm_file(tokens, encoding, sys.stdout)
print()
for i in range(encoding.length()):
print(""%s --> %d"" % (encoding.describe(i), i))
print()
",[],0,[],/classify/tadm.py_encoding_demo
5861,/home/amandapotts/git/nltk/nltk/classify/scikitlearn.py___init__,"def __init__(self, estimator, dtype=float, sparse=True):
""""""
:param estimator: scikit-learn classifier object.
:param dtype: data type used when building feature array.
scikit-learn estimators work exclusively on numeric data. The
default value should be fine for almost all situations.
:param sparse: Whether to use sparse matrices internally.
The estimator must support these
do (see their respective documentation and look for ""sparse
matrix""). The default value is True, since most NLP problems
involve sparse feature sets. Setting this to False may take a
great amount of memory.
:type sparse: boolean.
""""""
self._clf = estimator
self._encoder = LabelEncoder()
self._vectorizer = DictVectorizer(dtype=dtype, sparse=sparse)
",[],0,[],/classify/scikitlearn.py___init__
5862,/home/amandapotts/git/nltk/nltk/classify/scikitlearn.py___repr__,"def __repr__(self):
return ""<SklearnClassifier(%r)>"" % self._clf
",[],0,[],/classify/scikitlearn.py___repr__
5863,/home/amandapotts/git/nltk/nltk/classify/scikitlearn.py_classify_many,"def classify_many(self, featuresets):
""""""Classify a batch of samples.
:param featuresets: An iterable over featuresets, each a dict mapping
strings to either numbers, booleans or strings.
:return: The predicted class label for each input sample.
:rtype: list
""""""
X = self._vectorizer.transform(featuresets)
classes = self._encoder.classes_
return [classes[i] for i in self._clf.predict(X)]
",[],0,[],/classify/scikitlearn.py_classify_many
5864,/home/amandapotts/git/nltk/nltk/classify/scikitlearn.py_prob_classify_many,"def prob_classify_many(self, featuresets):
""""""Compute per-class probabilities for a batch of samples.
:param featuresets: An iterable over featuresets, each a dict mapping
strings to either numbers, booleans or strings.
:rtype: list of ``ProbDistI``
""""""
X = self._vectorizer.transform(featuresets)
y_proba_list = self._clf.predict_proba(X)
return [self._make_probdist(y_proba) for y_proba in y_proba_list]
",[],0,[],/classify/scikitlearn.py_prob_classify_many
5865,/home/amandapotts/git/nltk/nltk/classify/scikitlearn.py_labels,"def labels(self):
""""""The class labels used by this classifier.
:rtype: list
""""""
return list(self._encoder.classes_)
",[],0,[],/classify/scikitlearn.py_labels
5866,/home/amandapotts/git/nltk/nltk/classify/scikitlearn.py_train,"def train(self, labeled_featuresets):
""""""
Train (fit) the scikit-learn estimator.
:param labeled_featuresets: A list of ``(featureset, label)``
where each ``featureset`` is a dict mapping strings to either
numbers, booleans or strings.
""""""
X, y = list(zip(*labeled_featuresets))
X = self._vectorizer.fit_transform(X)
y = self._encoder.fit_transform(y)
self._clf.fit(X, y)
return self
",[],0,[],/classify/scikitlearn.py_train
5867,/home/amandapotts/git/nltk/nltk/classify/scikitlearn.py__make_probdist,"def _make_probdist(self, y_proba):
classes = self._encoder.classes_
return DictionaryProbDist({classes[i]: p for i, p in enumerate(y_proba)})
",[],0,[],/classify/scikitlearn.py__make_probdist
5868,/home/amandapotts/git/nltk/nltk/classify/weka.py_config_weka,"def config_weka(classpath=None):
global _weka_classpath
config_java()
if classpath is not None:
_weka_classpath = classpath
if _weka_classpath is None:
searchpath = _weka_search
if ""WEKAHOME"" in os.environ:
searchpath.insert(0, os.environ[""WEKAHOME""])
for path in searchpath:
if os.path.exists(os.path.join(path, ""weka.jar"")):
_weka_classpath = os.path.join(path, ""weka.jar"")
version = _check_weka_version(_weka_classpath)
if version:
print(f""[Found Weka: {_weka_classpath} (version {version})]"")
else:
print(""[Found Weka: %s]"" % _weka_classpath)
_check_weka_version(_weka_classpath)
if _weka_classpath is None:
raise LookupError(
""Unable to find weka.jar!  Use config_weka() ""
""or set the WEKAHOME environment variable. ""
""For more information about Weka, please see ""
""https://www.cs.waikato.ac.nz/ml/weka/""
)
",[],0,[],/classify/weka.py_config_weka
5869,/home/amandapotts/git/nltk/nltk/classify/weka.py__check_weka_version,"def _check_weka_version(jar):
try:
zf = zipfile.ZipFile(jar)
except (SystemExit, KeyboardInterrupt):
raise
except:
return None
try:
try:
return zf.read(""weka/core/version.txt"")
except KeyError:
return None
finally:
zf.close()
",[],0,[],/classify/weka.py__check_weka_version
5870,/home/amandapotts/git/nltk/nltk/classify/weka.py___init__,"def __init__(self, formatter, model_filename):
self._formatter = formatter
self._model = model_filename
",[],0,[],/classify/weka.py___init__
5871,/home/amandapotts/git/nltk/nltk/classify/weka.py_prob_classify_many,"def prob_classify_many(self, featuresets):
return self._classify_many(featuresets, [""-p"", ""0"", ""-distribution""])
",[],0,[],/classify/weka.py_prob_classify_many
5872,/home/amandapotts/git/nltk/nltk/classify/weka.py_classify_many,"def classify_many(self, featuresets):
return self._classify_many(featuresets, [""-p"", ""0""])
",[],0,[],/classify/weka.py_classify_many
5873,/home/amandapotts/git/nltk/nltk/classify/weka.py__classify_many,"def _classify_many(self, featuresets, options):
config_weka()
temp_dir = tempfile.mkdtemp()
try:
test_filename = os.path.join(temp_dir, ""test.arff"")
self._formatter.write(test_filename, featuresets)
cmd = [
""weka.classifiers.bayes.NaiveBayes"",
""-l"",
self._model,
""-T"",
test_filename,
] + options
(stdout, stderr) = java(
cmd,
classpath=_weka_classpath,
stdout=subprocess.PIPE,
stderr=subprocess.PIPE,
)
if stderr and not stdout:
if ""Illegal options: -distribution"" in stderr:
raise ValueError(
""The installed version of weka does ""
""not support probability distribution ""
""output.""
)
else:
raise ValueError(""Weka failed to generate output:\n%s"" % stderr)
return self.parse_weka_output(stdout.decode(stdin.encoding).split(""\n""))
finally:
for f in os.listdir(temp_dir):
os.remove(os.path.join(temp_dir, f))
os.rmdir(temp_dir)
",[],0,[],/classify/weka.py__classify_many
5874,/home/amandapotts/git/nltk/nltk/classify/weka.py_parse_weka_distribution,"def parse_weka_distribution(self, s):
probs = [float(v) for v in re.split(""[*,]+"", s) if v.strip()]
probs = dict(zip(self._formatter.labels(), probs))
return DictionaryProbDist(probs)
",[],0,[],/classify/weka.py_parse_weka_distribution
5875,/home/amandapotts/git/nltk/nltk/classify/weka.py_parse_weka_output,"def parse_weka_output(self, lines):
for i, line in enumerate(lines):
if line.strip().startswith(""inst#""):
lines = lines[i:]
break
if lines[0].split() == [""inst#"", ""actual"", ""predicted"", ""error"", ""prediction""]:
return [line.split()[2].split("":"")[1] for line in lines[1:] if line.strip()]
elif lines[0].split() == [
""inst#"",
""actual"",
""predicted"",
""error"",
""distribution"",
]:
return [
self.parse_weka_distribution(line.split()[-1])
for line in lines[1:]
if line.strip()
]
elif re.match(r""^0 \w+ [01]\.[0-9]* \?\s*$"", lines[0]):
return [line.split()[1] for line in lines if line.strip()]
else:
for line in lines[:10]:
print(line)
raise ValueError(
""Unhandled output format -- your version ""
""of weka may not be supported.\n""
""  Header: %s"" % lines[0]
)
",[],0,[],/classify/weka.py_parse_weka_output
5876,/home/amandapotts/git/nltk/nltk/classify/weka.py_train,"def train(
cls,
model_filename,
featuresets,
classifier=""naivebayes"",
options=[],
quiet=True,
",[],0,[],/classify/weka.py_train
5877,/home/amandapotts/git/nltk/nltk/classify/weka.py___init__,"def __init__(self, labels, features):
""""""
:param labels: A list of all class labels that can be generated.
:param features: A list of feature specifications, where
each feature specification is a tuple (fname, ftype)
and ftype is an ARFF type string such as NUMERIC or
STRING.
""""""
self._labels = labels
self._features = features
",[],0,[],/classify/weka.py___init__
5878,/home/amandapotts/git/nltk/nltk/classify/weka.py_format,"def format(self, tokens):
""""""Returns a string representation of ARFF output for the given data.""""""
return self.header_section() + self.data_section(tokens)
",[],0,[],/classify/weka.py_format
5879,/home/amandapotts/git/nltk/nltk/classify/weka.py_labels,"def labels(self):
""""""Returns the list of classes.""""""
return list(self._labels)
",[],0,[],/classify/weka.py_labels
5880,/home/amandapotts/git/nltk/nltk/classify/weka.py_write,"def write(self, outfile, tokens):
""""""Writes ARFF data to a file for the given data.""""""
if not hasattr(outfile, ""write""):
outfile = open(outfile, ""w"")
outfile.write(self.format(tokens))
outfile.close()
",[],0,[],/classify/weka.py_write
5881,/home/amandapotts/git/nltk/nltk/classify/weka.py_from_train,"def from_train(tokens):
""""""
Constructs an ARFF_Formatter instance with class labels and feature
types determined from the given data. Handles boolean, numeric and
string (note: not nominal) types.
""""""
labels = {label for (tok, label) in tokens}
features = {}
for tok, label in tokens:
for fname, fval in tok.items():
if issubclass(type(fval), bool):
ftype = ""{True, False}""
elif issubclass(type(fval), (int, float, bool)):
ftype = ""NUMERIC""
elif issubclass(type(fval), str):
ftype = ""STRING""
elif fval is None:
continue  # can't tell the type.
else:
raise ValueError(""Unsupported value type %r"" % ftype)
if features.get(fname, ftype) != ftype:
raise ValueError(""Inconsistent type for %s"" % fname)
features[fname] = ftype
features = sorted(features.items())
return ARFF_Formatter(labels, features)
",[],0,[],/classify/weka.py_from_train
5882,/home/amandapotts/git/nltk/nltk/classify/weka.py_header_section,"def header_section(self):
""""""Returns an ARFF header as a string.""""""
s = (
""% Weka ARFF file\n""
+ ""% Generated automatically by NLTK\n""
+ ""%% %s\n\n"" % time.ctime()
)
s += ""@RELATION rel\n\n""
for fname, ftype in self._features:
s += ""@ATTRIBUTE %-30r %s\n"" % (fname, ftype)
s += ""@ATTRIBUTE %-30r {%s}\n"" % (""-label-"", "","".join(self._labels))
return s
",[],0,[],/classify/weka.py_header_section
5883,/home/amandapotts/git/nltk/nltk/classify/weka.py_data_section,"def data_section(self, tokens, labeled=None):
""""""
Returns the ARFF data section for the given data.
:param tokens: a list of featuresets (dicts) or labelled featuresets
which are tuples (featureset, label).
:param labeled: Indicates whether the given tokens are labeled
or not.  If None, then the tokens will be assumed to be
labeled if the first token's value is a tuple or list.
""""""
if labeled is None:
labeled = tokens and isinstance(tokens[0], (tuple, list))
if not labeled:
tokens = [(tok, None) for tok in tokens]
s = ""\n@DATA\n""
for tok, label in tokens:
for fname, ftype in self._features:
s += ""%s,"" % self._fmt_arff_val(tok.get(fname))
s += ""%s\n"" % self._fmt_arff_val(label)
return s
",[],0,[],/classify/weka.py_data_section
5884,/home/amandapotts/git/nltk/nltk/classify/weka.py__fmt_arff_val,"def _fmt_arff_val(self, fval):
if fval is None:
return ""?""
elif isinstance(fval, (bool, int)):
return ""%s"" % fval
elif isinstance(fval, float):
return ""%r"" % fval
else:
return ""%r"" % fval
",[],0,[],/classify/weka.py__fmt_arff_val
5885,/home/amandapotts/git/nltk/nltk/classify/weka.py_make_classifier,"def make_classifier(featuresets):
return WekaClassifier.train(""/tmp/name.model"", featuresets, ""C4.5"")
",[],0,[],/classify/weka.py_make_classifier
